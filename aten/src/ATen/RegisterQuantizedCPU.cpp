// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// an external backend might generate file within its code tree
// and check all the source files within the tree with clang-format.
// so, disable it since the backend might have a different config.
// clang-format off

// NOTE: This condition is true for all PyTorch internal libraries, it
//       just excludes external projects such as torch_xla which
//       re-use some of the PyTorch codegen machinery.
#if defined(CAFFE2_BUILD_MAIN_LIB)        || \
    defined(TORCH_CUDA_BUILD_MAIN_LIB)    || \
    defined(TORCH_HIP_BUILD_MAIN_LIB)     || \
    defined(TORCH_CUDA_CU_BUILD_MAIN_LIB) || \
    defined(TORCH_CUDA_CPP_BUILD_MAIN_LIB)
#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
#endif

// @generated by torchgen/gen.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/Tensor.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>


#include <ATen/NativeFunctions.h>
#include <ATen/Functions.h>
#include <ATen/NativeFunctions.h>

// See template file RegisterDispatchDefinitions.ini
namespace at {
// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {
Tensor create_out(IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  if (strides.empty()) {
      return at::empty(sizes, options);
  } else {
      return at::empty_strided(sizes, strides, options);
  }
}
void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      // TODO: avoid the redispatch here
      out.as_strided_(sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}
void check_inplace(const Tensor &self, IntArrayRef sizes, const TensorOptions &options) {
  // These checks are needed on those operators that:
  //   1) don't use 'TensorIterator' (e.g. 'addmm' and 'baddbmm')
  //   2) have particular typing rules (e.g. 'cumsum' and 'cumprod')
  // For other operators (e.g. 'add'), 'TensorIterator' already checks
  // these things separately.
  TORCH_CHECK(options.dtype() == self.dtype(),
      "Bad in-place call: ",
      "input tensor dtype ", self.dtype(), " and output tensor dtype ", options.dtype(), " should match");
  TORCH_CHECK(options.device() == self.device(),
      "Bad in-place call: ",
      "input tensor device ", self.device(), " and output tensor device ", options.device(), " should match");
  TORCH_CHECK(sizes == self.sizes(),
      "Bad in-place call: ",
      "input tensor size ", self.sizes(), " and output tensor size ", sizes, " should match");
}
c10::optional<Tensor> maybe_create_proxy(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  if (out.strides() != strides) {
    return at::empty_strided(sizes, strides, options);
  }
  return c10::nullopt;
}
namespace {
at::Tensor wrapper_QuantizedCPU__as_strided(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset) {
    // No device check
  // DeviceGuard omitted
  return at::native::as_strided_qtensorimpl(self, C10_AS_INTARRAYREF_SLOW(size), C10_AS_INTARRAYREF_SLOW(stride), storage_offset.has_value() ? c10::make_optional(storage_offset->guard_int(__FILE__, __LINE__)) : c10::nullopt);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__quantized_batch_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & var, double eps, double output_scale, int64_t output_zero_point) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantized_batch_norm(input, weight, bias, mean, var, eps, output_scale, output_zero_point);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__cat(const at::ITensorListRef & tensors, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::cat_quantized_cpu(tensors, dim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_out_cat_out(const at::ITensorListRef & tensors, int64_t dim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::cat_out_quantized_cpu(tensors, dim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__clamp(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
    // No device check
  // DeviceGuard omitted
  return at::native::clamp_quantized_cpu(self, min, max);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_memory_format_empty(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::empty_unknown_quantized(C10_AS_INTARRAYREF_SLOW(size), dtype, layout, device, pin_memory, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU___empty_affine_quantized(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::empty_affine_quantized(C10_AS_INTARRAYREF_SLOW(size), dtype, layout, device, pin_memory, scale, zero_point, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU___empty_per_channel_affine_quantized(c10::SymIntArrayRef size, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::empty_per_channel_affine_quantized(C10_AS_INTARRAYREF_SLOW(size), scales, zero_points, axis, dtype, layout, device, pin_memory, memory_format);
}
} // anonymous namespace
namespace {
const at::Tensor & wrapper_QuantizedCPU__resize_(const at::Tensor & self, c10::SymIntArrayRef size, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantized_resize_cpu_(self, C10_AS_INTARRAYREF_SLOW(size), memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__empty_quantized(at::IntArrayRef size, const at::Tensor & qtensor, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::empty_quantized(size, qtensor, dtype, layout, device, pin_memory, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__empty_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::empty_like_quantized(self, dtype, layout, device, pin_memory, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__empty_strided(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return at::native::empty_strided_unknown_quantized(C10_AS_INTARRAYREF_SLOW(size), C10_AS_INTARRAYREF_SLOW(stride), dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Scalar_fill_(at::Tensor & self, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return at::native::fill_quantized_(self, value);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Tensor_fill_(at::Tensor & self, const at::Tensor & value) {
    // No device check
  // DeviceGuard omitted
  return at::native::fill_quantized_(self, value);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__grid_sampler_2d(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    // No device check
  // DeviceGuard omitted
  return at::native::grid_sampler_2d_cpu(input, grid, interpolation_mode, padding_mode, align_corners);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_Tensor_index(const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantized_index(self, indices);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU___index_put_impl_(at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate, bool unsafe) {
    // No device check
  // DeviceGuard omitted
  return at::native::_index_put_impl_quantized_cpu_(self, indices, values, accumulate, unsafe);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_QuantizedCPU_dim_max(const at::Tensor & self, int64_t dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::qmax(self, dim, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__quantized_max_pool1d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantized_max_pool1d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__quantized_max_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantized_max_pool2d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__quantized_max_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantized_max_pool3d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_dim_mean(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::mean_quantized_cpu(self, dim, keepdim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_out_mean_out(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::mean_out_quantized_cpu(self, dim, keepdim, dtype, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_QuantizedCPU_dim_min(const at::Tensor & self, int64_t dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::qmin(self, dim, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__channel_shuffle(const at::Tensor & self, c10::SymInt groups) {
    // No device check
  // DeviceGuard omitted
  return at::native::channel_shuffle_quantized_cpu(self, groups.guard_int(__FILE__, __LINE__));
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU___reshape_alias(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
    // No device check
  // DeviceGuard omitted
  return at::native::_reshape_alias(self, C10_AS_INTARRAYREF_SLOW(size), C10_AS_INTARRAYREF_SLOW(stride));
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__relu(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::relu_quantized_cpu(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU__relu_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::relu_quantized_cpu_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU___prelu_kernel(const at::Tensor & self, const at::Tensor & weight) {
    // No device check
  // DeviceGuard omitted
  return at::native::_prelu_kernel_quantized_cpu(self, weight);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__gelu(const at::Tensor & self, c10::string_view approximate) {
    // No device check
  // DeviceGuard omitted
  return at::native::gelu_quantized_cpu(self, approximate);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__sigmoid(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::sigmoid_quantized_cpu(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__squeeze(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::squeeze_quantized(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_dim_squeeze(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::squeeze_quantized(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_dims_squeeze(const at::Tensor & self, at::IntArrayRef dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::squeeze_quantized(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_correction_std(const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::std_quantized_cpu(self, dim, correction, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_correction_out_std_out(const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::std_out_quantized_cpu(self, dim, correction, keepdim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__tanh(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::tanh_quantized_cpu(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__threshold(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return at::native::threshold_quantized_cpu(self, threshold, value);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__flip(const at::Tensor & self, at::IntArrayRef dims) {
    // No device check
  // DeviceGuard omitted
  return at::native::flip(self, dims);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__unsqueeze(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::unsqueeze_quantized(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantized_clone(self, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_self_dequantize(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::dequantize_quantized(self);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_QuantizedCPU_tensors_dequantize(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::dequantize_tensors_quantized_cpu(tensors);
}
} // anonymous namespace
namespace {
double wrapper_QuantizedCPU__q_scale(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::q_scale_quant(self);
}
} // anonymous namespace
namespace {
int64_t wrapper_QuantizedCPU__q_zero_point(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::q_zero_point_quant(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__q_per_channel_scales(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::q_per_channel_scales(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__q_per_channel_zero_points(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::q_per_channel_zero_points(self);
}
} // anonymous namespace
namespace {
int64_t wrapper_QuantizedCPU__q_per_channel_axis(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::q_per_channel_axis(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__int_repr(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::int_repr_quantized_cpu(self);
}
} // anonymous namespace
namespace {
at::QScheme wrapper_QuantizedCPU__qscheme(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::qscheme_quant(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_source_Storage_storage_offset_set_(at::Tensor & self, at::Storage source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
    // No device check
  // DeviceGuard omitted
  return at::native::set_storage_quantized_(self, source, storage_offset.guard_int(__FILE__, __LINE__), C10_AS_INTARRAYREF_SLOW(size), C10_AS_INTARRAYREF_SLOW(stride));
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Scalar_masked_fill_(at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return at::native::masked_fill__quantized_cpu(self, mask, value);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Tensor_masked_fill_(at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
    // No device check
  // DeviceGuard omitted
  return at::native::masked_fill__quantized_cpu(self, mask, value);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__view(const at::Tensor & self, c10::SymIntArrayRef size) {
    // No device check
  // DeviceGuard omitted
  return at::native::view(self, C10_AS_INTARRAYREF_SLOW(size));
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_Scalar_eq(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::eq_quantized_cpu(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Scalar_out_eq_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::eq_out_quantized_cpu(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_Tensor_eq(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::eq_quantized_cpu(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Tensor_out_eq_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::eq_out_quantized_cpu(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_Scalar_ne(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::ne_quantized_cpu(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Scalar_out_ne_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::ne_out_quantized_cpu(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_Tensor_ne(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::ne_quantized_cpu(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Tensor_out_ne_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::ne_out_quantized_cpu(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_Scalar_ge(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::ge_quantized_cpu(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Scalar_out_ge_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::ge_out_quantized_cpu(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_Tensor_ge(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::ge_quantized_cpu(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Tensor_out_ge_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::ge_out_quantized_cpu(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_Scalar_le(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::le_quantized_cpu(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Scalar_out_le_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::le_out_quantized_cpu(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_Tensor_le(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::le_quantized_cpu(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Tensor_out_le_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::le_out_quantized_cpu(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_Scalar_gt(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::gt_quantized_cpu(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Scalar_out_gt_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::gt_out_quantized_cpu(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_Tensor_gt(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::gt_quantized_cpu(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Tensor_out_gt_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::gt_out_quantized_cpu(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_Scalar_lt(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::lt_quantized_cpu(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Scalar_out_lt_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::lt_out_quantized_cpu(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU_Tensor_lt(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::lt_quantized_cpu(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_Tensor_out_lt_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::lt_out_quantized_cpu(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__index_select(const at::Tensor & self, int64_t dim, const at::Tensor & index) {
    // No device check
  // DeviceGuard omitted
  return at::native::index_select_quantized_cpu_(self, dim, index);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_out_index_select_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::index_select_out_cpu_(self, dim, index, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__min(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::min_quantized_cpu(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_unary_out_min_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::min_quantized_unary_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__max(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::max_quantized_cpu(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_unary_out_max_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::max_quantized_unary_out(self, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_QuantizedCPU_stable_sort(const at::Tensor & self, c10::optional<bool> stable, int64_t dim, bool descending) {
    // No device check
  // DeviceGuard omitted
  return at::native::sort_quantized_cpu_stable(self, stable, dim, descending);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_QuantizedCPU__topk(const at::Tensor & self, c10::SymInt k, int64_t dim, bool largest, bool sorted) {
    // No device check
  // DeviceGuard omitted
  return at::native::topk_quantized_cpu(self, k.guard_int(__FILE__, __LINE__), dim, largest, sorted);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__unfold(const at::Tensor & self, int64_t dimension, int64_t size, int64_t step) {
    // No device check
  // DeviceGuard omitted
  return at::native::unfold(self, dimension, size, step);
}
} // anonymous namespace
namespace {
bool wrapper_QuantizedCPU__equal(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::equal_quantized_cpu(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__hardsigmoid(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::hardsigmoid_quantized_cpu(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_out_hardsigmoid_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::hardsigmoid_out_quantized_cpu(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__hardtanh(const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    // No device check
  // DeviceGuard omitted
  return at::native::hardtanh_quantized_cpu(self, min_val, max_val);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_out_hardtanh_out(const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::hardtanh_out_quantized_cpu(self, min_val, max_val, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU__hardtanh_(at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    // No device check
  // DeviceGuard omitted
  return at::native::hardtanh_quantized_cpu_(self, min_val, max_val);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__leaky_relu(const at::Tensor & self, const at::Scalar & negative_slope) {
    // No device check
  // DeviceGuard omitted
  return at::native::leaky_relu_quantized_cpu(self, negative_slope);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_out_leaky_relu_out(const at::Tensor & self, const at::Scalar & negative_slope, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::leaky_relu_out_quantized_cpu(self, negative_slope, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU__leaky_relu_(at::Tensor & self, const at::Scalar & negative_slope) {
    // No device check
  // DeviceGuard omitted
  return at::native::leaky_relu_quantized_cpu_(self, negative_slope);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU___adaptive_avg_pool2d(const at::Tensor & self, c10::SymIntArrayRef output_size) {
    // No device check
  // DeviceGuard omitted
  return at::native::adaptive_avg_pool2d_quantized_cpu(self, C10_AS_INTARRAYREF_SLOW(output_size));
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_out_adaptive_avg_pool3d_out(const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::adaptive_avg_pool3d_out_quantized_cpu(self, C10_AS_INTARRAYREF_SLOW(output_size), out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU___adaptive_avg_pool3d(const at::Tensor & self, c10::SymIntArrayRef output_size) {
    // No device check
  // DeviceGuard omitted
  return at::native::adaptive_avg_pool3d_quantized_cpu(self, C10_AS_INTARRAYREF_SLOW(output_size));
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__avg_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    // No device check
  // DeviceGuard omitted
  return at::native::avg_pool2d_quantized_cpu(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__avg_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    // No device check
  // DeviceGuard omitted
  return at::native::avg_pool3d_quantized_cpu(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_out_reflection_pad1d_out(const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::reflection_pad1d_out_quantized_cpu(self, C10_AS_INTARRAYREF_SLOW(padding), out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__reflection_pad2d(const at::Tensor & self, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return at::native::reflection_pad2d_quantized_cpu(self, C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedCPU_out_reflection_pad2d_out(const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::reflection_pad2d_out_cpu(self, C10_AS_INTARRAYREF_SLOW(padding), out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__upsample_bilinear2d(const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    // No device check
  // DeviceGuard omitted
  return at::native::upsample_bilinear2d_quantized_cpu(self, C10_AS_INTARRAYREF_SLOW(output_size), align_corners, scales_h, scales_w);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__upsample_nearest2d(const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    // No device check
  // DeviceGuard omitted
  return at::native::upsample_nearest2d_quantized_cpu(self, C10_AS_INTARRAYREF_SLOW(output_size), scales_h, scales_w);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU___upsample_nearest_exact2d(const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    // No device check
  // DeviceGuard omitted
  return at::native::_upsample_nearest_exact2d_quantized_cpu(self, C10_AS_INTARRAYREF_SLOW(output_size), scales_h, scales_w);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU__upsample_nearest3d(const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    // No device check
  // DeviceGuard omitted
  return at::native::upsample_nearest3d_quantized_cpu(self, C10_AS_INTARRAYREF_SLOW(output_size), scales_d, scales_h, scales_w);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedCPU___upsample_nearest_exact3d(const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    // No device check
  // DeviceGuard omitted
  return at::native::_upsample_nearest_exact3d_quantized_cpu(self, C10_AS_INTARRAYREF_SLOW(output_size), scales_d, scales_h, scales_w);
}
} // anonymous namespace
TORCH_LIBRARY_IMPL(aten, QuantizedCPU, m) {
    m.impl("as_strided",
TORCH_FN(wrapper_QuantizedCPU__as_strided));
m.impl("quantized_batch_norm",
TORCH_FN(wrapper_QuantizedCPU__quantized_batch_norm));
m.impl("cat",
TORCH_FN(wrapper_QuantizedCPU__cat));
m.impl("cat.out",
TORCH_FN(wrapper_QuantizedCPU_out_cat_out));
m.impl("clamp",
TORCH_FN(wrapper_QuantizedCPU__clamp));
m.impl("empty.memory_format",
TORCH_FN(wrapper_QuantizedCPU_memory_format_empty));
m.impl("_empty_affine_quantized",
TORCH_FN(wrapper_QuantizedCPU___empty_affine_quantized));
m.impl("_empty_per_channel_affine_quantized",
TORCH_FN(wrapper_QuantizedCPU___empty_per_channel_affine_quantized));
m.impl("resize_",
TORCH_FN(wrapper_QuantizedCPU__resize_));
m.impl("empty_quantized",
TORCH_FN(wrapper_QuantizedCPU__empty_quantized));
m.impl("empty_like",
TORCH_FN(wrapper_QuantizedCPU__empty_like));
m.impl("empty_strided",
TORCH_FN(wrapper_QuantizedCPU__empty_strided));
m.impl("fill_.Scalar",
TORCH_FN(wrapper_QuantizedCPU_Scalar_fill_));
m.impl("fill_.Tensor",
TORCH_FN(wrapper_QuantizedCPU_Tensor_fill_));
m.impl("grid_sampler_2d",
TORCH_FN(wrapper_QuantizedCPU__grid_sampler_2d));
m.impl("index.Tensor",
TORCH_FN(wrapper_QuantizedCPU_Tensor_index));
m.impl("_index_put_impl_",
TORCH_FN(wrapper_QuantizedCPU___index_put_impl_));
m.impl("max.dim",
TORCH_FN(wrapper_QuantizedCPU_dim_max));
m.impl("quantized_max_pool1d",
TORCH_FN(wrapper_QuantizedCPU__quantized_max_pool1d));
m.impl("quantized_max_pool2d",
TORCH_FN(wrapper_QuantizedCPU__quantized_max_pool2d));
m.impl("quantized_max_pool3d",
TORCH_FN(wrapper_QuantizedCPU__quantized_max_pool3d));
m.impl("mean.dim",
TORCH_FN(wrapper_QuantizedCPU_dim_mean));
m.impl("mean.out",
TORCH_FN(wrapper_QuantizedCPU_out_mean_out));
m.impl("min.dim",
TORCH_FN(wrapper_QuantizedCPU_dim_min));
m.impl("channel_shuffle",
TORCH_FN(wrapper_QuantizedCPU__channel_shuffle));
m.impl("_reshape_alias",
TORCH_FN(wrapper_QuantizedCPU___reshape_alias));
m.impl("relu",
TORCH_FN(wrapper_QuantizedCPU__relu));
m.impl("relu_",
TORCH_FN(wrapper_QuantizedCPU__relu_));
m.impl("_prelu_kernel",
TORCH_FN(wrapper_QuantizedCPU___prelu_kernel));
m.impl("gelu",
TORCH_FN(wrapper_QuantizedCPU__gelu));
m.impl("sigmoid",
TORCH_FN(wrapper_QuantizedCPU__sigmoid));
m.impl("squeeze",
TORCH_FN(wrapper_QuantizedCPU__squeeze));
m.impl("squeeze.dim",
TORCH_FN(wrapper_QuantizedCPU_dim_squeeze));
m.impl("squeeze.dims",
TORCH_FN(wrapper_QuantizedCPU_dims_squeeze));
m.impl("std.correction",
TORCH_FN(wrapper_QuantizedCPU_correction_std));
m.impl("std.correction_out",
TORCH_FN(wrapper_QuantizedCPU_correction_out_std_out));
m.impl("tanh",
TORCH_FN(wrapper_QuantizedCPU__tanh));
m.impl("threshold",
TORCH_FN(wrapper_QuantizedCPU__threshold));
m.impl("flip",
TORCH_FN(wrapper_QuantizedCPU__flip));
m.impl("unsqueeze",
TORCH_FN(wrapper_QuantizedCPU__unsqueeze));
m.impl("clone",
TORCH_FN(wrapper_QuantizedCPU__clone));
m.impl("dequantize.self",
TORCH_FN(wrapper_QuantizedCPU_self_dequantize));
m.impl("dequantize.tensors",
TORCH_FN(wrapper_QuantizedCPU_tensors_dequantize));
m.impl("q_scale",
TORCH_FN(wrapper_QuantizedCPU__q_scale));
m.impl("q_zero_point",
TORCH_FN(wrapper_QuantizedCPU__q_zero_point));
m.impl("q_per_channel_scales",
TORCH_FN(wrapper_QuantizedCPU__q_per_channel_scales));
m.impl("q_per_channel_zero_points",
TORCH_FN(wrapper_QuantizedCPU__q_per_channel_zero_points));
m.impl("q_per_channel_axis",
TORCH_FN(wrapper_QuantizedCPU__q_per_channel_axis));
m.impl("int_repr",
TORCH_FN(wrapper_QuantizedCPU__int_repr));
m.impl("qscheme",
TORCH_FN(wrapper_QuantizedCPU__qscheme));
m.impl("set_.source_Storage_storage_offset",
TORCH_FN(wrapper_QuantizedCPU_source_Storage_storage_offset_set_));
m.impl("masked_fill_.Scalar",
TORCH_FN(wrapper_QuantizedCPU_Scalar_masked_fill_));
m.impl("masked_fill_.Tensor",
TORCH_FN(wrapper_QuantizedCPU_Tensor_masked_fill_));
m.impl("view",
TORCH_FN(wrapper_QuantizedCPU__view));
m.impl("eq.Scalar",
TORCH_FN(wrapper_QuantizedCPU_Scalar_eq));
m.impl("eq.Scalar_out",
TORCH_FN(wrapper_QuantizedCPU_Scalar_out_eq_out));
m.impl("eq.Tensor",
TORCH_FN(wrapper_QuantizedCPU_Tensor_eq));
m.impl("eq.Tensor_out",
TORCH_FN(wrapper_QuantizedCPU_Tensor_out_eq_out));
m.impl("ne.Scalar",
TORCH_FN(wrapper_QuantizedCPU_Scalar_ne));
m.impl("ne.Scalar_out",
TORCH_FN(wrapper_QuantizedCPU_Scalar_out_ne_out));
m.impl("ne.Tensor",
TORCH_FN(wrapper_QuantizedCPU_Tensor_ne));
m.impl("ne.Tensor_out",
TORCH_FN(wrapper_QuantizedCPU_Tensor_out_ne_out));
m.impl("ge.Scalar",
TORCH_FN(wrapper_QuantizedCPU_Scalar_ge));
m.impl("ge.Scalar_out",
TORCH_FN(wrapper_QuantizedCPU_Scalar_out_ge_out));
m.impl("ge.Tensor",
TORCH_FN(wrapper_QuantizedCPU_Tensor_ge));
m.impl("ge.Tensor_out",
TORCH_FN(wrapper_QuantizedCPU_Tensor_out_ge_out));
m.impl("le.Scalar",
TORCH_FN(wrapper_QuantizedCPU_Scalar_le));
m.impl("le.Scalar_out",
TORCH_FN(wrapper_QuantizedCPU_Scalar_out_le_out));
m.impl("le.Tensor",
TORCH_FN(wrapper_QuantizedCPU_Tensor_le));
m.impl("le.Tensor_out",
TORCH_FN(wrapper_QuantizedCPU_Tensor_out_le_out));
m.impl("gt.Scalar",
TORCH_FN(wrapper_QuantizedCPU_Scalar_gt));
m.impl("gt.Scalar_out",
TORCH_FN(wrapper_QuantizedCPU_Scalar_out_gt_out));
m.impl("gt.Tensor",
TORCH_FN(wrapper_QuantizedCPU_Tensor_gt));
m.impl("gt.Tensor_out",
TORCH_FN(wrapper_QuantizedCPU_Tensor_out_gt_out));
m.impl("lt.Scalar",
TORCH_FN(wrapper_QuantizedCPU_Scalar_lt));
m.impl("lt.Scalar_out",
TORCH_FN(wrapper_QuantizedCPU_Scalar_out_lt_out));
m.impl("lt.Tensor",
TORCH_FN(wrapper_QuantizedCPU_Tensor_lt));
m.impl("lt.Tensor_out",
TORCH_FN(wrapper_QuantizedCPU_Tensor_out_lt_out));
m.impl("index_select",
TORCH_FN(wrapper_QuantizedCPU__index_select));
m.impl("index_select.out",
TORCH_FN(wrapper_QuantizedCPU_out_index_select_out));
m.impl("min",
TORCH_FN(wrapper_QuantizedCPU__min));
m.impl("min.unary_out",
TORCH_FN(wrapper_QuantizedCPU_unary_out_min_out));
m.impl("max",
TORCH_FN(wrapper_QuantizedCPU__max));
m.impl("max.unary_out",
TORCH_FN(wrapper_QuantizedCPU_unary_out_max_out));
m.impl("sort.stable",
TORCH_FN(wrapper_QuantizedCPU_stable_sort));
m.impl("topk",
TORCH_FN(wrapper_QuantizedCPU__topk));
m.impl("unfold",
TORCH_FN(wrapper_QuantizedCPU__unfold));
m.impl("equal",
TORCH_FN(wrapper_QuantizedCPU__equal));
m.impl("hardsigmoid",
TORCH_FN(wrapper_QuantizedCPU__hardsigmoid));
m.impl("hardsigmoid.out",
TORCH_FN(wrapper_QuantizedCPU_out_hardsigmoid_out));
m.impl("hardtanh",
TORCH_FN(wrapper_QuantizedCPU__hardtanh));
m.impl("hardtanh.out",
TORCH_FN(wrapper_QuantizedCPU_out_hardtanh_out));
m.impl("hardtanh_",
TORCH_FN(wrapper_QuantizedCPU__hardtanh_));
m.impl("leaky_relu",
TORCH_FN(wrapper_QuantizedCPU__leaky_relu));
m.impl("leaky_relu.out",
TORCH_FN(wrapper_QuantizedCPU_out_leaky_relu_out));
m.impl("leaky_relu_",
TORCH_FN(wrapper_QuantizedCPU__leaky_relu_));
m.impl("_adaptive_avg_pool2d",
TORCH_FN(wrapper_QuantizedCPU___adaptive_avg_pool2d));
m.impl("adaptive_avg_pool3d.out",
TORCH_FN(wrapper_QuantizedCPU_out_adaptive_avg_pool3d_out));
m.impl("_adaptive_avg_pool3d",
TORCH_FN(wrapper_QuantizedCPU___adaptive_avg_pool3d));
m.impl("avg_pool2d",
TORCH_FN(wrapper_QuantizedCPU__avg_pool2d));
m.impl("avg_pool3d",
TORCH_FN(wrapper_QuantizedCPU__avg_pool3d));
m.impl("reflection_pad1d.out",
TORCH_FN(wrapper_QuantizedCPU_out_reflection_pad1d_out));
m.impl("reflection_pad2d",
TORCH_FN(wrapper_QuantizedCPU__reflection_pad2d));
m.impl("reflection_pad2d.out",
TORCH_FN(wrapper_QuantizedCPU_out_reflection_pad2d_out));
m.impl("upsample_bilinear2d",
TORCH_FN(wrapper_QuantizedCPU__upsample_bilinear2d));
m.impl("upsample_nearest2d",
TORCH_FN(wrapper_QuantizedCPU__upsample_nearest2d));
m.impl("_upsample_nearest_exact2d",
TORCH_FN(wrapper_QuantizedCPU___upsample_nearest_exact2d));
m.impl("upsample_nearest3d",
TORCH_FN(wrapper_QuantizedCPU__upsample_nearest3d));
m.impl("_upsample_nearest_exact3d",
TORCH_FN(wrapper_QuantizedCPU___upsample_nearest_exact3d));
};
} // anonymous namespace
namespace quantizedcpu {
at::Tensor as_strided(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<int64_t> storage_offset) {
return wrapper_QuantizedCPU__as_strided(self, c10::fromIntArrayRefSlow(size), c10::fromIntArrayRefSlow(stride), storage_offset.has_value() ? c10::make_optional(c10::SymInt(*storage_offset)) : c10::nullopt);
}
at::Tensor as_strided_symint(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset) {
return wrapper_QuantizedCPU__as_strided(self, size, stride, storage_offset);
}
at::Tensor quantized_batch_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & var, double eps, double output_scale, int64_t output_zero_point) {
return wrapper_QuantizedCPU__quantized_batch_norm(input, weight, bias, mean, var, eps, output_scale, output_zero_point);
}
at::Tensor cat(const at::ITensorListRef & tensors, int64_t dim) {
return wrapper_QuantizedCPU__cat(tensors, dim);
}
at::Tensor & cat_out(at::Tensor & out, const at::ITensorListRef & tensors, int64_t dim) {
return wrapper_QuantizedCPU_out_cat_out(tensors, dim, out);
}
at::Tensor & cat_outf(const at::ITensorListRef & tensors, int64_t dim, at::Tensor & out) {
return wrapper_QuantizedCPU_out_cat_out(tensors, dim, out);
}
at::Tensor clamp(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
return wrapper_QuantizedCPU__clamp(self, min, max);
}
at::Tensor empty(at::IntArrayRef size, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU_memory_format_empty(c10::fromIntArrayRefSlow(size), c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor empty(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU_memory_format_empty(c10::fromIntArrayRefSlow(size), dtype, layout, device, pin_memory, memory_format);
}
at::Tensor empty_symint(c10::SymIntArrayRef size, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU_memory_format_empty(size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor empty_symint(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU_memory_format_empty(size, dtype, layout, device, pin_memory, memory_format);
}
at::Tensor _empty_affine_quantized(at::IntArrayRef size, at::TensorOptions options, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU___empty_affine_quantized(c10::fromIntArrayRefSlow(size), c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), scale, zero_point, c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor _empty_affine_quantized(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU___empty_affine_quantized(c10::fromIntArrayRefSlow(size), dtype, layout, device, pin_memory, scale, zero_point, memory_format);
}
at::Tensor _empty_affine_quantized_symint(c10::SymIntArrayRef size, at::TensorOptions options, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU___empty_affine_quantized(size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), scale, zero_point, c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor _empty_affine_quantized_symint(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU___empty_affine_quantized(size, dtype, layout, device, pin_memory, scale, zero_point, memory_format);
}
at::Tensor _empty_per_channel_affine_quantized(at::IntArrayRef size, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU___empty_per_channel_affine_quantized(c10::fromIntArrayRefSlow(size), scales, zero_points, axis, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor _empty_per_channel_affine_quantized(at::IntArrayRef size, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU___empty_per_channel_affine_quantized(c10::fromIntArrayRefSlow(size), scales, zero_points, axis, dtype, layout, device, pin_memory, memory_format);
}
at::Tensor _empty_per_channel_affine_quantized_symint(c10::SymIntArrayRef size, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU___empty_per_channel_affine_quantized(size, scales, zero_points, axis, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor _empty_per_channel_affine_quantized_symint(c10::SymIntArrayRef size, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU___empty_per_channel_affine_quantized(size, scales, zero_points, axis, dtype, layout, device, pin_memory, memory_format);
}
const at::Tensor & resize_(const at::Tensor & self, at::IntArrayRef size, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU__resize_(self, c10::fromIntArrayRefSlow(size), memory_format);
}
const at::Tensor & resize__symint(const at::Tensor & self, c10::SymIntArrayRef size, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU__resize_(self, size, memory_format);
}
at::Tensor empty_quantized(at::IntArrayRef size, const at::Tensor & qtensor, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU__empty_quantized(size, qtensor, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor empty_quantized(at::IntArrayRef size, const at::Tensor & qtensor, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU__empty_quantized(size, qtensor, dtype, layout, device, pin_memory, memory_format);
}
at::Tensor empty_like(const at::Tensor & self, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU__empty_like(self, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor empty_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU__empty_like(self, dtype, layout, device, pin_memory, memory_format);
}
at::Tensor empty_strided(at::IntArrayRef size, at::IntArrayRef stride, at::TensorOptions options) {
return wrapper_QuantizedCPU__empty_strided(c10::fromIntArrayRefSlow(size), c10::fromIntArrayRefSlow(stride), c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor empty_strided(at::IntArrayRef size, at::IntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_QuantizedCPU__empty_strided(c10::fromIntArrayRefSlow(size), c10::fromIntArrayRefSlow(stride), dtype, layout, device, pin_memory);
}
at::Tensor empty_strided_symint(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::TensorOptions options) {
return wrapper_QuantizedCPU__empty_strided(size, stride, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor empty_strided_symint(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_QuantizedCPU__empty_strided(size, stride, dtype, layout, device, pin_memory);
}
at::Tensor & fill_(at::Tensor & self, const at::Scalar & value) {
return wrapper_QuantizedCPU_Scalar_fill_(self, value);
}
at::Tensor & fill_(at::Tensor & self, const at::Tensor & value) {
return wrapper_QuantizedCPU_Tensor_fill_(self, value);
}
at::Tensor grid_sampler_2d(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
return wrapper_QuantizedCPU__grid_sampler_2d(input, grid, interpolation_mode, padding_mode, align_corners);
}
at::Tensor index(const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices) {
return wrapper_QuantizedCPU_Tensor_index(self, indices);
}
at::Tensor & _index_put_impl_(at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate, bool unsafe) {
return wrapper_QuantizedCPU___index_put_impl_(self, indices, values, accumulate, unsafe);
}
::std::tuple<at::Tensor,at::Tensor> max(const at::Tensor & self, int64_t dim, bool keepdim) {
return wrapper_QuantizedCPU_dim_max(self, dim, keepdim);
}
at::Tensor quantized_max_pool1d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper_QuantizedCPU__quantized_max_pool1d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
at::Tensor quantized_max_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper_QuantizedCPU__quantized_max_pool2d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
at::Tensor quantized_max_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper_QuantizedCPU__quantized_max_pool3d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
at::Tensor mean(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_QuantizedCPU_dim_mean(self, dim, keepdim, dtype);
}
at::Tensor & mean_out(at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_QuantizedCPU_out_mean_out(self, dim, keepdim, dtype, out);
}
at::Tensor & mean_outf(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_QuantizedCPU_out_mean_out(self, dim, keepdim, dtype, out);
}
::std::tuple<at::Tensor,at::Tensor> min(const at::Tensor & self, int64_t dim, bool keepdim) {
return wrapper_QuantizedCPU_dim_min(self, dim, keepdim);
}
at::Tensor channel_shuffle(const at::Tensor & self, int64_t groups) {
return wrapper_QuantizedCPU__channel_shuffle(self, groups);
}
at::Tensor channel_shuffle_symint(const at::Tensor & self, c10::SymInt groups) {
return wrapper_QuantizedCPU__channel_shuffle(self, groups);
}
at::Tensor _reshape_alias(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride) {
return wrapper_QuantizedCPU___reshape_alias(self, c10::fromIntArrayRefSlow(size), c10::fromIntArrayRefSlow(stride));
}
at::Tensor _reshape_alias_symint(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
return wrapper_QuantizedCPU___reshape_alias(self, size, stride);
}
at::Tensor relu(const at::Tensor & self) {
return wrapper_QuantizedCPU__relu(self);
}
at::Tensor & relu_(at::Tensor & self) {
return wrapper_QuantizedCPU__relu_(self);
}
at::Tensor _prelu_kernel(const at::Tensor & self, const at::Tensor & weight) {
return wrapper_QuantizedCPU___prelu_kernel(self, weight);
}
at::Tensor gelu(const at::Tensor & self, c10::string_view approximate) {
return wrapper_QuantizedCPU__gelu(self, approximate);
}
at::Tensor sigmoid(const at::Tensor & self) {
return wrapper_QuantizedCPU__sigmoid(self);
}
at::Tensor squeeze(const at::Tensor & self) {
return wrapper_QuantizedCPU__squeeze(self);
}
at::Tensor squeeze(const at::Tensor & self, int64_t dim) {
return wrapper_QuantizedCPU_dim_squeeze(self, dim);
}
at::Tensor squeeze(const at::Tensor & self, at::IntArrayRef dim) {
return wrapper_QuantizedCPU_dims_squeeze(self, dim);
}
at::Tensor std(const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
return wrapper_QuantizedCPU_correction_std(self, dim, correction, keepdim);
}
at::Tensor & std_out(at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
return wrapper_QuantizedCPU_correction_out_std_out(self, dim, correction, keepdim, out);
}
at::Tensor & std_outf(const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
return wrapper_QuantizedCPU_correction_out_std_out(self, dim, correction, keepdim, out);
}
at::Tensor tanh(const at::Tensor & self) {
return wrapper_QuantizedCPU__tanh(self);
}
at::Tensor threshold(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
return wrapper_QuantizedCPU__threshold(self, threshold, value);
}
at::Tensor flip(const at::Tensor & self, at::IntArrayRef dims) {
return wrapper_QuantizedCPU__flip(self, dims);
}
at::Tensor unsqueeze(const at::Tensor & self, int64_t dim) {
return wrapper_QuantizedCPU__unsqueeze(self, dim);
}
at::Tensor clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_QuantizedCPU__clone(self, memory_format);
}
at::Tensor dequantize(const at::Tensor & self) {
return wrapper_QuantizedCPU_self_dequantize(self);
}
::std::vector<at::Tensor> dequantize(at::TensorList tensors) {
return wrapper_QuantizedCPU_tensors_dequantize(tensors);
}
double q_scale(const at::Tensor & self) {
return wrapper_QuantizedCPU__q_scale(self);
}
int64_t q_zero_point(const at::Tensor & self) {
return wrapper_QuantizedCPU__q_zero_point(self);
}
at::Tensor q_per_channel_scales(const at::Tensor & self) {
return wrapper_QuantizedCPU__q_per_channel_scales(self);
}
at::Tensor q_per_channel_zero_points(const at::Tensor & self) {
return wrapper_QuantizedCPU__q_per_channel_zero_points(self);
}
int64_t q_per_channel_axis(const at::Tensor & self) {
return wrapper_QuantizedCPU__q_per_channel_axis(self);
}
at::Tensor int_repr(const at::Tensor & self) {
return wrapper_QuantizedCPU__int_repr(self);
}
at::QScheme qscheme(const at::Tensor & self) {
return wrapper_QuantizedCPU__qscheme(self);
}
at::Tensor & set_(at::Tensor & self, at::Storage source, int64_t storage_offset, at::IntArrayRef size, at::IntArrayRef stride) {
return wrapper_QuantizedCPU_source_Storage_storage_offset_set_(self, source, storage_offset, c10::fromIntArrayRefSlow(size), c10::fromIntArrayRefSlow(stride));
}
at::Tensor & set__symint(at::Tensor & self, at::Storage source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
return wrapper_QuantizedCPU_source_Storage_storage_offset_set_(self, source, storage_offset, size, stride);
}
at::Tensor & masked_fill_(at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
return wrapper_QuantizedCPU_Scalar_masked_fill_(self, mask, value);
}
at::Tensor & masked_fill_(at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
return wrapper_QuantizedCPU_Tensor_masked_fill_(self, mask, value);
}
at::Tensor view(const at::Tensor & self, at::IntArrayRef size) {
return wrapper_QuantizedCPU__view(self, c10::fromIntArrayRefSlow(size));
}
at::Tensor view_symint(const at::Tensor & self, c10::SymIntArrayRef size) {
return wrapper_QuantizedCPU__view(self, size);
}
at::Tensor eq(const at::Tensor & self, const at::Scalar & other) {
return wrapper_QuantizedCPU_Scalar_eq(self, other);
}
at::Tensor & eq_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_QuantizedCPU_Scalar_out_eq_out(self, other, out);
}
at::Tensor & eq_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_QuantizedCPU_Scalar_out_eq_out(self, other, out);
}
at::Tensor eq(const at::Tensor & self, const at::Tensor & other) {
return wrapper_QuantizedCPU_Tensor_eq(self, other);
}
at::Tensor & eq_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_QuantizedCPU_Tensor_out_eq_out(self, other, out);
}
at::Tensor & eq_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_QuantizedCPU_Tensor_out_eq_out(self, other, out);
}
at::Tensor ne(const at::Tensor & self, const at::Scalar & other) {
return wrapper_QuantizedCPU_Scalar_ne(self, other);
}
at::Tensor & ne_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_QuantizedCPU_Scalar_out_ne_out(self, other, out);
}
at::Tensor & ne_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_QuantizedCPU_Scalar_out_ne_out(self, other, out);
}
at::Tensor ne(const at::Tensor & self, const at::Tensor & other) {
return wrapper_QuantizedCPU_Tensor_ne(self, other);
}
at::Tensor & ne_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_QuantizedCPU_Tensor_out_ne_out(self, other, out);
}
at::Tensor & ne_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_QuantizedCPU_Tensor_out_ne_out(self, other, out);
}
at::Tensor ge(const at::Tensor & self, const at::Scalar & other) {
return wrapper_QuantizedCPU_Scalar_ge(self, other);
}
at::Tensor & ge_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_QuantizedCPU_Scalar_out_ge_out(self, other, out);
}
at::Tensor & ge_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_QuantizedCPU_Scalar_out_ge_out(self, other, out);
}
at::Tensor ge(const at::Tensor & self, const at::Tensor & other) {
return wrapper_QuantizedCPU_Tensor_ge(self, other);
}
at::Tensor & ge_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_QuantizedCPU_Tensor_out_ge_out(self, other, out);
}
at::Tensor & ge_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_QuantizedCPU_Tensor_out_ge_out(self, other, out);
}
at::Tensor le(const at::Tensor & self, const at::Scalar & other) {
return wrapper_QuantizedCPU_Scalar_le(self, other);
}
at::Tensor & le_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_QuantizedCPU_Scalar_out_le_out(self, other, out);
}
at::Tensor & le_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_QuantizedCPU_Scalar_out_le_out(self, other, out);
}
at::Tensor le(const at::Tensor & self, const at::Tensor & other) {
return wrapper_QuantizedCPU_Tensor_le(self, other);
}
at::Tensor & le_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_QuantizedCPU_Tensor_out_le_out(self, other, out);
}
at::Tensor & le_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_QuantizedCPU_Tensor_out_le_out(self, other, out);
}
at::Tensor gt(const at::Tensor & self, const at::Scalar & other) {
return wrapper_QuantizedCPU_Scalar_gt(self, other);
}
at::Tensor & gt_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_QuantizedCPU_Scalar_out_gt_out(self, other, out);
}
at::Tensor & gt_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_QuantizedCPU_Scalar_out_gt_out(self, other, out);
}
at::Tensor gt(const at::Tensor & self, const at::Tensor & other) {
return wrapper_QuantizedCPU_Tensor_gt(self, other);
}
at::Tensor & gt_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_QuantizedCPU_Tensor_out_gt_out(self, other, out);
}
at::Tensor & gt_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_QuantizedCPU_Tensor_out_gt_out(self, other, out);
}
at::Tensor lt(const at::Tensor & self, const at::Scalar & other) {
return wrapper_QuantizedCPU_Scalar_lt(self, other);
}
at::Tensor & lt_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_QuantizedCPU_Scalar_out_lt_out(self, other, out);
}
at::Tensor & lt_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_QuantizedCPU_Scalar_out_lt_out(self, other, out);
}
at::Tensor lt(const at::Tensor & self, const at::Tensor & other) {
return wrapper_QuantizedCPU_Tensor_lt(self, other);
}
at::Tensor & lt_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_QuantizedCPU_Tensor_out_lt_out(self, other, out);
}
at::Tensor & lt_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_QuantizedCPU_Tensor_out_lt_out(self, other, out);
}
at::Tensor index_select(const at::Tensor & self, int64_t dim, const at::Tensor & index) {
return wrapper_QuantizedCPU__index_select(self, dim, index);
}
at::Tensor & index_select_out(at::Tensor & out, const at::Tensor & self, int64_t dim, const at::Tensor & index) {
return wrapper_QuantizedCPU_out_index_select_out(self, dim, index, out);
}
at::Tensor & index_select_outf(const at::Tensor & self, int64_t dim, const at::Tensor & index, at::Tensor & out) {
return wrapper_QuantizedCPU_out_index_select_out(self, dim, index, out);
}
at::Tensor min(const at::Tensor & self) {
return wrapper_QuantizedCPU__min(self);
}
at::Tensor & min_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_QuantizedCPU_unary_out_min_out(self, out);
}
at::Tensor & min_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_QuantizedCPU_unary_out_min_out(self, out);
}
at::Tensor max(const at::Tensor & self) {
return wrapper_QuantizedCPU__max(self);
}
at::Tensor & max_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_QuantizedCPU_unary_out_max_out(self, out);
}
at::Tensor & max_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_QuantizedCPU_unary_out_max_out(self, out);
}
::std::tuple<at::Tensor,at::Tensor> sort(const at::Tensor & self, c10::optional<bool> stable, int64_t dim, bool descending) {
return wrapper_QuantizedCPU_stable_sort(self, stable, dim, descending);
}
::std::tuple<at::Tensor,at::Tensor> topk(const at::Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
return wrapper_QuantizedCPU__topk(self, k, dim, largest, sorted);
}
::std::tuple<at::Tensor,at::Tensor> topk_symint(const at::Tensor & self, c10::SymInt k, int64_t dim, bool largest, bool sorted) {
return wrapper_QuantizedCPU__topk(self, k, dim, largest, sorted);
}
at::Tensor unfold(const at::Tensor & self, int64_t dimension, int64_t size, int64_t step) {
return wrapper_QuantizedCPU__unfold(self, dimension, size, step);
}
bool equal(const at::Tensor & self, const at::Tensor & other) {
return wrapper_QuantizedCPU__equal(self, other);
}
at::Tensor hardsigmoid(const at::Tensor & self) {
return wrapper_QuantizedCPU__hardsigmoid(self);
}
at::Tensor & hardsigmoid_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_QuantizedCPU_out_hardsigmoid_out(self, out);
}
at::Tensor & hardsigmoid_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_QuantizedCPU_out_hardsigmoid_out(self, out);
}
at::Tensor hardtanh(const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
return wrapper_QuantizedCPU__hardtanh(self, min_val, max_val);
}
at::Tensor & hardtanh_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
return wrapper_QuantizedCPU_out_hardtanh_out(self, min_val, max_val, out);
}
at::Tensor & hardtanh_outf(const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & out) {
return wrapper_QuantizedCPU_out_hardtanh_out(self, min_val, max_val, out);
}
at::Tensor & hardtanh_(at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
return wrapper_QuantizedCPU__hardtanh_(self, min_val, max_val);
}
at::Tensor leaky_relu(const at::Tensor & self, const at::Scalar & negative_slope) {
return wrapper_QuantizedCPU__leaky_relu(self, negative_slope);
}
at::Tensor & leaky_relu_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & negative_slope) {
return wrapper_QuantizedCPU_out_leaky_relu_out(self, negative_slope, out);
}
at::Tensor & leaky_relu_outf(const at::Tensor & self, const at::Scalar & negative_slope, at::Tensor & out) {
return wrapper_QuantizedCPU_out_leaky_relu_out(self, negative_slope, out);
}
at::Tensor & leaky_relu_(at::Tensor & self, const at::Scalar & negative_slope) {
return wrapper_QuantizedCPU__leaky_relu_(self, negative_slope);
}
at::Tensor _adaptive_avg_pool2d(const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper_QuantizedCPU___adaptive_avg_pool2d(self, c10::fromIntArrayRefSlow(output_size));
}
at::Tensor _adaptive_avg_pool2d_symint(const at::Tensor & self, c10::SymIntArrayRef output_size) {
return wrapper_QuantizedCPU___adaptive_avg_pool2d(self, output_size);
}
at::Tensor & adaptive_avg_pool3d_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper_QuantizedCPU_out_adaptive_avg_pool3d_out(self, c10::fromIntArrayRefSlow(output_size), out);
}
at::Tensor & adaptive_avg_pool3d_outf(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out) {
return wrapper_QuantizedCPU_out_adaptive_avg_pool3d_out(self, c10::fromIntArrayRefSlow(output_size), out);
}
at::Tensor & adaptive_avg_pool3d_symint_out(at::Tensor & out, const at::Tensor & self, c10::SymIntArrayRef output_size) {
return wrapper_QuantizedCPU_out_adaptive_avg_pool3d_out(self, output_size, out);
}
at::Tensor & adaptive_avg_pool3d_symint_outf(const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
return wrapper_QuantizedCPU_out_adaptive_avg_pool3d_out(self, output_size, out);
}
at::Tensor _adaptive_avg_pool3d(const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper_QuantizedCPU___adaptive_avg_pool3d(self, c10::fromIntArrayRefSlow(output_size));
}
at::Tensor _adaptive_avg_pool3d_symint(const at::Tensor & self, c10::SymIntArrayRef output_size) {
return wrapper_QuantizedCPU___adaptive_avg_pool3d(self, output_size);
}
at::Tensor avg_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
return wrapper_QuantizedCPU__avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
at::Tensor avg_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
return wrapper_QuantizedCPU__avg_pool3d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
at::Tensor & reflection_pad1d_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef padding) {
return wrapper_QuantizedCPU_out_reflection_pad1d_out(self, c10::fromIntArrayRefSlow(padding), out);
}
at::Tensor & reflection_pad1d_outf(const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
return wrapper_QuantizedCPU_out_reflection_pad1d_out(self, c10::fromIntArrayRefSlow(padding), out);
}
at::Tensor & reflection_pad1d_symint_out(at::Tensor & out, const at::Tensor & self, c10::SymIntArrayRef padding) {
return wrapper_QuantizedCPU_out_reflection_pad1d_out(self, padding, out);
}
at::Tensor & reflection_pad1d_symint_outf(const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
return wrapper_QuantizedCPU_out_reflection_pad1d_out(self, padding, out);
}
at::Tensor reflection_pad2d(const at::Tensor & self, at::IntArrayRef padding) {
return wrapper_QuantizedCPU__reflection_pad2d(self, c10::fromIntArrayRefSlow(padding));
}
at::Tensor reflection_pad2d_symint(const at::Tensor & self, c10::SymIntArrayRef padding) {
return wrapper_QuantizedCPU__reflection_pad2d(self, padding);
}
at::Tensor & reflection_pad2d_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef padding) {
return wrapper_QuantizedCPU_out_reflection_pad2d_out(self, c10::fromIntArrayRefSlow(padding), out);
}
at::Tensor & reflection_pad2d_outf(const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
return wrapper_QuantizedCPU_out_reflection_pad2d_out(self, c10::fromIntArrayRefSlow(padding), out);
}
at::Tensor & reflection_pad2d_symint_out(at::Tensor & out, const at::Tensor & self, c10::SymIntArrayRef padding) {
return wrapper_QuantizedCPU_out_reflection_pad2d_out(self, padding, out);
}
at::Tensor & reflection_pad2d_symint_outf(const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
return wrapper_QuantizedCPU_out_reflection_pad2d_out(self, padding, out);
}
at::Tensor upsample_bilinear2d(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_QuantizedCPU__upsample_bilinear2d(self, c10::fromIntArrayRefSlow(output_size), align_corners, scales_h, scales_w);
}
at::Tensor upsample_bilinear2d_symint(const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_QuantizedCPU__upsample_bilinear2d(self, output_size, align_corners, scales_h, scales_w);
}
at::Tensor upsample_nearest2d(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_QuantizedCPU__upsample_nearest2d(self, c10::fromIntArrayRefSlow(output_size), scales_h, scales_w);
}
at::Tensor upsample_nearest2d_symint(const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_QuantizedCPU__upsample_nearest2d(self, output_size, scales_h, scales_w);
}
at::Tensor _upsample_nearest_exact2d(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_QuantizedCPU___upsample_nearest_exact2d(self, c10::fromIntArrayRefSlow(output_size), scales_h, scales_w);
}
at::Tensor _upsample_nearest_exact2d_symint(const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_QuantizedCPU___upsample_nearest_exact2d(self, output_size, scales_h, scales_w);
}
at::Tensor upsample_nearest3d(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_QuantizedCPU__upsample_nearest3d(self, c10::fromIntArrayRefSlow(output_size), scales_d, scales_h, scales_w);
}
at::Tensor upsample_nearest3d_symint(const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_QuantizedCPU__upsample_nearest3d(self, output_size, scales_d, scales_h, scales_w);
}
at::Tensor _upsample_nearest_exact3d(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_QuantizedCPU___upsample_nearest_exact3d(self, c10::fromIntArrayRefSlow(output_size), scales_d, scales_h, scales_w);
}
at::Tensor _upsample_nearest_exact3d_symint(const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_QuantizedCPU___upsample_nearest_exact3d(self, output_size, scales_d, scales_h, scales_w);
}
} // namespace quantizedcpu
} // namespace at
