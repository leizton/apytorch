// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// an external backend might generate file within its code tree
// and check all the source files within the tree with clang-format.
// so, disable it since the backend might have a different config.
// clang-format off

// NOTE: This condition is true for all PyTorch internal libraries, it
//       just excludes external projects such as torch_xla which
//       re-use some of the PyTorch codegen machinery.
#if defined(CAFFE2_BUILD_MAIN_LIB)        || \
    defined(TORCH_CUDA_BUILD_MAIN_LIB)    || \
    defined(TORCH_HIP_BUILD_MAIN_LIB)     || \
    defined(TORCH_CUDA_CU_BUILD_MAIN_LIB) || \
    defined(TORCH_CUDA_CPP_BUILD_MAIN_LIB)
#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
#endif

// @generated by torchgen/gen.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/Tensor.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/ATenCUDAGeneral.h>
#include <ATen/cuda/CUDADevice.h>
#include <ATen/cuda/CUDAContext.h>

#include <ATen/NativeFunctions.h>
#include <ATen/Functions.h>
#include <ATen/NativeFunctions.h>

// See template file RegisterDispatchDefinitions.ini
namespace at {
// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {
void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      // TODO: avoid the redispatch here
      out.as_strided_(sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}
void check_inplace(const Tensor &self, IntArrayRef sizes, const TensorOptions &options) {
  // These checks are needed on those operators that:
  //   1) don't use 'TensorIterator' (e.g. 'addmm' and 'baddbmm')
  //   2) have particular typing rules (e.g. 'cumsum' and 'cumprod')
  // For other operators (e.g. 'add'), 'TensorIterator' already checks
  // these things separately.
  TORCH_CHECK(options.dtype() == self.dtype(),
      "Bad in-place call: ",
      "input tensor dtype ", self.dtype(), " and output tensor dtype ", options.dtype(), " should match");
  TORCH_CHECK(options.device() == self.device(),
      "Bad in-place call: ",
      "input tensor device ", self.device(), " and output tensor device ", options.device(), " should match");
  TORCH_CHECK(sizes == self.sizes(),
      "Bad in-place call: ",
      "input tensor size ", self.sizes(), " and output tensor size ", sizes, " should match");
}
namespace {
at::Tensor wrapper_SparseCUDA__abs(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::abs_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_abs_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::abs_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__abs_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::abs_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__sgn(const at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__sgn", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sgn_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_sgn_out(const at::Tensor & self, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_sgn_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_out_sgn_out", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sgn_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__sgn_(at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__sgn_", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sgn_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_conj_physical_out(const at::Tensor & self, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_conj_physical_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_out_conj_physical_out", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::conj_physical_out_sparse(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA_Tensor_add(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::add_sparse(self, other, alpha);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_add_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::add_out_sparse_cuda(self, other, alpha, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_Tensor_add_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::add_sparse_(self, other, alpha);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__asinh(const at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__asinh", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::asinh_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_asinh_out(const at::Tensor & self, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_asinh_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_out_asinh_out", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::asinh_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__asinh_(at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__asinh_", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::asinh_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__atanh(const at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__atanh", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::atanh_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_atanh_out(const at::Tensor & self, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_atanh_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_out_atanh_out", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::atanh_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__atanh_(at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__atanh_", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::atanh_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__asin(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::asin_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_asin_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::asin_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__asin_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::asin_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__atan(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::atan_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_atan_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::atan_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__atan_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::atan_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__bmm(const at::Tensor & self, const at::Tensor & mat2) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__bmm", "self");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_SparseCUDA__bmm", "mat2");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::bmm_sparse_cuda(self, mat2);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_bmm_out(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_bmm_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_out_bmm_out", "self");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_SparseCUDA_out_bmm_out", "mat2");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::bmm_out_sparse_cuda(self, mat2, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___sparse_broadcast_to(const at::Tensor & self, at::IntArrayRef size) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___sparse_broadcast_to", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_broadcast_to(self, size);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__cat(const at::ITensorListRef & tensors, int64_t dim) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, tensors, "wrapper_SparseCUDA__cat", "tensors");
  const OptionalDeviceGuard device_guard(device_of(tensors));
  return at::native::cat_sparse(tensors, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__ceil(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::ceil_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_ceil_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::ceil_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__ceil_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::ceil_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__copy_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return at::native::copy_sparse_wrapper_(self, src, non_blocking);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA_Tensor_div(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::div_sparse(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_div_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::div_out_sparse_zerodim(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_Tensor_div_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::div_sparse_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA_Tensor_mode_div(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::div_sparse(self, other, rounding_mode);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_mode_div_out(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::div_out_sparse_zerodim(self, other, rounding_mode, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_Tensor_mode_div_(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::div_sparse_(self, other, rounding_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA_memory_format_empty(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  globalContext().lazyInitCUDA();
  const DeviceGuard device_guard(device_or_default(device));
  return at::native::empty_sparse(C10_AS_INTARRAYREF_SLOW(size), dtype, layout, device, pin_memory, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__empty_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::empty_like_sparse_coo(self, dtype, layout, device, pin_memory, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__erf(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::erf_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_erf_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::erf_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__erf_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::erf_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__expm1(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::expm1_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_expm1_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::expm1_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__expm1_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::expm1_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__floor(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::floor_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_floor_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::floor_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__floor_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::floor_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__floor_divide(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::floor_divide_sparse(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_floor_divide_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::floor_divide_out_sparse_zerodim(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_Tensor_floor_divide_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::floor_divide_sparse_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__frac(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::frac_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_frac_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::frac_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__frac_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::frac_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__isnan(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::isnan_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__nan_to_num(const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__nan_to_num", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::nan_to_num_sparse(self, nan, posinf, neginf);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_nan_to_num_out(const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_nan_to_num_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_out_nan_to_num_out", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::nan_to_num_sparse_out(self, nan, posinf, neginf, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__nan_to_num_(at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__nan_to_num_", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::nan_to_num_sparse_(self, nan, posinf, neginf);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__log1p(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::log1p_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_log1p_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::log1p_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__log1p_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::log1p_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__mm(const at::Tensor & self, const at::Tensor & mat2) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__mm", "self");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_SparseCUDA__mm", "mat2");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::_sparse_mm(self, mat2);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_mm_out(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_mm_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_out_mm_out", "self");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_SparseCUDA_out_mm_out", "mat2");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::_sparse_mm_out(self, mat2, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___sparse_sparse_matmul(const at::Tensor & self, const at::Tensor & other) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___sparse_sparse_matmul", "self");
  c10::impl::check_and_update_common_device(common_device, other, "wrapper_SparseCUDA___sparse_sparse_matmul", "other");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_sparse_matmul_cuda(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA_Tensor_mul(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::mul_sparse(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_mul_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::mul_out_sparse_cuda(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_Tensor_mul_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::mul_sparse_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__mv(const at::Tensor & self, const at::Tensor & vec) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__mv", "self");
  c10::impl::check_and_update_common_device(common_device, vec, "wrapper_SparseCUDA__mv", "vec");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::mv_sparse(self, vec);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__narrow_copy(const at::Tensor & self, int64_t dim, c10::SymInt start, c10::SymInt length) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__narrow_copy", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::narrow_copy_sparse(self, dim, start.guard_int(__FILE__, __LINE__), length.guard_int(__FILE__, __LINE__));
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__permute(const at::Tensor & self, at::IntArrayRef dims) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__permute", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::permute_sparse_coo(self, dims);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__rad2deg(const at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__rad2deg", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::rad2deg_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_rad2deg_out(const at::Tensor & self, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_rad2deg_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_out_rad2deg_out", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::rad2deg_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__rad2deg_(at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__rad2deg_", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::rad2deg_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__deg2rad(const at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__deg2rad", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::deg2rad_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_deg2rad_out(const at::Tensor & self, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_deg2rad_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_out_deg2rad_out", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::deg2rad_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__deg2rad_(at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__deg2rad_", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::deg2rad_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__neg(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::neg_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_neg_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::neg_out_sparse(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__neg_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::neg_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__round(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::round_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_round_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::round_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__round_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::round_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__relu(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::relu_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__relu_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::relu_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__sin(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sin_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_sin_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sin_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__sin_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sin_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__sinh(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sinh_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_sinh_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sinh_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__sinh_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sinh_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_sspaddmm_out(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_sspaddmm_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_out_sspaddmm_out", "self");
  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper_SparseCUDA_out_sspaddmm_out", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_SparseCUDA_out_sspaddmm_out", "mat2");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::_sspaddmm_out_cuda(self, mat1, mat2, beta, alpha, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__sum(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sum_coo(self, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA_dim_IntList_sum(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sum_sparse_coo(self, dim, keepdim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__sqrt(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sqrt_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_sqrt_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sqrt_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__sqrt_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sqrt_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__tan(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::tan_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_tan_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::tan_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__tan_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::tan_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__tanh(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::tanh_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_tanh_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::tanh_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__tanh_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::tanh_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__threshold_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, grad_output, "wrapper_SparseCUDA__threshold_backward", "grad_output");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__threshold_backward", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::threshold_backward_sparse(grad_output, self, threshold);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_grad_input_threshold_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold, at::Tensor & grad_input) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, grad_input, "wrapper_SparseCUDA_grad_input_threshold_backward_out", "grad_input");
  c10::impl::check_and_update_common_device(common_device, grad_output, "wrapper_SparseCUDA_grad_input_threshold_backward_out", "grad_output");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_grad_input_threshold_backward_out", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::threshold_backward_sparse_out(grad_output, self, threshold, grad_input);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__trunc(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::trunc_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_trunc_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::trunc_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__trunc_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::trunc_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__unsqueeze(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::unsqueeze_sparse(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_zeros_out(c10::SymIntArrayRef size, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_zeros_out", "out");
  const OptionalDeviceGuard device_guard(device_of(out));
  return at::native::zeros_sparse_out(C10_AS_INTARRAYREF_SLOW(size), out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__native_norm(const at::Tensor & self, const at::Scalar & p) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__native_norm", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::norm_sparse(self, p);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA_ScalarOpt_dim_dtype_native_norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_ScalarOpt_dim_dtype_native_norm", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::norm_sparse(self, p, dim, keepdim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___sparse_sum_backward(const at::Tensor & grad, const at::Tensor & self, at::IntArrayRef dim) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, grad, "wrapper_SparseCUDA___sparse_sum_backward", "grad");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___sparse_sum_backward", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::_sparse_sum_backward_cuda(grad, self, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___sparse_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___sparse_softmax", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::softmax_sparse_cuda(self, dim, half_to_float);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___sparse_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, grad_output, "wrapper_SparseCUDA___sparse_softmax_backward_data", "grad_output");
  c10::impl::check_and_update_common_device(common_device, output, "wrapper_SparseCUDA___sparse_softmax_backward_data", "output");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___sparse_softmax_backward_data", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::softmax_backward_sparse_cuda(grad_output, output, dim, self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___sparse_log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___sparse_log_softmax", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::log_softmax_sparse_cuda(self, dim, half_to_float);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___sparse_log_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, grad_output, "wrapper_SparseCUDA___sparse_log_softmax_backward_data", "grad_output");
  c10::impl::check_and_update_common_device(common_device, output, "wrapper_SparseCUDA___sparse_log_softmax_backward_data", "output");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___sparse_log_softmax_backward_data", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::log_softmax_backward_sparse_cuda(grad_output, output, dim, self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA_ScalarOpt_dim_dtype_norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_dtype_norm(self, p, dim, keepdim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA_ScalarOpt_dim_norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_norm(self, p, dim, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__clone", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::clone_sparse(self, memory_format);
}
} // anonymous namespace
namespace {
const at::Tensor & wrapper_SparseCUDA__resize_as_sparse_(const at::Tensor & self, const at::Tensor & the_template) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__resize_as_sparse_", "self");
  c10::impl::check_and_update_common_device(common_device, the_template, "wrapper_SparseCUDA__resize_as_sparse_", "the_template");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::resize_as_sparse_(self, the_template);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__zero_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::zero_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA_Tensor_sub(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sub_sparse(self, other, alpha);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_sub_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sub_out_sparse(self, other, alpha, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_Tensor_sub_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sub_sparse_(self, other, alpha);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__addmm", "self");
  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper_SparseCUDA__addmm", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_SparseCUDA__addmm", "mat2");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::addmm_sparse_dense_cuda(self, mat1, mat2, beta, alpha);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_addmm_out(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_addmm_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_out_addmm_out", "self");
  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper_SparseCUDA_out_addmm_out", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_SparseCUDA_out_addmm_out", "mat2");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::addmm_out_sparse_dense_cuda(self, mat1, mat2, beta, alpha, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__addmm_(at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__addmm_", "self");
  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper_SparseCUDA__addmm_", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_SparseCUDA__addmm_", "mat2");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::s_addmm_sparse_dense_cuda_(self, mat1, mat2, beta, alpha);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  globalContext().lazyInitCUDA();
  const DeviceGuard device_guard(device_or_default(device));
  return at::native::new_with_dims_sparse(sparse_dim, dense_dim, size, dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, c10::SymIntArrayRef size, const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<bool> is_coalesced) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, indices, "wrapper_SparseCUDA___sparse_coo_tensor_with_dims_and_tensors", "indices");
  c10::impl::check_and_update_common_device(common_device, values, "wrapper_SparseCUDA___sparse_coo_tensor_with_dims_and_tensors", "values");
  globalContext().lazyInitCUDA();
  const DeviceGuard device_guard(device_or_default(device));
  return at::native::new_with_dims_and_tensor_sparse_symint(sparse_dim, dense_dim, size, indices, values, dtype, layout, device, pin_memory, is_coalesced);
}
} // anonymous namespace
namespace {
const at::Tensor & wrapper_SparseCUDA__sparse_resize_(const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__sparse_resize_", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_resize_(self, size, sparse_dim, dense_dim);
}
} // anonymous namespace
namespace {
const at::Tensor & wrapper_SparseCUDA__sparse_resize_and_clear_(const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__sparse_resize_and_clear_", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_resize_and_clear_(self, size, sparse_dim, dense_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__sparse_mask(const at::Tensor & self, const at::Tensor & mask) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__sparse_mask", "self");
  c10::impl::check_and_update_common_device(common_device, mask, "wrapper_SparseCUDA__sparse_mask", "mask");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_mask(self, mask);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___sparse_mask_projection(const at::Tensor & self, const at::Tensor & mask, bool accumulate_matches) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___sparse_mask_projection", "self");
  c10::impl::check_and_update_common_device(common_device, mask, "wrapper_SparseCUDA___sparse_mask_projection", "mask");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_mask_projection(self, mask, accumulate_matches);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___to_dense(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<bool> masked_grad) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___to_dense", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_to_dense(self, dtype, masked_grad);
}
} // anonymous namespace
namespace {
int64_t wrapper_SparseCUDA__sparse_dim(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::sparse_dim_sparse(self);
}
} // anonymous namespace
namespace {
int64_t wrapper_SparseCUDA___dimI(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::sparse_dim_sparse(self);
}
} // anonymous namespace
namespace {
int64_t wrapper_SparseCUDA__dense_dim(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::dense_dim_sparse(self);
}
} // anonymous namespace
namespace {
int64_t wrapper_SparseCUDA___dimV(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::dense_dim_sparse(self);
}
} // anonymous namespace
namespace {
int64_t wrapper_SparseCUDA___nnz(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::_nnz_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___coalesce(const at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___coalesce", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::_coalesce_sparse_cuda(self);
}
} // anonymous namespace
namespace {
bool wrapper_SparseCUDA__is_coalesced(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::is_coalesced_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___indices(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::_indices_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___values(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::_values_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA___coalesced_(at::Tensor & self, bool coalesced) {
    // No device check
  // DeviceGuard omitted
  return at::native::_coalesced_sparse_(self, coalesced);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__indices(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::indices_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__values(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::values_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__hspmm(const at::Tensor & mat1, const at::Tensor & mat2) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper_SparseCUDA__hspmm", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_SparseCUDA__hspmm", "mat2");
  const OptionalDeviceGuard device_guard(device_of(mat1));
  return at::native::hspmm_sparse_cuda(mat1, mat2);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_hspmm_out(const at::Tensor & mat1, const at::Tensor & mat2, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_hspmm_out", "out");
  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper_SparseCUDA_out_hspmm_out", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_SparseCUDA_out_hspmm_out", "mat2");
  const OptionalDeviceGuard device_guard(device_of(out));
  return at::native::hspmm_out_sparse_cuda(mat1, mat2, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__copy_sparse_to_sparse_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::copy_sparse_(self, src, non_blocking);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA_sparse_dim__to_sparse(const at::Tensor & self, int64_t sparse_dim) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_sparse_dim__to_sparse", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_coo_to_sparse(self, sparse_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___to_sparse(const at::Tensor & self, c10::optional<at::Layout> layout, at::OptionalIntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___to_sparse", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_coo_to_sparse(self, layout, blocksize, dense_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___to_sparse_csr(const at::Tensor & self, c10::optional<int64_t> dense_dim) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___to_sparse_csr", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::coo_to_sparse_csr(self, dense_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___to_sparse_csc(const at::Tensor & self, c10::optional<int64_t> dense_dim) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___to_sparse_csc", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::coo_to_sparse_csc(self, dense_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___to_sparse_bsr(const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___to_sparse_bsr", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::coo_to_sparse_bsr(self, blocksize, dense_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA___to_sparse_bsc(const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA___to_sparse_bsc", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::coo_to_sparse_bsc(self, blocksize, dense_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__index_select(const at::Tensor & self, int64_t dim, const at::Tensor & index) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__index_select", "self");
  c10::impl::check_and_update_common_device(common_device, index, "wrapper_SparseCUDA__index_select", "index");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::index_select_sparse_cuda(self, dim, index);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__erfinv(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::erfinv_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_erfinv_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::erfinv_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__erfinv_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::erfinv_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__sign(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sign_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_sign_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sign_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA__sign_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sign_sparse_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__signbit(const at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__signbit", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::signbit_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_signbit_out(const at::Tensor & self, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_signbit_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_out_signbit_out", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::signbit_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__any(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::any_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA_Tensor_Scalar_pow(const at::Tensor & self, const at::Scalar & exponent) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::pow_sparse_scalar(self, exponent);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_Tensor_Scalar_out_pow_out(const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::pow_out_sparse_scalar(self, exponent, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__isinf(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::isinf_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__isposinf(const at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__isposinf", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::isposinf_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_isposinf_out(const at::Tensor & self, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_isposinf_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_out_isposinf_out", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::isposinf_sparse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseCUDA__isneginf(const at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA__isneginf", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::isneginf_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseCUDA_out_isneginf_out(const at::Tensor & self, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, out, "wrapper_SparseCUDA_out_isneginf_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseCUDA_out_isneginf_out", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::isneginf_sparse_out(self, out);
}
} // anonymous namespace
TORCH_LIBRARY_IMPL(aten, SparseCUDA, m) {
    m.impl("abs",
TORCH_FN(wrapper_SparseCUDA__abs));
m.impl("abs.out",
TORCH_FN(wrapper_SparseCUDA_out_abs_out));
m.impl("abs_",
TORCH_FN(wrapper_SparseCUDA__abs_));
m.impl("sgn",
TORCH_FN(wrapper_SparseCUDA__sgn));
m.impl("sgn.out",
TORCH_FN(wrapper_SparseCUDA_out_sgn_out));
m.impl("sgn_",
TORCH_FN(wrapper_SparseCUDA__sgn_));
m.impl("conj_physical.out",
TORCH_FN(wrapper_SparseCUDA_out_conj_physical_out));
m.impl("add.Tensor",
TORCH_FN(wrapper_SparseCUDA_Tensor_add));
m.impl("add.out",
TORCH_FN(wrapper_SparseCUDA_out_add_out));
m.impl("add_.Tensor",
TORCH_FN(wrapper_SparseCUDA_Tensor_add_));
m.impl("asinh",
TORCH_FN(wrapper_SparseCUDA__asinh));
m.impl("asinh.out",
TORCH_FN(wrapper_SparseCUDA_out_asinh_out));
m.impl("asinh_",
TORCH_FN(wrapper_SparseCUDA__asinh_));
m.impl("atanh",
TORCH_FN(wrapper_SparseCUDA__atanh));
m.impl("atanh.out",
TORCH_FN(wrapper_SparseCUDA_out_atanh_out));
m.impl("atanh_",
TORCH_FN(wrapper_SparseCUDA__atanh_));
m.impl("asin",
TORCH_FN(wrapper_SparseCUDA__asin));
m.impl("asin.out",
TORCH_FN(wrapper_SparseCUDA_out_asin_out));
m.impl("asin_",
TORCH_FN(wrapper_SparseCUDA__asin_));
m.impl("atan",
TORCH_FN(wrapper_SparseCUDA__atan));
m.impl("atan.out",
TORCH_FN(wrapper_SparseCUDA_out_atan_out));
m.impl("atan_",
TORCH_FN(wrapper_SparseCUDA__atan_));
m.impl("bmm",
TORCH_FN(wrapper_SparseCUDA__bmm));
m.impl("bmm.out",
TORCH_FN(wrapper_SparseCUDA_out_bmm_out));
m.impl("_sparse_broadcast_to",
TORCH_FN(wrapper_SparseCUDA___sparse_broadcast_to));
m.impl("cat",
TORCH_FN(wrapper_SparseCUDA__cat));
m.impl("ceil",
TORCH_FN(wrapper_SparseCUDA__ceil));
m.impl("ceil.out",
TORCH_FN(wrapper_SparseCUDA_out_ceil_out));
m.impl("ceil_",
TORCH_FN(wrapper_SparseCUDA__ceil_));
m.impl("copy_",
TORCH_FN(wrapper_SparseCUDA__copy_));
m.impl("div.Tensor",
TORCH_FN(wrapper_SparseCUDA_Tensor_div));
m.impl("div.out",
TORCH_FN(wrapper_SparseCUDA_out_div_out));
m.impl("div_.Tensor",
TORCH_FN(wrapper_SparseCUDA_Tensor_div_));
m.impl("div.Tensor_mode",
TORCH_FN(wrapper_SparseCUDA_Tensor_mode_div));
m.impl("div.out_mode",
TORCH_FN(wrapper_SparseCUDA_out_mode_div_out));
m.impl("div_.Tensor_mode",
TORCH_FN(wrapper_SparseCUDA_Tensor_mode_div_));
m.impl("empty.memory_format",
TORCH_FN(wrapper_SparseCUDA_memory_format_empty));
m.impl("empty_like",
TORCH_FN(wrapper_SparseCUDA__empty_like));
m.impl("erf",
TORCH_FN(wrapper_SparseCUDA__erf));
m.impl("erf.out",
TORCH_FN(wrapper_SparseCUDA_out_erf_out));
m.impl("erf_",
TORCH_FN(wrapper_SparseCUDA__erf_));
m.impl("expm1",
TORCH_FN(wrapper_SparseCUDA__expm1));
m.impl("expm1.out",
TORCH_FN(wrapper_SparseCUDA_out_expm1_out));
m.impl("expm1_",
TORCH_FN(wrapper_SparseCUDA__expm1_));
m.impl("floor",
TORCH_FN(wrapper_SparseCUDA__floor));
m.impl("floor.out",
TORCH_FN(wrapper_SparseCUDA_out_floor_out));
m.impl("floor_",
TORCH_FN(wrapper_SparseCUDA__floor_));
m.impl("floor_divide",
TORCH_FN(wrapper_SparseCUDA__floor_divide));
m.impl("floor_divide.out",
TORCH_FN(wrapper_SparseCUDA_out_floor_divide_out));
m.impl("floor_divide_.Tensor",
TORCH_FN(wrapper_SparseCUDA_Tensor_floor_divide_));
m.impl("frac",
TORCH_FN(wrapper_SparseCUDA__frac));
m.impl("frac.out",
TORCH_FN(wrapper_SparseCUDA_out_frac_out));
m.impl("frac_",
TORCH_FN(wrapper_SparseCUDA__frac_));
m.impl("isnan",
TORCH_FN(wrapper_SparseCUDA__isnan));
m.impl("nan_to_num",
TORCH_FN(wrapper_SparseCUDA__nan_to_num));
m.impl("nan_to_num.out",
TORCH_FN(wrapper_SparseCUDA_out_nan_to_num_out));
m.impl("nan_to_num_",
TORCH_FN(wrapper_SparseCUDA__nan_to_num_));
m.impl("log1p",
TORCH_FN(wrapper_SparseCUDA__log1p));
m.impl("log1p.out",
TORCH_FN(wrapper_SparseCUDA_out_log1p_out));
m.impl("log1p_",
TORCH_FN(wrapper_SparseCUDA__log1p_));
m.impl("mm",
TORCH_FN(wrapper_SparseCUDA__mm));
m.impl("mm.out",
TORCH_FN(wrapper_SparseCUDA_out_mm_out));
m.impl("_sparse_sparse_matmul",
TORCH_FN(wrapper_SparseCUDA___sparse_sparse_matmul));
m.impl("mul.Tensor",
TORCH_FN(wrapper_SparseCUDA_Tensor_mul));
m.impl("mul.out",
TORCH_FN(wrapper_SparseCUDA_out_mul_out));
m.impl("mul_.Tensor",
TORCH_FN(wrapper_SparseCUDA_Tensor_mul_));
m.impl("mv",
TORCH_FN(wrapper_SparseCUDA__mv));
m.impl("narrow_copy",
TORCH_FN(wrapper_SparseCUDA__narrow_copy));
m.impl("permute",
TORCH_FN(wrapper_SparseCUDA__permute));
m.impl("rad2deg",
TORCH_FN(wrapper_SparseCUDA__rad2deg));
m.impl("rad2deg.out",
TORCH_FN(wrapper_SparseCUDA_out_rad2deg_out));
m.impl("rad2deg_",
TORCH_FN(wrapper_SparseCUDA__rad2deg_));
m.impl("deg2rad",
TORCH_FN(wrapper_SparseCUDA__deg2rad));
m.impl("deg2rad.out",
TORCH_FN(wrapper_SparseCUDA_out_deg2rad_out));
m.impl("deg2rad_",
TORCH_FN(wrapper_SparseCUDA__deg2rad_));
m.impl("neg",
TORCH_FN(wrapper_SparseCUDA__neg));
m.impl("neg.out",
TORCH_FN(wrapper_SparseCUDA_out_neg_out));
m.impl("neg_",
TORCH_FN(wrapper_SparseCUDA__neg_));
m.impl("round",
TORCH_FN(wrapper_SparseCUDA__round));
m.impl("round.out",
TORCH_FN(wrapper_SparseCUDA_out_round_out));
m.impl("round_",
TORCH_FN(wrapper_SparseCUDA__round_));
m.impl("relu",
TORCH_FN(wrapper_SparseCUDA__relu));
m.impl("relu_",
TORCH_FN(wrapper_SparseCUDA__relu_));
m.impl("sin",
TORCH_FN(wrapper_SparseCUDA__sin));
m.impl("sin.out",
TORCH_FN(wrapper_SparseCUDA_out_sin_out));
m.impl("sin_",
TORCH_FN(wrapper_SparseCUDA__sin_));
m.impl("sinh",
TORCH_FN(wrapper_SparseCUDA__sinh));
m.impl("sinh.out",
TORCH_FN(wrapper_SparseCUDA_out_sinh_out));
m.impl("sinh_",
TORCH_FN(wrapper_SparseCUDA__sinh_));
m.impl("sspaddmm.out",
TORCH_FN(wrapper_SparseCUDA_out_sspaddmm_out));
m.impl("sum",
TORCH_FN(wrapper_SparseCUDA__sum));
m.impl("sum.dim_IntList",
TORCH_FN(wrapper_SparseCUDA_dim_IntList_sum));
m.impl("sqrt",
TORCH_FN(wrapper_SparseCUDA__sqrt));
m.impl("sqrt.out",
TORCH_FN(wrapper_SparseCUDA_out_sqrt_out));
m.impl("sqrt_",
TORCH_FN(wrapper_SparseCUDA__sqrt_));
m.impl("tan",
TORCH_FN(wrapper_SparseCUDA__tan));
m.impl("tan.out",
TORCH_FN(wrapper_SparseCUDA_out_tan_out));
m.impl("tan_",
TORCH_FN(wrapper_SparseCUDA__tan_));
m.impl("tanh",
TORCH_FN(wrapper_SparseCUDA__tanh));
m.impl("tanh.out",
TORCH_FN(wrapper_SparseCUDA_out_tanh_out));
m.impl("tanh_",
TORCH_FN(wrapper_SparseCUDA__tanh_));
m.impl("threshold_backward",
TORCH_FN(wrapper_SparseCUDA__threshold_backward));
m.impl("threshold_backward.grad_input",
TORCH_FN(wrapper_SparseCUDA_grad_input_threshold_backward_out));
m.impl("trunc",
TORCH_FN(wrapper_SparseCUDA__trunc));
m.impl("trunc.out",
TORCH_FN(wrapper_SparseCUDA_out_trunc_out));
m.impl("trunc_",
TORCH_FN(wrapper_SparseCUDA__trunc_));
m.impl("unsqueeze",
TORCH_FN(wrapper_SparseCUDA__unsqueeze));
m.impl("zeros.out",
TORCH_FN(wrapper_SparseCUDA_out_zeros_out));
m.impl("native_norm",
TORCH_FN(wrapper_SparseCUDA__native_norm));
m.impl("native_norm.ScalarOpt_dim_dtype",
TORCH_FN(wrapper_SparseCUDA_ScalarOpt_dim_dtype_native_norm));
m.impl("_sparse_sum_backward",
TORCH_FN(wrapper_SparseCUDA___sparse_sum_backward));
m.impl("_sparse_softmax",
TORCH_FN(wrapper_SparseCUDA___sparse_softmax));
m.impl("_sparse_softmax_backward_data",
TORCH_FN(wrapper_SparseCUDA___sparse_softmax_backward_data));
m.impl("_sparse_log_softmax",
TORCH_FN(wrapper_SparseCUDA___sparse_log_softmax));
m.impl("_sparse_log_softmax_backward_data",
TORCH_FN(wrapper_SparseCUDA___sparse_log_softmax_backward_data));
m.impl("norm.ScalarOpt_dim_dtype",
TORCH_FN(wrapper_SparseCUDA_ScalarOpt_dim_dtype_norm));
m.impl("norm.ScalarOpt_dim",
TORCH_FN(wrapper_SparseCUDA_ScalarOpt_dim_norm));
m.impl("clone",
TORCH_FN(wrapper_SparseCUDA__clone));
m.impl("resize_as_sparse_",
TORCH_FN(wrapper_SparseCUDA__resize_as_sparse_));
m.impl("zero_",
TORCH_FN(wrapper_SparseCUDA__zero_));
m.impl("sub.Tensor",
TORCH_FN(wrapper_SparseCUDA_Tensor_sub));
m.impl("sub.out",
TORCH_FN(wrapper_SparseCUDA_out_sub_out));
m.impl("sub_.Tensor",
TORCH_FN(wrapper_SparseCUDA_Tensor_sub_));
m.impl("addmm",
TORCH_FN(wrapper_SparseCUDA__addmm));
m.impl("addmm.out",
TORCH_FN(wrapper_SparseCUDA_out_addmm_out));
m.impl("addmm_",
TORCH_FN(wrapper_SparseCUDA__addmm_));
m.impl("_sparse_coo_tensor_with_dims",
TORCH_FN(wrapper_SparseCUDA___sparse_coo_tensor_with_dims));
m.impl("_sparse_coo_tensor_with_dims_and_tensors",
TORCH_FN(wrapper_SparseCUDA___sparse_coo_tensor_with_dims_and_tensors));
m.impl("sparse_resize_",
TORCH_FN(wrapper_SparseCUDA__sparse_resize_));
m.impl("sparse_resize_and_clear_",
TORCH_FN(wrapper_SparseCUDA__sparse_resize_and_clear_));
m.impl("sparse_mask",
TORCH_FN(wrapper_SparseCUDA__sparse_mask));
m.impl("_sparse_mask_projection",
TORCH_FN(wrapper_SparseCUDA___sparse_mask_projection));
m.impl("_to_dense",
TORCH_FN(wrapper_SparseCUDA___to_dense));
m.impl("sparse_dim",
TORCH_FN(wrapper_SparseCUDA__sparse_dim));
m.impl("_dimI",
TORCH_FN(wrapper_SparseCUDA___dimI));
m.impl("dense_dim",
TORCH_FN(wrapper_SparseCUDA__dense_dim));
m.impl("_dimV",
TORCH_FN(wrapper_SparseCUDA___dimV));
m.impl("_nnz",
TORCH_FN(wrapper_SparseCUDA___nnz));
m.impl("_coalesce",
TORCH_FN(wrapper_SparseCUDA___coalesce));
m.impl("is_coalesced",
TORCH_FN(wrapper_SparseCUDA__is_coalesced));
m.impl("_indices",
TORCH_FN(wrapper_SparseCUDA___indices));
m.impl("_values",
TORCH_FN(wrapper_SparseCUDA___values));
m.impl("_coalesced_",
TORCH_FN(wrapper_SparseCUDA___coalesced_));
m.impl("indices",
TORCH_FN(wrapper_SparseCUDA__indices));
m.impl("values",
TORCH_FN(wrapper_SparseCUDA__values));
m.impl("hspmm",
TORCH_FN(wrapper_SparseCUDA__hspmm));
m.impl("hspmm.out",
TORCH_FN(wrapper_SparseCUDA_out_hspmm_out));
m.impl("copy_sparse_to_sparse_",
TORCH_FN(wrapper_SparseCUDA__copy_sparse_to_sparse_));
m.impl("_to_sparse.sparse_dim",
TORCH_FN(wrapper_SparseCUDA_sparse_dim__to_sparse));
m.impl("_to_sparse",
TORCH_FN(wrapper_SparseCUDA___to_sparse));
m.impl("_to_sparse_csr",
TORCH_FN(wrapper_SparseCUDA___to_sparse_csr));
m.impl("_to_sparse_csc",
TORCH_FN(wrapper_SparseCUDA___to_sparse_csc));
m.impl("_to_sparse_bsr",
TORCH_FN(wrapper_SparseCUDA___to_sparse_bsr));
m.impl("_to_sparse_bsc",
TORCH_FN(wrapper_SparseCUDA___to_sparse_bsc));
m.impl("index_select",
TORCH_FN(wrapper_SparseCUDA__index_select));
m.impl("erfinv",
TORCH_FN(wrapper_SparseCUDA__erfinv));
m.impl("erfinv.out",
TORCH_FN(wrapper_SparseCUDA_out_erfinv_out));
m.impl("erfinv_",
TORCH_FN(wrapper_SparseCUDA__erfinv_));
m.impl("sign",
TORCH_FN(wrapper_SparseCUDA__sign));
m.impl("sign.out",
TORCH_FN(wrapper_SparseCUDA_out_sign_out));
m.impl("sign_",
TORCH_FN(wrapper_SparseCUDA__sign_));
m.impl("signbit",
TORCH_FN(wrapper_SparseCUDA__signbit));
m.impl("signbit.out",
TORCH_FN(wrapper_SparseCUDA_out_signbit_out));
m.impl("any",
TORCH_FN(wrapper_SparseCUDA__any));
m.impl("pow.Tensor_Scalar",
TORCH_FN(wrapper_SparseCUDA_Tensor_Scalar_pow));
m.impl("pow.Tensor_Scalar_out",
TORCH_FN(wrapper_SparseCUDA_Tensor_Scalar_out_pow_out));
m.impl("isinf",
TORCH_FN(wrapper_SparseCUDA__isinf));
m.impl("isposinf",
TORCH_FN(wrapper_SparseCUDA__isposinf));
m.impl("isposinf.out",
TORCH_FN(wrapper_SparseCUDA_out_isposinf_out));
m.impl("isneginf",
TORCH_FN(wrapper_SparseCUDA__isneginf));
m.impl("isneginf.out",
TORCH_FN(wrapper_SparseCUDA_out_isneginf_out));
};
} // anonymous namespace
namespace sparsecuda {
at::Tensor abs(const at::Tensor & self) {
return wrapper_SparseCUDA__abs(self);
}
at::Tensor & abs_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_abs_out(self, out);
}
at::Tensor & abs_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_abs_out(self, out);
}
at::Tensor & abs_(at::Tensor & self) {
return wrapper_SparseCUDA__abs_(self);
}
at::Tensor sgn(const at::Tensor & self) {
return wrapper_SparseCUDA__sgn(self);
}
at::Tensor & sgn_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_sgn_out(self, out);
}
at::Tensor & sgn_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_sgn_out(self, out);
}
at::Tensor & sgn_(at::Tensor & self) {
return wrapper_SparseCUDA__sgn_(self);
}
at::Tensor & conj_physical_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_conj_physical_out(self, out);
}
at::Tensor & conj_physical_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_conj_physical_out(self, out);
}
at::Tensor add(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_SparseCUDA_Tensor_add(self, other, alpha);
}
at::Tensor & add_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_SparseCUDA_out_add_out(self, other, alpha, out);
}
at::Tensor & add_outf(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
return wrapper_SparseCUDA_out_add_out(self, other, alpha, out);
}
at::Tensor & add_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_SparseCUDA_Tensor_add_(self, other, alpha);
}
at::Tensor asinh(const at::Tensor & self) {
return wrapper_SparseCUDA__asinh(self);
}
at::Tensor & asinh_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_asinh_out(self, out);
}
at::Tensor & asinh_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_asinh_out(self, out);
}
at::Tensor & asinh_(at::Tensor & self) {
return wrapper_SparseCUDA__asinh_(self);
}
at::Tensor atanh(const at::Tensor & self) {
return wrapper_SparseCUDA__atanh(self);
}
at::Tensor & atanh_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_atanh_out(self, out);
}
at::Tensor & atanh_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_atanh_out(self, out);
}
at::Tensor & atanh_(at::Tensor & self) {
return wrapper_SparseCUDA__atanh_(self);
}
at::Tensor asin(const at::Tensor & self) {
return wrapper_SparseCUDA__asin(self);
}
at::Tensor & asin_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_asin_out(self, out);
}
at::Tensor & asin_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_asin_out(self, out);
}
at::Tensor & asin_(at::Tensor & self) {
return wrapper_SparseCUDA__asin_(self);
}
at::Tensor atan(const at::Tensor & self) {
return wrapper_SparseCUDA__atan(self);
}
at::Tensor & atan_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_atan_out(self, out);
}
at::Tensor & atan_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_atan_out(self, out);
}
at::Tensor & atan_(at::Tensor & self) {
return wrapper_SparseCUDA__atan_(self);
}
at::Tensor bmm(const at::Tensor & self, const at::Tensor & mat2) {
return wrapper_SparseCUDA__bmm(self, mat2);
}
at::Tensor & bmm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat2) {
return wrapper_SparseCUDA_out_bmm_out(self, mat2, out);
}
at::Tensor & bmm_outf(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
return wrapper_SparseCUDA_out_bmm_out(self, mat2, out);
}
at::Tensor _sparse_broadcast_to(const at::Tensor & self, at::IntArrayRef size) {
return wrapper_SparseCUDA___sparse_broadcast_to(self, size);
}
at::Tensor cat(const at::ITensorListRef & tensors, int64_t dim) {
return wrapper_SparseCUDA__cat(tensors, dim);
}
at::Tensor ceil(const at::Tensor & self) {
return wrapper_SparseCUDA__ceil(self);
}
at::Tensor & ceil_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_ceil_out(self, out);
}
at::Tensor & ceil_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_ceil_out(self, out);
}
at::Tensor & ceil_(at::Tensor & self) {
return wrapper_SparseCUDA__ceil_(self);
}
at::Tensor & copy_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
return wrapper_SparseCUDA__copy_(self, src, non_blocking);
}
at::Tensor div(const at::Tensor & self, const at::Tensor & other) {
return wrapper_SparseCUDA_Tensor_div(self, other);
}
at::Tensor & div_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_SparseCUDA_out_div_out(self, other, out);
}
at::Tensor & div_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_SparseCUDA_out_div_out(self, other, out);
}
at::Tensor & div_(at::Tensor & self, const at::Tensor & other) {
return wrapper_SparseCUDA_Tensor_div_(self, other);
}
at::Tensor div(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_SparseCUDA_Tensor_mode_div(self, other, rounding_mode);
}
at::Tensor & div_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_SparseCUDA_out_mode_div_out(self, other, rounding_mode, out);
}
at::Tensor & div_outf(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
return wrapper_SparseCUDA_out_mode_div_out(self, other, rounding_mode, out);
}
at::Tensor & div_(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_SparseCUDA_Tensor_mode_div_(self, other, rounding_mode);
}
at::Tensor empty(at::IntArrayRef size, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_SparseCUDA_memory_format_empty(c10::fromIntArrayRefSlow(size), c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor empty(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_SparseCUDA_memory_format_empty(c10::fromIntArrayRefSlow(size), dtype, layout, device, pin_memory, memory_format);
}
at::Tensor empty_symint(c10::SymIntArrayRef size, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_SparseCUDA_memory_format_empty(size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor empty_symint(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_SparseCUDA_memory_format_empty(size, dtype, layout, device, pin_memory, memory_format);
}
at::Tensor empty_like(const at::Tensor & self, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_SparseCUDA__empty_like(self, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor empty_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_SparseCUDA__empty_like(self, dtype, layout, device, pin_memory, memory_format);
}
at::Tensor erf(const at::Tensor & self) {
return wrapper_SparseCUDA__erf(self);
}
at::Tensor & erf_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_erf_out(self, out);
}
at::Tensor & erf_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_erf_out(self, out);
}
at::Tensor & erf_(at::Tensor & self) {
return wrapper_SparseCUDA__erf_(self);
}
at::Tensor expm1(const at::Tensor & self) {
return wrapper_SparseCUDA__expm1(self);
}
at::Tensor & expm1_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_expm1_out(self, out);
}
at::Tensor & expm1_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_expm1_out(self, out);
}
at::Tensor & expm1_(at::Tensor & self) {
return wrapper_SparseCUDA__expm1_(self);
}
at::Tensor floor(const at::Tensor & self) {
return wrapper_SparseCUDA__floor(self);
}
at::Tensor & floor_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_floor_out(self, out);
}
at::Tensor & floor_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_floor_out(self, out);
}
at::Tensor & floor_(at::Tensor & self) {
return wrapper_SparseCUDA__floor_(self);
}
at::Tensor floor_divide(const at::Tensor & self, const at::Tensor & other) {
return wrapper_SparseCUDA__floor_divide(self, other);
}
at::Tensor & floor_divide_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_SparseCUDA_out_floor_divide_out(self, other, out);
}
at::Tensor & floor_divide_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_SparseCUDA_out_floor_divide_out(self, other, out);
}
at::Tensor & floor_divide_(at::Tensor & self, const at::Tensor & other) {
return wrapper_SparseCUDA_Tensor_floor_divide_(self, other);
}
at::Tensor frac(const at::Tensor & self) {
return wrapper_SparseCUDA__frac(self);
}
at::Tensor & frac_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_frac_out(self, out);
}
at::Tensor & frac_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_frac_out(self, out);
}
at::Tensor & frac_(at::Tensor & self) {
return wrapper_SparseCUDA__frac_(self);
}
at::Tensor isnan(const at::Tensor & self) {
return wrapper_SparseCUDA__isnan(self);
}
at::Tensor nan_to_num(const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
return wrapper_SparseCUDA__nan_to_num(self, nan, posinf, neginf);
}
at::Tensor & nan_to_num_out(at::Tensor & out, const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
return wrapper_SparseCUDA_out_nan_to_num_out(self, nan, posinf, neginf, out);
}
at::Tensor & nan_to_num_outf(const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf, at::Tensor & out) {
return wrapper_SparseCUDA_out_nan_to_num_out(self, nan, posinf, neginf, out);
}
at::Tensor & nan_to_num_(at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
return wrapper_SparseCUDA__nan_to_num_(self, nan, posinf, neginf);
}
at::Tensor log1p(const at::Tensor & self) {
return wrapper_SparseCUDA__log1p(self);
}
at::Tensor & log1p_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_log1p_out(self, out);
}
at::Tensor & log1p_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_log1p_out(self, out);
}
at::Tensor & log1p_(at::Tensor & self) {
return wrapper_SparseCUDA__log1p_(self);
}
at::Tensor mm(const at::Tensor & self, const at::Tensor & mat2) {
return wrapper_SparseCUDA__mm(self, mat2);
}
at::Tensor & mm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat2) {
return wrapper_SparseCUDA_out_mm_out(self, mat2, out);
}
at::Tensor & mm_outf(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
return wrapper_SparseCUDA_out_mm_out(self, mat2, out);
}
at::Tensor _sparse_sparse_matmul(const at::Tensor & self, const at::Tensor & other) {
return wrapper_SparseCUDA___sparse_sparse_matmul(self, other);
}
at::Tensor mul(const at::Tensor & self, const at::Tensor & other) {
return wrapper_SparseCUDA_Tensor_mul(self, other);
}
at::Tensor & mul_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_SparseCUDA_out_mul_out(self, other, out);
}
at::Tensor & mul_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_SparseCUDA_out_mul_out(self, other, out);
}
at::Tensor & mul_(at::Tensor & self, const at::Tensor & other) {
return wrapper_SparseCUDA_Tensor_mul_(self, other);
}
at::Tensor mv(const at::Tensor & self, const at::Tensor & vec) {
return wrapper_SparseCUDA__mv(self, vec);
}
at::Tensor narrow_copy(const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
return wrapper_SparseCUDA__narrow_copy(self, dim, start, length);
}
at::Tensor narrow_copy_symint(const at::Tensor & self, int64_t dim, c10::SymInt start, c10::SymInt length) {
return wrapper_SparseCUDA__narrow_copy(self, dim, start, length);
}
at::Tensor permute(const at::Tensor & self, at::IntArrayRef dims) {
return wrapper_SparseCUDA__permute(self, dims);
}
at::Tensor rad2deg(const at::Tensor & self) {
return wrapper_SparseCUDA__rad2deg(self);
}
at::Tensor & rad2deg_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_rad2deg_out(self, out);
}
at::Tensor & rad2deg_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_rad2deg_out(self, out);
}
at::Tensor & rad2deg_(at::Tensor & self) {
return wrapper_SparseCUDA__rad2deg_(self);
}
at::Tensor deg2rad(const at::Tensor & self) {
return wrapper_SparseCUDA__deg2rad(self);
}
at::Tensor & deg2rad_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_deg2rad_out(self, out);
}
at::Tensor & deg2rad_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_deg2rad_out(self, out);
}
at::Tensor & deg2rad_(at::Tensor & self) {
return wrapper_SparseCUDA__deg2rad_(self);
}
at::Tensor neg(const at::Tensor & self) {
return wrapper_SparseCUDA__neg(self);
}
at::Tensor & neg_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_neg_out(self, out);
}
at::Tensor & neg_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_neg_out(self, out);
}
at::Tensor & neg_(at::Tensor & self) {
return wrapper_SparseCUDA__neg_(self);
}
at::Tensor round(const at::Tensor & self) {
return wrapper_SparseCUDA__round(self);
}
at::Tensor & round_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_round_out(self, out);
}
at::Tensor & round_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_round_out(self, out);
}
at::Tensor & round_(at::Tensor & self) {
return wrapper_SparseCUDA__round_(self);
}
at::Tensor relu(const at::Tensor & self) {
return wrapper_SparseCUDA__relu(self);
}
at::Tensor & relu_(at::Tensor & self) {
return wrapper_SparseCUDA__relu_(self);
}
at::Tensor sin(const at::Tensor & self) {
return wrapper_SparseCUDA__sin(self);
}
at::Tensor & sin_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_sin_out(self, out);
}
at::Tensor & sin_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_sin_out(self, out);
}
at::Tensor & sin_(at::Tensor & self) {
return wrapper_SparseCUDA__sin_(self);
}
at::Tensor sinh(const at::Tensor & self) {
return wrapper_SparseCUDA__sinh(self);
}
at::Tensor & sinh_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_sinh_out(self, out);
}
at::Tensor & sinh_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_sinh_out(self, out);
}
at::Tensor & sinh_(at::Tensor & self) {
return wrapper_SparseCUDA__sinh_(self);
}
at::Tensor & sspaddmm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper_SparseCUDA_out_sspaddmm_out(self, mat1, mat2, beta, alpha, out);
}
at::Tensor & sspaddmm_outf(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
return wrapper_SparseCUDA_out_sspaddmm_out(self, mat1, mat2, beta, alpha, out);
}
at::Tensor sum(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
return wrapper_SparseCUDA__sum(self, dtype);
}
at::Tensor sum(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_SparseCUDA_dim_IntList_sum(self, dim, keepdim, dtype);
}
at::Tensor sqrt(const at::Tensor & self) {
return wrapper_SparseCUDA__sqrt(self);
}
at::Tensor & sqrt_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_sqrt_out(self, out);
}
at::Tensor & sqrt_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_sqrt_out(self, out);
}
at::Tensor & sqrt_(at::Tensor & self) {
return wrapper_SparseCUDA__sqrt_(self);
}
at::Tensor tan(const at::Tensor & self) {
return wrapper_SparseCUDA__tan(self);
}
at::Tensor & tan_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_tan_out(self, out);
}
at::Tensor & tan_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_tan_out(self, out);
}
at::Tensor & tan_(at::Tensor & self) {
return wrapper_SparseCUDA__tan_(self);
}
at::Tensor tanh(const at::Tensor & self) {
return wrapper_SparseCUDA__tanh(self);
}
at::Tensor & tanh_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_tanh_out(self, out);
}
at::Tensor & tanh_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_tanh_out(self, out);
}
at::Tensor & tanh_(at::Tensor & self) {
return wrapper_SparseCUDA__tanh_(self);
}
at::Tensor threshold_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
return wrapper_SparseCUDA__threshold_backward(grad_output, self, threshold);
}
at::Tensor & threshold_backward_out(at::Tensor & grad_input, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
return wrapper_SparseCUDA_grad_input_threshold_backward_out(grad_output, self, threshold, grad_input);
}
at::Tensor & threshold_backward_outf(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold, at::Tensor & grad_input) {
return wrapper_SparseCUDA_grad_input_threshold_backward_out(grad_output, self, threshold, grad_input);
}
at::Tensor trunc(const at::Tensor & self) {
return wrapper_SparseCUDA__trunc(self);
}
at::Tensor & trunc_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_trunc_out(self, out);
}
at::Tensor & trunc_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_trunc_out(self, out);
}
at::Tensor & trunc_(at::Tensor & self) {
return wrapper_SparseCUDA__trunc_(self);
}
at::Tensor unsqueeze(const at::Tensor & self, int64_t dim) {
return wrapper_SparseCUDA__unsqueeze(self, dim);
}
at::Tensor & zeros_out(at::Tensor & out, at::IntArrayRef size) {
return wrapper_SparseCUDA_out_zeros_out(c10::fromIntArrayRefSlow(size), out);
}
at::Tensor & zeros_outf(at::IntArrayRef size, at::Tensor & out) {
return wrapper_SparseCUDA_out_zeros_out(c10::fromIntArrayRefSlow(size), out);
}
at::Tensor & zeros_symint_out(at::Tensor & out, c10::SymIntArrayRef size) {
return wrapper_SparseCUDA_out_zeros_out(size, out);
}
at::Tensor & zeros_symint_outf(c10::SymIntArrayRef size, at::Tensor & out) {
return wrapper_SparseCUDA_out_zeros_out(size, out);
}
at::Tensor native_norm(const at::Tensor & self, const at::Scalar & p) {
return wrapper_SparseCUDA__native_norm(self, p);
}
at::Tensor native_norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_SparseCUDA_ScalarOpt_dim_dtype_native_norm(self, p, dim, keepdim, dtype);
}
at::Tensor _sparse_sum_backward(const at::Tensor & grad, const at::Tensor & self, at::IntArrayRef dim) {
return wrapper_SparseCUDA___sparse_sum_backward(grad, self, dim);
}
at::Tensor _sparse_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
return wrapper_SparseCUDA___sparse_softmax(self, dim, half_to_float);
}
at::Tensor _sparse_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
return wrapper_SparseCUDA___sparse_softmax_backward_data(grad_output, output, dim, self);
}
at::Tensor _sparse_log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
return wrapper_SparseCUDA___sparse_log_softmax(self, dim, half_to_float);
}
at::Tensor _sparse_log_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
return wrapper_SparseCUDA___sparse_log_softmax_backward_data(grad_output, output, dim, self);
}
at::Tensor norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype) {
return wrapper_SparseCUDA_ScalarOpt_dim_dtype_norm(self, p, dim, keepdim, dtype);
}
at::Tensor norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim) {
return wrapper_SparseCUDA_ScalarOpt_dim_norm(self, p, dim, keepdim);
}
at::Tensor clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_SparseCUDA__clone(self, memory_format);
}
const at::Tensor & resize_as_sparse_(const at::Tensor & self, const at::Tensor & the_template) {
return wrapper_SparseCUDA__resize_as_sparse_(self, the_template);
}
at::Tensor & zero_(at::Tensor & self) {
return wrapper_SparseCUDA__zero_(self);
}
at::Tensor sub(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_SparseCUDA_Tensor_sub(self, other, alpha);
}
at::Tensor & sub_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_SparseCUDA_out_sub_out(self, other, alpha, out);
}
at::Tensor & sub_outf(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
return wrapper_SparseCUDA_out_sub_out(self, other, alpha, out);
}
at::Tensor & sub_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_SparseCUDA_Tensor_sub_(self, other, alpha);
}
at::Tensor addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper_SparseCUDA__addmm(self, mat1, mat2, beta, alpha);
}
at::Tensor & addmm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper_SparseCUDA_out_addmm_out(self, mat1, mat2, beta, alpha, out);
}
at::Tensor & addmm_outf(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
return wrapper_SparseCUDA_out_addmm_out(self, mat1, mat2, beta, alpha, out);
}
at::Tensor & addmm_(at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper_SparseCUDA__addmm_(self, mat1, mat2, beta, alpha);
}
at::Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, at::TensorOptions options) {
return wrapper_SparseCUDA___sparse_coo_tensor_with_dims(sparse_dim, dense_dim, size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_SparseCUDA___sparse_coo_tensor_with_dims(sparse_dim, dense_dim, size, dtype, layout, device, pin_memory);
}
at::Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, const at::Tensor & indices, const at::Tensor & values, at::TensorOptions options, c10::optional<bool> is_coalesced) {
return wrapper_SparseCUDA___sparse_coo_tensor_with_dims_and_tensors(sparse_dim, dense_dim, c10::fromIntArrayRefSlow(size), indices, values, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), is_coalesced);
}
at::Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<bool> is_coalesced) {
return wrapper_SparseCUDA___sparse_coo_tensor_with_dims_and_tensors(sparse_dim, dense_dim, c10::fromIntArrayRefSlow(size), indices, values, dtype, layout, device, pin_memory, is_coalesced);
}
at::Tensor _sparse_coo_tensor_with_dims_and_tensors_symint(int64_t sparse_dim, int64_t dense_dim, c10::SymIntArrayRef size, const at::Tensor & indices, const at::Tensor & values, at::TensorOptions options, c10::optional<bool> is_coalesced) {
return wrapper_SparseCUDA___sparse_coo_tensor_with_dims_and_tensors(sparse_dim, dense_dim, size, indices, values, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), is_coalesced);
}
at::Tensor _sparse_coo_tensor_with_dims_and_tensors_symint(int64_t sparse_dim, int64_t dense_dim, c10::SymIntArrayRef size, const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<bool> is_coalesced) {
return wrapper_SparseCUDA___sparse_coo_tensor_with_dims_and_tensors(sparse_dim, dense_dim, size, indices, values, dtype, layout, device, pin_memory, is_coalesced);
}
const at::Tensor & sparse_resize_(const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
return wrapper_SparseCUDA__sparse_resize_(self, size, sparse_dim, dense_dim);
}
const at::Tensor & sparse_resize_and_clear_(const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
return wrapper_SparseCUDA__sparse_resize_and_clear_(self, size, sparse_dim, dense_dim);
}
at::Tensor sparse_mask(const at::Tensor & self, const at::Tensor & mask) {
return wrapper_SparseCUDA__sparse_mask(self, mask);
}
at::Tensor _sparse_mask_projection(const at::Tensor & self, const at::Tensor & mask, bool accumulate_matches) {
return wrapper_SparseCUDA___sparse_mask_projection(self, mask, accumulate_matches);
}
at::Tensor _to_dense(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<bool> masked_grad) {
return wrapper_SparseCUDA___to_dense(self, dtype, masked_grad);
}
int64_t sparse_dim(const at::Tensor & self) {
return wrapper_SparseCUDA__sparse_dim(self);
}
int64_t _dimI(const at::Tensor & self) {
return wrapper_SparseCUDA___dimI(self);
}
int64_t dense_dim(const at::Tensor & self) {
return wrapper_SparseCUDA__dense_dim(self);
}
int64_t _dimV(const at::Tensor & self) {
return wrapper_SparseCUDA___dimV(self);
}
int64_t _nnz(const at::Tensor & self) {
return wrapper_SparseCUDA___nnz(self);
}
at::Tensor _coalesce(const at::Tensor & self) {
return wrapper_SparseCUDA___coalesce(self);
}
bool is_coalesced(const at::Tensor & self) {
return wrapper_SparseCUDA__is_coalesced(self);
}
at::Tensor _indices(const at::Tensor & self) {
return wrapper_SparseCUDA___indices(self);
}
at::Tensor _values(const at::Tensor & self) {
return wrapper_SparseCUDA___values(self);
}
at::Tensor & _coalesced_(at::Tensor & self, bool coalesced) {
return wrapper_SparseCUDA___coalesced_(self, coalesced);
}
at::Tensor indices(const at::Tensor & self) {
return wrapper_SparseCUDA__indices(self);
}
at::Tensor values(const at::Tensor & self) {
return wrapper_SparseCUDA__values(self);
}
at::Tensor hspmm(const at::Tensor & mat1, const at::Tensor & mat2) {
return wrapper_SparseCUDA__hspmm(mat1, mat2);
}
at::Tensor & hspmm_out(at::Tensor & out, const at::Tensor & mat1, const at::Tensor & mat2) {
return wrapper_SparseCUDA_out_hspmm_out(mat1, mat2, out);
}
at::Tensor & hspmm_outf(const at::Tensor & mat1, const at::Tensor & mat2, at::Tensor & out) {
return wrapper_SparseCUDA_out_hspmm_out(mat1, mat2, out);
}
at::Tensor & copy_sparse_to_sparse_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
return wrapper_SparseCUDA__copy_sparse_to_sparse_(self, src, non_blocking);
}
at::Tensor _to_sparse(const at::Tensor & self, int64_t sparse_dim) {
return wrapper_SparseCUDA_sparse_dim__to_sparse(self, sparse_dim);
}
at::Tensor _to_sparse(const at::Tensor & self, c10::optional<at::Layout> layout, at::OptionalIntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
return wrapper_SparseCUDA___to_sparse(self, layout, blocksize, dense_dim);
}
at::Tensor _to_sparse_csr(const at::Tensor & self, c10::optional<int64_t> dense_dim) {
return wrapper_SparseCUDA___to_sparse_csr(self, dense_dim);
}
at::Tensor _to_sparse_csc(const at::Tensor & self, c10::optional<int64_t> dense_dim) {
return wrapper_SparseCUDA___to_sparse_csc(self, dense_dim);
}
at::Tensor _to_sparse_bsr(const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
return wrapper_SparseCUDA___to_sparse_bsr(self, blocksize, dense_dim);
}
at::Tensor _to_sparse_bsc(const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
return wrapper_SparseCUDA___to_sparse_bsc(self, blocksize, dense_dim);
}
at::Tensor index_select(const at::Tensor & self, int64_t dim, const at::Tensor & index) {
return wrapper_SparseCUDA__index_select(self, dim, index);
}
at::Tensor erfinv(const at::Tensor & self) {
return wrapper_SparseCUDA__erfinv(self);
}
at::Tensor & erfinv_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_erfinv_out(self, out);
}
at::Tensor & erfinv_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_erfinv_out(self, out);
}
at::Tensor & erfinv_(at::Tensor & self) {
return wrapper_SparseCUDA__erfinv_(self);
}
at::Tensor sign(const at::Tensor & self) {
return wrapper_SparseCUDA__sign(self);
}
at::Tensor & sign_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_sign_out(self, out);
}
at::Tensor & sign_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_sign_out(self, out);
}
at::Tensor & sign_(at::Tensor & self) {
return wrapper_SparseCUDA__sign_(self);
}
at::Tensor signbit(const at::Tensor & self) {
return wrapper_SparseCUDA__signbit(self);
}
at::Tensor & signbit_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_signbit_out(self, out);
}
at::Tensor & signbit_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_signbit_out(self, out);
}
at::Tensor any(const at::Tensor & self) {
return wrapper_SparseCUDA__any(self);
}
at::Tensor pow(const at::Tensor & self, const at::Scalar & exponent) {
return wrapper_SparseCUDA_Tensor_Scalar_pow(self, exponent);
}
at::Tensor & pow_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & exponent) {
return wrapper_SparseCUDA_Tensor_Scalar_out_pow_out(self, exponent, out);
}
at::Tensor & pow_outf(const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
return wrapper_SparseCUDA_Tensor_Scalar_out_pow_out(self, exponent, out);
}
at::Tensor isinf(const at::Tensor & self) {
return wrapper_SparseCUDA__isinf(self);
}
at::Tensor isposinf(const at::Tensor & self) {
return wrapper_SparseCUDA__isposinf(self);
}
at::Tensor & isposinf_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_isposinf_out(self, out);
}
at::Tensor & isposinf_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_isposinf_out(self, out);
}
at::Tensor isneginf(const at::Tensor & self) {
return wrapper_SparseCUDA__isneginf(self);
}
at::Tensor & isneginf_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_SparseCUDA_out_isneginf_out(self, out);
}
at::Tensor & isneginf_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_SparseCUDA_out_isneginf_out(self, out);
}
} // namespace sparsecuda
} // namespace at
