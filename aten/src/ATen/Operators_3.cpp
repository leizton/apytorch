#include <ATen/Tensor.h>
#include <ATen/core/dispatch/Dispatcher.h>

// @generated by torchgen/gen.py from Operators.cpp
// NOTE See [Sharded File] comment in VariableType

#ifndef AT_PER_OPERATOR_HEADERS
#include <ATen/Operators.h>
#else
#include <ATen/ops/_cast_Int.h>
#include <ATen/ops/_cast_Long.h>
#include <ATen/ops/_backward.h>
#include <ATen/ops/data.h>
#include <ATen/ops/retain_grad.h>
#include <ATen/ops/rename.h>
#include <ATen/ops/rename.h>
#include <ATen/ops/sym_constrain_range.h>
#include <ATen/ops/_functional_sym_constrain_range.h>
#include <ATen/ops/_cudnn_rnn_backward.h>
#include <ATen/ops/native_dropout_backward.h>
#include <ATen/ops/feature_dropout.h>
#include <ATen/ops/feature_dropout.h>
#include <ATen/ops/conj.h>
#include <ATen/ops/_add_relu.h>
#include <ATen/ops/_add_relu.h>
#include <ATen/ops/_add_relu.h>
#include <ATen/ops/_add_relu.h>
#include <ATen/ops/_add_relu.h>
#include <ATen/ops/affine_grid_generator.h>
#include <ATen/ops/_is_any_true.h>
#include <ATen/ops/_test_functorch_fallback.h>
#include <ATen/ops/arange.h>
#include <ATen/ops/arange.h>
#include <ATen/ops/arange.h>
#include <ATen/ops/arange.h>
#include <ATen/ops/arange.h>
#include <ATen/ops/_dim_arange.h>
#include <ATen/ops/arcsinh.h>
#include <ATen/ops/arcsinh.h>
#include <ATen/ops/arcsinh.h>
#include <ATen/ops/atanh.h>
#include <ATen/ops/atanh.h>
#include <ATen/ops/atanh.h>
#include <ATen/ops/arcsin.h>
#include <ATen/ops/arcsin.h>
#include <ATen/ops/arcsin.h>
#include <ATen/ops/bartlett_window.h>
#include <ATen/ops/bartlett_window.h>
#include <ATen/ops/binary_cross_entropy.h>
#include <ATen/ops/binary_cross_entropy.h>
#include <ATen/ops/bmm.h>
#include <ATen/ops/bmm.h>
#include <ATen/ops/_sparse_broadcast_to.h>
#include <ATen/ops/concat.h>
#include <ATen/ops/concat.h>
#include <ATen/ops/concat.h>
#include <ATen/ops/concat.h>
#include <ATen/ops/chain_matmul.h>
#include <ATen/ops/chain_matmul.h>
#include <ATen/ops/clamp_min.h>
#include <ATen/ops/clamp_min.h>
#include <ATen/ops/clamp_min.h>
#include <ATen/ops/clamp_min.h>
#include <ATen/ops/clamp_min.h>
#include <ATen/ops/clamp_min.h>
#include <ATen/ops/_convolution_mode.h>
#include <ATen/ops/conv1d.h>
#include <ATen/ops/conv3d.h>
#include <ATen/ops/conv1d.h>
#include <ATen/ops/conv3d.h>
#include <ATen/ops/conv_tbc_backward.h>
#include <ATen/ops/conv_transpose3d.h>
#include <ATen/ops/copy.h>
#include <ATen/ops/copy.h>
#include <ATen/ops/_copy_from_and_resize.h>
#include <ATen/ops/cudnn_convolution.h>
#include <ATen/ops/cudnn_convolution.h>
#include <ATen/ops/cudnn_convolution_relu.h>
#include <ATen/ops/cumprod.h>
#include <ATen/ops/cumprod.h>
#include <ATen/ops/cumprod.h>
#include <ATen/ops/cumprod.h>
#include <ATen/ops/cumprod.h>
#include <ATen/ops/cumprod.h>
#include <ATen/ops/cumulative_trapezoid.h>
#include <ATen/ops/cumulative_trapezoid.h>
#include <ATen/ops/ctc_loss.h>
#include <ATen/ops/ctc_loss.h>
#include <ATen/ops/diag_embed.h>
#include <ATen/ops/diagonal.h>
#include <ATen/ops/diagonal.h>
#include <ATen/ops/divide.h>
#include <ATen/ops/divide.h>
#include <ATen/ops/divide.h>
#include <ATen/ops/divide.h>
#include <ATen/ops/divide.h>
#include <ATen/ops/divide.h>
#include <ATen/ops/divide.h>
#include <ATen/ops/divide.h>
#include <ATen/ops/divide.h>
#include <ATen/ops/divide.h>
#include <ATen/ops/empty_permuted.h>
#include <ATen/ops/_empty_affine_quantized.h>
#include <ATen/ops/_resize_output.h>
#include <ATen/ops/empty_like.h>
#include <ATen/ops/expand.h>
#include <ATen/ops/flatten.h>
#include <ATen/ops/flatten.h>
#include <ATen/ops/flatten.h>
#include <ATen/ops/flatten.h>
#include <ATen/ops/floor.h>
#include <ATen/ops/floor.h>
#include <ATen/ops/floor.h>
#include <ATen/ops/grid_sampler_3d_backward.h>
#include <ATen/ops/hinge_embedding_loss.h>
#include <ATen/ops/native_group_norm.h>
#include <ATen/ops/_fft_r2c.h>
#include <ATen/ops/_fft_r2c.h>
#include <ATen/ops/_unsafe_index.h>
#include <ATen/ops/is_neg.h>
#include <ATen/ops/isreal.h>
#include <ATen/ops/linear_backward.h>
#include <ATen/ops/mkldnn_linear_backward_input.h>
#include <ATen/ops/mkldnn_linear_backward.h>
#include <ATen/ops/_sparse_semi_structured_linear.h>
#include <ATen/ops/_logcumsumexp.h>
#include <ATen/ops/_logcumsumexp.h>
#include <ATen/ops/value_selecting_reduction_backward.h>
#include <ATen/ops/max_pool1d.h>
#include <ATen/ops/max_pool2d.h>
#include <ATen/ops/mean.h>
#include <ATen/ops/mean.h>
#include <ATen/ops/mean.h>
#include <ATen/ops/mean.h>
#include <ATen/ops/mean.h>
#include <ATen/ops/nanmean.h>
#include <ATen/ops/nanmean.h>
#include <ATen/ops/min.h>
#include <ATen/ops/min.h>
#include <ATen/ops/min.h>
#include <ATen/ops/min.h>
#include <ATen/ops/mm.h>
#include <ATen/ops/mm.h>
#include <ATen/ops/_weight_int4pack_mm.h>
#include <ATen/ops/mv.h>
#include <ATen/ops/mv.h>
#include <ATen/ops/narrow_copy.h>
#include <ATen/ops/narrow_copy.h>
#include <ATen/ops/_native_batch_norm_legit_no_training.h>
#include <ATen/ops/batch_norm_gather_stats_with_counts.h>
#include <ATen/ops/pairwise_distance.h>
#include <ATen/ops/_pdist_backward.h>
#include <ATen/ops/permute.h>
#include <ATen/ops/matrix_H.h>
#include <ATen/ops/pixel_shuffle.h>
#include <ATen/ops/pinverse.h>
#include <ATen/ops/reshape.h>
#include <ATen/ops/_reshape_alias.h>
#include <ATen/ops/select.h>
#include <ATen/ops/select.h>
#include <ATen/ops/celu.h>
#include <ATen/ops/celu.h>
#include <ATen/ops/silu.h>
#include <ATen/ops/silu.h>
#include <ATen/ops/silu.h>
#include <ATen/ops/mish_backward.h>
#include <ATen/ops/logit.h>
#include <ATen/ops/logit.h>
#include <ATen/ops/logit.h>
#include <ATen/ops/sinh.h>
#include <ATen/ops/sinh.h>
#include <ATen/ops/sinh.h>
#include <ATen/ops/slice_backward.h>
#include <ATen/ops/softmax.h>
#include <ATen/ops/softmax.h>
#include <ATen/ops/softmax.h>
#include <ATen/ops/_softmax.h>
#include <ATen/ops/_softmax.h>
#include <ATen/ops/unsafe_split.h>
#include <ATen/ops/dsplit.h>
#include <ATen/ops/dsplit.h>
#include <ATen/ops/vstack.h>
#include <ATen/ops/vstack.h>
#include <ATen/ops/stft.h>
#include <ATen/ops/stft.h>
#include <ATen/ops/sym_stride.h>
#include <ATen/ops/_nested_sum_backward.h>
#include <ATen/ops/sum_to_size.h>
#include <ATen/ops/sqrt.h>
#include <ATen/ops/sqrt.h>
#include <ATen/ops/sqrt.h>
#include <ATen/ops/std.h>
#include <ATen/ops/std.h>
#include <ATen/ops/std.h>
#include <ATen/ops/std_mean.h>
#include <ATen/ops/std_mean.h>
#include <ATen/ops/std_mean.h>
#include <ATen/ops/std_mean.h>
#include <ATen/ops/std_mean.h>
#include <ATen/ops/std.h>
#include <ATen/ops/std.h>
#include <ATen/ops/std.h>
#include <ATen/ops/std.h>
#include <ATen/ops/std.h>
#include <ATen/ops/std.h>
#include <ATen/ops/t.h>
#include <ATen/ops/t.h>
#include <ATen/ops/threshold.h>
#include <ATen/ops/threshold.h>
#include <ATen/ops/threshold.h>
#include <ATen/ops/transpose.h>
#include <ATen/ops/transpose.h>
#include <ATen/ops/transpose.h>
#include <ATen/ops/flip.h>
#include <ATen/ops/roll.h>
#include <ATen/ops/_nested_from_padded.h>
#include <ATen/ops/_nested_view_from_buffer.h>
#include <ATen/ops/_trilinear.h>
#include <ATen/ops/type_as.h>
#include <ATen/ops/_has_compatible_shallow_copy_type.h>
#include <ATen/ops/_unique2.h>
#include <ATen/ops/_weight_norm_interface_backward.h>
#include <ATen/ops/zeros_like.h>
#include <ATen/ops/_sparse_csr_prod.h>
#include <ATen/ops/_sparse_softmax_backward_data.h>
#include <ATen/ops/_sparse_log_softmax.h>
#include <ATen/ops/_sparse_log_softmax.h>
#include <ATen/ops/_sparse_log_softmax.h>
#include <ATen/ops/_sparse_log_softmax_backward_data.h>
#include <ATen/ops/_spdiags.h>
#include <ATen/ops/frexp.h>
#include <ATen/ops/frexp.h>
#include <ATen/ops/zero.h>
#include <ATen/ops/rsub.h>
#include <ATen/ops/rsub.h>
#include <ATen/ops/_sparse_mm_reduce_impl.h>
#include <ATen/ops/_scaled_mm.h>
#include <ATen/ops/_scaled_mm.h>
#include <ATen/ops/_sparse_bsr_tensor_unsafe.h>
#include <ATen/ops/_validate_sparse_csc_tensor_args.h>
#include <ATen/ops/_sparse_coo_tensor_with_dims.h>
#include <ATen/ops/to_dense_backward.h>
#include <ATen/ops/_coalesce.h>
#include <ATen/ops/_values.h>
#include <ATen/ops/crow_indices.h>
#include <ATen/ops/_to_sparse.h>
#include <ATen/ops/_to_sparse.h>
#include <ATen/ops/q_zero_point.h>
#include <ATen/ops/q_per_channel_scales.h>
#include <ATen/ops/_fake_quantize_learnable_per_tensor_affine_backward.h>
#include <ATen/ops/_fake_quantize_learnable_per_channel_affine_backward.h>
#include <ATen/ops/fused_moving_avg_obs_fake_quant.h>
#include <ATen/ops/_choose_qparams_per_tensor.h>
#include <ATen/ops/meshgrid.h>
#include <ATen/ops/meshgrid.h>
#include <ATen/ops/can_cast.h>
#include <ATen/ops/lstm_mps_backward.h>
#include <ATen/ops/_thnn_fused_lstm_cell_backward_impl.h>
#include <ATen/ops/_thnn_fused_gru_cell.h>
#include <ATen/ops/quantized_rnn_tanh_cell.h>
#include <ATen/ops/_pack_padded_sequence.h>
#include <ATen/ops/is_set_to.h>
#include <ATen/ops/_masked_softmax.h>
#include <ATen/ops/view.h>
#include <ATen/ops/view.h>
#include <ATen/ops/xor.h>
#include <ATen/ops/xor.h>
#include <ATen/ops/xor.h>
#include <ATen/ops/xor.h>
#include <ATen/ops/triu.h>
#include <ATen/ops/lerp.h>
#include <ATen/ops/lerp.h>
#include <ATen/ops/addbmm.h>
#include <ATen/ops/addbmm.h>
#include <ATen/ops/addbmm.h>
#include <ATen/ops/triu.h>
#include <ATen/ops/triu.h>
#include <ATen/ops/not_equal.h>
#include <ATen/ops/not_equal.h>
#include <ATen/ops/not_equal.h>
#include <ATen/ops/not_equal.h>
#include <ATen/ops/not_equal.h>
#include <ATen/ops/not_equal.h>
#include <ATen/ops/greater.h>
#include <ATen/ops/greater.h>
#include <ATen/ops/greater.h>
#include <ATen/ops/greater.h>
#include <ATen/ops/greater.h>
#include <ATen/ops/greater.h>
#include <ATen/ops/gather.h>
#include <ATen/ops/gather.h>
#include <ATen/ops/gather_backward.h>
#include <ATen/ops/gather.h>
#include <ATen/ops/gather.h>
#include <ATen/ops/cross_entropy_loss.h>
#include <ATen/ops/triangular_solve.h>
#include <ATen/ops/triangular_solve.h>
#include <ATen/ops/_linalg_check_errors.h>
#include <ATen/ops/linalg_solve_triangular.h>
#include <ATen/ops/linalg_solve_triangular.h>
#include <ATen/ops/ormqr.h>
#include <ATen/ops/ormqr.h>
#include <ATen/ops/i0.h>
#include <ATen/ops/i0.h>
#include <ATen/ops/i0.h>
#include <ATen/ops/sign.h>
#include <ATen/ops/sign.h>
#include <ATen/ops/sign.h>
#include <ATen/ops/lerp.h>
#include <ATen/ops/lerp.h>
#include <ATen/ops/lerp.h>
#include <ATen/ops/lerp.h>
#include <ATen/ops/min.h>
#include <ATen/ops/min.h>
#include <ATen/ops/fmin.h>
#include <ATen/ops/fmin.h>
#include <ATen/ops/min.h>
#include <ATen/ops/min.h>
#include <ATen/ops/equal.h>
#include <ATen/ops/_foreach_mul.h>
#include <ATen/ops/_foreach_mul.h>
#include <ATen/ops/_foreach_mul.h>
#include <ATen/ops/_foreach_mul.h>
#include <ATen/ops/_foreach_mul.h>
#include <ATen/ops/_foreach_mul.h>
#include <ATen/ops/_foreach_mul.h>
#include <ATen/ops/_foreach_mul.h>
#include <ATen/ops/_foreach_div.h>
#include <ATen/ops/_foreach_div.h>
#include <ATen/ops/_foreach_div.h>
#include <ATen/ops/_foreach_div.h>
#include <ATen/ops/_foreach_div.h>
#include <ATen/ops/_foreach_div.h>
#include <ATen/ops/_foreach_div.h>
#include <ATen/ops/_foreach_div.h>
#include <ATen/ops/_foreach_addcmul.h>
#include <ATen/ops/_foreach_addcmul.h>
#include <ATen/ops/_foreach_addcmul.h>
#include <ATen/ops/_foreach_addcmul.h>
#include <ATen/ops/_foreach_addcmul.h>
#include <ATen/ops/_foreach_addcmul.h>
#include <ATen/ops/_foreach_asin.h>
#include <ATen/ops/_foreach_asin.h>
#include <ATen/ops/_foreach_cos.h>
#include <ATen/ops/_foreach_cos.h>
#include <ATen/ops/_foreach_floor.h>
#include <ATen/ops/_foreach_floor.h>
#include <ATen/ops/_foreach_tanh.h>
#include <ATen/ops/_foreach_tanh.h>
#include <ATen/ops/_foreach_zero.h>
#include <ATen/ops/_convert_indices_from_csr_to_coo.h>
#include <ATen/ops/_convert_indices_from_csr_to_coo.h>
#include <ATen/ops/nll_loss.h>
#include <ATen/ops/nll_loss.h>
#include <ATen/ops/nll_loss_backward.h>
#include <ATen/ops/nll_loss_backward.h>
#include <ATen/ops/smooth_l1_loss_backward.h>
#include <ATen/ops/smooth_l1_loss_backward.h>
#include <ATen/ops/huber_loss.h>
#include <ATen/ops/huber_loss.h>
#include <ATen/ops/huber_loss_backward.h>
#include <ATen/ops/huber_loss_backward.h>
#include <ATen/ops/hardsigmoid.h>
#include <ATen/ops/hardsigmoid.h>
#include <ATen/ops/hardsigmoid.h>
#include <ATen/ops/log_sigmoid.h>
#include <ATen/ops/log_sigmoid.h>
#include <ATen/ops/adaptive_avg_pool2d.h>
#include <ATen/ops/adaptive_avg_pool2d.h>
#include <ATen/ops/adaptive_avg_pool3d.h>
#include <ATen/ops/adaptive_avg_pool3d.h>
#include <ATen/ops/_adaptive_avg_pool3d.h>
#include <ATen/ops/adaptive_max_pool2d.h>
#include <ATen/ops/adaptive_max_pool2d.h>
#include <ATen/ops/adaptive_max_pool3d.h>
#include <ATen/ops/adaptive_max_pool3d.h>
#include <ATen/ops/avg_pool2d_backward.h>
#include <ATen/ops/avg_pool2d_backward.h>
#include <ATen/ops/fractional_max_pool2d.h>
#include <ATen/ops/fractional_max_pool2d.h>
#include <ATen/ops/max_unpool2d.h>
#include <ATen/ops/max_unpool2d.h>
#include <ATen/ops/max_unpool3d.h>
#include <ATen/ops/max_unpool3d.h>
#include <ATen/ops/reflection_pad3d_backward.h>
#include <ATen/ops/reflection_pad3d_backward.h>
#include <ATen/ops/replication_pad2d_backward.h>
#include <ATen/ops/replication_pad2d_backward.h>
#include <ATen/ops/replication_pad3d.h>
#include <ATen/ops/replication_pad3d.h>
#include <ATen/ops/upsample_linear1d.h>
#include <ATen/ops/upsample_bilinear2d.h>
#include <ATen/ops/upsample_bicubic2d.h>
#include <ATen/ops/upsample_nearest2d.h>
#include <ATen/ops/upsample_linear1d.h>
#include <ATen/ops/upsample_linear1d.h>
#include <ATen/ops/upsample_bilinear2d.h>
#include <ATen/ops/upsample_bilinear2d.h>
#include <ATen/ops/upsample_bicubic2d.h>
#include <ATen/ops/upsample_bicubic2d.h>
#include <ATen/ops/upsample_bicubic2d_backward.h>
#include <ATen/ops/upsample_bicubic2d_backward.h>
#include <ATen/ops/upsample_trilinear3d_backward.h>
#include <ATen/ops/upsample_trilinear3d_backward.h>
#include <ATen/ops/upsample_nearest2d.h>
#include <ATen/ops/upsample_nearest2d.h>
#include <ATen/ops/upsample_nearest3d_backward.h>
#include <ATen/ops/_upsample_nearest_exact3d_backward.h>
#include <ATen/ops/upsample_nearest3d_backward.h>
#include <ATen/ops/_upsample_nearest_exact3d_backward.h>
#include <ATen/ops/logit_backward.h>
#include <ATen/ops/logit_backward.h>
#include <ATen/ops/slow_conv_transpose2d.h>
#include <ATen/ops/slow_conv_transpose2d.h>
#include <ATen/ops/_slow_conv2d_backward.h>
#include <ATen/ops/_slow_conv2d_backward.h>
#include <ATen/ops/conv_depthwise3d.h>
#include <ATen/ops/slow_conv_dilated2d.h>
#include <ATen/ops/col2im.h>
#include <ATen/ops/col2im.h>
#include <ATen/ops/isfinite.h>
#include <ATen/ops/record_stream.h>
#include <ATen/ops/isposinf.h>
#include <ATen/ops/isposinf.h>
#include <ATen/ops/special_expm1.h>
#include <ATen/ops/special_expm1.h>
#include <ATen/ops/special_exp2.h>
#include <ATen/ops/special_exp2.h>
#include <ATen/ops/special_gammaln.h>
#include <ATen/ops/special_gammaln.h>
#include <ATen/ops/special_erfinv.h>
#include <ATen/ops/special_erfinv.h>
#include <ATen/ops/special_xlog1py.h>
#include <ATen/ops/special_xlog1py.h>
#include <ATen/ops/special_xlog1py.h>
#include <ATen/ops/special_xlog1py.h>
#include <ATen/ops/special_xlog1py.h>
#include <ATen/ops/special_xlog1py.h>
#include <ATen/ops/special_i0.h>
#include <ATen/ops/special_i0.h>
#include <ATen/ops/special_polygamma.h>
#include <ATen/ops/special_polygamma.h>
#include <ATen/ops/special_log1p.h>
#include <ATen/ops/special_log1p.h>
#include <ATen/ops/fft_irfft.h>
#include <ATen/ops/fft_irfft.h>
#include <ATen/ops/fft_ifft2.h>
#include <ATen/ops/fft_ifft2.h>
#include <ATen/ops/fft_irfft2.h>
#include <ATen/ops/fft_irfft2.h>
#include <ATen/ops/fft_rfftn.h>
#include <ATen/ops/fft_rfftn.h>
#include <ATen/ops/linalg_cholesky.h>
#include <ATen/ops/linalg_cholesky.h>
#include <ATen/ops/_linalg_det.h>
#include <ATen/ops/_linalg_det.h>
#include <ATen/ops/linalg_ldl_factor.h>
#include <ATen/ops/linalg_ldl_factor.h>
#include <ATen/ops/linalg_matmul.h>
#include <ATen/ops/linalg_matmul.h>
#include <ATen/ops/linalg_slogdet.h>
#include <ATen/ops/linalg_slogdet.h>
#include <ATen/ops/logdet.h>
#include <ATen/ops/linalg_eigvals.h>
#include <ATen/ops/linalg_eigvals.h>
#include <ATen/ops/linalg_inv_ex.h>
#include <ATen/ops/linalg_inv_ex.h>
#include <ATen/ops/inner.h>
#include <ATen/ops/inner.h>
#include <ATen/ops/linalg_vector_norm.h>
#include <ATen/ops/linalg_vector_norm.h>
#include <ATen/ops/linalg_solve.h>
#include <ATen/ops/linalg_solve.h>
#include <ATen/ops/linalg_tensorinv.h>
#include <ATen/ops/linalg_tensorinv.h>
#include <ATen/ops/linalg_matrix_rank.h>
#include <ATen/ops/linalg_matrix_rank.h>
#include <ATen/ops/linalg_matrix_rank.h>
#include <ATen/ops/linalg_matrix_rank.h>
#include <ATen/ops/linalg_matrix_rank.h>
#include <ATen/ops/linalg_matrix_rank.h>
#include <ATen/ops/linalg_matrix_rank.h>
#include <ATen/ops/linalg_matrix_rank.h>
#include <ATen/ops/_test_optional_filled_intlist.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_view_copy.h>
#include <ATen/ops/pad_sequence.h>
#include <ATen/ops/_fw_primal_copy.h>
#include <ATen/ops/view_as_real_copy.h>
#include <ATen/ops/as_strided_copy.h>
#include <ATen/ops/_reshape_alias_copy.h>
#include <ATen/ops/split_copy.h>
#include <ATen/ops/squeeze_copy.h>
#include <ATen/ops/squeeze_copy.h>
#include <ATen/ops/squeeze_copy.h>
#include <ATen/ops/indices_copy.h>
#include <ATen/ops/ccol_indices_copy.h>
#include <ATen/ops/split_copy.h>
#include <ATen/ops/_scaled_dot_product_flash_attention_for_cpu_backward.h>
#include <ATen/ops/_scaled_dot_product_efficient_attention.h>
#include <ATen/ops/_efficient_attention_forward.h>
#include <ATen/ops/special_bessel_j1.h>
#include <ATen/ops/special_bessel_j1.h>
#include <ATen/ops/special_chebyshev_polynomial_v.h>
#include <ATen/ops/special_chebyshev_polynomial_v.h>
#include <ATen/ops/special_chebyshev_polynomial_v.h>
#include <ATen/ops/special_chebyshev_polynomial_v.h>
#include <ATen/ops/special_chebyshev_polynomial_v.h>
#include <ATen/ops/special_chebyshev_polynomial_v.h>
#include <ATen/ops/_cudnn_rnn_backward.h>
#include <ATen/ops/native_dropout_backward.h>
#include <ATen/ops/_add_relu.h>
#include <ATen/ops/affine_grid_generator.h>
#include <ATen/ops/_test_functorch_fallback.h>
#include <ATen/ops/bartlett_window.h>
#include <ATen/ops/bartlett_window.h>
#include <ATen/ops/copy.h>
#include <ATen/ops/_copy_from_and_resize.h>
#include <ATen/ops/cudnn_convolution_relu.h>
#include <ATen/ops/diag_embed.h>
#include <ATen/ops/empty_permuted.h>
#include <ATen/ops/_empty_affine_quantized.h>
#include <ATen/ops/_resize_output.h>
#include <ATen/ops/_resize_output.h>
#include <ATen/ops/empty_like.h>
#include <ATen/ops/grid_sampler_3d_backward.h>
#include <ATen/ops/native_group_norm.h>
#include <ATen/ops/linear_backward.h>
#include <ATen/ops/mkldnn_linear_backward_input.h>
#include <ATen/ops/mkldnn_linear_backward.h>
#include <ATen/ops/_native_batch_norm_legit_no_training.h>
#include <ATen/ops/batch_norm_gather_stats_with_counts.h>
#include <ATen/ops/_pdist_backward.h>
#include <ATen/ops/pixel_shuffle.h>
#include <ATen/ops/celu.h>
#include <ATen/ops/slice_backward.h>
#include <ATen/ops/unsafe_split.h>
#include <ATen/ops/std_mean.h>
#include <ATen/ops/flip.h>
#include <ATen/ops/roll.h>
#include <ATen/ops/_nested_from_padded.h>
#include <ATen/ops/_trilinear.h>
#include <ATen/ops/_unique2.h>
#include <ATen/ops/_weight_norm_interface_backward.h>
#include <ATen/ops/zeros_like.h>
#include <ATen/ops/_sparse_csr_prod.h>
#include <ATen/ops/_sparse_softmax_backward_data.h>
#include <ATen/ops/_sparse_log_softmax.h>
#include <ATen/ops/_sparse_log_softmax_backward_data.h>
#include <ATen/ops/_spdiags.h>
#include <ATen/ops/zero.h>
#include <ATen/ops/zero.h>
#include <ATen/ops/rsub.h>
#include <ATen/ops/rsub.h>
#include <ATen/ops/_sparse_coo_tensor_with_dims.h>
#include <ATen/ops/_coalesce.h>
#include <ATen/ops/_to_sparse.h>
#include <ATen/ops/_to_sparse.h>
#include <ATen/ops/q_per_channel_scales.h>
#include <ATen/ops/lstm_mps_backward.h>
#include <ATen/ops/_thnn_fused_lstm_cell_backward_impl.h>
#include <ATen/ops/_thnn_fused_gru_cell.h>
#include <ATen/ops/_pack_padded_sequence.h>
#include <ATen/ops/_masked_softmax.h>
#include <ATen/ops/_foreach_mul.h>
#include <ATen/ops/_foreach_mul.h>
#include <ATen/ops/_foreach_mul.h>
#include <ATen/ops/_foreach_mul.h>
#include <ATen/ops/_foreach_div.h>
#include <ATen/ops/_foreach_div.h>
#include <ATen/ops/_foreach_div.h>
#include <ATen/ops/_foreach_div.h>
#include <ATen/ops/_foreach_addcmul.h>
#include <ATen/ops/_foreach_addcmul.h>
#include <ATen/ops/_foreach_addcmul.h>
#include <ATen/ops/_foreach_asin.h>
#include <ATen/ops/_foreach_cos.h>
#include <ATen/ops/_foreach_floor.h>
#include <ATen/ops/_foreach_tanh.h>
#include <ATen/ops/_foreach_zero.h>
#include <ATen/ops/_foreach_zero.h>
#include <ATen/ops/_adaptive_avg_pool3d.h>
#include <ATen/ops/_slow_conv2d_backward.h>
#include <ATen/ops/conv_depthwise3d.h>
#include <ATen/ops/slow_conv_dilated2d.h>
#include <ATen/ops/_test_optional_filled_intlist.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_view_copy.h>
#include <ATen/ops/_fw_primal_copy.h>
#include <ATen/ops/view_as_real_copy.h>
#include <ATen/ops/as_strided_copy.h>
#include <ATen/ops/_reshape_alias_copy.h>
#include <ATen/ops/squeeze_copy.h>
#include <ATen/ops/squeeze_copy.h>
#include <ATen/ops/squeeze_copy.h>
#include <ATen/ops/indices_copy.h>
#include <ATen/ops/ccol_indices_copy.h>
#endif



namespace at { namespace _ops {


STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Int, name, "aten::_cast_Int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Int, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Int, schema_str, "_cast_Int(Tensor self, bool non_blocking=False) -> Tensor")

// aten::_cast_Int(Tensor self, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cast_Int::schema> create__cast_Int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cast_Int::name, _cast_Int::overload_name)
      .typed<_cast_Int::schema>();
}

// aten::_cast_Int(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Int::call(const at::Tensor & self, bool non_blocking) {
    
    static auto op = create__cast_Int_typed_handle();
    return op.call(self, non_blocking);
}

// aten::_cast_Int(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
    
    static auto op = create__cast_Int_typed_handle();
    return op.redispatch(dispatchKeySet, self, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Long, name, "aten::_cast_Long")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Long, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Long, schema_str, "_cast_Long(Tensor self, bool non_blocking=False) -> Tensor")

// aten::_cast_Long(Tensor self, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cast_Long::schema> create__cast_Long_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cast_Long::name, _cast_Long::overload_name)
      .typed<_cast_Long::schema>();
}

// aten::_cast_Long(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Long::call(const at::Tensor & self, bool non_blocking) {
    
    static auto op = create__cast_Long_typed_handle();
    return op.call(self, non_blocking);
}

// aten::_cast_Long(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Long::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
    
    static auto op = create__cast_Long_typed_handle();
    return op.redispatch(dispatchKeySet, self, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_backward, name, "aten::_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_backward, schema_str, "_backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -> ()")

// aten::_backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_backward::schema> create__backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_backward::name, _backward::overload_name)
      .typed<_backward::schema>();
}

// aten::_backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -> ()
void _backward::call(const at::Tensor & self, at::TensorList inputs, const c10::optional<at::Tensor> & gradient, c10::optional<bool> retain_graph, bool create_graph) {
    
    static auto op = create__backward_typed_handle();
    return op.call(self, inputs, gradient, retain_graph, create_graph);
}

// aten::_backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -> ()
void _backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::TensorList inputs, const c10::optional<at::Tensor> & gradient, c10::optional<bool> retain_graph, bool create_graph) {
    
    static auto op = create__backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, inputs, gradient, retain_graph, create_graph);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(data, name, "aten::data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(data, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(data, schema_str, "data(Tensor self) -> Tensor")

// aten::data(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<data::schema> create_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(data::name, data::overload_name)
      .typed<data::schema>();
}

// aten::data(Tensor self) -> Tensor
at::Tensor data::call(const at::Tensor & self) {
    
    static auto op = create_data_typed_handle();
    return op.call(self);
}

// aten::data(Tensor self) -> Tensor
at::Tensor data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_data_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(retain_grad, name, "aten::retain_grad")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(retain_grad, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(retain_grad, schema_str, "retain_grad(Tensor(a!) self) -> ()")

// aten::retain_grad(Tensor(a!) self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<retain_grad::schema> create_retain_grad_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(retain_grad::name, retain_grad::overload_name)
      .typed<retain_grad::schema>();
}

// aten::retain_grad(Tensor(a!) self) -> ()
void retain_grad::call(at::Tensor & self) {
    
    static auto op = create_retain_grad_typed_handle();
    return op.call(self);
}

// aten::retain_grad(Tensor(a!) self) -> ()
void retain_grad::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_retain_grad_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rename_, name, "aten::rename_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rename_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rename_, schema_str, "rename_(Tensor(a!) self, Dimname[]? names) -> Tensor(a!)")

// aten::rename_(Tensor(a!) self, Dimname[]? names) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<rename_::schema> create_rename__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rename_::name, rename_::overload_name)
      .typed<rename_::schema>();
}

// aten::rename_(Tensor(a!) self, Dimname[]? names) -> Tensor(a!)
at::Tensor & rename_::call(at::Tensor & self, c10::optional<at::DimnameList> names) {
    
    static auto op = create_rename__typed_handle();
    return op.call(self, names);
}

// aten::rename_(Tensor(a!) self, Dimname[]? names) -> Tensor(a!)
at::Tensor & rename_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<at::DimnameList> names) {
    
    static auto op = create_rename__typed_handle();
    return op.redispatch(dispatchKeySet, self, names);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rename, name, "aten::rename")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rename, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rename, schema_str, "rename(Tensor(a) self, Dimname[]? names) -> Tensor(a)")

// aten::rename(Tensor(a) self, Dimname[]? names) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<rename::schema> create_rename_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rename::name, rename::overload_name)
      .typed<rename::schema>();
}

// aten::rename(Tensor(a) self, Dimname[]? names) -> Tensor(a)
at::Tensor rename::call(const at::Tensor & self, c10::optional<at::DimnameList> names) {
    
    static auto op = create_rename_typed_handle();
    return op.call(self, names);
}

// aten::rename(Tensor(a) self, Dimname[]? names) -> Tensor(a)
at::Tensor rename::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::DimnameList> names) {
    
    static auto op = create_rename_typed_handle();
    return op.redispatch(dispatchKeySet, self, names);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sym_constrain_range, name, "aten::sym_constrain_range")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sym_constrain_range, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sym_constrain_range, schema_str, "sym_constrain_range(Scalar size, *, int? min=None, int? max=None) -> ()")

// aten::sym_constrain_range(Scalar size, *, int? min=None, int? max=None) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<sym_constrain_range::schema> create_sym_constrain_range_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sym_constrain_range::name, sym_constrain_range::overload_name)
      .typed<sym_constrain_range::schema>();
}

// aten::sym_constrain_range(Scalar size, *, int? min=None, int? max=None) -> ()
void sym_constrain_range::call(const at::Scalar & size, c10::optional<int64_t> min, c10::optional<int64_t> max) {
    
    static auto op = create_sym_constrain_range_typed_handle();
    return op.call(size, min, max);
}

// aten::sym_constrain_range(Scalar size, *, int? min=None, int? max=None) -> ()
void sym_constrain_range::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & size, c10::optional<int64_t> min, c10::optional<int64_t> max) {
    
    static auto op = create_sym_constrain_range_typed_handle();
    return op.redispatch(dispatchKeySet, size, min, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_functional_sym_constrain_range, name, "aten::_functional_sym_constrain_range")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_functional_sym_constrain_range, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_functional_sym_constrain_range, schema_str, "_functional_sym_constrain_range(Scalar size, int? min, int? max, Tensor dep_token) -> Tensor")

// aten::_functional_sym_constrain_range(Scalar size, int? min, int? max, Tensor dep_token) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_functional_sym_constrain_range::schema> create__functional_sym_constrain_range_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_functional_sym_constrain_range::name, _functional_sym_constrain_range::overload_name)
      .typed<_functional_sym_constrain_range::schema>();
}

// aten::_functional_sym_constrain_range(Scalar size, int? min, int? max, Tensor dep_token) -> Tensor
at::Tensor _functional_sym_constrain_range::call(const at::Scalar & size, c10::optional<int64_t> min, c10::optional<int64_t> max, const at::Tensor & dep_token) {
    
    static auto op = create__functional_sym_constrain_range_typed_handle();
    return op.call(size, min, max, dep_token);
}

// aten::_functional_sym_constrain_range(Scalar size, int? min, int? max, Tensor dep_token) -> Tensor
at::Tensor _functional_sym_constrain_range::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & size, c10::optional<int64_t> min, c10::optional<int64_t> max, const at::Tensor & dep_token) {
    
    static auto op = create__functional_sym_constrain_range_typed_handle();
    return op.redispatch(dispatchKeySet, size, min, max, dep_token);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_backward, name, "aten::_cudnn_rnn_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_backward, schema_str, "_cudnn_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])")

// aten::_cudnn_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
static C10_NOINLINE c10::TypedOperatorHandle<_cudnn_rnn_backward::schema> create__cudnn_rnn_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cudnn_rnn_backward::name, _cudnn_rnn_backward::overload_name)
      .typed<_cudnn_rnn_backward::schema>();
}

// aten::_cudnn_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
::std::tuple<at::Tensor,at::Tensor,at::Tensor,::std::vector<at::Tensor>> _cudnn_rnn_backward::call(const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, c10::SymIntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, ::std::array<bool,4> output_mask) {
    
    static auto op = create__cudnn_rnn_backward_typed_handle();
    return op.call(input, weight, weight_stride0, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve, output_mask);
}

// aten::_cudnn_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
::std::tuple<at::Tensor,at::Tensor,at::Tensor,::std::vector<at::Tensor>> _cudnn_rnn_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, c10::SymIntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, ::std::array<bool,4> output_mask) {
    
    static auto op = create__cudnn_rnn_backward_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, weight_stride0, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_dropout_backward, name, "aten::native_dropout_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_dropout_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_dropout_backward, schema_str, "native_dropout_backward(Tensor grad_output, Tensor mask, float scale) -> Tensor")

// aten::native_dropout_backward(Tensor grad_output, Tensor mask, float scale) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<native_dropout_backward::schema> create_native_dropout_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_dropout_backward::name, native_dropout_backward::overload_name)
      .typed<native_dropout_backward::schema>();
}

// aten::native_dropout_backward(Tensor grad_output, Tensor mask, float scale) -> Tensor
at::Tensor native_dropout_backward::call(const at::Tensor & grad_output, const at::Tensor & mask, double scale) {
    
    static auto op = create_native_dropout_backward_typed_handle();
    return op.call(grad_output, mask, scale);
}

// aten::native_dropout_backward(Tensor grad_output, Tensor mask, float scale) -> Tensor
at::Tensor native_dropout_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & mask, double scale) {
    
    static auto op = create_native_dropout_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, mask, scale);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_dropout, name, "aten::feature_dropout")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_dropout, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_dropout, schema_str, "feature_dropout(Tensor input, float p, bool train) -> Tensor")

// aten::feature_dropout(Tensor input, float p, bool train) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<feature_dropout::schema> create_feature_dropout_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(feature_dropout::name, feature_dropout::overload_name)
      .typed<feature_dropout::schema>();
}

// aten::feature_dropout(Tensor input, float p, bool train) -> Tensor
at::Tensor feature_dropout::call(const at::Tensor & input, double p, bool train) {
    
    static auto op = create_feature_dropout_typed_handle();
    return op.call(input, p, train);
}

// aten::feature_dropout(Tensor input, float p, bool train) -> Tensor
at::Tensor feature_dropout::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double p, bool train) {
    
    static auto op = create_feature_dropout_typed_handle();
    return op.redispatch(dispatchKeySet, input, p, train);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_dropout_, name, "aten::feature_dropout_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_dropout_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_dropout_, schema_str, "feature_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)")

// aten::feature_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<feature_dropout_::schema> create_feature_dropout__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(feature_dropout_::name, feature_dropout_::overload_name)
      .typed<feature_dropout_::schema>();
}

// aten::feature_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
at::Tensor & feature_dropout_::call(at::Tensor & self, double p, bool train) {
    
    static auto op = create_feature_dropout__typed_handle();
    return op.call(self, p, train);
}

// aten::feature_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
at::Tensor & feature_dropout_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, bool train) {
    
    static auto op = create_feature_dropout__typed_handle();
    return op.redispatch(dispatchKeySet, self, p, train);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj, name, "aten::conj")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj, schema_str, "conj(Tensor(a) self) -> Tensor(a)")

// aten::conj(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<conj::schema> create_conj_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conj::name, conj::overload_name)
      .typed<conj::schema>();
}

// aten::conj(Tensor(a) self) -> Tensor(a)
at::Tensor conj::call(const at::Tensor & self) {
    
    static auto op = create_conj_typed_handle();
    return op.call(self);
}

// aten::conj(Tensor(a) self) -> Tensor(a)
at::Tensor conj::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_conj_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Tensor, name, "aten::_add_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Tensor, schema_str, "_add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor")

// aten::_add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_add_relu_Tensor::schema> create__add_relu_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_add_relu_Tensor::name, _add_relu_Tensor::overload_name)
      .typed<_add_relu_Tensor::schema>();
}

// aten::_add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
at::Tensor _add_relu_Tensor::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    
    static auto op = create__add_relu_Tensor_typed_handle();
    return op.call(self, other, alpha);
}

// aten::_add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
at::Tensor _add_relu_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    
    static auto op = create__add_relu_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu__Tensor, name, "aten::_add_relu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu__Tensor, schema_str, "_add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)")

// aten::_add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_add_relu__Tensor::schema> create__add_relu__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_add_relu__Tensor::name, _add_relu__Tensor::overload_name)
      .typed<_add_relu__Tensor::schema>();
}

// aten::_add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
at::Tensor & _add_relu__Tensor::call(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    
    static auto op = create__add_relu__Tensor_typed_handle();
    return op.call(self, other, alpha);
}

// aten::_add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
at::Tensor & _add_relu__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    
    static auto op = create__add_relu__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_out, name, "aten::_add_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_out, schema_str, "_add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::_add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_add_relu_out::schema> create__add_relu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_add_relu_out::name, _add_relu_out::overload_name)
      .typed<_add_relu_out::schema>();
}

// aten::_add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _add_relu_out::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create__add_relu_out_typed_handle();
    return op.call(self, other, alpha, out);
}

// aten::_add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _add_relu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create__add_relu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Scalar, name, "aten::_add_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Scalar, schema_str, "_add_relu.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor")

// aten::_add_relu.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_add_relu_Scalar::schema> create__add_relu_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_add_relu_Scalar::name, _add_relu_Scalar::overload_name)
      .typed<_add_relu_Scalar::schema>();
}

// aten::_add_relu.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
at::Tensor _add_relu_Scalar::call(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    
    static auto op = create__add_relu_Scalar_typed_handle();
    return op.call(self, other, alpha);
}

// aten::_add_relu.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
at::Tensor _add_relu_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    
    static auto op = create__add_relu_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu__Scalar, name, "aten::_add_relu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu__Scalar, schema_str, "_add_relu_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)")

// aten::_add_relu_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_add_relu__Scalar::schema> create__add_relu__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_add_relu__Scalar::name, _add_relu__Scalar::overload_name)
      .typed<_add_relu__Scalar::schema>();
}

// aten::_add_relu_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
at::Tensor & _add_relu__Scalar::call(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    
    static auto op = create__add_relu__Scalar_typed_handle();
    return op.call(self, other, alpha);
}

// aten::_add_relu_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
at::Tensor & _add_relu__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    
    static auto op = create__add_relu__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator, name, "aten::affine_grid_generator")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator, schema_str, "affine_grid_generator(Tensor theta, SymInt[] size, bool align_corners) -> Tensor")

// aten::affine_grid_generator(Tensor theta, SymInt[] size, bool align_corners) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<affine_grid_generator::schema> create_affine_grid_generator_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(affine_grid_generator::name, affine_grid_generator::overload_name)
      .typed<affine_grid_generator::schema>();
}

// aten::affine_grid_generator(Tensor theta, SymInt[] size, bool align_corners) -> Tensor
at::Tensor affine_grid_generator::call(const at::Tensor & theta, c10::SymIntArrayRef size, bool align_corners) {
    
    static auto op = create_affine_grid_generator_typed_handle();
    return op.call(theta, size, align_corners);
}

// aten::affine_grid_generator(Tensor theta, SymInt[] size, bool align_corners) -> Tensor
at::Tensor affine_grid_generator::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & theta, c10::SymIntArrayRef size, bool align_corners) {
    
    static auto op = create_affine_grid_generator_typed_handle();
    return op.redispatch(dispatchKeySet, theta, size, align_corners);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_is_any_true, name, "aten::_is_any_true")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_is_any_true, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_is_any_true, schema_str, "_is_any_true(Tensor self) -> Tensor")

// aten::_is_any_true(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_is_any_true::schema> create__is_any_true_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_is_any_true::name, _is_any_true::overload_name)
      .typed<_is_any_true::schema>();
}

// aten::_is_any_true(Tensor self) -> Tensor
at::Tensor _is_any_true::call(const at::Tensor & self) {
    
    static auto op = create__is_any_true_typed_handle();
    return op.call(self);
}

// aten::_is_any_true(Tensor self) -> Tensor
at::Tensor _is_any_true::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create__is_any_true_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_functorch_fallback, name, "aten::_test_functorch_fallback")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_functorch_fallback, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_functorch_fallback, schema_str, "_test_functorch_fallback(Tensor self, Tensor other) -> Tensor")

// aten::_test_functorch_fallback(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_test_functorch_fallback::schema> create__test_functorch_fallback_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_test_functorch_fallback::name, _test_functorch_fallback::overload_name)
      .typed<_test_functorch_fallback::schema>();
}

// aten::_test_functorch_fallback(Tensor self, Tensor other) -> Tensor
at::Tensor _test_functorch_fallback::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create__test_functorch_fallback_typed_handle();
    return op.call(self, other);
}

// aten::_test_functorch_fallback(Tensor self, Tensor other) -> Tensor
at::Tensor _test_functorch_fallback::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create__test_functorch_fallback_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange, name, "aten::arange")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange, schema_str, "arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arange::schema> create_arange_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arange::name, arange::overload_name)
      .typed<arange::schema>();
}

// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor arange::call(const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_arange_typed_handle();
    return op.call(end, dtype, layout, device, pin_memory);
}

// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor arange::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_arange_typed_handle();
    return op.redispatch(dispatchKeySet, end, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start, name, "aten::arange")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start, overload_name, "start")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start, schema_str, "arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arange_start::schema> create_arange_start_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arange_start::name, arange_start::overload_name)
      .typed<arange_start::schema>();
}

// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor arange_start::call(const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_arange_start_typed_handle();
    return op.call(start, end, dtype, layout, device, pin_memory);
}

// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor arange_start::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_arange_start_typed_handle();
    return op.redispatch(dispatchKeySet, start, end, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start_step, name, "aten::arange")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start_step, overload_name, "start_step")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start_step, schema_str, "arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arange_start_step::schema> create_arange_start_step_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arange_start_step::name, arange_start_step::overload_name)
      .typed<arange_start_step::schema>();
}

// aten::arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor arange_start_step::call(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_arange_start_step_typed_handle();
    return op.call(start, end, step, dtype, layout, device, pin_memory);
}

// aten::arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor arange_start_step::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_arange_start_step_typed_handle();
    return op.redispatch(dispatchKeySet, start, end, step, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_out, name, "aten::arange")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_out, schema_str, "arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)")

// aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arange_out::schema> create_arange_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arange_out::name, arange_out::overload_name)
      .typed<arange_out::schema>();
}

// aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arange_out::call(const at::Scalar & end, at::Tensor & out) {
    
    static auto op = create_arange_out_typed_handle();
    return op.call(end, out);
}

// aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arange_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & end, at::Tensor & out) {
    
    static auto op = create_arange_out_typed_handle();
    return op.redispatch(dispatchKeySet, end, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start_out, name, "aten::arange")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start_out, overload_name, "start_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start_out, schema_str, "arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arange_start_out::schema> create_arange_start_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arange_start_out::name, arange_start_out::overload_name)
      .typed<arange_start_out::schema>();
}

// aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arange_start_out::call(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
    
    static auto op = create_arange_start_out_typed_handle();
    return op.call(start, end, step, out);
}

// aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arange_start_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
    
    static auto op = create_arange_start_out_typed_handle();
    return op.redispatch(dispatchKeySet, start, end, step, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dim_arange, name, "aten::_dim_arange")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dim_arange, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dim_arange, schema_str, "_dim_arange(Tensor like, int dim) -> Tensor")

// aten::_dim_arange(Tensor like, int dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_dim_arange::schema> create__dim_arange_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_dim_arange::name, _dim_arange::overload_name)
      .typed<_dim_arange::schema>();
}

// aten::_dim_arange(Tensor like, int dim) -> Tensor
at::Tensor _dim_arange::call(const at::Tensor & like, int64_t dim) {
    
    static auto op = create__dim_arange_typed_handle();
    return op.call(like, dim);
}

// aten::_dim_arange(Tensor like, int dim) -> Tensor
at::Tensor _dim_arange::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & like, int64_t dim) {
    
    static auto op = create__dim_arange_typed_handle();
    return op.redispatch(dispatchKeySet, like, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh, name, "aten::arcsinh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh, schema_str, "arcsinh(Tensor self) -> Tensor")

// aten::arcsinh(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arcsinh::schema> create_arcsinh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arcsinh::name, arcsinh::overload_name)
      .typed<arcsinh::schema>();
}

// aten::arcsinh(Tensor self) -> Tensor
at::Tensor arcsinh::call(const at::Tensor & self) {
    
    static auto op = create_arcsinh_typed_handle();
    return op.call(self);
}

// aten::arcsinh(Tensor self) -> Tensor
at::Tensor arcsinh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_arcsinh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh_, name, "aten::arcsinh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh_, schema_str, "arcsinh_(Tensor(a!) self) -> Tensor(a!)")

// aten::arcsinh_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arcsinh_::schema> create_arcsinh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arcsinh_::name, arcsinh_::overload_name)
      .typed<arcsinh_::schema>();
}

// aten::arcsinh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arcsinh_::call(at::Tensor & self) {
    
    static auto op = create_arcsinh__typed_handle();
    return op.call(self);
}

// aten::arcsinh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arcsinh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_arcsinh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh_out, name, "aten::arcsinh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh_out, schema_str, "arcsinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::arcsinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arcsinh_out::schema> create_arcsinh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arcsinh_out::name, arcsinh_out::overload_name)
      .typed<arcsinh_out::schema>();
}

// aten::arcsinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arcsinh_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_arcsinh_out_typed_handle();
    return op.call(self, out);
}

// aten::arcsinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arcsinh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_arcsinh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh, name, "aten::atanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh, schema_str, "atanh(Tensor self) -> Tensor")

// aten::atanh(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<atanh::schema> create_atanh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atanh::name, atanh::overload_name)
      .typed<atanh::schema>();
}

// aten::atanh(Tensor self) -> Tensor
at::Tensor atanh::call(const at::Tensor & self) {
    
    static auto op = create_atanh_typed_handle();
    return op.call(self);
}

// aten::atanh(Tensor self) -> Tensor
at::Tensor atanh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_atanh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh_, name, "aten::atanh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh_, schema_str, "atanh_(Tensor(a!) self) -> Tensor(a!)")

// aten::atanh_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<atanh_::schema> create_atanh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atanh_::name, atanh_::overload_name)
      .typed<atanh_::schema>();
}

// aten::atanh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & atanh_::call(at::Tensor & self) {
    
    static auto op = create_atanh__typed_handle();
    return op.call(self);
}

// aten::atanh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & atanh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_atanh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh_out, name, "aten::atanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh_out, schema_str, "atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<atanh_out::schema> create_atanh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atanh_out::name, atanh_out::overload_name)
      .typed<atanh_out::schema>();
}

// aten::atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & atanh_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_atanh_out_typed_handle();
    return op.call(self, out);
}

// aten::atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & atanh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_atanh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin, name, "aten::arcsin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin, schema_str, "arcsin(Tensor self) -> Tensor")

// aten::arcsin(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arcsin::schema> create_arcsin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arcsin::name, arcsin::overload_name)
      .typed<arcsin::schema>();
}

// aten::arcsin(Tensor self) -> Tensor
at::Tensor arcsin::call(const at::Tensor & self) {
    
    static auto op = create_arcsin_typed_handle();
    return op.call(self);
}

// aten::arcsin(Tensor self) -> Tensor
at::Tensor arcsin::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_arcsin_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin_, name, "aten::arcsin_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin_, schema_str, "arcsin_(Tensor(a!) self) -> Tensor(a!)")

// aten::arcsin_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arcsin_::schema> create_arcsin__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arcsin_::name, arcsin_::overload_name)
      .typed<arcsin_::schema>();
}

// aten::arcsin_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arcsin_::call(at::Tensor & self) {
    
    static auto op = create_arcsin__typed_handle();
    return op.call(self);
}

// aten::arcsin_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arcsin_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_arcsin__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin_out, name, "aten::arcsin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin_out, schema_str, "arcsin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::arcsin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arcsin_out::schema> create_arcsin_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arcsin_out::name, arcsin_out::overload_name)
      .typed<arcsin_out::schema>();
}

// aten::arcsin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arcsin_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_arcsin_out_typed_handle();
    return op.call(self, out);
}

// aten::arcsin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arcsin_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_arcsin_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window, name, "aten::bartlett_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window, schema_str, "bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bartlett_window::schema> create_bartlett_window_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bartlett_window::name, bartlett_window::overload_name)
      .typed<bartlett_window::schema>();
}

// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor bartlett_window::call(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_bartlett_window_typed_handle();
    return op.call(window_length, dtype, layout, device, pin_memory);
}

// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor bartlett_window::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_bartlett_window_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window_periodic, name, "aten::bartlett_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window_periodic, overload_name, "periodic")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window_periodic, schema_str, "bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bartlett_window_periodic::schema> create_bartlett_window_periodic_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bartlett_window_periodic::name, bartlett_window_periodic::overload_name)
      .typed<bartlett_window_periodic::schema>();
}

// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor bartlett_window_periodic::call(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_bartlett_window_periodic_typed_handle();
    return op.call(window_length, periodic, dtype, layout, device, pin_memory);
}

// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor bartlett_window_periodic::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_bartlett_window_periodic_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy, name, "aten::binary_cross_entropy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy, schema_str, "binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor")

// aten::binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<binary_cross_entropy::schema> create_binary_cross_entropy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(binary_cross_entropy::name, binary_cross_entropy::overload_name)
      .typed<binary_cross_entropy::schema>();
}

// aten::binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
at::Tensor binary_cross_entropy::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
    
    static auto op = create_binary_cross_entropy_typed_handle();
    return op.call(self, target, weight, reduction);
}

// aten::binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
at::Tensor binary_cross_entropy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
    
    static auto op = create_binary_cross_entropy_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_out, name, "aten::binary_cross_entropy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_out, schema_str, "binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)")

// aten::binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<binary_cross_entropy_out::schema> create_binary_cross_entropy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(binary_cross_entropy_out::name, binary_cross_entropy_out::overload_name)
      .typed<binary_cross_entropy_out::schema>();
}

// aten::binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & binary_cross_entropy_out::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
    
    static auto op = create_binary_cross_entropy_out_typed_handle();
    return op.call(self, target, weight, reduction, out);
}

// aten::binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & binary_cross_entropy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
    
    static auto op = create_binary_cross_entropy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bmm, name, "aten::bmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bmm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bmm, schema_str, "bmm(Tensor self, Tensor mat2) -> Tensor")

// aten::bmm(Tensor self, Tensor mat2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bmm::schema> create_bmm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bmm::name, bmm::overload_name)
      .typed<bmm::schema>();
}

// aten::bmm(Tensor self, Tensor mat2) -> Tensor
at::Tensor bmm::call(const at::Tensor & self, const at::Tensor & mat2) {
    
    static auto op = create_bmm_typed_handle();
    return op.call(self, mat2);
}

// aten::bmm(Tensor self, Tensor mat2) -> Tensor
at::Tensor bmm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2) {
    
    static auto op = create_bmm_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bmm_out, name, "aten::bmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bmm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bmm_out, schema_str, "bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bmm_out::schema> create_bmm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bmm_out::name, bmm_out::overload_name)
      .typed<bmm_out::schema>();
}

// aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bmm_out::call(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
    
    static auto op = create_bmm_out_typed_handle();
    return op.call(self, mat2, out);
}

// aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bmm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
    
    static auto op = create_bmm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat2, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_broadcast_to, name, "aten::_sparse_broadcast_to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_broadcast_to, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_broadcast_to, schema_str, "_sparse_broadcast_to(Tensor(a) self, int[] size) -> Tensor(a)")

// aten::_sparse_broadcast_to(Tensor(a) self, int[] size) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_broadcast_to::schema> create__sparse_broadcast_to_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_broadcast_to::name, _sparse_broadcast_to::overload_name)
      .typed<_sparse_broadcast_to::schema>();
}

// aten::_sparse_broadcast_to(Tensor(a) self, int[] size) -> Tensor(a)
at::Tensor _sparse_broadcast_to::call(const at::Tensor & self, at::IntArrayRef size) {
    
    static auto op = create__sparse_broadcast_to_typed_handle();
    return op.call(self, size);
}

// aten::_sparse_broadcast_to(Tensor(a) self, int[] size) -> Tensor(a)
at::Tensor _sparse_broadcast_to::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size) {
    
    static auto op = create__sparse_broadcast_to_typed_handle();
    return op.redispatch(dispatchKeySet, self, size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat, name, "aten::concat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat, schema_str, "concat(Tensor[] tensors, int dim=0) -> Tensor")

// aten::concat(Tensor[] tensors, int dim=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<concat::schema> create_concat_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(concat::name, concat::overload_name)
      .typed<concat::schema>();
}

// aten::concat(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor concat::call(at::TensorList tensors, int64_t dim) {
    
    static auto op = create_concat_typed_handle();
    return op.call(tensors, dim);
}

// aten::concat(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor concat::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim) {
    
    static auto op = create_concat_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_out, name, "aten::concat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_out, schema_str, "concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<concat_out::schema> create_concat_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(concat_out::name, concat_out::overload_name)
      .typed<concat_out::schema>();
}

// aten::concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & concat_out::call(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    
    static auto op = create_concat_out_typed_handle();
    return op.call(tensors, dim, out);
}

// aten::concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & concat_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
    
    static auto op = create_concat_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_names, name, "aten::concat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_names, overload_name, "names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_names, schema_str, "concat.names(Tensor[] tensors, Dimname dim) -> Tensor")

// aten::concat.names(Tensor[] tensors, Dimname dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<concat_names::schema> create_concat_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(concat_names::name, concat_names::overload_name)
      .typed<concat_names::schema>();
}

// aten::concat.names(Tensor[] tensors, Dimname dim) -> Tensor
at::Tensor concat_names::call(at::TensorList tensors, at::Dimname dim) {
    
    static auto op = create_concat_names_typed_handle();
    return op.call(tensors, dim);
}

// aten::concat.names(Tensor[] tensors, Dimname dim) -> Tensor
at::Tensor concat_names::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Dimname dim) {
    
    static auto op = create_concat_names_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_names_out, name, "aten::concat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_names_out, overload_name, "names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_names_out, schema_str, "concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)")

// aten::concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<concat_names_out::schema> create_concat_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(concat_names_out::name, concat_names_out::overload_name)
      .typed<concat_names_out::schema>();
}

// aten::concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & concat_names_out::call(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
    
    static auto op = create_concat_names_out_typed_handle();
    return op.call(tensors, dim, out);
}

// aten::concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & concat_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
    
    static auto op = create_concat_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chain_matmul, name, "aten::chain_matmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chain_matmul, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chain_matmul, schema_str, "chain_matmul(Tensor[] matrices) -> Tensor")

// aten::chain_matmul(Tensor[] matrices) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<chain_matmul::schema> create_chain_matmul_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(chain_matmul::name, chain_matmul::overload_name)
      .typed<chain_matmul::schema>();
}

// aten::chain_matmul(Tensor[] matrices) -> Tensor
at::Tensor chain_matmul::call(at::TensorList matrices) {
    
    static auto op = create_chain_matmul_typed_handle();
    return op.call(matrices);
}

// aten::chain_matmul(Tensor[] matrices) -> Tensor
at::Tensor chain_matmul::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList matrices) {
    
    static auto op = create_chain_matmul_typed_handle();
    return op.redispatch(dispatchKeySet, matrices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chain_matmul_out, name, "aten::chain_matmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chain_matmul_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chain_matmul_out, schema_str, "chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -> Tensor(a!)")

// aten::chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<chain_matmul_out::schema> create_chain_matmul_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(chain_matmul_out::name, chain_matmul_out::overload_name)
      .typed<chain_matmul_out::schema>();
}

// aten::chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & chain_matmul_out::call(at::TensorList matrices, at::Tensor & out) {
    
    static auto op = create_chain_matmul_out_typed_handle();
    return op.call(matrices, out);
}

// aten::chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & chain_matmul_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList matrices, at::Tensor & out) {
    
    static auto op = create_chain_matmul_out_typed_handle();
    return op.redispatch(dispatchKeySet, matrices, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min, name, "aten::clamp_min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min, schema_str, "clamp_min(Tensor self, Scalar min) -> Tensor")

// aten::clamp_min(Tensor self, Scalar min) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<clamp_min::schema> create_clamp_min_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_min::name, clamp_min::overload_name)
      .typed<clamp_min::schema>();
}

// aten::clamp_min(Tensor self, Scalar min) -> Tensor
at::Tensor clamp_min::call(const at::Tensor & self, const at::Scalar & min) {
    
    static auto op = create_clamp_min_typed_handle();
    return op.call(self, min);
}

// aten::clamp_min(Tensor self, Scalar min) -> Tensor
at::Tensor clamp_min::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & min) {
    
    static auto op = create_clamp_min_typed_handle();
    return op.redispatch(dispatchKeySet, self, min);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_Tensor, name, "aten::clamp_min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_Tensor, schema_str, "clamp_min.Tensor(Tensor self, Tensor min) -> Tensor")

// aten::clamp_min.Tensor(Tensor self, Tensor min) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<clamp_min_Tensor::schema> create_clamp_min_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_min_Tensor::name, clamp_min_Tensor::overload_name)
      .typed<clamp_min_Tensor::schema>();
}

// aten::clamp_min.Tensor(Tensor self, Tensor min) -> Tensor
at::Tensor clamp_min_Tensor::call(const at::Tensor & self, const at::Tensor & min) {
    
    static auto op = create_clamp_min_Tensor_typed_handle();
    return op.call(self, min);
}

// aten::clamp_min.Tensor(Tensor self, Tensor min) -> Tensor
at::Tensor clamp_min_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & min) {
    
    static auto op = create_clamp_min_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, min);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_, name, "aten::clamp_min_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_, schema_str, "clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)")

// aten::clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_min_::schema> create_clamp_min__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_min_::name, clamp_min_::overload_name)
      .typed<clamp_min_::schema>();
}

// aten::clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)
at::Tensor & clamp_min_::call(at::Tensor & self, const at::Scalar & min) {
    
    static auto op = create_clamp_min__typed_handle();
    return op.call(self, min);
}

// aten::clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)
at::Tensor & clamp_min_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & min) {
    
    static auto op = create_clamp_min__typed_handle();
    return op.redispatch(dispatchKeySet, self, min);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min__Tensor, name, "aten::clamp_min_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min__Tensor, schema_str, "clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)")

// aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_min__Tensor::schema> create_clamp_min__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_min__Tensor::name, clamp_min__Tensor::overload_name)
      .typed<clamp_min__Tensor::schema>();
}

// aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)
at::Tensor & clamp_min__Tensor::call(at::Tensor & self, const at::Tensor & min) {
    
    static auto op = create_clamp_min__Tensor_typed_handle();
    return op.call(self, min);
}

// aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)
at::Tensor & clamp_min__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & min) {
    
    static auto op = create_clamp_min__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, min);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_out, name, "aten::clamp_min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_out, schema_str, "clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)")

// aten::clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_min_out::schema> create_clamp_min_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_min_out::name, clamp_min_out::overload_name)
      .typed<clamp_min_out::schema>();
}

// aten::clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_min_out::call(const at::Tensor & self, const at::Scalar & min, at::Tensor & out) {
    
    static auto op = create_clamp_min_out_typed_handle();
    return op.call(self, min, out);
}

// aten::clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_min_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & min, at::Tensor & out) {
    
    static auto op = create_clamp_min_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, min, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_Tensor_out, name, "aten::clamp_min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_Tensor_out, schema_str, "clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)")

// aten::clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_min_Tensor_out::schema> create_clamp_min_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_min_Tensor_out::name, clamp_min_Tensor_out::overload_name)
      .typed<clamp_min_Tensor_out::schema>();
}

// aten::clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_min_Tensor_out::call(const at::Tensor & self, const at::Tensor & min, at::Tensor & out) {
    
    static auto op = create_clamp_min_Tensor_out_typed_handle();
    return op.call(self, min, out);
}

// aten::clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_min_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & min, at::Tensor & out) {
    
    static auto op = create_clamp_min_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, min, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_mode, name, "aten::_convolution_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_mode, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_mode, schema_str, "_convolution_mode(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, str padding, SymInt[] dilation, SymInt groups) -> Tensor")

// aten::_convolution_mode(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, str padding, SymInt[] dilation, SymInt groups) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_convolution_mode::schema> create__convolution_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_convolution_mode::name, _convolution_mode::overload_name)
      .typed<_convolution_mode::schema>();
}

// aten::_convolution_mode(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, str padding, SymInt[] dilation, SymInt groups) -> Tensor
at::Tensor _convolution_mode::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    
    static auto op = create__convolution_mode_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, groups);
}

// aten::_convolution_mode(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, str padding, SymInt[] dilation, SymInt groups) -> Tensor
at::Tensor _convolution_mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    
    static auto op = create__convolution_mode_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv1d, name, "aten::conv1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv1d, schema_str, "conv1d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=1, SymInt[1] padding=0, SymInt[1] dilation=1, SymInt groups=1) -> Tensor")

// aten::conv1d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=1, SymInt[1] padding=0, SymInt[1] dilation=1, SymInt groups=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv1d::schema> create_conv1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv1d::name, conv1d::overload_name)
      .typed<conv1d::schema>();
}

// aten::conv1d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=1, SymInt[1] padding=0, SymInt[1] dilation=1, SymInt groups=1) -> Tensor
at::Tensor conv1d::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    
    static auto op = create_conv1d_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, groups);
}

// aten::conv1d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=1, SymInt[1] padding=0, SymInt[1] dilation=1, SymInt groups=1) -> Tensor
at::Tensor conv1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    
    static auto op = create_conv1d_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv3d, name, "aten::conv3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv3d, schema_str, "conv3d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] dilation=1, SymInt groups=1) -> Tensor")

// aten::conv3d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] dilation=1, SymInt groups=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv3d::schema> create_conv3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv3d::name, conv3d::overload_name)
      .typed<conv3d::schema>();
}

// aten::conv3d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] dilation=1, SymInt groups=1) -> Tensor
at::Tensor conv3d::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    
    static auto op = create_conv3d_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, groups);
}

// aten::conv3d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] dilation=1, SymInt groups=1) -> Tensor
at::Tensor conv3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    
    static auto op = create_conv3d_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv1d_padding, name, "aten::conv1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv1d_padding, overload_name, "padding")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv1d_padding, schema_str, "conv1d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=1, str padding=\"valid\", SymInt[1] dilation=1, SymInt groups=1) -> Tensor")

// aten::conv1d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=1, str padding="valid", SymInt[1] dilation=1, SymInt groups=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv1d_padding::schema> create_conv1d_padding_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv1d_padding::name, conv1d_padding::overload_name)
      .typed<conv1d_padding::schema>();
}

// aten::conv1d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=1, str padding="valid", SymInt[1] dilation=1, SymInt groups=1) -> Tensor
at::Tensor conv1d_padding::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    
    static auto op = create_conv1d_padding_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, groups);
}

// aten::conv1d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=1, str padding="valid", SymInt[1] dilation=1, SymInt groups=1) -> Tensor
at::Tensor conv1d_padding::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    
    static auto op = create_conv1d_padding_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv3d_padding, name, "aten::conv3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv3d_padding, overload_name, "padding")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv3d_padding, schema_str, "conv3d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, str padding=\"valid\", SymInt[3] dilation=1, SymInt groups=1) -> Tensor")

// aten::conv3d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, str padding="valid", SymInt[3] dilation=1, SymInt groups=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv3d_padding::schema> create_conv3d_padding_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv3d_padding::name, conv3d_padding::overload_name)
      .typed<conv3d_padding::schema>();
}

// aten::conv3d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, str padding="valid", SymInt[3] dilation=1, SymInt groups=1) -> Tensor
at::Tensor conv3d_padding::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    
    static auto op = create_conv3d_padding_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, groups);
}

// aten::conv3d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, str padding="valid", SymInt[3] dilation=1, SymInt groups=1) -> Tensor
at::Tensor conv3d_padding::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    
    static auto op = create_conv3d_padding_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc_backward, name, "aten::conv_tbc_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc_backward, schema_str, "conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)")

// aten::conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<conv_tbc_backward::schema> create_conv_tbc_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv_tbc_backward::name, conv_tbc_backward::overload_name)
      .typed<conv_tbc_backward::schema>();
}

// aten::conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> conv_tbc_backward::call(const at::Tensor & self, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
    
    static auto op = create_conv_tbc_backward_typed_handle();
    return op.call(self, input, weight, bias, pad);
}

// aten::conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> conv_tbc_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
    
    static auto op = create_conv_tbc_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, input, weight, bias, pad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_transpose3d_input, name, "aten::conv_transpose3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_transpose3d_input, overload_name, "input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_transpose3d_input, schema_str, "conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, SymInt groups=1, SymInt[3] dilation=1) -> Tensor")

// aten::conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, SymInt groups=1, SymInt[3] dilation=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv_transpose3d_input::schema> create_conv_transpose3d_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv_transpose3d_input::name, conv_transpose3d_input::overload_name)
      .typed<conv_transpose3d_input::schema>();
}

// aten::conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, SymInt groups=1, SymInt[3] dilation=1) -> Tensor
at::Tensor conv_transpose3d_input::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymInt groups, c10::SymIntArrayRef dilation) {
    
    static auto op = create_conv_transpose3d_input_typed_handle();
    return op.call(input, weight, bias, stride, padding, output_padding, groups, dilation);
}

// aten::conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, SymInt groups=1, SymInt[3] dilation=1) -> Tensor
at::Tensor conv_transpose3d_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymInt groups, c10::SymIntArrayRef dilation) {
    
    static auto op = create_conv_transpose3d_input_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, output_padding, groups, dilation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy, name, "aten::copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy, schema_str, "copy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor")

// aten::copy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<copy::schema> create_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(copy::name, copy::overload_name)
      .typed<copy::schema>();
}

// aten::copy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor
at::Tensor copy::call(const at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    
    static auto op = create_copy_typed_handle();
    return op.call(self, src, non_blocking);
}

// aten::copy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor
at::Tensor copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    
    static auto op = create_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self, src, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy_, name, "aten::copy_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy_, schema_str, "copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)")

// aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<copy_::schema> create_copy__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(copy_::name, copy_::overload_name)
      .typed<copy_::schema>();
}

// aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
at::Tensor & copy_::call(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    
    static auto op = create_copy__typed_handle();
    return op.call(self, src, non_blocking);
}

// aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
at::Tensor & copy_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    
    static auto op = create_copy__typed_handle();
    return op.redispatch(dispatchKeySet, self, src, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_copy_from_and_resize, name, "aten::_copy_from_and_resize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_copy_from_and_resize, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_copy_from_and_resize, schema_str, "_copy_from_and_resize(Tensor self, Tensor dst) -> Tensor")

// aten::_copy_from_and_resize(Tensor self, Tensor dst) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_copy_from_and_resize::schema> create__copy_from_and_resize_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_copy_from_and_resize::name, _copy_from_and_resize::overload_name)
      .typed<_copy_from_and_resize::schema>();
}

// aten::_copy_from_and_resize(Tensor self, Tensor dst) -> Tensor
at::Tensor _copy_from_and_resize::call(const at::Tensor & self, const at::Tensor & dst) {
    
    static auto op = create__copy_from_and_resize_typed_handle();
    return op.call(self, dst);
}

// aten::_copy_from_and_resize(Tensor self, Tensor dst) -> Tensor
at::Tensor _copy_from_and_resize::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & dst) {
    
    static auto op = create__copy_from_and_resize_typed_handle();
    return op.redispatch(dispatchKeySet, self, dst);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution, name, "aten::cudnn_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution, schema_str, "cudnn_convolution(Tensor self, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor")

// aten::cudnn_convolution(Tensor self, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution::schema> create_cudnn_convolution_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution::name, cudnn_convolution::overload_name)
      .typed<cudnn_convolution::schema>();
}

// aten::cudnn_convolution(Tensor self, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
at::Tensor cudnn_convolution::call(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic, bool allow_tf32) {
    
    static auto op = create_cudnn_convolution_typed_handle();
    return op.call(self, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
}

// aten::cudnn_convolution(Tensor self, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
at::Tensor cudnn_convolution::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic, bool allow_tf32) {
    
    static auto op = create_cudnn_convolution_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_out, name, "aten::cudnn_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_out, schema_str, "cudnn_convolution.out(Tensor self, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cudnn_convolution.out(Tensor self, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_out::schema> create_cudnn_convolution_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_out::name, cudnn_convolution_out::overload_name)
      .typed<cudnn_convolution_out::schema>();
}

// aten::cudnn_convolution.out(Tensor self, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cudnn_convolution_out::call(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic, bool allow_tf32, at::Tensor & out) {
    
    static auto op = create_cudnn_convolution_out_typed_handle();
    return op.call(self, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32, out);
}

// aten::cudnn_convolution.out(Tensor self, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cudnn_convolution_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic, bool allow_tf32, at::Tensor & out) {
    
    static auto op = create_cudnn_convolution_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_relu, name, "aten::cudnn_convolution_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_relu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_relu, schema_str, "cudnn_convolution_relu(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups) -> Tensor")

// aten::cudnn_convolution_relu(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_relu::schema> create_cudnn_convolution_relu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_relu::name, cudnn_convolution_relu::overload_name)
      .typed<cudnn_convolution_relu::schema>();
}

// aten::cudnn_convolution_relu(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups) -> Tensor
at::Tensor cudnn_convolution_relu::call(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    
    static auto op = create_cudnn_convolution_relu_typed_handle();
    return op.call(self, weight, bias, stride, padding, dilation, groups);
}

// aten::cudnn_convolution_relu(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups) -> Tensor
at::Tensor cudnn_convolution_relu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    
    static auto op = create_cudnn_convolution_relu_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod, name, "aten::cumprod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod, schema_str, "cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor")

// aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cumprod::schema> create_cumprod_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumprod::name, cumprod::overload_name)
      .typed<cumprod::schema>();
}

// aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor cumprod::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_cumprod_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor cumprod::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_cumprod_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_, name, "aten::cumprod_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_, schema_str, "cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)")

// aten::cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cumprod_::schema> create_cumprod__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumprod_::name, cumprod_::overload_name)
      .typed<cumprod_::schema>();
}

// aten::cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)
at::Tensor & cumprod_::call(at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_cumprod__typed_handle();
    return op.call(self, dim, dtype);
}

// aten::cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)
at::Tensor & cumprod_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_cumprod__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_out, name, "aten::cumprod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_out, schema_str, "cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cumprod_out::schema> create_cumprod_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumprod_out::name, cumprod_out::overload_name)
      .typed<cumprod_out::schema>();
}

// aten::cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cumprod_out::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create_cumprod_out_typed_handle();
    return op.call(self, dim, dtype, out);
}

// aten::cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cumprod_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create_cumprod_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_dimname, name, "aten::cumprod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_dimname, schema_str, "cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor")

// aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cumprod_dimname::schema> create_cumprod_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumprod_dimname::name, cumprod_dimname::overload_name)
      .typed<cumprod_dimname::schema>();
}

// aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor cumprod_dimname::call(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_cumprod_dimname_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor cumprod_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_cumprod_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod__dimname, name, "aten::cumprod_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod__dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod__dimname, schema_str, "cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)")

// aten::cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cumprod__dimname::schema> create_cumprod__dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumprod__dimname::name, cumprod__dimname::overload_name)
      .typed<cumprod__dimname::schema>();
}

// aten::cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)
at::Tensor & cumprod__dimname::call(at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_cumprod__dimname_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)
at::Tensor & cumprod__dimname::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_cumprod__dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_dimname_out, name, "aten::cumprod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_dimname_out, schema_str, "cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cumprod_dimname_out::schema> create_cumprod_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumprod_dimname_out::name, cumprod_dimname_out::overload_name)
      .typed<cumprod_dimname_out::schema>();
}

// aten::cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cumprod_dimname_out::call(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create_cumprod_dimname_out_typed_handle();
    return op.call(self, dim, dtype, out);
}

// aten::cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cumprod_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create_cumprod_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumulative_trapezoid_x, name, "aten::cumulative_trapezoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumulative_trapezoid_x, overload_name, "x")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumulative_trapezoid_x, schema_str, "cumulative_trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor")

// aten::cumulative_trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cumulative_trapezoid_x::schema> create_cumulative_trapezoid_x_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumulative_trapezoid_x::name, cumulative_trapezoid_x::overload_name)
      .typed<cumulative_trapezoid_x::schema>();
}

// aten::cumulative_trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
at::Tensor cumulative_trapezoid_x::call(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
    
    static auto op = create_cumulative_trapezoid_x_typed_handle();
    return op.call(y, x, dim);
}

// aten::cumulative_trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
at::Tensor cumulative_trapezoid_x::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & y, const at::Tensor & x, int64_t dim) {
    
    static auto op = create_cumulative_trapezoid_x_typed_handle();
    return op.redispatch(dispatchKeySet, y, x, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumulative_trapezoid_dx, name, "aten::cumulative_trapezoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumulative_trapezoid_dx, overload_name, "dx")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumulative_trapezoid_dx, schema_str, "cumulative_trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor")

// aten::cumulative_trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cumulative_trapezoid_dx::schema> create_cumulative_trapezoid_dx_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumulative_trapezoid_dx::name, cumulative_trapezoid_dx::overload_name)
      .typed<cumulative_trapezoid_dx::schema>();
}

// aten::cumulative_trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
at::Tensor cumulative_trapezoid_dx::call(const at::Tensor & y, const at::Scalar & dx, int64_t dim) {
    
    static auto op = create_cumulative_trapezoid_dx_typed_handle();
    return op.call(y, dx, dim);
}

// aten::cumulative_trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
at::Tensor cumulative_trapezoid_dx::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & y, const at::Scalar & dx, int64_t dim) {
    
    static auto op = create_cumulative_trapezoid_dx_typed_handle();
    return op.redispatch(dispatchKeySet, y, dx, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ctc_loss_IntList, name, "aten::ctc_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ctc_loss_IntList, overload_name, "IntList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ctc_loss_IntList, schema_str, "ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor")

// aten::ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ctc_loss_IntList::schema> create_ctc_loss_IntList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ctc_loss_IntList::name, ctc_loss_IntList::overload_name)
      .typed<ctc_loss_IntList::schema>();
}

// aten::ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
at::Tensor ctc_loss_IntList::call(const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
    
    static auto op = create_ctc_loss_IntList_typed_handle();
    return op.call(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}

// aten::ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
at::Tensor ctc_loss_IntList::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
    
    static auto op = create_ctc_loss_IntList_typed_handle();
    return op.redispatch(dispatchKeySet, log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ctc_loss_Tensor, name, "aten::ctc_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ctc_loss_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ctc_loss_Tensor, schema_str, "ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor")

// aten::ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ctc_loss_Tensor::schema> create_ctc_loss_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ctc_loss_Tensor::name, ctc_loss_Tensor::overload_name)
      .typed<ctc_loss_Tensor::schema>();
}

// aten::ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
at::Tensor ctc_loss_Tensor::call(const at::Tensor & log_probs, const at::Tensor & targets, const at::Tensor & input_lengths, const at::Tensor & target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
    
    static auto op = create_ctc_loss_Tensor_typed_handle();
    return op.call(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}

// aten::ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
at::Tensor ctc_loss_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, const at::Tensor & input_lengths, const at::Tensor & target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
    
    static auto op = create_ctc_loss_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_embed, name, "aten::diag_embed")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_embed, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_embed, schema_str, "diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor")

// aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<diag_embed::schema> create_diag_embed_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(diag_embed::name, diag_embed::overload_name)
      .typed<diag_embed::schema>();
}

// aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor
at::Tensor diag_embed::call(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
    
    static auto op = create_diag_embed_typed_handle();
    return op.call(self, offset, dim1, dim2);
}

// aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor
at::Tensor diag_embed::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
    
    static auto op = create_diag_embed_typed_handle();
    return op.redispatch(dispatchKeySet, self, offset, dim1, dim2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal, name, "aten::diagonal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal, schema_str, "diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)")

// aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<diagonal::schema> create_diagonal_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(diagonal::name, diagonal::overload_name)
      .typed<diagonal::schema>();
}

// aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)
at::Tensor diagonal::call(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
    
    static auto op = create_diagonal_typed_handle();
    return op.call(self, offset, dim1, dim2);
}

// aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)
at::Tensor diagonal::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
    
    static auto op = create_diagonal_typed_handle();
    return op.redispatch(dispatchKeySet, self, offset, dim1, dim2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal_Dimname, name, "aten::diagonal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal_Dimname, schema_str, "diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -> Tensor(a)")

// aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<diagonal_Dimname::schema> create_diagonal_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(diagonal_Dimname::name, diagonal_Dimname::overload_name)
      .typed<diagonal_Dimname::schema>();
}

// aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -> Tensor(a)
at::Tensor diagonal_Dimname::call(const at::Tensor & self, at::Dimname outdim, at::Dimname dim1, at::Dimname dim2, int64_t offset) {
    
    static auto op = create_diagonal_Dimname_typed_handle();
    return op.call(self, outdim, dim1, dim2, offset);
}

// aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -> Tensor(a)
at::Tensor diagonal_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname outdim, at::Dimname dim1, at::Dimname dim2, int64_t offset) {
    
    static auto op = create_diagonal_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, outdim, dim1, dim2, offset);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Tensor, name, "aten::divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Tensor, schema_str, "divide.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::divide.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<divide_Tensor::schema> create_divide_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide_Tensor::name, divide_Tensor::overload_name)
      .typed<divide_Tensor::schema>();
}

// aten::divide.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor divide_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_divide_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::divide.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor divide_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_divide_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Tensor, name, "aten::divide_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Tensor, schema_str, "divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<divide__Tensor::schema> create_divide__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide__Tensor::name, divide__Tensor::overload_name)
      .typed<divide__Tensor::schema>();
}

// aten::divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & divide__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_divide__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & divide__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_divide__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_out, name, "aten::divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_out, schema_str, "divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<divide_out::schema> create_divide_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide_out::name, divide_out::overload_name)
      .typed<divide_out::schema>();
}

// aten::divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & divide_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_divide_out_typed_handle();
    return op.call(self, other, out);
}

// aten::divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & divide_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_divide_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Scalar, name, "aten::divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Scalar, schema_str, "divide.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::divide.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<divide_Scalar::schema> create_divide_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide_Scalar::name, divide_Scalar::overload_name)
      .typed<divide_Scalar::schema>();
}

// aten::divide.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor divide_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_divide_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::divide.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor divide_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_divide_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Scalar, name, "aten::divide_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Scalar, schema_str, "divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<divide__Scalar::schema> create_divide__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide__Scalar::name, divide__Scalar::overload_name)
      .typed<divide__Scalar::schema>();
}

// aten::divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & divide__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_divide__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & divide__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_divide__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Tensor_mode, name, "aten::divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Tensor_mode, overload_name, "Tensor_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Tensor_mode, schema_str, "divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor")

// aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<divide_Tensor_mode::schema> create_divide_Tensor_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide_Tensor_mode::name, divide_Tensor_mode::overload_name)
      .typed<divide_Tensor_mode::schema>();
}

// aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
at::Tensor divide_Tensor_mode::call(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_divide_Tensor_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
at::Tensor divide_Tensor_mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_divide_Tensor_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Tensor_mode, name, "aten::divide_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Tensor_mode, overload_name, "Tensor_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Tensor_mode, schema_str, "divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)")

// aten::divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<divide__Tensor_mode::schema> create_divide__Tensor_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide__Tensor_mode::name, divide__Tensor_mode::overload_name)
      .typed<divide__Tensor_mode::schema>();
}

// aten::divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & divide__Tensor_mode::call(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_divide__Tensor_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & divide__Tensor_mode::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_divide__Tensor_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_out_mode, name, "aten::divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_out_mode, overload_name, "out_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_out_mode, schema_str, "divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)")

// aten::divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<divide_out_mode::schema> create_divide_out_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide_out_mode::name, divide_out_mode::overload_name)
      .typed<divide_out_mode::schema>();
}

// aten::divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
at::Tensor & divide_out_mode::call(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    
    static auto op = create_divide_out_mode_typed_handle();
    return op.call(self, other, rounding_mode, out);
}

// aten::divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
at::Tensor & divide_out_mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    
    static auto op = create_divide_out_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Scalar_mode, name, "aten::divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Scalar_mode, overload_name, "Scalar_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Scalar_mode, schema_str, "divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor")

// aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<divide_Scalar_mode::schema> create_divide_Scalar_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide_Scalar_mode::name, divide_Scalar_mode::overload_name)
      .typed<divide_Scalar_mode::schema>();
}

// aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
at::Tensor divide_Scalar_mode::call(const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_divide_Scalar_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
at::Tensor divide_Scalar_mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_divide_Scalar_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Scalar_mode, name, "aten::divide_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Scalar_mode, overload_name, "Scalar_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Scalar_mode, schema_str, "divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)")

// aten::divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<divide__Scalar_mode::schema> create_divide__Scalar_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide__Scalar_mode::name, divide__Scalar_mode::overload_name)
      .typed<divide__Scalar_mode::schema>();
}

// aten::divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & divide__Scalar_mode::call(at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_divide__Scalar_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & divide__Scalar_mode::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_divide__Scalar_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_permuted, name, "aten::empty_permuted")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_permuted, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_permuted, schema_str, "empty_permuted(SymInt[] size, int[] physical_layout, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::empty_permuted(SymInt[] size, int[] physical_layout, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<empty_permuted::schema> create_empty_permuted_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(empty_permuted::name, empty_permuted::overload_name)
      .typed<empty_permuted::schema>();
}

// aten::empty_permuted(SymInt[] size, int[] physical_layout, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor empty_permuted::call(c10::SymIntArrayRef size, at::IntArrayRef physical_layout, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_empty_permuted_typed_handle();
    return op.call(size, physical_layout, dtype, layout, device, pin_memory);
}

// aten::empty_permuted(SymInt[] size, int[] physical_layout, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor empty_permuted::redispatch(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, at::IntArrayRef physical_layout, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_empty_permuted_typed_handle();
    return op.redispatch(dispatchKeySet, size, physical_layout, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_empty_affine_quantized, name, "aten::_empty_affine_quantized")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_empty_affine_quantized, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_empty_affine_quantized, schema_str, "_empty_affine_quantized(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor")

// aten::_empty_affine_quantized(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_empty_affine_quantized::schema> create__empty_affine_quantized_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_empty_affine_quantized::name, _empty_affine_quantized::overload_name)
      .typed<_empty_affine_quantized::schema>();
}

// aten::_empty_affine_quantized(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor
at::Tensor _empty_affine_quantized::call(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create__empty_affine_quantized_typed_handle();
    return op.call(size, dtype, layout, device, pin_memory, scale, zero_point, memory_format);
}

// aten::_empty_affine_quantized(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor
at::Tensor _empty_affine_quantized::redispatch(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create__empty_affine_quantized_typed_handle();
    return op.redispatch(dispatchKeySet, size, dtype, layout, device, pin_memory, scale, zero_point, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_resize_output_, name, "aten::_resize_output_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_resize_output_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_resize_output_, schema_str, "_resize_output_(Tensor(a!) self, SymInt[] size, Device device) -> Tensor(a!)")

// aten::_resize_output_(Tensor(a!) self, SymInt[] size, Device device) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_resize_output_::schema> create__resize_output__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_resize_output_::name, _resize_output_::overload_name)
      .typed<_resize_output_::schema>();
}

// aten::_resize_output_(Tensor(a!) self, SymInt[] size, Device device) -> Tensor(a!)
const at::Tensor & _resize_output_::call(const at::Tensor & self, c10::SymIntArrayRef size, at::Device device) {
    
    static auto op = create__resize_output__typed_handle();
    return op.call(self, size, device);
}

// aten::_resize_output_(Tensor(a!) self, SymInt[] size, Device device) -> Tensor(a!)
const at::Tensor & _resize_output_::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, at::Device device) {
    
    static auto op = create__resize_output__typed_handle();
    return op.redispatch(dispatchKeySet, self, size, device);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_like, name, "aten::empty_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_like, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_like, schema_str, "empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<empty_like::schema> create_empty_like_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(empty_like::name, empty_like::overload_name)
      .typed<empty_like::schema>();
}

// aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor empty_like::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_empty_like_typed_handle();
    return op.call(self, dtype, layout, device, pin_memory, memory_format);
}

// aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor empty_like::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_empty_like_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expand, name, "aten::expand")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expand, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expand, schema_str, "expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)")

// aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<expand::schema> create_expand_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(expand::name, expand::overload_name)
      .typed<expand::schema>();
}

// aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)
at::Tensor expand::call(const at::Tensor & self, c10::SymIntArrayRef size, bool implicit) {
    
    static auto op = create_expand_typed_handle();
    return op.call(self, size, implicit);
}

// aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)
at::Tensor expand::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, bool implicit) {
    
    static auto op = create_expand_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, implicit);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_using_ints, name, "aten::flatten")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_using_ints, overload_name, "using_ints")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_using_ints, schema_str, "flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)")

// aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<flatten_using_ints::schema> create_flatten_using_ints_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(flatten_using_ints::name, flatten_using_ints::overload_name)
      .typed<flatten_using_ints::schema>();
}

// aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)
at::Tensor flatten_using_ints::call(const at::Tensor & self, int64_t start_dim, int64_t end_dim) {
    
    static auto op = create_flatten_using_ints_typed_handle();
    return op.call(self, start_dim, end_dim);
}

// aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)
at::Tensor flatten_using_ints::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t start_dim, int64_t end_dim) {
    
    static auto op = create_flatten_using_ints_typed_handle();
    return op.redispatch(dispatchKeySet, self, start_dim, end_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_named_out_dim, name, "aten::flatten")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_named_out_dim, overload_name, "named_out_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_named_out_dim, schema_str, "flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -> Tensor(a)")

// aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<flatten_named_out_dim::schema> create_flatten_named_out_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(flatten_named_out_dim::name, flatten_named_out_dim::overload_name)
      .typed<flatten_named_out_dim::schema>();
}

// aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -> Tensor(a)
at::Tensor flatten_named_out_dim::call(const at::Tensor & self, int64_t start_dim, int64_t end_dim, at::Dimname out_dim) {
    
    static auto op = create_flatten_named_out_dim_typed_handle();
    return op.call(self, start_dim, end_dim, out_dim);
}

// aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -> Tensor(a)
at::Tensor flatten_named_out_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t start_dim, int64_t end_dim, at::Dimname out_dim) {
    
    static auto op = create_flatten_named_out_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, start_dim, end_dim, out_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_using_names, name, "aten::flatten")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_using_names, overload_name, "using_names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_using_names, schema_str, "flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -> Tensor(a)")

// aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<flatten_using_names::schema> create_flatten_using_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(flatten_using_names::name, flatten_using_names::overload_name)
      .typed<flatten_using_names::schema>();
}

// aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -> Tensor(a)
at::Tensor flatten_using_names::call(const at::Tensor & self, at::Dimname start_dim, at::Dimname end_dim, at::Dimname out_dim) {
    
    static auto op = create_flatten_using_names_typed_handle();
    return op.call(self, start_dim, end_dim, out_dim);
}

// aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -> Tensor(a)
at::Tensor flatten_using_names::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname start_dim, at::Dimname end_dim, at::Dimname out_dim) {
    
    static auto op = create_flatten_using_names_typed_handle();
    return op.redispatch(dispatchKeySet, self, start_dim, end_dim, out_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_DimnameList, name, "aten::flatten")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_DimnameList, overload_name, "DimnameList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_DimnameList, schema_str, "flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -> Tensor(a)")

// aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<flatten_DimnameList::schema> create_flatten_DimnameList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(flatten_DimnameList::name, flatten_DimnameList::overload_name)
      .typed<flatten_DimnameList::schema>();
}

// aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -> Tensor(a)
at::Tensor flatten_DimnameList::call(const at::Tensor & self, at::DimnameList dims, at::Dimname out_dim) {
    
    static auto op = create_flatten_DimnameList_typed_handle();
    return op.call(self, dims, out_dim);
}

// aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -> Tensor(a)
at::Tensor flatten_DimnameList::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dims, at::Dimname out_dim) {
    
    static auto op = create_flatten_DimnameList_typed_handle();
    return op.redispatch(dispatchKeySet, self, dims, out_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor, name, "aten::floor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor, schema_str, "floor(Tensor self) -> Tensor")

// aten::floor(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<floor::schema> create_floor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(floor::name, floor::overload_name)
      .typed<floor::schema>();
}

// aten::floor(Tensor self) -> Tensor
at::Tensor floor::call(const at::Tensor & self) {
    
    static auto op = create_floor_typed_handle();
    return op.call(self);
}

// aten::floor(Tensor self) -> Tensor
at::Tensor floor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_floor_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_, name, "aten::floor_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_, schema_str, "floor_(Tensor(a!) self) -> Tensor(a!)")

// aten::floor_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<floor_::schema> create_floor__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(floor_::name, floor_::overload_name)
      .typed<floor_::schema>();
}

// aten::floor_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & floor_::call(at::Tensor & self) {
    
    static auto op = create_floor__typed_handle();
    return op.call(self);
}

// aten::floor_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & floor_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_floor__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_out, name, "aten::floor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_out, schema_str, "floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<floor_out::schema> create_floor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(floor_out::name, floor_out::overload_name)
      .typed<floor_out::schema>();
}

// aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & floor_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_floor_out_typed_handle();
    return op.call(self, out);
}

// aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & floor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_floor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d_backward, name, "aten::grid_sampler_3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d_backward, schema_str, "grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -> (Tensor, Tensor)")

// aten::grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<grid_sampler_3d_backward::schema> create_grid_sampler_3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(grid_sampler_3d_backward::name, grid_sampler_3d_backward::overload_name)
      .typed<grid_sampler_3d_backward::schema>();
}

// aten::grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> grid_sampler_3d_backward::call(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, ::std::array<bool,2> output_mask) {
    
    static auto op = create_grid_sampler_3d_backward_typed_handle();
    return op.call(grad_output, input, grid, interpolation_mode, padding_mode, align_corners, output_mask);
}

// aten::grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> grid_sampler_3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, ::std::array<bool,2> output_mask) {
    
    static auto op = create_grid_sampler_3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input, grid, interpolation_mode, padding_mode, align_corners, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hinge_embedding_loss, name, "aten::hinge_embedding_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hinge_embedding_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hinge_embedding_loss, schema_str, "hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -> Tensor")

// aten::hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hinge_embedding_loss::schema> create_hinge_embedding_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hinge_embedding_loss::name, hinge_embedding_loss::overload_name)
      .typed<hinge_embedding_loss::schema>();
}

// aten::hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -> Tensor
at::Tensor hinge_embedding_loss::call(const at::Tensor & self, const at::Tensor & target, double margin, int64_t reduction) {
    
    static auto op = create_hinge_embedding_loss_typed_handle();
    return op.call(self, target, margin, reduction);
}

// aten::hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -> Tensor
at::Tensor hinge_embedding_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, double margin, int64_t reduction) {
    
    static auto op = create_hinge_embedding_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, margin, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm, name, "aten::native_group_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm, schema_str, "native_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -> (Tensor, Tensor, Tensor)")

// aten::native_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<native_group_norm::schema> create_native_group_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_group_norm::name, native_group_norm::overload_name)
      .typed<native_group_norm::schema>();
}

// aten::native_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_group_norm::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, double eps) {
    
    static auto op = create_native_group_norm_typed_handle();
    return op.call(input, weight, bias, N, C, HxW, group, eps);
}

// aten::native_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_group_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, double eps) {
    
    static auto op = create_native_group_norm_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, N, C, HxW, group, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_r2c, name, "aten::_fft_r2c")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_r2c, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_r2c, schema_str, "_fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor")

// aten::_fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_fft_r2c::schema> create__fft_r2c_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fft_r2c::name, _fft_r2c::overload_name)
      .typed<_fft_r2c::schema>();
}

// aten::_fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor
at::Tensor _fft_r2c::call(const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided) {
    
    static auto op = create__fft_r2c_typed_handle();
    return op.call(self, dim, normalization, onesided);
}

// aten::_fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor
at::Tensor _fft_r2c::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided) {
    
    static auto op = create__fft_r2c_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, normalization, onesided);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_r2c_out, name, "aten::_fft_r2c")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_r2c_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_r2c_out, schema_str, "_fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_fft_r2c_out::schema> create__fft_r2c_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fft_r2c_out::name, _fft_r2c_out::overload_name)
      .typed<_fft_r2c_out::schema>();
}

// aten::_fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _fft_r2c_out::call(const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided, at::Tensor & out) {
    
    static auto op = create__fft_r2c_out_typed_handle();
    return op.call(self, dim, normalization, onesided, out);
}

// aten::_fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _fft_r2c_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided, at::Tensor & out) {
    
    static auto op = create__fft_r2c_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, normalization, onesided, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unsafe_index_Tensor, name, "aten::_unsafe_index")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unsafe_index_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unsafe_index_Tensor, schema_str, "_unsafe_index.Tensor(Tensor self, Tensor?[] indices) -> Tensor")

// aten::_unsafe_index.Tensor(Tensor self, Tensor?[] indices) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_unsafe_index_Tensor::schema> create__unsafe_index_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_unsafe_index_Tensor::name, _unsafe_index_Tensor::overload_name)
      .typed<_unsafe_index_Tensor::schema>();
}

// aten::_unsafe_index.Tensor(Tensor self, Tensor?[] indices) -> Tensor
at::Tensor _unsafe_index_Tensor::call(const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices) {
    
    static auto op = create__unsafe_index_Tensor_typed_handle();
    return op.call(self, indices);
}

// aten::_unsafe_index.Tensor(Tensor self, Tensor?[] indices) -> Tensor
at::Tensor _unsafe_index_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices) {
    
    static auto op = create__unsafe_index_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_neg, name, "aten::is_neg")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_neg, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_neg, schema_str, "is_neg(Tensor self) -> bool")

// aten::is_neg(Tensor self) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_neg::schema> create_is_neg_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_neg::name, is_neg::overload_name)
      .typed<is_neg::schema>();
}

// aten::is_neg(Tensor self) -> bool
bool is_neg::call(const at::Tensor & self) {
    
    static auto op = create_is_neg_typed_handle();
    return op.call(self);
}

// aten::is_neg(Tensor self) -> bool
bool is_neg::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_is_neg_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isreal, name, "aten::isreal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isreal, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isreal, schema_str, "isreal(Tensor self) -> Tensor")

// aten::isreal(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<isreal::schema> create_isreal_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isreal::name, isreal::overload_name)
      .typed<isreal::schema>();
}

// aten::isreal(Tensor self) -> Tensor
at::Tensor isreal::call(const at::Tensor & self) {
    
    static auto op = create_isreal_typed_handle();
    return op.call(self);
}

// aten::isreal(Tensor self) -> Tensor
at::Tensor isreal::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_isreal_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linear_backward, name, "aten::linear_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linear_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linear_backward, schema_str, "linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)")

// aten::linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<linear_backward::schema> create_linear_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linear_backward::name, linear_backward::overload_name)
      .typed<linear_backward::schema>();
}

// aten::linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> linear_backward::call(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask) {
    
    static auto op = create_linear_backward_typed_handle();
    return op.call(self, grad_output, weight, output_mask);
}

// aten::linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> linear_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask) {
    
    static auto op = create_linear_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, grad_output, weight, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_input, name, "aten::mkldnn_linear_backward_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_input, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_input, schema_str, "mkldnn_linear_backward_input(int[] input_size, Tensor grad_output, Tensor weight) -> Tensor")

// aten::mkldnn_linear_backward_input(int[] input_size, Tensor grad_output, Tensor weight) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_linear_backward_input::schema> create_mkldnn_linear_backward_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_linear_backward_input::name, mkldnn_linear_backward_input::overload_name)
      .typed<mkldnn_linear_backward_input::schema>();
}

// aten::mkldnn_linear_backward_input(int[] input_size, Tensor grad_output, Tensor weight) -> Tensor
at::Tensor mkldnn_linear_backward_input::call(at::IntArrayRef input_size, const at::Tensor & grad_output, const at::Tensor & weight) {
    
    static auto op = create_mkldnn_linear_backward_input_typed_handle();
    return op.call(input_size, grad_output, weight);
}

// aten::mkldnn_linear_backward_input(int[] input_size, Tensor grad_output, Tensor weight) -> Tensor
at::Tensor mkldnn_linear_backward_input::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef input_size, const at::Tensor & grad_output, const at::Tensor & weight) {
    
    static auto op = create_mkldnn_linear_backward_input_typed_handle();
    return op.redispatch(dispatchKeySet, input_size, grad_output, weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward, name, "aten::mkldnn_linear_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward, schema_str, "mkldnn_linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)")

// aten::mkldnn_linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_linear_backward::schema> create_mkldnn_linear_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_linear_backward::name, mkldnn_linear_backward::overload_name)
      .typed<mkldnn_linear_backward::schema>();
}

// aten::mkldnn_linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> mkldnn_linear_backward::call(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask) {
    
    static auto op = create_mkldnn_linear_backward_typed_handle();
    return op.call(self, grad_output, weight, output_mask);
}

// aten::mkldnn_linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> mkldnn_linear_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask) {
    
    static auto op = create_mkldnn_linear_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, grad_output, weight, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_semi_structured_linear, name, "aten::_sparse_semi_structured_linear")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_semi_structured_linear, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_semi_structured_linear, schema_str, "_sparse_semi_structured_linear(Tensor input, Tensor weight, Tensor meta, *, Tensor? bias=None, str? activation=None, ScalarType? out_dtype=None) -> Tensor")

// aten::_sparse_semi_structured_linear(Tensor input, Tensor weight, Tensor meta, *, Tensor? bias=None, str? activation=None, ScalarType? out_dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_semi_structured_linear::schema> create__sparse_semi_structured_linear_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_semi_structured_linear::name, _sparse_semi_structured_linear::overload_name)
      .typed<_sparse_semi_structured_linear::schema>();
}

// aten::_sparse_semi_structured_linear(Tensor input, Tensor weight, Tensor meta, *, Tensor? bias=None, str? activation=None, ScalarType? out_dtype=None) -> Tensor
at::Tensor _sparse_semi_structured_linear::call(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & meta, const c10::optional<at::Tensor> & bias, c10::optional<c10::string_view> activation, c10::optional<at::ScalarType> out_dtype) {
    
    static auto op = create__sparse_semi_structured_linear_typed_handle();
    return op.call(input, weight, meta, bias, activation, out_dtype);
}

// aten::_sparse_semi_structured_linear(Tensor input, Tensor weight, Tensor meta, *, Tensor? bias=None, str? activation=None, ScalarType? out_dtype=None) -> Tensor
at::Tensor _sparse_semi_structured_linear::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & meta, const c10::optional<at::Tensor> & bias, c10::optional<c10::string_view> activation, c10::optional<at::ScalarType> out_dtype) {
    
    static auto op = create__sparse_semi_structured_linear_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, meta, bias, activation, out_dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_logcumsumexp, name, "aten::_logcumsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_logcumsumexp, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_logcumsumexp, schema_str, "_logcumsumexp(Tensor self, int dim) -> Tensor")

// aten::_logcumsumexp(Tensor self, int dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_logcumsumexp::schema> create__logcumsumexp_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_logcumsumexp::name, _logcumsumexp::overload_name)
      .typed<_logcumsumexp::schema>();
}

// aten::_logcumsumexp(Tensor self, int dim) -> Tensor
at::Tensor _logcumsumexp::call(const at::Tensor & self, int64_t dim) {
    
    static auto op = create__logcumsumexp_typed_handle();
    return op.call(self, dim);
}

// aten::_logcumsumexp(Tensor self, int dim) -> Tensor
at::Tensor _logcumsumexp::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    
    static auto op = create__logcumsumexp_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_logcumsumexp_out, name, "aten::_logcumsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_logcumsumexp_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_logcumsumexp_out, schema_str, "_logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_logcumsumexp_out::schema> create__logcumsumexp_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_logcumsumexp_out::name, _logcumsumexp_out::overload_name)
      .typed<_logcumsumexp_out::schema>();
}

// aten::_logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _logcumsumexp_out::call(const at::Tensor & self, int64_t dim, at::Tensor & out) {
    
    static auto op = create__logcumsumexp_out_typed_handle();
    return op.call(self, dim, out);
}

// aten::_logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _logcumsumexp_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
    
    static auto op = create__logcumsumexp_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(value_selecting_reduction_backward, name, "aten::value_selecting_reduction_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(value_selecting_reduction_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(value_selecting_reduction_backward, schema_str, "value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor")

// aten::value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<value_selecting_reduction_backward::schema> create_value_selecting_reduction_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(value_selecting_reduction_backward::name, value_selecting_reduction_backward::overload_name)
      .typed<value_selecting_reduction_backward::schema>();
}

// aten::value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor
at::Tensor value_selecting_reduction_backward::call(const at::Tensor & grad, int64_t dim, const at::Tensor & indices, c10::SymIntArrayRef sizes, bool keepdim) {
    
    static auto op = create_value_selecting_reduction_backward_typed_handle();
    return op.call(grad, dim, indices, sizes, keepdim);
}

// aten::value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor
at::Tensor value_selecting_reduction_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, int64_t dim, const at::Tensor & indices, c10::SymIntArrayRef sizes, bool keepdim) {
    
    static auto op = create_value_selecting_reduction_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, dim, indices, sizes, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool1d, name, "aten::max_pool1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool1d, schema_str, "max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor")

// aten::max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max_pool1d::schema> create_max_pool1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_pool1d::name, max_pool1d::overload_name)
      .typed<max_pool1d::schema>();
}

// aten::max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor max_pool1d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    
    static auto op = create_max_pool1d_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor max_pool1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    
    static auto op = create_max_pool1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d, name, "aten::max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d, schema_str, "max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor")

// aten::max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max_pool2d::schema> create_max_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_pool2d::name, max_pool2d::overload_name)
      .typed<max_pool2d::schema>();
}

// aten::max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor max_pool2d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    
    static auto op = create_max_pool2d_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor max_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    
    static auto op = create_max_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean, name, "aten::mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean, schema_str, "mean(Tensor self, *, ScalarType? dtype=None) -> Tensor")

// aten::mean(Tensor self, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mean::schema> create_mean_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mean::name, mean::overload_name)
      .typed<mean::schema>();
}

// aten::mean(Tensor self, *, ScalarType? dtype=None) -> Tensor
at::Tensor mean::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_mean_typed_handle();
    return op.call(self, dtype);
}

// aten::mean(Tensor self, *, ScalarType? dtype=None) -> Tensor
at::Tensor mean::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_mean_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_dim, name, "aten::mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_dim, schema_str, "mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mean_dim::schema> create_mean_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mean_dim::name, mean_dim::overload_name)
      .typed<mean_dim::schema>();
}

// aten::mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor mean_dim::call(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_mean_dim_typed_handle();
    return op.call(self, dim, keepdim, dtype);
}

// aten::mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor mean_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_mean_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_out, name, "aten::mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_out, schema_str, "mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mean_out::schema> create_mean_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mean_out::name, mean_out::overload_name)
      .typed<mean_out::schema>();
}

// aten::mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mean_out::call(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create_mean_out_typed_handle();
    return op.call(self, dim, keepdim, dtype, out);
}

// aten::mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mean_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create_mean_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_names_dim, name, "aten::mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_names_dim, overload_name, "names_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_names_dim, schema_str, "mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mean_names_dim::schema> create_mean_names_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mean_names_dim::name, mean_names_dim::overload_name)
      .typed<mean_names_dim::schema>();
}

// aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor mean_names_dim::call(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_mean_names_dim_typed_handle();
    return op.call(self, dim, keepdim, dtype);
}

// aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor mean_names_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_mean_names_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_names_out, name, "aten::mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_names_out, overload_name, "names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_names_out, schema_str, "mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mean_names_out::schema> create_mean_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mean_names_out::name, mean_names_out::overload_name)
      .typed<mean_names_out::schema>();
}

// aten::mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mean_names_out::call(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create_mean_names_out_typed_handle();
    return op.call(self, dim, keepdim, dtype, out);
}

// aten::mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mean_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create_mean_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmean, name, "aten::nanmean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmean, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmean, schema_str, "nanmean(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::nanmean(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nanmean::schema> create_nanmean_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanmean::name, nanmean::overload_name)
      .typed<nanmean::schema>();
}

// aten::nanmean(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor nanmean::call(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_nanmean_typed_handle();
    return op.call(self, dim, keepdim, dtype);
}

// aten::nanmean(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor nanmean::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_nanmean_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmean_out, name, "aten::nanmean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmean_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmean_out, schema_str, "nanmean.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::nanmean.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nanmean_out::schema> create_nanmean_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanmean_out::name, nanmean_out::overload_name)
      .typed<nanmean_out::schema>();
}

// aten::nanmean.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nanmean_out::call(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create_nanmean_out_typed_handle();
    return op.call(self, dim, keepdim, dtype, out);
}

// aten::nanmean.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nanmean_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create_nanmean_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_dim, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_dim, schema_str, "min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<min_dim::schema> create_min_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min_dim::name, min_dim::overload_name)
      .typed<min_dim::schema>();
}

// aten::min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> min_dim::call(const at::Tensor & self, int64_t dim, bool keepdim) {
    
    static auto op = create_min_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> min_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
    
    static auto op = create_min_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_dim_min, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_dim_min, overload_name, "dim_min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_dim_min, schema_str, "min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<min_dim_min::schema> create_min_dim_min_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min_dim_min::name, min_dim_min::overload_name)
      .typed<min_dim_min::schema>();
}

// aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> min_dim_min::call(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
    
    static auto op = create_min_dim_min_typed_handle();
    return op.call(self, dim, keepdim, min, min_indices);
}

// aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> min_dim_min::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
    
    static auto op = create_min_dim_min_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, min, min_indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_names_dim, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_names_dim, overload_name, "names_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_names_dim, schema_str, "min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<min_names_dim::schema> create_min_names_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min_names_dim::name, min_names_dim::overload_name)
      .typed<min_names_dim::schema>();
}

// aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> min_names_dim::call(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    
    static auto op = create_min_names_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> min_names_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
    
    static auto op = create_min_names_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_names_dim_min, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_names_dim_min, overload_name, "names_dim_min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_names_dim_min, schema_str, "min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<min_names_dim_min::schema> create_min_names_dim_min_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min_names_dim_min::name, min_names_dim_min::overload_name)
      .typed<min_names_dim_min::schema>();
}

// aten::min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> min_names_dim_min::call(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
    
    static auto op = create_min_names_dim_min_typed_handle();
    return op.call(self, dim, keepdim, min, min_indices);
}

// aten::min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> min_names_dim_min::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
    
    static auto op = create_min_names_dim_min_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, min, min_indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mm, name, "aten::mm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mm, schema_str, "mm(Tensor self, Tensor mat2) -> Tensor")

// aten::mm(Tensor self, Tensor mat2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mm::schema> create_mm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mm::name, mm::overload_name)
      .typed<mm::schema>();
}

// aten::mm(Tensor self, Tensor mat2) -> Tensor
at::Tensor mm::call(const at::Tensor & self, const at::Tensor & mat2) {
    
    static auto op = create_mm_typed_handle();
    return op.call(self, mat2);
}

// aten::mm(Tensor self, Tensor mat2) -> Tensor
at::Tensor mm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2) {
    
    static auto op = create_mm_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mm_out, name, "aten::mm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mm_out, schema_str, "mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)")

// aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mm_out::schema> create_mm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mm_out::name, mm_out::overload_name)
      .typed<mm_out::schema>();
}

// aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mm_out::call(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
    
    static auto op = create_mm_out_typed_handle();
    return op.call(self, mat2, out);
}

// aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
    
    static auto op = create_mm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat2, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_int4pack_mm, name, "aten::_weight_int4pack_mm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_int4pack_mm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_int4pack_mm, schema_str, "_weight_int4pack_mm(Tensor self, Tensor mat2, int qGroupSize, Tensor qScaleAndZeros) -> Tensor")

// aten::_weight_int4pack_mm(Tensor self, Tensor mat2, int qGroupSize, Tensor qScaleAndZeros) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_weight_int4pack_mm::schema> create__weight_int4pack_mm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_weight_int4pack_mm::name, _weight_int4pack_mm::overload_name)
      .typed<_weight_int4pack_mm::schema>();
}

// aten::_weight_int4pack_mm(Tensor self, Tensor mat2, int qGroupSize, Tensor qScaleAndZeros) -> Tensor
at::Tensor _weight_int4pack_mm::call(const at::Tensor & self, const at::Tensor & mat2, int64_t qGroupSize, const at::Tensor & qScaleAndZeros) {
    
    static auto op = create__weight_int4pack_mm_typed_handle();
    return op.call(self, mat2, qGroupSize, qScaleAndZeros);
}

// aten::_weight_int4pack_mm(Tensor self, Tensor mat2, int qGroupSize, Tensor qScaleAndZeros) -> Tensor
at::Tensor _weight_int4pack_mm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, int64_t qGroupSize, const at::Tensor & qScaleAndZeros) {
    
    static auto op = create__weight_int4pack_mm_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat2, qGroupSize, qScaleAndZeros);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mv, name, "aten::mv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mv, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mv, schema_str, "mv(Tensor self, Tensor vec) -> Tensor")

// aten::mv(Tensor self, Tensor vec) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mv::schema> create_mv_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mv::name, mv::overload_name)
      .typed<mv::schema>();
}

// aten::mv(Tensor self, Tensor vec) -> Tensor
at::Tensor mv::call(const at::Tensor & self, const at::Tensor & vec) {
    
    static auto op = create_mv_typed_handle();
    return op.call(self, vec);
}

// aten::mv(Tensor self, Tensor vec) -> Tensor
at::Tensor mv::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec) {
    
    static auto op = create_mv_typed_handle();
    return op.redispatch(dispatchKeySet, self, vec);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mv_out, name, "aten::mv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mv_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mv_out, schema_str, "mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)")

// aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mv_out::schema> create_mv_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mv_out::name, mv_out::overload_name)
      .typed<mv_out::schema>();
}

// aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mv_out::call(const at::Tensor & self, const at::Tensor & vec, at::Tensor & out) {
    
    static auto op = create_mv_out_typed_handle();
    return op.call(self, vec, out);
}

// aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mv_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec, at::Tensor & out) {
    
    static auto op = create_mv_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, vec, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_copy, name, "aten::narrow_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_copy, schema_str, "narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -> Tensor")

// aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<narrow_copy::schema> create_narrow_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(narrow_copy::name, narrow_copy::overload_name)
      .typed<narrow_copy::schema>();
}

// aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -> Tensor
at::Tensor narrow_copy::call(const at::Tensor & self, int64_t dim, c10::SymInt start, c10::SymInt length) {
    
    static auto op = create_narrow_copy_typed_handle();
    return op.call(self, dim, start, length);
}

// aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -> Tensor
at::Tensor narrow_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::SymInt start, c10::SymInt length) {
    
    static auto op = create_narrow_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, start, length);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_copy_out, name, "aten::narrow_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_copy_out, schema_str, "narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -> Tensor(a!)")

// aten::narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<narrow_copy_out::schema> create_narrow_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(narrow_copy_out::name, narrow_copy_out::overload_name)
      .typed<narrow_copy_out::schema>();
}

// aten::narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & narrow_copy_out::call(const at::Tensor & self, int64_t dim, c10::SymInt start, c10::SymInt length, at::Tensor & out) {
    
    static auto op = create_narrow_copy_out_typed_handle();
    return op.call(self, dim, start, length, out);
}

// aten::narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & narrow_copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::SymInt start, c10::SymInt length, at::Tensor & out) {
    
    static auto op = create_narrow_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, start, length, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_native_batch_norm_legit_no_training, name, "aten::_native_batch_norm_legit_no_training")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_native_batch_norm_legit_no_training, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_native_batch_norm_legit_no_training, schema_str, "_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)")

// aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_native_batch_norm_legit_no_training::schema> create__native_batch_norm_legit_no_training_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_native_batch_norm_legit_no_training::name, _native_batch_norm_legit_no_training::overload_name)
      .typed<_native_batch_norm_legit_no_training::schema>();
}

// aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _native_batch_norm_legit_no_training::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & running_mean, const at::Tensor & running_var, double momentum, double eps) {
    
    static auto op = create__native_batch_norm_legit_no_training_typed_handle();
    return op.call(input, weight, bias, running_mean, running_var, momentum, eps);
}

// aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _native_batch_norm_legit_no_training::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & running_mean, const at::Tensor & running_var, double momentum, double eps) {
    
    static auto op = create__native_batch_norm_legit_no_training_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, running_mean, running_var, momentum, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats_with_counts, name, "aten::batch_norm_gather_stats_with_counts")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats_with_counts, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats_with_counts, schema_str, "batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)")

// aten::batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_gather_stats_with_counts::schema> create_batch_norm_gather_stats_with_counts_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_gather_stats_with_counts::name, batch_norm_gather_stats_with_counts::overload_name)
      .typed<batch_norm_gather_stats_with_counts::schema>();
}

// aten::batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> batch_norm_gather_stats_with_counts::call(const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, const at::Tensor & counts) {
    
    static auto op = create_batch_norm_gather_stats_with_counts_typed_handle();
    return op.call(input, mean, invstd, running_mean, running_var, momentum, eps, counts);
}

// aten::batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> batch_norm_gather_stats_with_counts::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, const at::Tensor & counts) {
    
    static auto op = create_batch_norm_gather_stats_with_counts_typed_handle();
    return op.redispatch(dispatchKeySet, input, mean, invstd, running_mean, running_var, momentum, eps, counts);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pairwise_distance, name, "aten::pairwise_distance")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pairwise_distance, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pairwise_distance, schema_str, "pairwise_distance(Tensor x1, Tensor x2, float p=2, float eps=1e-06, bool keepdim=False) -> Tensor")

// aten::pairwise_distance(Tensor x1, Tensor x2, float p=2, float eps=1e-06, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<pairwise_distance::schema> create_pairwise_distance_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pairwise_distance::name, pairwise_distance::overload_name)
      .typed<pairwise_distance::schema>();
}

// aten::pairwise_distance(Tensor x1, Tensor x2, float p=2, float eps=1e-06, bool keepdim=False) -> Tensor
at::Tensor pairwise_distance::call(const at::Tensor & x1, const at::Tensor & x2, double p, double eps, bool keepdim) {
    
    static auto op = create_pairwise_distance_typed_handle();
    return op.call(x1, x2, p, eps, keepdim);
}

// aten::pairwise_distance(Tensor x1, Tensor x2, float p=2, float eps=1e-06, bool keepdim=False) -> Tensor
at::Tensor pairwise_distance::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2, double p, double eps, bool keepdim) {
    
    static auto op = create_pairwise_distance_typed_handle();
    return op.redispatch(dispatchKeySet, x1, x2, p, eps, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pdist_backward, name, "aten::_pdist_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pdist_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pdist_backward, schema_str, "_pdist_backward(Tensor grad, Tensor self, float p, Tensor pdist) -> Tensor")

// aten::_pdist_backward(Tensor grad, Tensor self, float p, Tensor pdist) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_pdist_backward::schema> create__pdist_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_pdist_backward::name, _pdist_backward::overload_name)
      .typed<_pdist_backward::schema>();
}

// aten::_pdist_backward(Tensor grad, Tensor self, float p, Tensor pdist) -> Tensor
at::Tensor _pdist_backward::call(const at::Tensor & grad, const at::Tensor & self, double p, const at::Tensor & pdist) {
    
    static auto op = create__pdist_backward_typed_handle();
    return op.call(grad, self, p, pdist);
}

// aten::_pdist_backward(Tensor grad, Tensor self, float p, Tensor pdist) -> Tensor
at::Tensor _pdist_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, double p, const at::Tensor & pdist) {
    
    static auto op = create__pdist_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self, p, pdist);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(permute, name, "aten::permute")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(permute, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(permute, schema_str, "permute(Tensor(a) self, int[] dims) -> Tensor(a)")

// aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<permute::schema> create_permute_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(permute::name, permute::overload_name)
      .typed<permute::schema>();
}

// aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)
at::Tensor permute::call(const at::Tensor & self, at::IntArrayRef dims) {
    
    static auto op = create_permute_typed_handle();
    return op.call(self, dims);
}

// aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)
at::Tensor permute::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dims) {
    
    static auto op = create_permute_typed_handle();
    return op.redispatch(dispatchKeySet, self, dims);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_H, name, "aten::matrix_H")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_H, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_H, schema_str, "matrix_H(Tensor(a) self) -> Tensor(a)")

// aten::matrix_H(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<matrix_H::schema> create_matrix_H_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(matrix_H::name, matrix_H::overload_name)
      .typed<matrix_H::schema>();
}

// aten::matrix_H(Tensor(a) self) -> Tensor(a)
at::Tensor matrix_H::call(const at::Tensor & self) {
    
    static auto op = create_matrix_H_typed_handle();
    return op.call(self);
}

// aten::matrix_H(Tensor(a) self) -> Tensor(a)
at::Tensor matrix_H::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_matrix_H_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pixel_shuffle, name, "aten::pixel_shuffle")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pixel_shuffle, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pixel_shuffle, schema_str, "pixel_shuffle(Tensor self, int upscale_factor) -> Tensor")

// aten::pixel_shuffle(Tensor self, int upscale_factor) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<pixel_shuffle::schema> create_pixel_shuffle_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pixel_shuffle::name, pixel_shuffle::overload_name)
      .typed<pixel_shuffle::schema>();
}

// aten::pixel_shuffle(Tensor self, int upscale_factor) -> Tensor
at::Tensor pixel_shuffle::call(const at::Tensor & self, int64_t upscale_factor) {
    
    static auto op = create_pixel_shuffle_typed_handle();
    return op.call(self, upscale_factor);
}

// aten::pixel_shuffle(Tensor self, int upscale_factor) -> Tensor
at::Tensor pixel_shuffle::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t upscale_factor) {
    
    static auto op = create_pixel_shuffle_typed_handle();
    return op.redispatch(dispatchKeySet, self, upscale_factor);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pinverse, name, "aten::pinverse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pinverse, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pinverse, schema_str, "pinverse(Tensor self, float rcond=1e-15) -> Tensor")

// aten::pinverse(Tensor self, float rcond=1e-15) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<pinverse::schema> create_pinverse_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pinverse::name, pinverse::overload_name)
      .typed<pinverse::schema>();
}

// aten::pinverse(Tensor self, float rcond=1e-15) -> Tensor
at::Tensor pinverse::call(const at::Tensor & self, double rcond) {
    
    static auto op = create_pinverse_typed_handle();
    return op.call(self, rcond);
}

// aten::pinverse(Tensor self, float rcond=1e-15) -> Tensor
at::Tensor pinverse::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double rcond) {
    
    static auto op = create_pinverse_typed_handle();
    return op.redispatch(dispatchKeySet, self, rcond);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reshape, name, "aten::reshape")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reshape, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reshape, schema_str, "reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)")

// aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<reshape::schema> create_reshape_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reshape::name, reshape::overload_name)
      .typed<reshape::schema>();
}

// aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)
at::Tensor reshape::call(const at::Tensor & self, c10::SymIntArrayRef shape) {
    
    static auto op = create_reshape_typed_handle();
    return op.call(self, shape);
}

// aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)
at::Tensor reshape::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef shape) {
    
    static auto op = create_reshape_typed_handle();
    return op.redispatch(dispatchKeySet, self, shape);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_alias, name, "aten::_reshape_alias")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_alias, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_alias, schema_str, "_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -> Tensor(a)")

// aten::_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<_reshape_alias::schema> create__reshape_alias_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_reshape_alias::name, _reshape_alias::overload_name)
      .typed<_reshape_alias::schema>();
}

// aten::_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -> Tensor(a)
at::Tensor _reshape_alias::call(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
    
    static auto op = create__reshape_alias_typed_handle();
    return op.call(self, size, stride);
}

// aten::_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -> Tensor(a)
at::Tensor _reshape_alias::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
    
    static auto op = create__reshape_alias_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, stride);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_Dimname, name, "aten::select")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_Dimname, schema_str, "select.Dimname(Tensor(a) self, Dimname dim, int index) -> Tensor(a)")

// aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<select_Dimname::schema> create_select_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(select_Dimname::name, select_Dimname::overload_name)
      .typed<select_Dimname::schema>();
}

// aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -> Tensor(a)
at::Tensor select_Dimname::call(const at::Tensor & self, at::Dimname dim, int64_t index) {
    
    static auto op = create_select_Dimname_typed_handle();
    return op.call(self, dim, index);
}

// aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -> Tensor(a)
at::Tensor select_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, int64_t index) {
    
    static auto op = create_select_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_int, name, "aten::select")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_int, schema_str, "select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)")

// aten::select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<select_int::schema> create_select_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(select_int::name, select_int::overload_name)
      .typed<select_int::schema>();
}

// aten::select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)
at::Tensor select_int::call(const at::Tensor & self, int64_t dim, c10::SymInt index) {
    
    static auto op = create_select_int_typed_handle();
    return op.call(self, dim, index);
}

// aten::select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)
at::Tensor select_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::SymInt index) {
    
    static auto op = create_select_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu, name, "aten::celu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu, schema_str, "celu(Tensor self, Scalar alpha=1.0) -> Tensor")

// aten::celu(Tensor self, Scalar alpha=1.0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<celu::schema> create_celu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(celu::name, celu::overload_name)
      .typed<celu::schema>();
}

// aten::celu(Tensor self, Scalar alpha=1.0) -> Tensor
at::Tensor celu::call(const at::Tensor & self, const at::Scalar & alpha) {
    
    static auto op = create_celu_typed_handle();
    return op.call(self, alpha);
}

// aten::celu(Tensor self, Scalar alpha=1.0) -> Tensor
at::Tensor celu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & alpha) {
    
    static auto op = create_celu_typed_handle();
    return op.redispatch(dispatchKeySet, self, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu_, name, "aten::celu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu_, schema_str, "celu_(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)")

// aten::celu_(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<celu_::schema> create_celu__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(celu_::name, celu_::overload_name)
      .typed<celu_::schema>();
}

// aten::celu_(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)
at::Tensor & celu_::call(at::Tensor & self, const at::Scalar & alpha) {
    
    static auto op = create_celu__typed_handle();
    return op.call(self, alpha);
}

// aten::celu_(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)
at::Tensor & celu_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & alpha) {
    
    static auto op = create_celu__typed_handle();
    return op.redispatch(dispatchKeySet, self, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu, name, "aten::silu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu, schema_str, "silu(Tensor self) -> Tensor")

// aten::silu(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<silu::schema> create_silu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(silu::name, silu::overload_name)
      .typed<silu::schema>();
}

// aten::silu(Tensor self) -> Tensor
at::Tensor silu::call(const at::Tensor & self) {
    
    static auto op = create_silu_typed_handle();
    return op.call(self);
}

// aten::silu(Tensor self) -> Tensor
at::Tensor silu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_silu_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_, name, "aten::silu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_, schema_str, "silu_(Tensor(a!) self) -> Tensor(a!)")

// aten::silu_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<silu_::schema> create_silu__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(silu_::name, silu_::overload_name)
      .typed<silu_::schema>();
}

// aten::silu_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & silu_::call(at::Tensor & self) {
    
    static auto op = create_silu__typed_handle();
    return op.call(self);
}

// aten::silu_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & silu_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_silu__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_out, name, "aten::silu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_out, schema_str, "silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<silu_out::schema> create_silu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(silu_out::name, silu_out::overload_name)
      .typed<silu_out::schema>();
}

// aten::silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & silu_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_silu_out_typed_handle();
    return op.call(self, out);
}

// aten::silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & silu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_silu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_backward, name, "aten::mish_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_backward, schema_str, "mish_backward(Tensor grad_output, Tensor self) -> Tensor")

// aten::mish_backward(Tensor grad_output, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mish_backward::schema> create_mish_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mish_backward::name, mish_backward::overload_name)
      .typed<mish_backward::schema>();
}

// aten::mish_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor mish_backward::call(const at::Tensor & grad_output, const at::Tensor & self) {
    
    static auto op = create_mish_backward_typed_handle();
    return op.call(grad_output, self);
}

// aten::mish_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor mish_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
    
    static auto op = create_mish_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit, name, "aten::logit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit, schema_str, "logit(Tensor self, float? eps=None) -> Tensor")

// aten::logit(Tensor self, float? eps=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logit::schema> create_logit_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logit::name, logit::overload_name)
      .typed<logit::schema>();
}

// aten::logit(Tensor self, float? eps=None) -> Tensor
at::Tensor logit::call(const at::Tensor & self, c10::optional<double> eps) {
    
    static auto op = create_logit_typed_handle();
    return op.call(self, eps);
}

// aten::logit(Tensor self, float? eps=None) -> Tensor
at::Tensor logit::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> eps) {
    
    static auto op = create_logit_typed_handle();
    return op.redispatch(dispatchKeySet, self, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_, name, "aten::logit_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_, schema_str, "logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)")

// aten::logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logit_::schema> create_logit__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logit_::name, logit_::overload_name)
      .typed<logit_::schema>();
}

// aten::logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)
at::Tensor & logit_::call(at::Tensor & self, c10::optional<double> eps) {
    
    static auto op = create_logit__typed_handle();
    return op.call(self, eps);
}

// aten::logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)
at::Tensor & logit_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<double> eps) {
    
    static auto op = create_logit__typed_handle();
    return op.redispatch(dispatchKeySet, self, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_out, name, "aten::logit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_out, schema_str, "logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logit_out::schema> create_logit_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logit_out::name, logit_out::overload_name)
      .typed<logit_out::schema>();
}

// aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logit_out::call(const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
    
    static auto op = create_logit_out_typed_handle();
    return op.call(self, eps, out);
}

// aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logit_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
    
    static auto op = create_logit_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, eps, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh, name, "aten::sinh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh, schema_str, "sinh(Tensor self) -> Tensor")

// aten::sinh(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sinh::schema> create_sinh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sinh::name, sinh::overload_name)
      .typed<sinh::schema>();
}

// aten::sinh(Tensor self) -> Tensor
at::Tensor sinh::call(const at::Tensor & self) {
    
    static auto op = create_sinh_typed_handle();
    return op.call(self);
}

// aten::sinh(Tensor self) -> Tensor
at::Tensor sinh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_sinh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh_, name, "aten::sinh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh_, schema_str, "sinh_(Tensor(a!) self) -> Tensor(a!)")

// aten::sinh_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sinh_::schema> create_sinh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sinh_::name, sinh_::overload_name)
      .typed<sinh_::schema>();
}

// aten::sinh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sinh_::call(at::Tensor & self) {
    
    static auto op = create_sinh__typed_handle();
    return op.call(self);
}

// aten::sinh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sinh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_sinh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh_out, name, "aten::sinh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh_out, schema_str, "sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sinh_out::schema> create_sinh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sinh_out::name, sinh_out::overload_name)
      .typed<sinh_out::schema>();
}

// aten::sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sinh_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_sinh_out_typed_handle();
    return op.call(self, out);
}

// aten::sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sinh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_sinh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_backward, name, "aten::slice_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_backward, schema_str, "slice_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step) -> Tensor")

// aten::slice_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<slice_backward::schema> create_slice_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slice_backward::name, slice_backward::overload_name)
      .typed<slice_backward::schema>();
}

// aten::slice_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step) -> Tensor
at::Tensor slice_backward::call(const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt start, c10::SymInt end, c10::SymInt step) {
    
    static auto op = create_slice_backward_typed_handle();
    return op.call(grad_output, input_sizes, dim, start, end, step);
}

// aten::slice_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step) -> Tensor
at::Tensor slice_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt start, c10::SymInt end, c10::SymInt step) {
    
    static auto op = create_slice_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input_sizes, dim, start, end, step);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_int, name, "aten::softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_int, schema_str, "softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor")

// aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<softmax_int::schema> create_softmax_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softmax_int::name, softmax_int::overload_name)
      .typed<softmax_int::schema>();
}

// aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
at::Tensor softmax_int::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_softmax_int_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
at::Tensor softmax_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_softmax_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_int_out, name, "aten::softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_int_out, overload_name, "int_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_int_out, schema_str, "softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<softmax_int_out::schema> create_softmax_int_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softmax_int_out::name, softmax_int_out::overload_name)
      .typed<softmax_int_out::schema>();
}

// aten::softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & softmax_int_out::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create_softmax_int_out_typed_handle();
    return op.call(self, dim, dtype, out);
}

// aten::softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & softmax_int_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create_softmax_int_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_Dimname, name, "aten::softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_Dimname, schema_str, "softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor")

// aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<softmax_Dimname::schema> create_softmax_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softmax_Dimname::name, softmax_Dimname::overload_name)
      .typed<softmax_Dimname::schema>();
}

// aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor softmax_Dimname::call(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_softmax_Dimname_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor softmax_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_softmax_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax, name, "aten::_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax, schema_str, "_softmax(Tensor self, int dim, bool half_to_float) -> Tensor")

// aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_softmax::schema> create__softmax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_softmax::name, _softmax::overload_name)
      .typed<_softmax::schema>();
}

// aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
at::Tensor _softmax::call(const at::Tensor & self, int64_t dim, bool half_to_float) {
    
    static auto op = create__softmax_typed_handle();
    return op.call(self, dim, half_to_float);
}

// aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
at::Tensor _softmax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float) {
    
    static auto op = create__softmax_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, half_to_float);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_out, name, "aten::_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_out, schema_str, "_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_softmax_out::schema> create__softmax_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_softmax_out::name, _softmax_out::overload_name)
      .typed<_softmax_out::schema>();
}

// aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _softmax_out::call(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
    
    static auto op = create__softmax_out_typed_handle();
    return op.call(self, dim, half_to_float, out);
}

// aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _softmax_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
    
    static auto op = create__softmax_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, half_to_float, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_split_Tensor, name, "aten::unsafe_split")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_split_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_split_Tensor, schema_str, "unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]")

// aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<unsafe_split_Tensor::schema> create_unsafe_split_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unsafe_split_Tensor::name, unsafe_split_Tensor::overload_name)
      .typed<unsafe_split_Tensor::schema>();
}

// aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]
::std::vector<at::Tensor> unsafe_split_Tensor::call(const at::Tensor & self, c10::SymInt split_size, int64_t dim) {
    
    static auto op = create_unsafe_split_Tensor_typed_handle();
    return op.call(self, split_size, dim);
}

// aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]
::std::vector<at::Tensor> unsafe_split_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt split_size, int64_t dim) {
    
    static auto op = create_unsafe_split_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, split_size, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dsplit_int, name, "aten::dsplit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dsplit_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dsplit_int, schema_str, "dsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]")

// aten::dsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<dsplit_int::schema> create_dsplit_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dsplit_int::name, dsplit_int::overload_name)
      .typed<dsplit_int::schema>();
}

// aten::dsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]
::std::vector<at::Tensor> dsplit_int::call(const at::Tensor & self, int64_t sections) {
    
    static auto op = create_dsplit_int_typed_handle();
    return op.call(self, sections);
}

// aten::dsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]
::std::vector<at::Tensor> dsplit_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sections) {
    
    static auto op = create_dsplit_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, sections);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dsplit_array, name, "aten::dsplit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dsplit_array, overload_name, "array")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dsplit_array, schema_str, "dsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]")

// aten::dsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<dsplit_array::schema> create_dsplit_array_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dsplit_array::name, dsplit_array::overload_name)
      .typed<dsplit_array::schema>();
}

// aten::dsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]
::std::vector<at::Tensor> dsplit_array::call(const at::Tensor & self, at::IntArrayRef indices) {
    
    static auto op = create_dsplit_array_typed_handle();
    return op.call(self, indices);
}

// aten::dsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]
::std::vector<at::Tensor> dsplit_array::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef indices) {
    
    static auto op = create_dsplit_array_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vstack, name, "aten::vstack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vstack, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vstack, schema_str, "vstack(Tensor[] tensors) -> Tensor")

// aten::vstack(Tensor[] tensors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<vstack::schema> create_vstack_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(vstack::name, vstack::overload_name)
      .typed<vstack::schema>();
}

// aten::vstack(Tensor[] tensors) -> Tensor
at::Tensor vstack::call(at::TensorList tensors) {
    
    static auto op = create_vstack_typed_handle();
    return op.call(tensors);
}

// aten::vstack(Tensor[] tensors) -> Tensor
at::Tensor vstack::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    
    static auto op = create_vstack_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vstack_out, name, "aten::vstack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vstack_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vstack_out, schema_str, "vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)")

// aten::vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<vstack_out::schema> create_vstack_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(vstack_out::name, vstack_out::overload_name)
      .typed<vstack_out::schema>();
}

// aten::vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & vstack_out::call(at::TensorList tensors, at::Tensor & out) {
    
    static auto op = create_vstack_out_typed_handle();
    return op.call(tensors, out);
}

// aten::vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & vstack_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
    
    static auto op = create_vstack_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stft, name, "aten::stft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stft, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stft, schema_str, "stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor")

// aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<stft::schema> create_stft_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(stft::name, stft::overload_name)
      .typed<stft::schema>();
}

// aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
at::Tensor stft::call(const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex) {
    
    static auto op = create_stft_typed_handle();
    return op.call(self, n_fft, hop_length, win_length, window, normalized, onesided, return_complex);
}

// aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
at::Tensor stft::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex) {
    
    static auto op = create_stft_typed_handle();
    return op.redispatch(dispatchKeySet, self, n_fft, hop_length, win_length, window, normalized, onesided, return_complex);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stft_center, name, "aten::stft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stft_center, overload_name, "center")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stft_center, schema_str, "stft.center(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, str pad_mode=\"reflect\", bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor")

// aten::stft.center(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, str pad_mode="reflect", bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<stft_center::schema> create_stft_center_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(stft_center::name, stft_center::overload_name)
      .typed<stft_center::schema>();
}

// aten::stft.center(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, str pad_mode="reflect", bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
at::Tensor stft_center::call(const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool center, c10::string_view pad_mode, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex) {
    
    static auto op = create_stft_center_typed_handle();
    return op.call(self, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex);
}

// aten::stft.center(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, str pad_mode="reflect", bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
at::Tensor stft_center::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool center, c10::string_view pad_mode, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex) {
    
    static auto op = create_stft_center_typed_handle();
    return op.redispatch(dispatchKeySet, self, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sym_stride_int, name, "aten::sym_stride")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sym_stride_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sym_stride_int, schema_str, "sym_stride.int(Tensor self, int dim) -> SymInt")

// aten::sym_stride.int(Tensor self, int dim) -> SymInt
static C10_NOINLINE c10::TypedOperatorHandle<sym_stride_int::schema> create_sym_stride_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sym_stride_int::name, sym_stride_int::overload_name)
      .typed<sym_stride_int::schema>();
}

// aten::sym_stride.int(Tensor self, int dim) -> SymInt
c10::SymInt sym_stride_int::call(const at::Tensor & self, int64_t dim) {
    
    static auto op = create_sym_stride_int_typed_handle();
    return op.call(self, dim);
}

// aten::sym_stride.int(Tensor self, int dim) -> SymInt
c10::SymInt sym_stride_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    
    static auto op = create_sym_stride_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_sum_backward, name, "aten::_nested_sum_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_sum_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_sum_backward, schema_str, "_nested_sum_backward(Tensor grad, Tensor self, int[1]? dim, bool keepdim=False) -> Tensor")

// aten::_nested_sum_backward(Tensor grad, Tensor self, int[1]? dim, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_nested_sum_backward::schema> create__nested_sum_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_nested_sum_backward::name, _nested_sum_backward::overload_name)
      .typed<_nested_sum_backward::schema>();
}

// aten::_nested_sum_backward(Tensor grad, Tensor self, int[1]? dim, bool keepdim=False) -> Tensor
at::Tensor _nested_sum_backward::call(const at::Tensor & grad, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim) {
    
    static auto op = create__nested_sum_backward_typed_handle();
    return op.call(grad, self, dim, keepdim);
}

// aten::_nested_sum_backward(Tensor grad, Tensor self, int[1]? dim, bool keepdim=False) -> Tensor
at::Tensor _nested_sum_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim) {
    
    static auto op = create__nested_sum_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_to_size, name, "aten::sum_to_size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_to_size, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_to_size, schema_str, "sum_to_size(Tensor self, SymInt[] size) -> Tensor")

// aten::sum_to_size(Tensor self, SymInt[] size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sum_to_size::schema> create_sum_to_size_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sum_to_size::name, sum_to_size::overload_name)
      .typed<sum_to_size::schema>();
}

// aten::sum_to_size(Tensor self, SymInt[] size) -> Tensor
at::Tensor sum_to_size::call(const at::Tensor & self, c10::SymIntArrayRef size) {
    
    static auto op = create_sum_to_size_typed_handle();
    return op.call(self, size);
}

// aten::sum_to_size(Tensor self, SymInt[] size) -> Tensor
at::Tensor sum_to_size::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size) {
    
    static auto op = create_sum_to_size_typed_handle();
    return op.redispatch(dispatchKeySet, self, size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt, name, "aten::sqrt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt, schema_str, "sqrt(Tensor self) -> Tensor")

// aten::sqrt(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sqrt::schema> create_sqrt_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sqrt::name, sqrt::overload_name)
      .typed<sqrt::schema>();
}

// aten::sqrt(Tensor self) -> Tensor
at::Tensor sqrt::call(const at::Tensor & self) {
    
    static auto op = create_sqrt_typed_handle();
    return op.call(self);
}

// aten::sqrt(Tensor self) -> Tensor
at::Tensor sqrt::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_sqrt_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt_, name, "aten::sqrt_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt_, schema_str, "sqrt_(Tensor(a!) self) -> Tensor(a!)")

// aten::sqrt_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sqrt_::schema> create_sqrt__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sqrt_::name, sqrt_::overload_name)
      .typed<sqrt_::schema>();
}

// aten::sqrt_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sqrt_::call(at::Tensor & self) {
    
    static auto op = create_sqrt__typed_handle();
    return op.call(self);
}

// aten::sqrt_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sqrt_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_sqrt__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt_out, name, "aten::sqrt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt_out, schema_str, "sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sqrt_out::schema> create_sqrt_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sqrt_out::name, sqrt_out::overload_name)
      .typed<sqrt_out::schema>();
}

// aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sqrt_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_sqrt_out_typed_handle();
    return op.call(self, out);
}

// aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sqrt_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_sqrt_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std, schema_str, "std(Tensor self, bool unbiased=True) -> Tensor")

// aten::std(Tensor self, bool unbiased=True) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<std::schema> create_std_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std::name, std::overload_name)
      .typed<std::schema>();
}

// aten::std(Tensor self, bool unbiased=True) -> Tensor
at::Tensor std::call(const at::Tensor & self, bool unbiased) {
    
    static auto op = create_std_typed_handle();
    return op.call(self, unbiased);
}

// aten::std(Tensor self, bool unbiased=True) -> Tensor
at::Tensor std::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool unbiased) {
    
    static auto op = create_std_typed_handle();
    return op.redispatch(dispatchKeySet, self, unbiased);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_dim, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_dim, schema_str, "std.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor")

// aten::std.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<std_dim::schema> create_std_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_dim::name, std_dim::overload_name)
      .typed<std_dim::schema>();
}

// aten::std.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor
at::Tensor std_dim::call(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
    
    static auto op = create_std_dim_typed_handle();
    return op.call(self, dim, unbiased, keepdim);
}

// aten::std.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor
at::Tensor std_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
    
    static auto op = create_std_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction, overload_name, "correction")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction, schema_str, "std.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor")

// aten::std.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<std_correction::schema> create_std_correction_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_correction::name, std_correction::overload_name)
      .typed<std_correction::schema>();
}

// aten::std.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor
at::Tensor std_correction::call(const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
    
    static auto op = create_std_correction_typed_handle();
    return op.call(self, dim, correction, keepdim);
}

// aten::std.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor
at::Tensor std_correction::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
    
    static auto op = create_std_correction_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean, name, "aten::std_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean, schema_str, "std_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)")

// aten::std_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<std_mean::schema> create_std_mean_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_mean::name, std_mean::overload_name)
      .typed<std_mean::schema>();
}

// aten::std_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean::call(const at::Tensor & self, bool unbiased) {
    
    static auto op = create_std_mean_typed_handle();
    return op.call(self, unbiased);
}

// aten::std_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool unbiased) {
    
    static auto op = create_std_mean_typed_handle();
    return op.redispatch(dispatchKeySet, self, unbiased);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_dim, name, "aten::std_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_dim, schema_str, "std_mean.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)")

// aten::std_mean.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<std_mean_dim::schema> create_std_mean_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_mean_dim::name, std_mean_dim::overload_name)
      .typed<std_mean_dim::schema>();
}

// aten::std_mean.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_dim::call(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
    
    static auto op = create_std_mean_dim_typed_handle();
    return op.call(self, dim, unbiased, keepdim);
}

// aten::std_mean.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
    
    static auto op = create_std_mean_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction, name, "aten::std_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction, overload_name, "correction")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction, schema_str, "std_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)")

// aten::std_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<std_mean_correction::schema> create_std_mean_correction_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_mean_correction::name, std_mean_correction::overload_name)
      .typed<std_mean_correction::schema>();
}

// aten::std_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_correction::call(const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
    
    static auto op = create_std_mean_correction_typed_handle();
    return op.call(self, dim, correction, keepdim);
}

// aten::std_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_correction::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
    
    static auto op = create_std_mean_correction_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_names_dim, name, "aten::std_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_names_dim, overload_name, "names_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_names_dim, schema_str, "std_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)")

// aten::std_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<std_mean_names_dim::schema> create_std_mean_names_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_mean_names_dim::name, std_mean_names_dim::overload_name)
      .typed<std_mean_names_dim::schema>();
}

// aten::std_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_names_dim::call(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    
    static auto op = create_std_mean_names_dim_typed_handle();
    return op.call(self, dim, unbiased, keepdim);
}

// aten::std_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_names_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    
    static auto op = create_std_mean_names_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction_names, name, "aten::std_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction_names, overload_name, "correction_names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction_names, schema_str, "std_mean.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)")

// aten::std_mean.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<std_mean_correction_names::schema> create_std_mean_correction_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_mean_correction_names::name, std_mean_correction_names::overload_name)
      .typed<std_mean_correction_names::schema>();
}

// aten::std_mean.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_correction_names::call(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
    
    static auto op = create_std_mean_correction_names_typed_handle();
    return op.call(self, dim, correction, keepdim);
}

// aten::std_mean.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_correction_names::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
    
    static auto op = create_std_mean_correction_names_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_out, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_out, schema_str, "std.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::std.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<std_out::schema> create_std_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_out::name, std_out::overload_name)
      .typed<std_out::schema>();
}

// aten::std.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_out::call(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
    
    static auto op = create_std_out_typed_handle();
    return op.call(self, dim, unbiased, keepdim, out);
}

// aten::std.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
    
    static auto op = create_std_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_out, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_out, overload_name, "correction_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_out, schema_str, "std.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)")

// aten::std.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<std_correction_out::schema> create_std_correction_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_correction_out::name, std_correction_out::overload_name)
      .typed<std_correction_out::schema>();
}

// aten::std.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_correction_out::call(const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
    
    static auto op = create_std_correction_out_typed_handle();
    return op.call(self, dim, correction, keepdim, out);
}

// aten::std.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_correction_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
    
    static auto op = create_std_correction_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_names_dim, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_names_dim, overload_name, "names_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_names_dim, schema_str, "std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor")

// aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<std_names_dim::schema> create_std_names_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_names_dim::name, std_names_dim::overload_name)
      .typed<std_names_dim::schema>();
}

// aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
at::Tensor std_names_dim::call(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    
    static auto op = create_std_names_dim_typed_handle();
    return op.call(self, dim, unbiased, keepdim);
}

// aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
at::Tensor std_names_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    
    static auto op = create_std_names_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_names_out, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_names_out, overload_name, "names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_names_out, schema_str, "std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<std_names_out::schema> create_std_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_names_out::name, std_names_out::overload_name)
      .typed<std_names_out::schema>();
}

// aten::std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_names_out::call(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
    
    static auto op = create_std_names_out_typed_handle();
    return op.call(self, dim, unbiased, keepdim, out);
}

// aten::std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
    
    static auto op = create_std_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_names, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_names, overload_name, "correction_names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_names, schema_str, "std.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -> Tensor")

// aten::std.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<std_correction_names::schema> create_std_correction_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_correction_names::name, std_correction_names::overload_name)
      .typed<std_correction_names::schema>();
}

// aten::std.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -> Tensor
at::Tensor std_correction_names::call(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
    
    static auto op = create_std_correction_names_typed_handle();
    return op.call(self, dim, correction, keepdim);
}

// aten::std.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -> Tensor
at::Tensor std_correction_names::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
    
    static auto op = create_std_correction_names_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_names_out, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_names_out, overload_name, "correction_names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_names_out, schema_str, "std.correction_names_out(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)")

// aten::std.correction_names_out(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<std_correction_names_out::schema> create_std_correction_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_correction_names_out::name, std_correction_names_out::overload_name)
      .typed<std_correction_names_out::schema>();
}

// aten::std.correction_names_out(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_correction_names_out::call(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
    
    static auto op = create_std_correction_names_out_typed_handle();
    return op.call(self, dim, correction, keepdim, out);
}

// aten::std.correction_names_out(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_correction_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
    
    static auto op = create_std_correction_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(t, name, "aten::t")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(t, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(t, schema_str, "t(Tensor(a) self) -> Tensor(a)")

// aten::t(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<t::schema> create_t_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(t::name, t::overload_name)
      .typed<t::schema>();
}

// aten::t(Tensor(a) self) -> Tensor(a)
at::Tensor t::call(const at::Tensor & self) {
    
    static auto op = create_t_typed_handle();
    return op.call(self);
}

// aten::t(Tensor(a) self) -> Tensor(a)
at::Tensor t::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_t_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(t_, name, "aten::t_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(t_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(t_, schema_str, "t_(Tensor(a!) self) -> Tensor(a!)")

// aten::t_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<t_::schema> create_t__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(t_::name, t_::overload_name)
      .typed<t_::schema>();
}

// aten::t_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & t_::call(at::Tensor & self) {
    
    static auto op = create_t__typed_handle();
    return op.call(self);
}

// aten::t_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & t_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_t__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold, name, "aten::threshold")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold, schema_str, "threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor")

// aten::threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<threshold::schema> create_threshold_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(threshold::name, threshold::overload_name)
      .typed<threshold::schema>();
}

// aten::threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor
at::Tensor threshold::call(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
    
    static auto op = create_threshold_typed_handle();
    return op.call(self, threshold, value);
}

// aten::threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor
at::Tensor threshold::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
    
    static auto op = create_threshold_typed_handle();
    return op.redispatch(dispatchKeySet, self, threshold, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_, name, "aten::threshold_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_, schema_str, "threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)")

// aten::threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<threshold_::schema> create_threshold__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(threshold_::name, threshold_::overload_name)
      .typed<threshold_::schema>();
}

// aten::threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)
at::Tensor & threshold_::call(at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
    
    static auto op = create_threshold__typed_handle();
    return op.call(self, threshold, value);
}

// aten::threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)
at::Tensor & threshold_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
    
    static auto op = create_threshold__typed_handle();
    return op.redispatch(dispatchKeySet, self, threshold, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_out, name, "aten::threshold")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_out, schema_str, "threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)")

// aten::threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<threshold_out::schema> create_threshold_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(threshold_out::name, threshold_out::overload_name)
      .typed<threshold_out::schema>();
}

// aten::threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & threshold_out::call(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value, at::Tensor & out) {
    
    static auto op = create_threshold_out_typed_handle();
    return op.call(self, threshold, value, out);
}

// aten::threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & threshold_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value, at::Tensor & out) {
    
    static auto op = create_threshold_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, threshold, value, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_int, name, "aten::transpose")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_int, schema_str, "transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)")

// aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<transpose_int::schema> create_transpose_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(transpose_int::name, transpose_int::overload_name)
      .typed<transpose_int::schema>();
}

// aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
at::Tensor transpose_int::call(const at::Tensor & self, int64_t dim0, int64_t dim1) {
    
    static auto op = create_transpose_int_typed_handle();
    return op.call(self, dim0, dim1);
}

// aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
at::Tensor transpose_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim0, int64_t dim1) {
    
    static auto op = create_transpose_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim0, dim1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_Dimname, name, "aten::transpose")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_Dimname, schema_str, "transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -> Tensor(a)")

// aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<transpose_Dimname::schema> create_transpose_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(transpose_Dimname::name, transpose_Dimname::overload_name)
      .typed<transpose_Dimname::schema>();
}

// aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -> Tensor(a)
at::Tensor transpose_Dimname::call(const at::Tensor & self, at::Dimname dim0, at::Dimname dim1) {
    
    static auto op = create_transpose_Dimname_typed_handle();
    return op.call(self, dim0, dim1);
}

// aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -> Tensor(a)
at::Tensor transpose_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim0, at::Dimname dim1) {
    
    static auto op = create_transpose_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim0, dim1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_, name, "aten::transpose_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_, schema_str, "transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)")

// aten::transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<transpose_::schema> create_transpose__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(transpose_::name, transpose_::overload_name)
      .typed<transpose_::schema>();
}

// aten::transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
at::Tensor & transpose_::call(at::Tensor & self, int64_t dim0, int64_t dim1) {
    
    static auto op = create_transpose__typed_handle();
    return op.call(self, dim0, dim1);
}

// aten::transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
at::Tensor & transpose_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim0, int64_t dim1) {
    
    static auto op = create_transpose__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim0, dim1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flip, name, "aten::flip")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flip, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flip, schema_str, "flip(Tensor self, int[] dims) -> Tensor")

// aten::flip(Tensor self, int[] dims) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<flip::schema> create_flip_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(flip::name, flip::overload_name)
      .typed<flip::schema>();
}

// aten::flip(Tensor self, int[] dims) -> Tensor
at::Tensor flip::call(const at::Tensor & self, at::IntArrayRef dims) {
    
    static auto op = create_flip_typed_handle();
    return op.call(self, dims);
}

// aten::flip(Tensor self, int[] dims) -> Tensor
at::Tensor flip::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dims) {
    
    static auto op = create_flip_typed_handle();
    return op.redispatch(dispatchKeySet, self, dims);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(roll, name, "aten::roll")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(roll, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(roll, schema_str, "roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -> Tensor")

// aten::roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<roll::schema> create_roll_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(roll::name, roll::overload_name)
      .typed<roll::schema>();
}

// aten::roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -> Tensor
at::Tensor roll::call(const at::Tensor & self, c10::SymIntArrayRef shifts, at::IntArrayRef dims) {
    
    static auto op = create_roll_typed_handle();
    return op.call(self, shifts, dims);
}

// aten::roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -> Tensor
at::Tensor roll::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef shifts, at::IntArrayRef dims) {
    
    static auto op = create_roll_typed_handle();
    return op.redispatch(dispatchKeySet, self, shifts, dims);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_from_padded, name, "aten::_nested_from_padded")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_from_padded, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_from_padded, schema_str, "_nested_from_padded(Tensor padded, Tensor cpu_nested_shape_example, bool fuse_transform_0213=False) -> Tensor")

// aten::_nested_from_padded(Tensor padded, Tensor cpu_nested_shape_example, bool fuse_transform_0213=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_nested_from_padded::schema> create__nested_from_padded_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_nested_from_padded::name, _nested_from_padded::overload_name)
      .typed<_nested_from_padded::schema>();
}

// aten::_nested_from_padded(Tensor padded, Tensor cpu_nested_shape_example, bool fuse_transform_0213=False) -> Tensor
at::Tensor _nested_from_padded::call(const at::Tensor & padded, const at::Tensor & cpu_nested_shape_example, bool fuse_transform_0213) {
    
    static auto op = create__nested_from_padded_typed_handle();
    return op.call(padded, cpu_nested_shape_example, fuse_transform_0213);
}

// aten::_nested_from_padded(Tensor padded, Tensor cpu_nested_shape_example, bool fuse_transform_0213=False) -> Tensor
at::Tensor _nested_from_padded::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & padded, const at::Tensor & cpu_nested_shape_example, bool fuse_transform_0213) {
    
    static auto op = create__nested_from_padded_typed_handle();
    return op.redispatch(dispatchKeySet, padded, cpu_nested_shape_example, fuse_transform_0213);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_view_from_buffer, name, "aten::_nested_view_from_buffer")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_view_from_buffer, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_view_from_buffer, schema_str, "_nested_view_from_buffer(Tensor(a) self, Tensor nested_size, Tensor nested_strides, Tensor offsets) -> Tensor(a)")

// aten::_nested_view_from_buffer(Tensor(a) self, Tensor nested_size, Tensor nested_strides, Tensor offsets) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<_nested_view_from_buffer::schema> create__nested_view_from_buffer_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_nested_view_from_buffer::name, _nested_view_from_buffer::overload_name)
      .typed<_nested_view_from_buffer::schema>();
}

// aten::_nested_view_from_buffer(Tensor(a) self, Tensor nested_size, Tensor nested_strides, Tensor offsets) -> Tensor(a)
at::Tensor _nested_view_from_buffer::call(const at::Tensor & self, const at::Tensor & nested_size, const at::Tensor & nested_strides, const at::Tensor & offsets) {
    
    static auto op = create__nested_view_from_buffer_typed_handle();
    return op.call(self, nested_size, nested_strides, offsets);
}

// aten::_nested_view_from_buffer(Tensor(a) self, Tensor nested_size, Tensor nested_strides, Tensor offsets) -> Tensor(a)
at::Tensor _nested_view_from_buffer::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & nested_size, const at::Tensor & nested_strides, const at::Tensor & offsets) {
    
    static auto op = create__nested_view_from_buffer_typed_handle();
    return op.redispatch(dispatchKeySet, self, nested_size, nested_strides, offsets);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_trilinear, name, "aten::_trilinear")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_trilinear, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_trilinear, schema_str, "_trilinear(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1) -> Tensor")

// aten::_trilinear(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_trilinear::schema> create__trilinear_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_trilinear::name, _trilinear::overload_name)
      .typed<_trilinear::schema>();
}

// aten::_trilinear(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1) -> Tensor
at::Tensor _trilinear::call(const at::Tensor & i1, const at::Tensor & i2, const at::Tensor & i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim) {
    
    static auto op = create__trilinear_typed_handle();
    return op.call(i1, i2, i3, expand1, expand2, expand3, sumdim, unroll_dim);
}

// aten::_trilinear(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1) -> Tensor
at::Tensor _trilinear::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & i1, const at::Tensor & i2, const at::Tensor & i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim) {
    
    static auto op = create__trilinear_typed_handle();
    return op.redispatch(dispatchKeySet, i1, i2, i3, expand1, expand2, expand3, sumdim, unroll_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(type_as, name, "aten::type_as")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(type_as, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(type_as, schema_str, "type_as(Tensor self, Tensor other) -> Tensor")

// aten::type_as(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<type_as::schema> create_type_as_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(type_as::name, type_as::overload_name)
      .typed<type_as::schema>();
}

// aten::type_as(Tensor self, Tensor other) -> Tensor
at::Tensor type_as::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_type_as_typed_handle();
    return op.call(self, other);
}

// aten::type_as(Tensor self, Tensor other) -> Tensor
at::Tensor type_as::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_type_as_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_has_compatible_shallow_copy_type, name, "aten::_has_compatible_shallow_copy_type")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_has_compatible_shallow_copy_type, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_has_compatible_shallow_copy_type, schema_str, "_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool")

// aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<_has_compatible_shallow_copy_type::schema> create__has_compatible_shallow_copy_type_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_has_compatible_shallow_copy_type::name, _has_compatible_shallow_copy_type::overload_name)
      .typed<_has_compatible_shallow_copy_type::schema>();
}

// aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool
bool _has_compatible_shallow_copy_type::call(const at::Tensor & self, const at::Tensor & from) {
    
    static auto op = create__has_compatible_shallow_copy_type_typed_handle();
    return op.call(self, from);
}

// aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool
bool _has_compatible_shallow_copy_type::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & from) {
    
    static auto op = create__has_compatible_shallow_copy_type_typed_handle();
    return op.redispatch(dispatchKeySet, self, from);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unique2, name, "aten::_unique2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unique2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unique2, schema_str, "_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)")

// aten::_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_unique2::schema> create__unique2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_unique2::name, _unique2::overload_name)
      .typed<_unique2::schema>();
}

// aten::_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _unique2::call(const at::Tensor & self, bool sorted, bool return_inverse, bool return_counts) {
    
    static auto op = create__unique2_typed_handle();
    return op.call(self, sorted, return_inverse, return_counts);
}

// aten::_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _unique2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool sorted, bool return_inverse, bool return_counts) {
    
    static auto op = create__unique2_typed_handle();
    return op.redispatch(dispatchKeySet, self, sorted, return_inverse, return_counts);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_interface_backward, name, "aten::_weight_norm_interface_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_interface_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_interface_backward, schema_str, "_weight_norm_interface_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)")

// aten::_weight_norm_interface_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_weight_norm_interface_backward::schema> create__weight_norm_interface_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_weight_norm_interface_backward::name, _weight_norm_interface_backward::overload_name)
      .typed<_weight_norm_interface_backward::schema>();
}

// aten::_weight_norm_interface_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _weight_norm_interface_backward::call(const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim) {
    
    static auto op = create__weight_norm_interface_backward_typed_handle();
    return op.call(grad_w, saved_v, saved_g, saved_norms, dim);
}

// aten::_weight_norm_interface_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _weight_norm_interface_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim) {
    
    static auto op = create__weight_norm_interface_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_w, saved_v, saved_g, saved_norms, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_like, name, "aten::zeros_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_like, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_like, schema_str, "zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<zeros_like::schema> create_zeros_like_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(zeros_like::name, zeros_like::overload_name)
      .typed<zeros_like::schema>();
}

// aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor zeros_like::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_zeros_like_typed_handle();
    return op.call(self, dtype, layout, device, pin_memory, memory_format);
}

// aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor zeros_like::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_zeros_like_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_csr_prod_dim_dtype, name, "aten::_sparse_csr_prod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_csr_prod_dim_dtype, overload_name, "dim_dtype")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_csr_prod_dim_dtype, schema_str, "_sparse_csr_prod.dim_dtype(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::_sparse_csr_prod.dim_dtype(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_csr_prod_dim_dtype::schema> create__sparse_csr_prod_dim_dtype_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_csr_prod_dim_dtype::name, _sparse_csr_prod_dim_dtype::overload_name)
      .typed<_sparse_csr_prod_dim_dtype::schema>();
}

// aten::_sparse_csr_prod.dim_dtype(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor _sparse_csr_prod_dim_dtype::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create__sparse_csr_prod_dim_dtype_typed_handle();
    return op.call(self, dim, keepdim, dtype);
}

// aten::_sparse_csr_prod.dim_dtype(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor _sparse_csr_prod_dim_dtype::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create__sparse_csr_prod_dim_dtype_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_backward_data, name, "aten::_sparse_softmax_backward_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_backward_data, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_backward_data, schema_str, "_sparse_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor")

// aten::_sparse_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_softmax_backward_data::schema> create__sparse_softmax_backward_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_softmax_backward_data::name, _sparse_softmax_backward_data::overload_name)
      .typed<_sparse_softmax_backward_data::schema>();
}

// aten::_sparse_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
at::Tensor _sparse_softmax_backward_data::call(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
    
    static auto op = create__sparse_softmax_backward_data_typed_handle();
    return op.call(grad_output, output, dim, self);
}

// aten::_sparse_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
at::Tensor _sparse_softmax_backward_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
    
    static auto op = create__sparse_softmax_backward_data_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, dim, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_int, name, "aten::_sparse_log_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_int, schema_str, "_sparse_log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor")

// aten::_sparse_log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_log_softmax_int::schema> create__sparse_log_softmax_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_log_softmax_int::name, _sparse_log_softmax_int::overload_name)
      .typed<_sparse_log_softmax_int::schema>();
}

// aten::_sparse_log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
at::Tensor _sparse_log_softmax_int::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create__sparse_log_softmax_int_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::_sparse_log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
at::Tensor _sparse_log_softmax_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create__sparse_log_softmax_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_Dimname, name, "aten::_sparse_log_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_Dimname, schema_str, "_sparse_log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor")

// aten::_sparse_log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_log_softmax_Dimname::schema> create__sparse_log_softmax_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_log_softmax_Dimname::name, _sparse_log_softmax_Dimname::overload_name)
      .typed<_sparse_log_softmax_Dimname::schema>();
}

// aten::_sparse_log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor _sparse_log_softmax_Dimname::call(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create__sparse_log_softmax_Dimname_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::_sparse_log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor _sparse_log_softmax_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create__sparse_log_softmax_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax, name, "aten::_sparse_log_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax, schema_str, "_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor")

// aten::_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_log_softmax::schema> create__sparse_log_softmax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_log_softmax::name, _sparse_log_softmax::overload_name)
      .typed<_sparse_log_softmax::schema>();
}

// aten::_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
at::Tensor _sparse_log_softmax::call(const at::Tensor & self, int64_t dim, bool half_to_float) {
    
    static auto op = create__sparse_log_softmax_typed_handle();
    return op.call(self, dim, half_to_float);
}

// aten::_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
at::Tensor _sparse_log_softmax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float) {
    
    static auto op = create__sparse_log_softmax_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, half_to_float);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_backward_data, name, "aten::_sparse_log_softmax_backward_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_backward_data, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_backward_data, schema_str, "_sparse_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor")

// aten::_sparse_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_log_softmax_backward_data::schema> create__sparse_log_softmax_backward_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_log_softmax_backward_data::name, _sparse_log_softmax_backward_data::overload_name)
      .typed<_sparse_log_softmax_backward_data::schema>();
}

// aten::_sparse_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
at::Tensor _sparse_log_softmax_backward_data::call(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
    
    static auto op = create__sparse_log_softmax_backward_data_typed_handle();
    return op.call(grad_output, output, dim, self);
}

// aten::_sparse_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
at::Tensor _sparse_log_softmax_backward_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
    
    static auto op = create__sparse_log_softmax_backward_data_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, dim, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_spdiags, name, "aten::_spdiags")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_spdiags, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_spdiags, schema_str, "_spdiags(Tensor diagonals, Tensor offsets, int[] shape, Layout? layout=None) -> Tensor")

// aten::_spdiags(Tensor diagonals, Tensor offsets, int[] shape, Layout? layout=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_spdiags::schema> create__spdiags_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_spdiags::name, _spdiags::overload_name)
      .typed<_spdiags::schema>();
}

// aten::_spdiags(Tensor diagonals, Tensor offsets, int[] shape, Layout? layout=None) -> Tensor
at::Tensor _spdiags::call(const at::Tensor & diagonals, const at::Tensor & offsets, at::IntArrayRef shape, c10::optional<at::Layout> layout) {
    
    static auto op = create__spdiags_typed_handle();
    return op.call(diagonals, offsets, shape, layout);
}

// aten::_spdiags(Tensor diagonals, Tensor offsets, int[] shape, Layout? layout=None) -> Tensor
at::Tensor _spdiags::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & diagonals, const at::Tensor & offsets, at::IntArrayRef shape, c10::optional<at::Layout> layout) {
    
    static auto op = create__spdiags_typed_handle();
    return op.redispatch(dispatchKeySet, diagonals, offsets, shape, layout);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frexp_Tensor, name, "aten::frexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frexp_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frexp_Tensor, schema_str, "frexp.Tensor(Tensor self) -> (Tensor mantissa, Tensor exponent)")

// aten::frexp.Tensor(Tensor self) -> (Tensor mantissa, Tensor exponent)
static C10_NOINLINE c10::TypedOperatorHandle<frexp_Tensor::schema> create_frexp_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(frexp_Tensor::name, frexp_Tensor::overload_name)
      .typed<frexp_Tensor::schema>();
}

// aten::frexp.Tensor(Tensor self) -> (Tensor mantissa, Tensor exponent)
::std::tuple<at::Tensor,at::Tensor> frexp_Tensor::call(const at::Tensor & self) {
    
    static auto op = create_frexp_Tensor_typed_handle();
    return op.call(self);
}

// aten::frexp.Tensor(Tensor self) -> (Tensor mantissa, Tensor exponent)
::std::tuple<at::Tensor,at::Tensor> frexp_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_frexp_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frexp_Tensor_out, name, "aten::frexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frexp_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frexp_Tensor_out, schema_str, "frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -> (Tensor(a!) mantissa, Tensor(b!) exponent)")

// aten::frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -> (Tensor(a!) mantissa, Tensor(b!) exponent)
static C10_NOINLINE c10::TypedOperatorHandle<frexp_Tensor_out::schema> create_frexp_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(frexp_Tensor_out::name, frexp_Tensor_out::overload_name)
      .typed<frexp_Tensor_out::schema>();
}

// aten::frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -> (Tensor(a!) mantissa, Tensor(b!) exponent)
::std::tuple<at::Tensor &,at::Tensor &> frexp_Tensor_out::call(const at::Tensor & self, at::Tensor & mantissa, at::Tensor & exponent) {
    
    static auto op = create_frexp_Tensor_out_typed_handle();
    return op.call(self, mantissa, exponent);
}

// aten::frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -> (Tensor(a!) mantissa, Tensor(b!) exponent)
::std::tuple<at::Tensor &,at::Tensor &> frexp_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & mantissa, at::Tensor & exponent) {
    
    static auto op = create_frexp_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mantissa, exponent);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zero_, name, "aten::zero_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zero_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zero_, schema_str, "zero_(Tensor(a!) self) -> Tensor(a!)")

// aten::zero_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<zero_::schema> create_zero__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(zero_::name, zero_::overload_name)
      .typed<zero_::schema>();
}

// aten::zero_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & zero_::call(at::Tensor & self) {
    
    static auto op = create_zero__typed_handle();
    return op.call(self);
}

// aten::zero_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & zero_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_zero__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Tensor, name, "aten::rsub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Tensor, schema_str, "rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor")

// aten::rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rsub_Tensor::schema> create_rsub_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rsub_Tensor::name, rsub_Tensor::overload_name)
      .typed<rsub_Tensor::schema>();
}

// aten::rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
at::Tensor rsub_Tensor::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    
    static auto op = create_rsub_Tensor_typed_handle();
    return op.call(self, other, alpha);
}

// aten::rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
at::Tensor rsub_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    
    static auto op = create_rsub_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Scalar, name, "aten::rsub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Scalar, schema_str, "rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor")

// aten::rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rsub_Scalar::schema> create_rsub_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rsub_Scalar::name, rsub_Scalar::overload_name)
      .typed<rsub_Scalar::schema>();
}

// aten::rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
at::Tensor rsub_Scalar::call(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    
    static auto op = create_rsub_Scalar_typed_handle();
    return op.call(self, other, alpha);
}

// aten::rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
at::Tensor rsub_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    
    static auto op = create_rsub_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_mm_reduce_impl, name, "aten::_sparse_mm_reduce_impl")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_mm_reduce_impl, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_mm_reduce_impl, schema_str, "_sparse_mm_reduce_impl(Tensor self, Tensor other, str reduce) -> (Tensor, Tensor)")

// aten::_sparse_mm_reduce_impl(Tensor self, Tensor other, str reduce) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_mm_reduce_impl::schema> create__sparse_mm_reduce_impl_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_mm_reduce_impl::name, _sparse_mm_reduce_impl::overload_name)
      .typed<_sparse_mm_reduce_impl::schema>();
}

// aten::_sparse_mm_reduce_impl(Tensor self, Tensor other, str reduce) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _sparse_mm_reduce_impl::call(const at::Tensor & self, const at::Tensor & other, c10::string_view reduce) {
    
    static auto op = create__sparse_mm_reduce_impl_typed_handle();
    return op.call(self, other, reduce);
}

// aten::_sparse_mm_reduce_impl(Tensor self, Tensor other, str reduce) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _sparse_mm_reduce_impl::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::string_view reduce) {
    
    static auto op = create__sparse_mm_reduce_impl_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, reduce);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_mm, name, "aten::_scaled_mm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_mm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_mm, schema_str, "_scaled_mm(Tensor self, Tensor mat2, *, Tensor? bias=None, ScalarType? out_dtype=None, Tensor? scale_a=None, Tensor? scale_b=None, Tensor? scale_result=None, bool use_fast_accum=False) -> (Tensor, Tensor)")

// aten::_scaled_mm(Tensor self, Tensor mat2, *, Tensor? bias=None, ScalarType? out_dtype=None, Tensor? scale_a=None, Tensor? scale_b=None, Tensor? scale_result=None, bool use_fast_accum=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_scaled_mm::schema> create__scaled_mm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_scaled_mm::name, _scaled_mm::overload_name)
      .typed<_scaled_mm::schema>();
}

// aten::_scaled_mm(Tensor self, Tensor mat2, *, Tensor? bias=None, ScalarType? out_dtype=None, Tensor? scale_a=None, Tensor? scale_b=None, Tensor? scale_result=None, bool use_fast_accum=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _scaled_mm::call(const at::Tensor & self, const at::Tensor & mat2, const c10::optional<at::Tensor> & bias, c10::optional<at::ScalarType> out_dtype, const c10::optional<at::Tensor> & scale_a, const c10::optional<at::Tensor> & scale_b, const c10::optional<at::Tensor> & scale_result, bool use_fast_accum) {
    
    static auto op = create__scaled_mm_typed_handle();
    return op.call(self, mat2, bias, out_dtype, scale_a, scale_b, scale_result, use_fast_accum);
}

// aten::_scaled_mm(Tensor self, Tensor mat2, *, Tensor? bias=None, ScalarType? out_dtype=None, Tensor? scale_a=None, Tensor? scale_b=None, Tensor? scale_result=None, bool use_fast_accum=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _scaled_mm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, const c10::optional<at::Tensor> & bias, c10::optional<at::ScalarType> out_dtype, const c10::optional<at::Tensor> & scale_a, const c10::optional<at::Tensor> & scale_b, const c10::optional<at::Tensor> & scale_result, bool use_fast_accum) {
    
    static auto op = create__scaled_mm_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat2, bias, out_dtype, scale_a, scale_b, scale_result, use_fast_accum);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_mm_out, name, "aten::_scaled_mm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_mm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_mm_out, schema_str, "_scaled_mm.out(Tensor self, Tensor mat2, *, Tensor? bias=None, ScalarType? out_dtype=None, Tensor? scale_a=None, Tensor? scale_b=None, Tensor? scale_result=None, bool use_fast_accum=False, Tensor(a!) out, Tensor(b!) out_amax) -> (Tensor(a!), Tensor(b!))")

// aten::_scaled_mm.out(Tensor self, Tensor mat2, *, Tensor? bias=None, ScalarType? out_dtype=None, Tensor? scale_a=None, Tensor? scale_b=None, Tensor? scale_result=None, bool use_fast_accum=False, Tensor(a!) out, Tensor(b!) out_amax) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<_scaled_mm_out::schema> create__scaled_mm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_scaled_mm_out::name, _scaled_mm_out::overload_name)
      .typed<_scaled_mm_out::schema>();
}

// aten::_scaled_mm.out(Tensor self, Tensor mat2, *, Tensor? bias=None, ScalarType? out_dtype=None, Tensor? scale_a=None, Tensor? scale_b=None, Tensor? scale_result=None, bool use_fast_accum=False, Tensor(a!) out, Tensor(b!) out_amax) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> _scaled_mm_out::call(const at::Tensor & self, const at::Tensor & mat2, const c10::optional<at::Tensor> & bias, c10::optional<at::ScalarType> out_dtype, const c10::optional<at::Tensor> & scale_a, const c10::optional<at::Tensor> & scale_b, const c10::optional<at::Tensor> & scale_result, bool use_fast_accum, at::Tensor & out, at::Tensor & out_amax) {
    
    static auto op = create__scaled_mm_out_typed_handle();
    return op.call(self, mat2, bias, out_dtype, scale_a, scale_b, scale_result, use_fast_accum, out, out_amax);
}

// aten::_scaled_mm.out(Tensor self, Tensor mat2, *, Tensor? bias=None, ScalarType? out_dtype=None, Tensor? scale_a=None, Tensor? scale_b=None, Tensor? scale_result=None, bool use_fast_accum=False, Tensor(a!) out, Tensor(b!) out_amax) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> _scaled_mm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, const c10::optional<at::Tensor> & bias, c10::optional<at::ScalarType> out_dtype, const c10::optional<at::Tensor> & scale_a, const c10::optional<at::Tensor> & scale_b, const c10::optional<at::Tensor> & scale_result, bool use_fast_accum, at::Tensor & out, at::Tensor & out_amax) {
    
    static auto op = create__scaled_mm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat2, bias, out_dtype, scale_a, scale_b, scale_result, use_fast_accum, out, out_amax);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_bsr_tensor_unsafe, name, "aten::_sparse_bsr_tensor_unsafe")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_bsr_tensor_unsafe, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_bsr_tensor_unsafe, schema_str, "_sparse_bsr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::_sparse_bsr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_bsr_tensor_unsafe::schema> create__sparse_bsr_tensor_unsafe_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_bsr_tensor_unsafe::name, _sparse_bsr_tensor_unsafe::overload_name)
      .typed<_sparse_bsr_tensor_unsafe::schema>();
}

// aten::_sparse_bsr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor _sparse_bsr_tensor_unsafe::call(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create__sparse_bsr_tensor_unsafe_typed_handle();
    return op.call(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}

// aten::_sparse_bsr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor _sparse_bsr_tensor_unsafe::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create__sparse_bsr_tensor_unsafe_typed_handle();
    return op.redispatch(dispatchKeySet, crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_validate_sparse_csc_tensor_args, name, "aten::_validate_sparse_csc_tensor_args")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_validate_sparse_csc_tensor_args, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_validate_sparse_csc_tensor_args, schema_str, "_validate_sparse_csc_tensor_args(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size) -> ()")

// aten::_validate_sparse_csc_tensor_args(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_validate_sparse_csc_tensor_args::schema> create__validate_sparse_csc_tensor_args_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_validate_sparse_csc_tensor_args::name, _validate_sparse_csc_tensor_args::overload_name)
      .typed<_validate_sparse_csc_tensor_args::schema>();
}

// aten::_validate_sparse_csc_tensor_args(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size) -> ()
void _validate_sparse_csc_tensor_args::call(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size) {
    
    static auto op = create__validate_sparse_csc_tensor_args_typed_handle();
    return op.call(ccol_indices, row_indices, values, size);
}

// aten::_validate_sparse_csc_tensor_args(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size) -> ()
void _validate_sparse_csc_tensor_args::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size) {
    
    static auto op = create__validate_sparse_csc_tensor_args_typed_handle();
    return op.redispatch(dispatchKeySet, ccol_indices, row_indices, values, size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_with_dims, name, "aten::_sparse_coo_tensor_with_dims")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_with_dims, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_with_dims, schema_str, "_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor")

// aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_coo_tensor_with_dims::schema> create__sparse_coo_tensor_with_dims_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_coo_tensor_with_dims::name, _sparse_coo_tensor_with_dims::overload_name)
      .typed<_sparse_coo_tensor_with_dims::schema>();
}

// aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor _sparse_coo_tensor_with_dims::call(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create__sparse_coo_tensor_with_dims_typed_handle();
    return op.call(sparse_dim, dense_dim, size, dtype, layout, device, pin_memory);
}

// aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor _sparse_coo_tensor_with_dims::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create__sparse_coo_tensor_with_dims_typed_handle();
    return op.redispatch(dispatchKeySet, sparse_dim, dense_dim, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dense_backward, name, "aten::to_dense_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dense_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dense_backward, schema_str, "to_dense_backward(Tensor grad, Tensor input, bool? masked_grad=None) -> Tensor")

// aten::to_dense_backward(Tensor grad, Tensor input, bool? masked_grad=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<to_dense_backward::schema> create_to_dense_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_dense_backward::name, to_dense_backward::overload_name)
      .typed<to_dense_backward::schema>();
}

// aten::to_dense_backward(Tensor grad, Tensor input, bool? masked_grad=None) -> Tensor
at::Tensor to_dense_backward::call(const at::Tensor & grad, const at::Tensor & input, c10::optional<bool> masked_grad) {
    
    static auto op = create_to_dense_backward_typed_handle();
    return op.call(grad, input, masked_grad);
}

// aten::to_dense_backward(Tensor grad, Tensor input, bool? masked_grad=None) -> Tensor
at::Tensor to_dense_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & input, c10::optional<bool> masked_grad) {
    
    static auto op = create_to_dense_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, input, masked_grad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_coalesce, name, "aten::_coalesce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_coalesce, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_coalesce, schema_str, "_coalesce(Tensor self) -> Tensor")

// aten::_coalesce(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_coalesce::schema> create__coalesce_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_coalesce::name, _coalesce::overload_name)
      .typed<_coalesce::schema>();
}

// aten::_coalesce(Tensor self) -> Tensor
at::Tensor _coalesce::call(const at::Tensor & self) {
    
    static auto op = create__coalesce_typed_handle();
    return op.call(self);
}

// aten::_coalesce(Tensor self) -> Tensor
at::Tensor _coalesce::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create__coalesce_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_values, name, "aten::_values")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_values, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_values, schema_str, "_values(Tensor(a) self) -> Tensor(a)")

// aten::_values(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<_values::schema> create__values_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_values::name, _values::overload_name)
      .typed<_values::schema>();
}

// aten::_values(Tensor(a) self) -> Tensor(a)
at::Tensor _values::call(const at::Tensor & self) {
    
    static auto op = create__values_typed_handle();
    return op.call(self);
}

// aten::_values(Tensor(a) self) -> Tensor(a)
at::Tensor _values::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create__values_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(crow_indices, name, "aten::crow_indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(crow_indices, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(crow_indices, schema_str, "crow_indices(Tensor(a) self) -> Tensor(a)")

// aten::crow_indices(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<crow_indices::schema> create_crow_indices_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(crow_indices::name, crow_indices::overload_name)
      .typed<crow_indices::schema>();
}

// aten::crow_indices(Tensor(a) self) -> Tensor(a)
at::Tensor crow_indices::call(const at::Tensor & self) {
    
    static auto op = create_crow_indices_typed_handle();
    return op.call(self);
}

// aten::crow_indices(Tensor(a) self) -> Tensor(a)
at::Tensor crow_indices::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_crow_indices_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_sparse_dim, name, "aten::_to_sparse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_sparse_dim, overload_name, "sparse_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_sparse_dim, schema_str, "_to_sparse.sparse_dim(Tensor self, int sparse_dim) -> Tensor")

// aten::_to_sparse.sparse_dim(Tensor self, int sparse_dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_to_sparse_sparse_dim::schema> create__to_sparse_sparse_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_to_sparse_sparse_dim::name, _to_sparse_sparse_dim::overload_name)
      .typed<_to_sparse_sparse_dim::schema>();
}

// aten::_to_sparse.sparse_dim(Tensor self, int sparse_dim) -> Tensor
at::Tensor _to_sparse_sparse_dim::call(const at::Tensor & self, int64_t sparse_dim) {
    
    static auto op = create__to_sparse_sparse_dim_typed_handle();
    return op.call(self, sparse_dim);
}

// aten::_to_sparse.sparse_dim(Tensor self, int sparse_dim) -> Tensor
at::Tensor _to_sparse_sparse_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sparse_dim) {
    
    static auto op = create__to_sparse_sparse_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, sparse_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse, name, "aten::_to_sparse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse, schema_str, "_to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -> Tensor")

// aten::_to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_to_sparse::schema> create__to_sparse_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_to_sparse::name, _to_sparse::overload_name)
      .typed<_to_sparse::schema>();
}

// aten::_to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -> Tensor
at::Tensor _to_sparse::call(const at::Tensor & self, c10::optional<at::Layout> layout, at::OptionalIntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
    
    static auto op = create__to_sparse_typed_handle();
    return op.call(self, layout, blocksize, dense_dim);
}

// aten::_to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -> Tensor
at::Tensor _to_sparse::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Layout> layout, at::OptionalIntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
    
    static auto op = create__to_sparse_typed_handle();
    return op.redispatch(dispatchKeySet, self, layout, blocksize, dense_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_zero_point, name, "aten::q_zero_point")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_zero_point, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_zero_point, schema_str, "q_zero_point(Tensor self) -> int")

// aten::q_zero_point(Tensor self) -> int
static C10_NOINLINE c10::TypedOperatorHandle<q_zero_point::schema> create_q_zero_point_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(q_zero_point::name, q_zero_point::overload_name)
      .typed<q_zero_point::schema>();
}

// aten::q_zero_point(Tensor self) -> int
int64_t q_zero_point::call(const at::Tensor & self) {
    
    static auto op = create_q_zero_point_typed_handle();
    return op.call(self);
}

// aten::q_zero_point(Tensor self) -> int
int64_t q_zero_point::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_q_zero_point_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_scales, name, "aten::q_per_channel_scales")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_scales, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_scales, schema_str, "q_per_channel_scales(Tensor self) -> Tensor")

// aten::q_per_channel_scales(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<q_per_channel_scales::schema> create_q_per_channel_scales_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(q_per_channel_scales::name, q_per_channel_scales::overload_name)
      .typed<q_per_channel_scales::schema>();
}

// aten::q_per_channel_scales(Tensor self) -> Tensor
at::Tensor q_per_channel_scales::call(const at::Tensor & self) {
    
    static auto op = create_q_per_channel_scales_typed_handle();
    return op.call(self);
}

// aten::q_per_channel_scales(Tensor self) -> Tensor
at::Tensor q_per_channel_scales::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_q_per_channel_scales_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_tensor_affine_backward, name, "aten::_fake_quantize_learnable_per_tensor_affine_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_tensor_affine_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_tensor_affine_backward, schema_str, "_fake_quantize_learnable_per_tensor_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)")

// aten::_fake_quantize_learnable_per_tensor_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_fake_quantize_learnable_per_tensor_affine_backward::schema> create__fake_quantize_learnable_per_tensor_affine_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fake_quantize_learnable_per_tensor_affine_backward::name, _fake_quantize_learnable_per_tensor_affine_backward::overload_name)
      .typed<_fake_quantize_learnable_per_tensor_affine_backward::schema>();
}

// aten::_fake_quantize_learnable_per_tensor_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _fake_quantize_learnable_per_tensor_affine_backward::call(const at::Tensor & grad, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max, double grad_factor) {
    
    static auto op = create__fake_quantize_learnable_per_tensor_affine_backward_typed_handle();
    return op.call(grad, self, scale, zero_point, quant_min, quant_max, grad_factor);
}

// aten::_fake_quantize_learnable_per_tensor_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _fake_quantize_learnable_per_tensor_affine_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max, double grad_factor) {
    
    static auto op = create__fake_quantize_learnable_per_tensor_affine_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self, scale, zero_point, quant_min, quant_max, grad_factor);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine_backward, name, "aten::_fake_quantize_learnable_per_channel_affine_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine_backward, schema_str, "_fake_quantize_learnable_per_channel_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)")

// aten::_fake_quantize_learnable_per_channel_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_fake_quantize_learnable_per_channel_affine_backward::schema> create__fake_quantize_learnable_per_channel_affine_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fake_quantize_learnable_per_channel_affine_backward::name, _fake_quantize_learnable_per_channel_affine_backward::overload_name)
      .typed<_fake_quantize_learnable_per_channel_affine_backward::schema>();
}

// aten::_fake_quantize_learnable_per_channel_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _fake_quantize_learnable_per_channel_affine_backward::call(const at::Tensor & grad, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
    
    static auto op = create__fake_quantize_learnable_per_channel_affine_backward_typed_handle();
    return op.call(grad, self, scale, zero_point, axis, quant_min, quant_max, grad_factor);
}

// aten::_fake_quantize_learnable_per_channel_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _fake_quantize_learnable_per_channel_affine_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
    
    static auto op = create__fake_quantize_learnable_per_channel_affine_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self, scale, zero_point, axis, quant_min, quant_max, grad_factor);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fused_moving_avg_obs_fake_quant, name, "aten::fused_moving_avg_obs_fake_quant")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fused_moving_avg_obs_fake_quant, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fused_moving_avg_obs_fake_quant, schema_str, "fused_moving_avg_obs_fake_quant(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> Tensor")

// aten::fused_moving_avg_obs_fake_quant(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fused_moving_avg_obs_fake_quant::schema> create_fused_moving_avg_obs_fake_quant_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fused_moving_avg_obs_fake_quant::name, fused_moving_avg_obs_fake_quant::overload_name)
      .typed<fused_moving_avg_obs_fake_quant::schema>();
}

// aten::fused_moving_avg_obs_fake_quant(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> Tensor
at::Tensor fused_moving_avg_obs_fake_quant::call(const at::Tensor & self, const at::Tensor & observer_on, const at::Tensor & fake_quant_on, at::Tensor & running_min, at::Tensor & running_max, at::Tensor & scale, at::Tensor & zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, bool per_row_fake_quant, bool symmetric_quant) {
    
    static auto op = create_fused_moving_avg_obs_fake_quant_typed_handle();
    return op.call(self, observer_on, fake_quant_on, running_min, running_max, scale, zero_point, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
}

// aten::fused_moving_avg_obs_fake_quant(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> Tensor
at::Tensor fused_moving_avg_obs_fake_quant::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & observer_on, const at::Tensor & fake_quant_on, at::Tensor & running_min, at::Tensor & running_max, at::Tensor & scale, at::Tensor & zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, bool per_row_fake_quant, bool symmetric_quant) {
    
    static auto op = create_fused_moving_avg_obs_fake_quant_typed_handle();
    return op.redispatch(dispatchKeySet, self, observer_on, fake_quant_on, running_min, running_max, scale, zero_point, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_choose_qparams_per_tensor, name, "aten::_choose_qparams_per_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_choose_qparams_per_tensor, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_choose_qparams_per_tensor, schema_str, "_choose_qparams_per_tensor(Tensor self, bool reduce_range=False) -> (float, int)")

// aten::_choose_qparams_per_tensor(Tensor self, bool reduce_range=False) -> (float, int)
static C10_NOINLINE c10::TypedOperatorHandle<_choose_qparams_per_tensor::schema> create__choose_qparams_per_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_choose_qparams_per_tensor::name, _choose_qparams_per_tensor::overload_name)
      .typed<_choose_qparams_per_tensor::schema>();
}

// aten::_choose_qparams_per_tensor(Tensor self, bool reduce_range=False) -> (float, int)
::std::tuple<double,int64_t> _choose_qparams_per_tensor::call(const at::Tensor & self, bool reduce_range) {
    
    static auto op = create__choose_qparams_per_tensor_typed_handle();
    return op.call(self, reduce_range);
}

// aten::_choose_qparams_per_tensor(Tensor self, bool reduce_range=False) -> (float, int)
::std::tuple<double,int64_t> _choose_qparams_per_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool reduce_range) {
    
    static auto op = create__choose_qparams_per_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, reduce_range);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(meshgrid, name, "aten::meshgrid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(meshgrid, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(meshgrid, schema_str, "meshgrid(Tensor[] tensors) -> Tensor[]")

// aten::meshgrid(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<meshgrid::schema> create_meshgrid_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(meshgrid::name, meshgrid::overload_name)
      .typed<meshgrid::schema>();
}

// aten::meshgrid(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> meshgrid::call(at::TensorList tensors) {
    
    static auto op = create_meshgrid_typed_handle();
    return op.call(tensors);
}

// aten::meshgrid(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> meshgrid::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    
    static auto op = create_meshgrid_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(meshgrid_indexing, name, "aten::meshgrid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(meshgrid_indexing, overload_name, "indexing")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(meshgrid_indexing, schema_str, "meshgrid.indexing(Tensor[] tensors, *, str indexing) -> Tensor[]")

// aten::meshgrid.indexing(Tensor[] tensors, *, str indexing) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<meshgrid_indexing::schema> create_meshgrid_indexing_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(meshgrid_indexing::name, meshgrid_indexing::overload_name)
      .typed<meshgrid_indexing::schema>();
}

// aten::meshgrid.indexing(Tensor[] tensors, *, str indexing) -> Tensor[]
::std::vector<at::Tensor> meshgrid_indexing::call(at::TensorList tensors, c10::string_view indexing) {
    
    static auto op = create_meshgrid_indexing_typed_handle();
    return op.call(tensors, indexing);
}

// aten::meshgrid.indexing(Tensor[] tensors, *, str indexing) -> Tensor[]
::std::vector<at::Tensor> meshgrid_indexing::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, c10::string_view indexing) {
    
    static auto op = create_meshgrid_indexing_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, indexing);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(can_cast, name, "aten::can_cast")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(can_cast, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(can_cast, schema_str, "can_cast(ScalarType from, ScalarType to) -> bool")

// aten::can_cast(ScalarType from, ScalarType to) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<can_cast::schema> create_can_cast_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(can_cast::name, can_cast::overload_name)
      .typed<can_cast::schema>();
}

// aten::can_cast(ScalarType from, ScalarType to) -> bool
bool can_cast::call(at::ScalarType from, at::ScalarType to) {
    
    static auto op = create_can_cast_typed_handle();
    return op.call(from, to);
}

// aten::can_cast(ScalarType from, ScalarType to) -> bool
bool can_cast::redispatch(c10::DispatchKeySet dispatchKeySet, at::ScalarType from, at::ScalarType to) {
    
    static auto op = create_can_cast_typed_handle();
    return op.redispatch(dispatchKeySet, from, to);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_mps_backward, name, "aten::lstm_mps_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_mps_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_mps_backward, schema_str, "lstm_mps_backward(Tensor? grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor[], Tensor[])")

// aten::lstm_mps_backward(Tensor? grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor[], Tensor[])
static C10_NOINLINE c10::TypedOperatorHandle<lstm_mps_backward::schema> create_lstm_mps_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lstm_mps_backward::name, lstm_mps_backward::overload_name)
      .typed<lstm_mps_backward::schema>();
}

// aten::lstm_mps_backward(Tensor? grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor[], Tensor[])
::std::tuple<at::Tensor,::std::vector<at::Tensor>,::std::vector<at::Tensor>> lstm_mps_backward::call(const c10::optional<at::Tensor> & grad_y, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & z_state, const at::Tensor & cell_state_fwd, const at::Tensor & input, const at::Tensor & layersOutputs, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    
    static auto op = create_lstm_mps_backward_typed_handle();
    return op.call(grad_y, grad_hy, grad_cy, z_state, cell_state_fwd, input, layersOutputs, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

// aten::lstm_mps_backward(Tensor? grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor[], Tensor[])
::std::tuple<at::Tensor,::std::vector<at::Tensor>,::std::vector<at::Tensor>> lstm_mps_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const c10::optional<at::Tensor> & grad_y, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & z_state, const at::Tensor & cell_state_fwd, const at::Tensor & input, const at::Tensor & layersOutputs, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    
    static auto op = create_lstm_mps_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_y, grad_hy, grad_cy, z_state, cell_state_fwd, input, layersOutputs, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell_backward_impl, name, "aten::_thnn_fused_lstm_cell_backward_impl")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell_backward_impl, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell_backward_impl, schema_str, "_thnn_fused_lstm_cell_backward_impl(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor)")

// aten::_thnn_fused_lstm_cell_backward_impl(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_thnn_fused_lstm_cell_backward_impl::schema> create__thnn_fused_lstm_cell_backward_impl_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_thnn_fused_lstm_cell_backward_impl::name, _thnn_fused_lstm_cell_backward_impl::overload_name)
      .typed<_thnn_fused_lstm_cell_backward_impl::schema>();
}

// aten::_thnn_fused_lstm_cell_backward_impl(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _thnn_fused_lstm_cell_backward_impl::call(const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & cx, const at::Tensor & cy, const at::Tensor & workspace, bool has_bias) {
    
    static auto op = create__thnn_fused_lstm_cell_backward_impl_typed_handle();
    return op.call(grad_hy, grad_cy, cx, cy, workspace, has_bias);
}

// aten::_thnn_fused_lstm_cell_backward_impl(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _thnn_fused_lstm_cell_backward_impl::redispatch(c10::DispatchKeySet dispatchKeySet, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & cx, const at::Tensor & cy, const at::Tensor & workspace, bool has_bias) {
    
    static auto op = create__thnn_fused_lstm_cell_backward_impl_typed_handle();
    return op.redispatch(dispatchKeySet, grad_hy, grad_cy, cx, cy, workspace, has_bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_gru_cell, name, "aten::_thnn_fused_gru_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_gru_cell, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_gru_cell, schema_str, "_thnn_fused_gru_cell(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor)")

// aten::_thnn_fused_gru_cell(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_thnn_fused_gru_cell::schema> create__thnn_fused_gru_cell_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_thnn_fused_gru_cell::name, _thnn_fused_gru_cell::overload_name)
      .typed<_thnn_fused_gru_cell::schema>();
}

// aten::_thnn_fused_gru_cell(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _thnn_fused_gru_cell::call(const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
    
    static auto op = create__thnn_fused_gru_cell_typed_handle();
    return op.call(input_gates, hidden_gates, hx, input_bias, hidden_bias);
}

// aten::_thnn_fused_gru_cell(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _thnn_fused_gru_cell::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
    
    static auto op = create__thnn_fused_gru_cell_typed_handle();
    return op.redispatch(dispatchKeySet, input_gates, hidden_gates, hx, input_bias, hidden_bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_rnn_tanh_cell, name, "aten::quantized_rnn_tanh_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_rnn_tanh_cell, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_rnn_tanh_cell, schema_str, "quantized_rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor")

// aten::quantized_rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantized_rnn_tanh_cell::schema> create_quantized_rnn_tanh_cell_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantized_rnn_tanh_cell::name, quantized_rnn_tanh_cell::overload_name)
      .typed<quantized_rnn_tanh_cell::schema>();
}

// aten::quantized_rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
at::Tensor quantized_rnn_tanh_cell::call(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    
    static auto op = create_quantized_rnn_tanh_cell_typed_handle();
    return op.call(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

// aten::quantized_rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
at::Tensor quantized_rnn_tanh_cell::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    
    static auto op = create_quantized_rnn_tanh_cell_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pack_padded_sequence, name, "aten::_pack_padded_sequence")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pack_padded_sequence, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pack_padded_sequence, schema_str, "_pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)")

// aten::_pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_pack_padded_sequence::schema> create__pack_padded_sequence_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_pack_padded_sequence::name, _pack_padded_sequence::overload_name)
      .typed<_pack_padded_sequence::schema>();
}

// aten::_pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _pack_padded_sequence::call(const at::Tensor & input, const at::Tensor & lengths, bool batch_first) {
    
    static auto op = create__pack_padded_sequence_typed_handle();
    return op.call(input, lengths, batch_first);
}

// aten::_pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _pack_padded_sequence::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & lengths, bool batch_first) {
    
    static auto op = create__pack_padded_sequence_typed_handle();
    return op.redispatch(dispatchKeySet, input, lengths, batch_first);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_set_to, name, "aten::is_set_to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_set_to, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_set_to, schema_str, "is_set_to(Tensor self, Tensor tensor) -> bool")

// aten::is_set_to(Tensor self, Tensor tensor) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_set_to::schema> create_is_set_to_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_set_to::name, is_set_to::overload_name)
      .typed<is_set_to::schema>();
}

// aten::is_set_to(Tensor self, Tensor tensor) -> bool
bool is_set_to::call(const at::Tensor & self, const at::Tensor & tensor) {
    
    static auto op = create_is_set_to_typed_handle();
    return op.call(self, tensor);
}

// aten::is_set_to(Tensor self, Tensor tensor) -> bool
bool is_set_to::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor) {
    
    static auto op = create_is_set_to_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_masked_softmax, name, "aten::_masked_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_masked_softmax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_masked_softmax, schema_str, "_masked_softmax(Tensor self, Tensor mask, int? dim=None, int? mask_type=None) -> Tensor")

// aten::_masked_softmax(Tensor self, Tensor mask, int? dim=None, int? mask_type=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_masked_softmax::schema> create__masked_softmax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_masked_softmax::name, _masked_softmax::overload_name)
      .typed<_masked_softmax::schema>();
}

// aten::_masked_softmax(Tensor self, Tensor mask, int? dim=None, int? mask_type=None) -> Tensor
at::Tensor _masked_softmax::call(const at::Tensor & self, const at::Tensor & mask, c10::optional<int64_t> dim, c10::optional<int64_t> mask_type) {
    
    static auto op = create__masked_softmax_typed_handle();
    return op.call(self, mask, dim, mask_type);
}

// aten::_masked_softmax(Tensor self, Tensor mask, int? dim=None, int? mask_type=None) -> Tensor
at::Tensor _masked_softmax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, c10::optional<int64_t> dim, c10::optional<int64_t> mask_type) {
    
    static auto op = create__masked_softmax_typed_handle();
    return op.redispatch(dispatchKeySet, self, mask, dim, mask_type);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view, name, "aten::view")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view, schema_str, "view(Tensor(a) self, SymInt[] size) -> Tensor(a)")

// aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<view::schema> create_view_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(view::name, view::overload_name)
      .typed<view::schema>();
}

// aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)
at::Tensor view::call(const at::Tensor & self, c10::SymIntArrayRef size) {
    
    static auto op = create_view_typed_handle();
    return op.call(self, size);
}

// aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)
at::Tensor view::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size) {
    
    static auto op = create_view_typed_handle();
    return op.redispatch(dispatchKeySet, self, size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_dtype, name, "aten::view")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_dtype, overload_name, "dtype")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_dtype, schema_str, "view.dtype(Tensor(a) self, ScalarType dtype) -> Tensor(a)")

// aten::view.dtype(Tensor(a) self, ScalarType dtype) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<view_dtype::schema> create_view_dtype_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(view_dtype::name, view_dtype::overload_name)
      .typed<view_dtype::schema>();
}

// aten::view.dtype(Tensor(a) self, ScalarType dtype) -> Tensor(a)
at::Tensor view_dtype::call(const at::Tensor & self, at::ScalarType dtype) {
    
    static auto op = create_view_dtype_typed_handle();
    return op.call(self, dtype);
}

// aten::view.dtype(Tensor(a) self, ScalarType dtype) -> Tensor(a)
at::Tensor view_dtype::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ScalarType dtype) {
    
    static auto op = create_view_dtype_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__xor___Scalar, name, "aten::__xor__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__xor___Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__xor___Scalar, schema_str, "__xor__.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::__xor__.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<__xor___Scalar::schema> create___xor___Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__xor___Scalar::name, __xor___Scalar::overload_name)
      .typed<__xor___Scalar::schema>();
}

// aten::__xor__.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor __xor___Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create___xor___Scalar_typed_handle();
    return op.call(self, other);
}

// aten::__xor__.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor __xor___Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create___xor___Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__xor___Tensor, name, "aten::__xor__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__xor___Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__xor___Tensor, schema_str, "__xor__.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::__xor__.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<__xor___Tensor::schema> create___xor___Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__xor___Tensor::name, __xor___Tensor::overload_name)
      .typed<__xor___Tensor::schema>();
}

// aten::__xor__.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor __xor___Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create___xor___Tensor_typed_handle();
    return op.call(self, other);
}

// aten::__xor__.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor __xor___Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create___xor___Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ixor___Scalar, name, "aten::__ixor__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ixor___Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ixor___Scalar, schema_str, "__ixor__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::__ixor__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<__ixor___Scalar::schema> create___ixor___Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__ixor___Scalar::name, __ixor___Scalar::overload_name)
      .typed<__ixor___Scalar::schema>();
}

// aten::__ixor__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & __ixor___Scalar::call(at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create___ixor___Scalar_typed_handle();
    return op.call(self, other);
}

// aten::__ixor__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & __ixor___Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create___ixor___Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ixor___Tensor, name, "aten::__ixor__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ixor___Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ixor___Tensor, schema_str, "__ixor__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::__ixor__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<__ixor___Tensor::schema> create___ixor___Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__ixor___Tensor::name, __ixor___Tensor::overload_name)
      .typed<__ixor___Tensor::schema>();
}

// aten::__ixor__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & __ixor___Tensor::call(at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create___ixor___Tensor_typed_handle();
    return op.call(self, other);
}

// aten::__ixor__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & __ixor___Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create___ixor___Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_, name, "aten::triu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_, schema_str, "triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)")

// aten::triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<triu_::schema> create_triu__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(triu_::name, triu_::overload_name)
      .typed<triu_::schema>();
}

// aten::triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)
at::Tensor & triu_::call(at::Tensor & self, int64_t diagonal) {
    
    static auto op = create_triu__typed_handle();
    return op.call(self, diagonal);
}

// aten::triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)
at::Tensor & triu_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t diagonal) {
    
    static auto op = create_triu__typed_handle();
    return op.redispatch(dispatchKeySet, self, diagonal);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp__Scalar, name, "aten::lerp_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp__Scalar, schema_str, "lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)")

// aten::lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lerp__Scalar::schema> create_lerp__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lerp__Scalar::name, lerp__Scalar::overload_name)
      .typed<lerp__Scalar::schema>();
}

// aten::lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)
at::Tensor & lerp__Scalar::call(at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
    
    static auto op = create_lerp__Scalar_typed_handle();
    return op.call(self, end, weight);
}

// aten::lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)
at::Tensor & lerp__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
    
    static auto op = create_lerp__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, end, weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp__Tensor, name, "aten::lerp_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp__Tensor, schema_str, "lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)")

// aten::lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lerp__Tensor::schema> create_lerp__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lerp__Tensor::name, lerp__Tensor::overload_name)
      .typed<lerp__Tensor::schema>();
}

// aten::lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)
at::Tensor & lerp__Tensor::call(at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
    
    static auto op = create_lerp__Tensor_typed_handle();
    return op.call(self, end, weight);
}

// aten::lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)
at::Tensor & lerp__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
    
    static auto op = create_lerp__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, end, weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm_, name, "aten::addbmm_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm_, schema_str, "addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")

// aten::addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addbmm_::schema> create_addbmm__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addbmm_::name, addbmm_::overload_name)
      .typed<addbmm_::schema>();
}

// aten::addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & addbmm_::call(at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    
    static auto op = create_addbmm__typed_handle();
    return op.call(self, batch1, batch2, beta, alpha);
}

// aten::addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & addbmm_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    
    static auto op = create_addbmm__typed_handle();
    return op.redispatch(dispatchKeySet, self, batch1, batch2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm_out, name, "aten::addbmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm_out, schema_str, "addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addbmm_out::schema> create_addbmm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addbmm_out::name, addbmm_out::overload_name)
      .typed<addbmm_out::schema>();
}

// aten::addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addbmm_out::call(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create_addbmm_out_typed_handle();
    return op.call(self, batch1, batch2, beta, alpha, out);
}

// aten::addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addbmm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create_addbmm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, batch1, batch2, beta, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm, name, "aten::addbmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm, schema_str, "addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor")

// aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<addbmm::schema> create_addbmm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addbmm::name, addbmm::overload_name)
      .typed<addbmm::schema>();
}

// aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor addbmm::call(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    
    static auto op = create_addbmm_typed_handle();
    return op.call(self, batch1, batch2, beta, alpha);
}

// aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor addbmm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    
    static auto op = create_addbmm_typed_handle();
    return op.redispatch(dispatchKeySet, self, batch1, batch2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_out, name, "aten::triu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_out, schema_str, "triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<triu_out::schema> create_triu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(triu_out::name, triu_out::overload_name)
      .typed<triu_out::schema>();
}

// aten::triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & triu_out::call(const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
    
    static auto op = create_triu_out_typed_handle();
    return op.call(self, diagonal, out);
}

// aten::triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & triu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
    
    static auto op = create_triu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, diagonal, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu, name, "aten::triu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu, schema_str, "triu(Tensor self, int diagonal=0) -> Tensor")

// aten::triu(Tensor self, int diagonal=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<triu::schema> create_triu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(triu::name, triu::overload_name)
      .typed<triu::schema>();
}

// aten::triu(Tensor self, int diagonal=0) -> Tensor
at::Tensor triu::call(const at::Tensor & self, int64_t diagonal) {
    
    static auto op = create_triu_typed_handle();
    return op.call(self, diagonal);
}

// aten::triu(Tensor self, int diagonal=0) -> Tensor
at::Tensor triu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal) {
    
    static auto op = create_triu_typed_handle();
    return op.redispatch(dispatchKeySet, self, diagonal);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Scalar_out, name, "aten::not_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Scalar_out, schema_str, "not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<not_equal_Scalar_out::schema> create_not_equal_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(not_equal_Scalar_out::name, not_equal_Scalar_out::overload_name)
      .typed<not_equal_Scalar_out::schema>();
}

// aten::not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & not_equal_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_not_equal_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & not_equal_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_not_equal_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Scalar, name, "aten::not_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Scalar, schema_str, "not_equal.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::not_equal.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<not_equal_Scalar::schema> create_not_equal_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(not_equal_Scalar::name, not_equal_Scalar::overload_name)
      .typed<not_equal_Scalar::schema>();
}

// aten::not_equal.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor not_equal_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_not_equal_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::not_equal.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor not_equal_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_not_equal_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Tensor_out, name, "aten::not_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Tensor_out, schema_str, "not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<not_equal_Tensor_out::schema> create_not_equal_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(not_equal_Tensor_out::name, not_equal_Tensor_out::overload_name)
      .typed<not_equal_Tensor_out::schema>();
}

// aten::not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & not_equal_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_not_equal_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & not_equal_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_not_equal_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Tensor, name, "aten::not_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Tensor, schema_str, "not_equal.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::not_equal.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<not_equal_Tensor::schema> create_not_equal_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(not_equal_Tensor::name, not_equal_Tensor::overload_name)
      .typed<not_equal_Tensor::schema>();
}

// aten::not_equal.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor not_equal_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_not_equal_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::not_equal.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor not_equal_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_not_equal_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal__Scalar, name, "aten::not_equal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal__Scalar, schema_str, "not_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::not_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<not_equal__Scalar::schema> create_not_equal__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(not_equal__Scalar::name, not_equal__Scalar::overload_name)
      .typed<not_equal__Scalar::schema>();
}

// aten::not_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & not_equal__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_not_equal__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::not_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & not_equal__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_not_equal__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal__Tensor, name, "aten::not_equal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal__Tensor, schema_str, "not_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::not_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<not_equal__Tensor::schema> create_not_equal__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(not_equal__Tensor::name, not_equal__Tensor::overload_name)
      .typed<not_equal__Tensor::schema>();
}

// aten::not_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & not_equal__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_not_equal__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::not_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & not_equal__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_not_equal__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Scalar_out, name, "aten::greater")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Scalar_out, schema_str, "greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<greater_Scalar_out::schema> create_greater_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater_Scalar_out::name, greater_Scalar_out::overload_name)
      .typed<greater_Scalar_out::schema>();
}

// aten::greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & greater_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_greater_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & greater_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_greater_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Scalar, name, "aten::greater")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Scalar, schema_str, "greater.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::greater.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<greater_Scalar::schema> create_greater_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater_Scalar::name, greater_Scalar::overload_name)
      .typed<greater_Scalar::schema>();
}

// aten::greater.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor greater_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_greater_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::greater.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor greater_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_greater_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Tensor_out, name, "aten::greater")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Tensor_out, schema_str, "greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<greater_Tensor_out::schema> create_greater_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater_Tensor_out::name, greater_Tensor_out::overload_name)
      .typed<greater_Tensor_out::schema>();
}

// aten::greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & greater_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_greater_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & greater_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_greater_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Tensor, name, "aten::greater")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Tensor, schema_str, "greater.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::greater.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<greater_Tensor::schema> create_greater_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater_Tensor::name, greater_Tensor::overload_name)
      .typed<greater_Tensor::schema>();
}

// aten::greater.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor greater_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_greater_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::greater.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor greater_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_greater_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater__Scalar, name, "aten::greater_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater__Scalar, schema_str, "greater_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::greater_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<greater__Scalar::schema> create_greater__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater__Scalar::name, greater__Scalar::overload_name)
      .typed<greater__Scalar::schema>();
}

// aten::greater_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & greater__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_greater__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::greater_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & greater__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_greater__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater__Tensor, name, "aten::greater_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater__Tensor, schema_str, "greater_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::greater_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<greater__Tensor::schema> create_greater__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater__Tensor::name, greater__Tensor::overload_name)
      .typed<greater__Tensor::schema>();
}

// aten::greater_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & greater__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_greater__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::greater_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & greater__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_greater__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_out, name, "aten::gather")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_out, schema_str, "gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)")

// aten::gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<gather_out::schema> create_gather_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gather_out::name, gather_out::overload_name)
      .typed<gather_out::schema>();
}

// aten::gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gather_out::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
    
    static auto op = create_gather_out_typed_handle();
    return op.call(self, dim, index, sparse_grad, out);
}

// aten::gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gather_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
    
    static auto op = create_gather_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, sparse_grad, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather, name, "aten::gather")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather, schema_str, "gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor")

// aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<gather::schema> create_gather_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gather::name, gather::overload_name)
      .typed<gather::schema>();
}

// aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor
at::Tensor gather::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
    
    static auto op = create_gather_typed_handle();
    return op.call(self, dim, index, sparse_grad);
}

// aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor
at::Tensor gather::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
    
    static auto op = create_gather_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, sparse_grad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_backward, name, "aten::gather_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_backward, schema_str, "gather_backward(Tensor grad, Tensor self, int dim, Tensor index, bool sparse_grad) -> Tensor")

// aten::gather_backward(Tensor grad, Tensor self, int dim, Tensor index, bool sparse_grad) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<gather_backward::schema> create_gather_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gather_backward::name, gather_backward::overload_name)
      .typed<gather_backward::schema>();
}

// aten::gather_backward(Tensor grad, Tensor self, int dim, Tensor index, bool sparse_grad) -> Tensor
at::Tensor gather_backward::call(const at::Tensor & grad, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
    
    static auto op = create_gather_backward_typed_handle();
    return op.call(grad, self, dim, index, sparse_grad);
}

// aten::gather_backward(Tensor grad, Tensor self, int dim, Tensor index, bool sparse_grad) -> Tensor
at::Tensor gather_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
    
    static auto op = create_gather_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self, dim, index, sparse_grad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_dimname_out, name, "aten::gather")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_dimname_out, schema_str, "gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)")

// aten::gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<gather_dimname_out::schema> create_gather_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gather_dimname_out::name, gather_dimname_out::overload_name)
      .typed<gather_dimname_out::schema>();
}

// aten::gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gather_dimname_out::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
    
    static auto op = create_gather_dimname_out_typed_handle();
    return op.call(self, dim, index, sparse_grad, out);
}

// aten::gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gather_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
    
    static auto op = create_gather_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, sparse_grad, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_dimname, name, "aten::gather")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_dimname, schema_str, "gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor")

// aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<gather_dimname::schema> create_gather_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gather_dimname::name, gather_dimname::overload_name)
      .typed<gather_dimname::schema>();
}

// aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor
at::Tensor gather_dimname::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad) {
    
    static auto op = create_gather_dimname_typed_handle();
    return op.call(self, dim, index, sparse_grad);
}

// aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor
at::Tensor gather_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad) {
    
    static auto op = create_gather_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, sparse_grad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross_entropy_loss, name, "aten::cross_entropy_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross_entropy_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross_entropy_loss, schema_str, "cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor")

// aten::cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cross_entropy_loss::schema> create_cross_entropy_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cross_entropy_loss::name, cross_entropy_loss::overload_name)
      .typed<cross_entropy_loss::schema>();
}

// aten::cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor
at::Tensor cross_entropy_loss::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, double label_smoothing) {
    
    static auto op = create_cross_entropy_loss_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index, label_smoothing);
}

// aten::cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor
at::Tensor cross_entropy_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, double label_smoothing) {
    
    static auto op = create_cross_entropy_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index, label_smoothing);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triangular_solve_X, name, "aten::triangular_solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triangular_solve_X, overload_name, "X")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triangular_solve_X, schema_str, "triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)")

// aten::triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)
static C10_NOINLINE c10::TypedOperatorHandle<triangular_solve_X::schema> create_triangular_solve_X_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(triangular_solve_X::name, triangular_solve_X::overload_name)
      .typed<triangular_solve_X::schema>();
}

// aten::triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)
::std::tuple<at::Tensor &,at::Tensor &> triangular_solve_X::call(const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular, at::Tensor & X, at::Tensor & M) {
    
    static auto op = create_triangular_solve_X_typed_handle();
    return op.call(self, A, upper, transpose, unitriangular, X, M);
}

// aten::triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)
::std::tuple<at::Tensor &,at::Tensor &> triangular_solve_X::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular, at::Tensor & X, at::Tensor & M) {
    
    static auto op = create_triangular_solve_X_typed_handle();
    return op.redispatch(dispatchKeySet, self, A, upper, transpose, unitriangular, X, M);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triangular_solve, name, "aten::triangular_solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triangular_solve, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triangular_solve, schema_str, "triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)")

// aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)
static C10_NOINLINE c10::TypedOperatorHandle<triangular_solve::schema> create_triangular_solve_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(triangular_solve::name, triangular_solve::overload_name)
      .typed<triangular_solve::schema>();
}

// aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)
::std::tuple<at::Tensor,at::Tensor> triangular_solve::call(const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular) {
    
    static auto op = create_triangular_solve_typed_handle();
    return op.call(self, A, upper, transpose, unitriangular);
}

// aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)
::std::tuple<at::Tensor,at::Tensor> triangular_solve::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular) {
    
    static auto op = create_triangular_solve_typed_handle();
    return op.redispatch(dispatchKeySet, self, A, upper, transpose, unitriangular);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_check_errors, name, "aten::_linalg_check_errors")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_check_errors, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_check_errors, schema_str, "_linalg_check_errors(Tensor info, str api_name, *, bool is_matrix) -> ()")

// aten::_linalg_check_errors(Tensor info, str api_name, *, bool is_matrix) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_linalg_check_errors::schema> create__linalg_check_errors_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_linalg_check_errors::name, _linalg_check_errors::overload_name)
      .typed<_linalg_check_errors::schema>();
}

// aten::_linalg_check_errors(Tensor info, str api_name, *, bool is_matrix) -> ()
void _linalg_check_errors::call(const at::Tensor & info, c10::string_view api_name, bool is_matrix) {
    
    static auto op = create__linalg_check_errors_typed_handle();
    return op.call(info, api_name, is_matrix);
}

// aten::_linalg_check_errors(Tensor info, str api_name, *, bool is_matrix) -> ()
void _linalg_check_errors::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & info, c10::string_view api_name, bool is_matrix) {
    
    static auto op = create__linalg_check_errors_typed_handle();
    return op.redispatch(dispatchKeySet, info, api_name, is_matrix);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_triangular_out, name, "aten::linalg_solve_triangular")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_triangular_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_triangular_out, schema_str, "linalg_solve_triangular.out(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_solve_triangular.out(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_solve_triangular_out::schema> create_linalg_solve_triangular_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_solve_triangular_out::name, linalg_solve_triangular_out::overload_name)
      .typed<linalg_solve_triangular_out::schema>();
}

// aten::linalg_solve_triangular.out(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_solve_triangular_out::call(const at::Tensor & self, const at::Tensor & B, bool upper, bool left, bool unitriangular, at::Tensor & out) {
    
    static auto op = create_linalg_solve_triangular_out_typed_handle();
    return op.call(self, B, upper, left, unitriangular, out);
}

// aten::linalg_solve_triangular.out(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_solve_triangular_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & B, bool upper, bool left, bool unitriangular, at::Tensor & out) {
    
    static auto op = create_linalg_solve_triangular_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, B, upper, left, unitriangular, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_triangular, name, "aten::linalg_solve_triangular")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_triangular, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_triangular, schema_str, "linalg_solve_triangular(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False) -> Tensor")

// aten::linalg_solve_triangular(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_solve_triangular::schema> create_linalg_solve_triangular_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_solve_triangular::name, linalg_solve_triangular::overload_name)
      .typed<linalg_solve_triangular::schema>();
}

// aten::linalg_solve_triangular(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False) -> Tensor
at::Tensor linalg_solve_triangular::call(const at::Tensor & self, const at::Tensor & B, bool upper, bool left, bool unitriangular) {
    
    static auto op = create_linalg_solve_triangular_typed_handle();
    return op.call(self, B, upper, left, unitriangular);
}

// aten::linalg_solve_triangular(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False) -> Tensor
at::Tensor linalg_solve_triangular::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & B, bool upper, bool left, bool unitriangular) {
    
    static auto op = create_linalg_solve_triangular_typed_handle();
    return op.redispatch(dispatchKeySet, self, B, upper, left, unitriangular);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ormqr_out, name, "aten::ormqr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ormqr_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ormqr_out, schema_str, "ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ormqr_out::schema> create_ormqr_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ormqr_out::name, ormqr_out::overload_name)
      .typed<ormqr_out::schema>();
}

// aten::ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ormqr_out::call(const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose, at::Tensor & out) {
    
    static auto op = create_ormqr_out_typed_handle();
    return op.call(self, input2, input3, left, transpose, out);
}

// aten::ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ormqr_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose, at::Tensor & out) {
    
    static auto op = create_ormqr_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, input2, input3, left, transpose, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ormqr, name, "aten::ormqr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ormqr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ormqr, schema_str, "ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor")

// aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ormqr::schema> create_ormqr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ormqr::name, ormqr::overload_name)
      .typed<ormqr::schema>();
}

// aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor
at::Tensor ormqr::call(const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose) {
    
    static auto op = create_ormqr_typed_handle();
    return op.call(self, input2, input3, left, transpose);
}

// aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor
at::Tensor ormqr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose) {
    
    static auto op = create_ormqr_typed_handle();
    return op.redispatch(dispatchKeySet, self, input2, input3, left, transpose);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0, name, "aten::i0")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0, schema_str, "i0(Tensor self) -> Tensor")

// aten::i0(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<i0::schema> create_i0_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(i0::name, i0::overload_name)
      .typed<i0::schema>();
}

// aten::i0(Tensor self) -> Tensor
at::Tensor i0::call(const at::Tensor & self) {
    
    static auto op = create_i0_typed_handle();
    return op.call(self);
}

// aten::i0(Tensor self) -> Tensor
at::Tensor i0::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_i0_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0_, name, "aten::i0_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0_, schema_str, "i0_(Tensor(a!) self) -> Tensor(a!)")

// aten::i0_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<i0_::schema> create_i0__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(i0_::name, i0_::overload_name)
      .typed<i0_::schema>();
}

// aten::i0_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & i0_::call(at::Tensor & self) {
    
    static auto op = create_i0__typed_handle();
    return op.call(self);
}

// aten::i0_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & i0_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_i0__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0_out, name, "aten::i0")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0_out, schema_str, "i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<i0_out::schema> create_i0_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(i0_out::name, i0_out::overload_name)
      .typed<i0_out::schema>();
}

// aten::i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & i0_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_i0_out_typed_handle();
    return op.call(self, out);
}

// aten::i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & i0_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_i0_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign, name, "aten::sign")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign, schema_str, "sign(Tensor self) -> Tensor")

// aten::sign(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sign::schema> create_sign_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sign::name, sign::overload_name)
      .typed<sign::schema>();
}

// aten::sign(Tensor self) -> Tensor
at::Tensor sign::call(const at::Tensor & self) {
    
    static auto op = create_sign_typed_handle();
    return op.call(self);
}

// aten::sign(Tensor self) -> Tensor
at::Tensor sign::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_sign_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign_, name, "aten::sign_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign_, schema_str, "sign_(Tensor(a!) self) -> Tensor(a!)")

// aten::sign_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sign_::schema> create_sign__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sign_::name, sign_::overload_name)
      .typed<sign_::schema>();
}

// aten::sign_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sign_::call(at::Tensor & self) {
    
    static auto op = create_sign__typed_handle();
    return op.call(self);
}

// aten::sign_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sign_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_sign__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign_out, name, "aten::sign")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign_out, schema_str, "sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sign_out::schema> create_sign_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sign_out::name, sign_out::overload_name)
      .typed<sign_out::schema>();
}

// aten::sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sign_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_sign_out_typed_handle();
    return op.call(self, out);
}

// aten::sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sign_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_sign_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Scalar_out, name, "aten::lerp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Scalar_out, schema_str, "lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)")

// aten::lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lerp_Scalar_out::schema> create_lerp_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lerp_Scalar_out::name, lerp_Scalar_out::overload_name)
      .typed<lerp_Scalar_out::schema>();
}

// aten::lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lerp_Scalar_out::call(const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight, at::Tensor & out) {
    
    static auto op = create_lerp_Scalar_out_typed_handle();
    return op.call(self, end, weight, out);
}

// aten::lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lerp_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight, at::Tensor & out) {
    
    static auto op = create_lerp_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, end, weight, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Tensor_out, name, "aten::lerp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Tensor_out, schema_str, "lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)")

// aten::lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lerp_Tensor_out::schema> create_lerp_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lerp_Tensor_out::name, lerp_Tensor_out::overload_name)
      .typed<lerp_Tensor_out::schema>();
}

// aten::lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lerp_Tensor_out::call(const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight, at::Tensor & out) {
    
    static auto op = create_lerp_Tensor_out_typed_handle();
    return op.call(self, end, weight, out);
}

// aten::lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lerp_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight, at::Tensor & out) {
    
    static auto op = create_lerp_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, end, weight, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Scalar, name, "aten::lerp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Scalar, schema_str, "lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor")

// aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<lerp_Scalar::schema> create_lerp_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lerp_Scalar::name, lerp_Scalar::overload_name)
      .typed<lerp_Scalar::schema>();
}

// aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor
at::Tensor lerp_Scalar::call(const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
    
    static auto op = create_lerp_Scalar_typed_handle();
    return op.call(self, end, weight);
}

// aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor
at::Tensor lerp_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
    
    static auto op = create_lerp_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, end, weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Tensor, name, "aten::lerp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Tensor, schema_str, "lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor")

// aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<lerp_Tensor::schema> create_lerp_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lerp_Tensor::name, lerp_Tensor::overload_name)
      .typed<lerp_Tensor::schema>();
}

// aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor
at::Tensor lerp_Tensor::call(const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
    
    static auto op = create_lerp_Tensor_typed_handle();
    return op.call(self, end, weight);
}

// aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor
at::Tensor lerp_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
    
    static auto op = create_lerp_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, end, weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min, schema_str, "min(Tensor self) -> Tensor")

// aten::min(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<min::schema> create_min_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min::name, min::overload_name)
      .typed<min::schema>();
}

// aten::min(Tensor self) -> Tensor
at::Tensor min::call(const at::Tensor & self) {
    
    static auto op = create_min_typed_handle();
    return op.call(self);
}

// aten::min(Tensor self) -> Tensor
at::Tensor min::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_min_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_unary_out, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_unary_out, overload_name, "unary_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_unary_out, schema_str, "min.unary_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::min.unary_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<min_unary_out::schema> create_min_unary_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min_unary_out::name, min_unary_out::overload_name)
      .typed<min_unary_out::schema>();
}

// aten::min.unary_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & min_unary_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_min_unary_out_typed_handle();
    return op.call(self, out);
}

// aten::min.unary_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & min_unary_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_min_unary_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmin, name, "aten::fmin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmin, schema_str, "fmin(Tensor self, Tensor other) -> Tensor")

// aten::fmin(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fmin::schema> create_fmin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fmin::name, fmin::overload_name)
      .typed<fmin::schema>();
}

// aten::fmin(Tensor self, Tensor other) -> Tensor
at::Tensor fmin::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_fmin_typed_handle();
    return op.call(self, other);
}

// aten::fmin(Tensor self, Tensor other) -> Tensor
at::Tensor fmin::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_fmin_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmin_out, name, "aten::fmin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmin_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmin_out, schema_str, "fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fmin_out::schema> create_fmin_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fmin_out::name, fmin_out::overload_name)
      .typed<fmin_out::schema>();
}

// aten::fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fmin_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_fmin_out_typed_handle();
    return op.call(self, other, out);
}

// aten::fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fmin_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_fmin_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_out, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_out, schema_str, "min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<min_out::schema> create_min_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min_out::name, min_out::overload_name)
      .typed<min_out::schema>();
}

// aten::min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & min_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_min_out_typed_handle();
    return op.call(self, other, out);
}

// aten::min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & min_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_min_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_other, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_other, overload_name, "other")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_other, schema_str, "min.other(Tensor self, Tensor other) -> Tensor")

// aten::min.other(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<min_other::schema> create_min_other_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min_other::name, min_other::overload_name)
      .typed<min_other::schema>();
}

// aten::min.other(Tensor self, Tensor other) -> Tensor
at::Tensor min_other::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_min_other_typed_handle();
    return op.call(self, other);
}

// aten::min.other(Tensor self, Tensor other) -> Tensor
at::Tensor min_other::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_min_other_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(equal, name, "aten::equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(equal, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(equal, schema_str, "equal(Tensor self, Tensor other) -> bool")

// aten::equal(Tensor self, Tensor other) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<equal::schema> create_equal_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(equal::name, equal::overload_name)
      .typed<equal::schema>();
}

// aten::equal(Tensor self, Tensor other) -> bool
bool equal::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_equal_typed_handle();
    return op.call(self, other);
}

// aten::equal(Tensor self, Tensor other) -> bool
bool equal::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_equal_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Scalar, name, "aten::_foreach_mul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Scalar, schema_str, "_foreach_mul.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]")

// aten::_foreach_mul.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul_Scalar::schema> create__foreach_mul_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul_Scalar::name, _foreach_mul_Scalar::overload_name)
      .typed<_foreach_mul_Scalar::schema>();
}

// aten::_foreach_mul.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_mul_Scalar::call(at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_mul_Scalar_typed_handle();
    return op.call(self, scalar);
}

// aten::_foreach_mul.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_mul_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_mul_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__Scalar, name, "aten::_foreach_mul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__Scalar, schema_str, "_foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()")

// aten::_foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul__Scalar::schema> create__foreach_mul__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul__Scalar::name, _foreach_mul__Scalar::overload_name)
      .typed<_foreach_mul__Scalar::schema>();
}

// aten::_foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_mul__Scalar::call(at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_mul__Scalar_typed_handle();
    return op.call(self, scalar);
}

// aten::_foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_mul__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_mul__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_List, name, "aten::_foreach_mul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_List, schema_str, "_foreach_mul.List(Tensor[] self, Tensor[] other) -> Tensor[]")

// aten::_foreach_mul.List(Tensor[] self, Tensor[] other) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul_List::schema> create__foreach_mul_List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul_List::name, _foreach_mul_List::overload_name)
      .typed<_foreach_mul_List::schema>();
}

// aten::_foreach_mul.List(Tensor[] self, Tensor[] other) -> Tensor[]
::std::vector<at::Tensor> _foreach_mul_List::call(at::TensorList self, at::TensorList other) {
    
    static auto op = create__foreach_mul_List_typed_handle();
    return op.call(self, other);
}

// aten::_foreach_mul.List(Tensor[] self, Tensor[] other) -> Tensor[]
::std::vector<at::Tensor> _foreach_mul_List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
    
    static auto op = create__foreach_mul_List_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__List, name, "aten::_foreach_mul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__List, schema_str, "_foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -> ()")

// aten::_foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul__List::schema> create__foreach_mul__List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul__List::name, _foreach_mul__List::overload_name)
      .typed<_foreach_mul__List::schema>();
}

// aten::_foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -> ()
void _foreach_mul__List::call(at::TensorList self, at::TensorList other) {
    
    static auto op = create__foreach_mul__List_typed_handle();
    return op.call(self, other);
}

// aten::_foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -> ()
void _foreach_mul__List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
    
    static auto op = create__foreach_mul__List_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_ScalarList, name, "aten::_foreach_mul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_ScalarList, schema_str, "_foreach_mul.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]")

// aten::_foreach_mul.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul_ScalarList::schema> create__foreach_mul_ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul_ScalarList::name, _foreach_mul_ScalarList::overload_name)
      .typed<_foreach_mul_ScalarList::schema>();
}

// aten::_foreach_mul.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_mul_ScalarList::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_mul_ScalarList_typed_handle();
    return op.call(self, scalars);
}

// aten::_foreach_mul.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_mul_ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_mul_ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__ScalarList, name, "aten::_foreach_mul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__ScalarList, schema_str, "_foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()")

// aten::_foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul__ScalarList::schema> create__foreach_mul__ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul__ScalarList::name, _foreach_mul__ScalarList::overload_name)
      .typed<_foreach_mul__ScalarList::schema>();
}

// aten::_foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_mul__ScalarList::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_mul__ScalarList_typed_handle();
    return op.call(self, scalars);
}

// aten::_foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_mul__ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_mul__ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Tensor, name, "aten::_foreach_mul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Tensor, schema_str, "_foreach_mul.Tensor(Tensor[] self, Tensor other) -> Tensor[]")

// aten::_foreach_mul.Tensor(Tensor[] self, Tensor other) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul_Tensor::schema> create__foreach_mul_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul_Tensor::name, _foreach_mul_Tensor::overload_name)
      .typed<_foreach_mul_Tensor::schema>();
}

// aten::_foreach_mul.Tensor(Tensor[] self, Tensor other) -> Tensor[]
::std::vector<at::Tensor> _foreach_mul_Tensor::call(at::TensorList self, const at::Tensor & other) {
    
    static auto op = create__foreach_mul_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::_foreach_mul.Tensor(Tensor[] self, Tensor other) -> Tensor[]
::std::vector<at::Tensor> _foreach_mul_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Tensor & other) {
    
    static auto op = create__foreach_mul_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__Tensor, name, "aten::_foreach_mul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__Tensor, schema_str, "_foreach_mul_.Tensor(Tensor(a!)[] self, Tensor other) -> ()")

// aten::_foreach_mul_.Tensor(Tensor(a!)[] self, Tensor other) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul__Tensor::schema> create__foreach_mul__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul__Tensor::name, _foreach_mul__Tensor::overload_name)
      .typed<_foreach_mul__Tensor::schema>();
}

// aten::_foreach_mul_.Tensor(Tensor(a!)[] self, Tensor other) -> ()
void _foreach_mul__Tensor::call(at::TensorList self, const at::Tensor & other) {
    
    static auto op = create__foreach_mul__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::_foreach_mul_.Tensor(Tensor(a!)[] self, Tensor other) -> ()
void _foreach_mul__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Tensor & other) {
    
    static auto op = create__foreach_mul__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Scalar, name, "aten::_foreach_div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Scalar, schema_str, "_foreach_div.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]")

// aten::_foreach_div.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div_Scalar::schema> create__foreach_div_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div_Scalar::name, _foreach_div_Scalar::overload_name)
      .typed<_foreach_div_Scalar::schema>();
}

// aten::_foreach_div.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_div_Scalar::call(at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_div_Scalar_typed_handle();
    return op.call(self, scalar);
}

// aten::_foreach_div.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_div_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_div_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__Scalar, name, "aten::_foreach_div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__Scalar, schema_str, "_foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()")

// aten::_foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div__Scalar::schema> create__foreach_div__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div__Scalar::name, _foreach_div__Scalar::overload_name)
      .typed<_foreach_div__Scalar::schema>();
}

// aten::_foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_div__Scalar::call(at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_div__Scalar_typed_handle();
    return op.call(self, scalar);
}

// aten::_foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_div__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_div__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_List, name, "aten::_foreach_div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_List, schema_str, "_foreach_div.List(Tensor[] self, Tensor[] other) -> Tensor[]")

// aten::_foreach_div.List(Tensor[] self, Tensor[] other) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div_List::schema> create__foreach_div_List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div_List::name, _foreach_div_List::overload_name)
      .typed<_foreach_div_List::schema>();
}

// aten::_foreach_div.List(Tensor[] self, Tensor[] other) -> Tensor[]
::std::vector<at::Tensor> _foreach_div_List::call(at::TensorList self, at::TensorList other) {
    
    static auto op = create__foreach_div_List_typed_handle();
    return op.call(self, other);
}

// aten::_foreach_div.List(Tensor[] self, Tensor[] other) -> Tensor[]
::std::vector<at::Tensor> _foreach_div_List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
    
    static auto op = create__foreach_div_List_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__List, name, "aten::_foreach_div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__List, schema_str, "_foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -> ()")

// aten::_foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div__List::schema> create__foreach_div__List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div__List::name, _foreach_div__List::overload_name)
      .typed<_foreach_div__List::schema>();
}

// aten::_foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -> ()
void _foreach_div__List::call(at::TensorList self, at::TensorList other) {
    
    static auto op = create__foreach_div__List_typed_handle();
    return op.call(self, other);
}

// aten::_foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -> ()
void _foreach_div__List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
    
    static auto op = create__foreach_div__List_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_ScalarList, name, "aten::_foreach_div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_ScalarList, schema_str, "_foreach_div.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]")

// aten::_foreach_div.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div_ScalarList::schema> create__foreach_div_ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div_ScalarList::name, _foreach_div_ScalarList::overload_name)
      .typed<_foreach_div_ScalarList::schema>();
}

// aten::_foreach_div.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_div_ScalarList::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_div_ScalarList_typed_handle();
    return op.call(self, scalars);
}

// aten::_foreach_div.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_div_ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_div_ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__ScalarList, name, "aten::_foreach_div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__ScalarList, schema_str, "_foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()")

// aten::_foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div__ScalarList::schema> create__foreach_div__ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div__ScalarList::name, _foreach_div__ScalarList::overload_name)
      .typed<_foreach_div__ScalarList::schema>();
}

// aten::_foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_div__ScalarList::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_div__ScalarList_typed_handle();
    return op.call(self, scalars);
}

// aten::_foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_div__ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_div__ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Tensor, name, "aten::_foreach_div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Tensor, schema_str, "_foreach_div.Tensor(Tensor[] self, Tensor other) -> Tensor[]")

// aten::_foreach_div.Tensor(Tensor[] self, Tensor other) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div_Tensor::schema> create__foreach_div_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div_Tensor::name, _foreach_div_Tensor::overload_name)
      .typed<_foreach_div_Tensor::schema>();
}

// aten::_foreach_div.Tensor(Tensor[] self, Tensor other) -> Tensor[]
::std::vector<at::Tensor> _foreach_div_Tensor::call(at::TensorList self, const at::Tensor & other) {
    
    static auto op = create__foreach_div_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::_foreach_div.Tensor(Tensor[] self, Tensor other) -> Tensor[]
::std::vector<at::Tensor> _foreach_div_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Tensor & other) {
    
    static auto op = create__foreach_div_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__Tensor, name, "aten::_foreach_div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__Tensor, schema_str, "_foreach_div_.Tensor(Tensor(a!)[] self, Tensor other) -> ()")

// aten::_foreach_div_.Tensor(Tensor(a!)[] self, Tensor other) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div__Tensor::schema> create__foreach_div__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div__Tensor::name, _foreach_div__Tensor::overload_name)
      .typed<_foreach_div__Tensor::schema>();
}

// aten::_foreach_div_.Tensor(Tensor(a!)[] self, Tensor other) -> ()
void _foreach_div__Tensor::call(at::TensorList self, const at::Tensor & other) {
    
    static auto op = create__foreach_div__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::_foreach_div_.Tensor(Tensor(a!)[] self, Tensor other) -> ()
void _foreach_div__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Tensor & other) {
    
    static auto op = create__foreach_div__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Scalar, name, "aten::_foreach_addcmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Scalar, schema_str, "_foreach_addcmul.Scalar(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]")

// aten::_foreach_addcmul.Scalar(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcmul_Scalar::schema> create__foreach_addcmul_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcmul_Scalar::name, _foreach_addcmul_Scalar::overload_name)
      .typed<_foreach_addcmul_Scalar::schema>();
}

// aten::_foreach_addcmul.Scalar(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
::std::vector<at::Tensor> _foreach_addcmul_Scalar::call(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
    
    static auto op = create__foreach_addcmul_Scalar_typed_handle();
    return op.call(self, tensor1, tensor2, value);
}

// aten::_foreach_addcmul.Scalar(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
::std::vector<at::Tensor> _foreach_addcmul_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
    
    static auto op = create__foreach_addcmul_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_ScalarList, name, "aten::_foreach_addcmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_ScalarList, schema_str, "_foreach_addcmul.ScalarList(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]")

// aten::_foreach_addcmul.ScalarList(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcmul_ScalarList::schema> create__foreach_addcmul_ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcmul_ScalarList::name, _foreach_addcmul_ScalarList::overload_name)
      .typed<_foreach_addcmul_ScalarList::schema>();
}

// aten::_foreach_addcmul.ScalarList(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_addcmul_ScalarList::call(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_addcmul_ScalarList_typed_handle();
    return op.call(self, tensor1, tensor2, scalars);
}

// aten::_foreach_addcmul.ScalarList(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_addcmul_ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_addcmul_ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Tensor, name, "aten::_foreach_addcmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Tensor, schema_str, "_foreach_addcmul.Tensor(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> Tensor[]")

// aten::_foreach_addcmul.Tensor(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcmul_Tensor::schema> create__foreach_addcmul_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcmul_Tensor::name, _foreach_addcmul_Tensor::overload_name)
      .typed<_foreach_addcmul_Tensor::schema>();
}

// aten::_foreach_addcmul.Tensor(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_addcmul_Tensor::call(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Tensor & scalars) {
    
    static auto op = create__foreach_addcmul_Tensor_typed_handle();
    return op.call(self, tensor1, tensor2, scalars);
}

// aten::_foreach_addcmul.Tensor(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_addcmul_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Tensor & scalars) {
    
    static auto op = create__foreach_addcmul_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__Scalar, name, "aten::_foreach_addcmul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__Scalar, schema_str, "_foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()")

// aten::_foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcmul__Scalar::schema> create__foreach_addcmul__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcmul__Scalar::name, _foreach_addcmul__Scalar::overload_name)
      .typed<_foreach_addcmul__Scalar::schema>();
}

// aten::_foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
void _foreach_addcmul__Scalar::call(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
    
    static auto op = create__foreach_addcmul__Scalar_typed_handle();
    return op.call(self, tensor1, tensor2, value);
}

// aten::_foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
void _foreach_addcmul__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
    
    static auto op = create__foreach_addcmul__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__ScalarList, name, "aten::_foreach_addcmul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__ScalarList, schema_str, "_foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()")

// aten::_foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcmul__ScalarList::schema> create__foreach_addcmul__ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcmul__ScalarList::name, _foreach_addcmul__ScalarList::overload_name)
      .typed<_foreach_addcmul__ScalarList::schema>();
}

// aten::_foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
void _foreach_addcmul__ScalarList::call(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_addcmul__ScalarList_typed_handle();
    return op.call(self, tensor1, tensor2, scalars);
}

// aten::_foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
void _foreach_addcmul__ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_addcmul__ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__Tensor, name, "aten::_foreach_addcmul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__Tensor, schema_str, "_foreach_addcmul_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> ()")

// aten::_foreach_addcmul_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcmul__Tensor::schema> create__foreach_addcmul__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcmul__Tensor::name, _foreach_addcmul__Tensor::overload_name)
      .typed<_foreach_addcmul__Tensor::schema>();
}

// aten::_foreach_addcmul_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> ()
void _foreach_addcmul__Tensor::call(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Tensor & scalars) {
    
    static auto op = create__foreach_addcmul__Tensor_typed_handle();
    return op.call(self, tensor1, tensor2, scalars);
}

// aten::_foreach_addcmul_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> ()
void _foreach_addcmul__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Tensor & scalars) {
    
    static auto op = create__foreach_addcmul__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin, name, "aten::_foreach_asin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin, schema_str, "_foreach_asin(Tensor[] self) -> Tensor[]")

// aten::_foreach_asin(Tensor[] self) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_asin::schema> create__foreach_asin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_asin::name, _foreach_asin::overload_name)
      .typed<_foreach_asin::schema>();
}

// aten::_foreach_asin(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_asin::call(at::TensorList self) {
    
    static auto op = create__foreach_asin_typed_handle();
    return op.call(self);
}

// aten::_foreach_asin(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_asin::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_asin_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin_, name, "aten::_foreach_asin_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin_, schema_str, "_foreach_asin_(Tensor(a!)[] self) -> ()")

// aten::_foreach_asin_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_asin_::schema> create__foreach_asin__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_asin_::name, _foreach_asin_::overload_name)
      .typed<_foreach_asin_::schema>();
}

// aten::_foreach_asin_(Tensor(a!)[] self) -> ()
void _foreach_asin_::call(at::TensorList self) {
    
    static auto op = create__foreach_asin__typed_handle();
    return op.call(self);
}

// aten::_foreach_asin_(Tensor(a!)[] self) -> ()
void _foreach_asin_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_asin__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos, name, "aten::_foreach_cos")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos, schema_str, "_foreach_cos(Tensor[] self) -> Tensor[]")

// aten::_foreach_cos(Tensor[] self) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_cos::schema> create__foreach_cos_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_cos::name, _foreach_cos::overload_name)
      .typed<_foreach_cos::schema>();
}

// aten::_foreach_cos(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_cos::call(at::TensorList self) {
    
    static auto op = create__foreach_cos_typed_handle();
    return op.call(self);
}

// aten::_foreach_cos(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_cos::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_cos_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos_, name, "aten::_foreach_cos_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos_, schema_str, "_foreach_cos_(Tensor(a!)[] self) -> ()")

// aten::_foreach_cos_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_cos_::schema> create__foreach_cos__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_cos_::name, _foreach_cos_::overload_name)
      .typed<_foreach_cos_::schema>();
}

// aten::_foreach_cos_(Tensor(a!)[] self) -> ()
void _foreach_cos_::call(at::TensorList self) {
    
    static auto op = create__foreach_cos__typed_handle();
    return op.call(self);
}

// aten::_foreach_cos_(Tensor(a!)[] self) -> ()
void _foreach_cos_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_cos__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor, name, "aten::_foreach_floor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor, schema_str, "_foreach_floor(Tensor[] self) -> Tensor[]")

// aten::_foreach_floor(Tensor[] self) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_floor::schema> create__foreach_floor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_floor::name, _foreach_floor::overload_name)
      .typed<_foreach_floor::schema>();
}

// aten::_foreach_floor(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_floor::call(at::TensorList self) {
    
    static auto op = create__foreach_floor_typed_handle();
    return op.call(self);
}

// aten::_foreach_floor(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_floor::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_floor_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor_, name, "aten::_foreach_floor_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor_, schema_str, "_foreach_floor_(Tensor(a!)[] self) -> ()")

// aten::_foreach_floor_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_floor_::schema> create__foreach_floor__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_floor_::name, _foreach_floor_::overload_name)
      .typed<_foreach_floor_::schema>();
}

// aten::_foreach_floor_(Tensor(a!)[] self) -> ()
void _foreach_floor_::call(at::TensorList self) {
    
    static auto op = create__foreach_floor__typed_handle();
    return op.call(self);
}

// aten::_foreach_floor_(Tensor(a!)[] self) -> ()
void _foreach_floor_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_floor__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh, name, "aten::_foreach_tanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh, schema_str, "_foreach_tanh(Tensor[] self) -> Tensor[]")

// aten::_foreach_tanh(Tensor[] self) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_tanh::schema> create__foreach_tanh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_tanh::name, _foreach_tanh::overload_name)
      .typed<_foreach_tanh::schema>();
}

// aten::_foreach_tanh(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_tanh::call(at::TensorList self) {
    
    static auto op = create__foreach_tanh_typed_handle();
    return op.call(self);
}

// aten::_foreach_tanh(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_tanh::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_tanh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh_, name, "aten::_foreach_tanh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh_, schema_str, "_foreach_tanh_(Tensor(a!)[] self) -> ()")

// aten::_foreach_tanh_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_tanh_::schema> create__foreach_tanh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_tanh_::name, _foreach_tanh_::overload_name)
      .typed<_foreach_tanh_::schema>();
}

// aten::_foreach_tanh_(Tensor(a!)[] self) -> ()
void _foreach_tanh_::call(at::TensorList self) {
    
    static auto op = create__foreach_tanh__typed_handle();
    return op.call(self);
}

// aten::_foreach_tanh_(Tensor(a!)[] self) -> ()
void _foreach_tanh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_tanh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_zero_, name, "aten::_foreach_zero_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_zero_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_zero_, schema_str, "_foreach_zero_(Tensor(a!)[] self) -> ()")

// aten::_foreach_zero_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_zero_::schema> create__foreach_zero__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_zero_::name, _foreach_zero_::overload_name)
      .typed<_foreach_zero_::schema>();
}

// aten::_foreach_zero_(Tensor(a!)[] self) -> ()
void _foreach_zero_::call(at::TensorList self) {
    
    static auto op = create__foreach_zero__typed_handle();
    return op.call(self);
}

// aten::_foreach_zero_(Tensor(a!)[] self) -> ()
void _foreach_zero_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_zero__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convert_indices_from_csr_to_coo, name, "aten::_convert_indices_from_csr_to_coo")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convert_indices_from_csr_to_coo, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convert_indices_from_csr_to_coo, schema_str, "_convert_indices_from_csr_to_coo(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False) -> Tensor")

// aten::_convert_indices_from_csr_to_coo(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_convert_indices_from_csr_to_coo::schema> create__convert_indices_from_csr_to_coo_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_convert_indices_from_csr_to_coo::name, _convert_indices_from_csr_to_coo::overload_name)
      .typed<_convert_indices_from_csr_to_coo::schema>();
}

// aten::_convert_indices_from_csr_to_coo(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False) -> Tensor
at::Tensor _convert_indices_from_csr_to_coo::call(const at::Tensor & crow_indices, const at::Tensor & col_indices, bool out_int32, bool transpose) {
    
    static auto op = create__convert_indices_from_csr_to_coo_typed_handle();
    return op.call(crow_indices, col_indices, out_int32, transpose);
}

// aten::_convert_indices_from_csr_to_coo(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False) -> Tensor
at::Tensor _convert_indices_from_csr_to_coo::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & crow_indices, const at::Tensor & col_indices, bool out_int32, bool transpose) {
    
    static auto op = create__convert_indices_from_csr_to_coo_typed_handle();
    return op.redispatch(dispatchKeySet, crow_indices, col_indices, out_int32, transpose);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convert_indices_from_csr_to_coo_out, name, "aten::_convert_indices_from_csr_to_coo")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convert_indices_from_csr_to_coo_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convert_indices_from_csr_to_coo_out, schema_str, "_convert_indices_from_csr_to_coo.out(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False, Tensor(a!) out) -> Tensor(a!)")

// aten::_convert_indices_from_csr_to_coo.out(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_convert_indices_from_csr_to_coo_out::schema> create__convert_indices_from_csr_to_coo_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_convert_indices_from_csr_to_coo_out::name, _convert_indices_from_csr_to_coo_out::overload_name)
      .typed<_convert_indices_from_csr_to_coo_out::schema>();
}

// aten::_convert_indices_from_csr_to_coo.out(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _convert_indices_from_csr_to_coo_out::call(const at::Tensor & crow_indices, const at::Tensor & col_indices, bool out_int32, bool transpose, at::Tensor & out) {
    
    static auto op = create__convert_indices_from_csr_to_coo_out_typed_handle();
    return op.call(crow_indices, col_indices, out_int32, transpose, out);
}

// aten::_convert_indices_from_csr_to_coo.out(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _convert_indices_from_csr_to_coo_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & crow_indices, const at::Tensor & col_indices, bool out_int32, bool transpose, at::Tensor & out) {
    
    static auto op = create__convert_indices_from_csr_to_coo_out_typed_handle();
    return op.redispatch(dispatchKeySet, crow_indices, col_indices, out_int32, transpose, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_out, name, "aten::nll_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_out, schema_str, "nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)")

// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss_out::schema> create_nll_loss_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss_out::name, nll_loss_out::overload_name)
      .typed<nll_loss_out::schema>();
}

// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nll_loss_out::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & out) {
    
    static auto op = create_nll_loss_out_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index, out);
}

// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nll_loss_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & out) {
    
    static auto op = create_nll_loss_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss, name, "aten::nll_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss, schema_str, "nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor")

// aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss::schema> create_nll_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss::name, nll_loss::overload_name)
      .typed<nll_loss::schema>();
}

// aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
at::Tensor nll_loss::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
    
    static auto op = create_nll_loss_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index);
}

// aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
at::Tensor nll_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
    
    static auto op = create_nll_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_backward_grad_input, name, "aten::nll_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_backward_grad_input, schema_str, "nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss_backward_grad_input::schema> create_nll_loss_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss_backward_grad_input::name, nll_loss_backward_grad_input::overload_name)
      .typed<nll_loss_backward_grad_input::schema>();
}

// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & nll_loss_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
    
    static auto op = create_nll_loss_backward_grad_input_typed_handle();
    return op.call(grad_output, self, target, weight, reduction, ignore_index, total_weight, grad_input);
}

// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & nll_loss_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
    
    static auto op = create_nll_loss_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, weight, reduction, ignore_index, total_weight, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_backward, name, "aten::nll_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_backward, schema_str, "nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor")

// aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss_backward::schema> create_nll_loss_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss_backward::name, nll_loss_backward::overload_name)
      .typed<nll_loss_backward::schema>();
}

// aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
at::Tensor nll_loss_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight) {
    
    static auto op = create_nll_loss_backward_typed_handle();
    return op.call(grad_output, self, target, weight, reduction, ignore_index, total_weight);
}

// aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
at::Tensor nll_loss_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight) {
    
    static auto op = create_nll_loss_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, weight, reduction, ignore_index, total_weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_backward_grad_input, name, "aten::smooth_l1_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_backward_grad_input, schema_str, "smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<smooth_l1_loss_backward_grad_input::schema> create_smooth_l1_loss_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(smooth_l1_loss_backward_grad_input::name, smooth_l1_loss_backward_grad_input::overload_name)
      .typed<smooth_l1_loss_backward_grad_input::schema>();
}

// aten::smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & smooth_l1_loss_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & grad_input) {
    
    static auto op = create_smooth_l1_loss_backward_grad_input_typed_handle();
    return op.call(grad_output, self, target, reduction, beta, grad_input);
}

// aten::smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & smooth_l1_loss_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & grad_input) {
    
    static auto op = create_smooth_l1_loss_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction, beta, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_backward, name, "aten::smooth_l1_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_backward, schema_str, "smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor")

// aten::smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<smooth_l1_loss_backward::schema> create_smooth_l1_loss_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(smooth_l1_loss_backward::name, smooth_l1_loss_backward::overload_name)
      .typed<smooth_l1_loss_backward::schema>();
}

// aten::smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor
at::Tensor smooth_l1_loss_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
    
    static auto op = create_smooth_l1_loss_backward_typed_handle();
    return op.call(grad_output, self, target, reduction, beta);
}

// aten::smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor
at::Tensor smooth_l1_loss_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
    
    static auto op = create_smooth_l1_loss_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction, beta);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_out, name, "aten::huber_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_out, schema_str, "huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<huber_loss_out::schema> create_huber_loss_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(huber_loss_out::name, huber_loss_out::overload_name)
      .typed<huber_loss_out::schema>();
}

// aten::huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & huber_loss_out::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & out) {
    
    static auto op = create_huber_loss_out_typed_handle();
    return op.call(self, target, reduction, delta, out);
}

// aten::huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & huber_loss_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & out) {
    
    static auto op = create_huber_loss_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction, delta, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss, name, "aten::huber_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss, schema_str, "huber_loss(Tensor self, Tensor target, int reduction=Mean, float delta=1.0) -> Tensor")

// aten::huber_loss(Tensor self, Tensor target, int reduction=Mean, float delta=1.0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<huber_loss::schema> create_huber_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(huber_loss::name, huber_loss::overload_name)
      .typed<huber_loss::schema>();
}

// aten::huber_loss(Tensor self, Tensor target, int reduction=Mean, float delta=1.0) -> Tensor
at::Tensor huber_loss::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
    
    static auto op = create_huber_loss_typed_handle();
    return op.call(self, target, reduction, delta);
}

// aten::huber_loss(Tensor self, Tensor target, int reduction=Mean, float delta=1.0) -> Tensor
at::Tensor huber_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
    
    static auto op = create_huber_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction, delta);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_backward_out, name, "aten::huber_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_backward_out, schema_str, "huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<huber_loss_backward_out::schema> create_huber_loss_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(huber_loss_backward_out::name, huber_loss_backward_out::overload_name)
      .typed<huber_loss_backward_out::schema>();
}

// aten::huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & huber_loss_backward_out::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & grad_input) {
    
    static auto op = create_huber_loss_backward_out_typed_handle();
    return op.call(grad_output, self, target, reduction, delta, grad_input);
}

// aten::huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & huber_loss_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & grad_input) {
    
    static auto op = create_huber_loss_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction, delta, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_backward, name, "aten::huber_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_backward, schema_str, "huber_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta) -> Tensor")

// aten::huber_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<huber_loss_backward::schema> create_huber_loss_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(huber_loss_backward::name, huber_loss_backward::overload_name)
      .typed<huber_loss_backward::schema>();
}

// aten::huber_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta) -> Tensor
at::Tensor huber_loss_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
    
    static auto op = create_huber_loss_backward_typed_handle();
    return op.call(grad_output, self, target, reduction, delta);
}

// aten::huber_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta) -> Tensor
at::Tensor huber_loss_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
    
    static auto op = create_huber_loss_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction, delta);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_out, name, "aten::hardsigmoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_out, schema_str, "hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardsigmoid_out::schema> create_hardsigmoid_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardsigmoid_out::name, hardsigmoid_out::overload_name)
      .typed<hardsigmoid_out::schema>();
}

// aten::hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardsigmoid_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_hardsigmoid_out_typed_handle();
    return op.call(self, out);
}

// aten::hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardsigmoid_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_hardsigmoid_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid, name, "aten::hardsigmoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid, schema_str, "hardsigmoid(Tensor self) -> Tensor")

// aten::hardsigmoid(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hardsigmoid::schema> create_hardsigmoid_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardsigmoid::name, hardsigmoid::overload_name)
      .typed<hardsigmoid::schema>();
}

// aten::hardsigmoid(Tensor self) -> Tensor
at::Tensor hardsigmoid::call(const at::Tensor & self) {
    
    static auto op = create_hardsigmoid_typed_handle();
    return op.call(self);
}

// aten::hardsigmoid(Tensor self) -> Tensor
at::Tensor hardsigmoid::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_hardsigmoid_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_, name, "aten::hardsigmoid_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_, schema_str, "hardsigmoid_(Tensor(a!) self) -> Tensor(a!)")

// aten::hardsigmoid_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardsigmoid_::schema> create_hardsigmoid__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardsigmoid_::name, hardsigmoid_::overload_name)
      .typed<hardsigmoid_::schema>();
}

// aten::hardsigmoid_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & hardsigmoid_::call(at::Tensor & self) {
    
    static auto op = create_hardsigmoid__typed_handle();
    return op.call(self);
}

// aten::hardsigmoid_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & hardsigmoid_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_hardsigmoid__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_out, name, "aten::log_sigmoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_out, schema_str, "log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log_sigmoid_out::schema> create_log_sigmoid_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_sigmoid_out::name, log_sigmoid_out::overload_name)
      .typed<log_sigmoid_out::schema>();
}

// aten::log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & log_sigmoid_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_log_sigmoid_out_typed_handle();
    return op.call(self, out);
}

// aten::log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & log_sigmoid_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_log_sigmoid_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid, name, "aten::log_sigmoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid, schema_str, "log_sigmoid(Tensor self) -> Tensor")

// aten::log_sigmoid(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<log_sigmoid::schema> create_log_sigmoid_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_sigmoid::name, log_sigmoid::overload_name)
      .typed<log_sigmoid::schema>();
}

// aten::log_sigmoid(Tensor self) -> Tensor
at::Tensor log_sigmoid::call(const at::Tensor & self) {
    
    static auto op = create_log_sigmoid_typed_handle();
    return op.call(self);
}

// aten::log_sigmoid(Tensor self) -> Tensor
at::Tensor log_sigmoid::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_log_sigmoid_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool2d_out, name, "aten::adaptive_avg_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool2d_out, schema_str, "adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_avg_pool2d_out::schema> create_adaptive_avg_pool2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_avg_pool2d_out::name, adaptive_avg_pool2d_out::overload_name)
      .typed<adaptive_avg_pool2d_out::schema>();
}

// aten::adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & adaptive_avg_pool2d_out::call(const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
    
    static auto op = create_adaptive_avg_pool2d_out_typed_handle();
    return op.call(self, output_size, out);
}

// aten::adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & adaptive_avg_pool2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
    
    static auto op = create_adaptive_avg_pool2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool2d, name, "aten::adaptive_avg_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool2d, schema_str, "adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor")

// aten::adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_avg_pool2d::schema> create_adaptive_avg_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_avg_pool2d::name, adaptive_avg_pool2d::overload_name)
      .typed<adaptive_avg_pool2d::schema>();
}

// aten::adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor
at::Tensor adaptive_avg_pool2d::call(const at::Tensor & self, c10::SymIntArrayRef output_size) {
    
    static auto op = create_adaptive_avg_pool2d_typed_handle();
    return op.call(self, output_size);
}

// aten::adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor
at::Tensor adaptive_avg_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size) {
    
    static auto op = create_adaptive_avg_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d_out, name, "aten::adaptive_avg_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d_out, schema_str, "adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_avg_pool3d_out::schema> create_adaptive_avg_pool3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_avg_pool3d_out::name, adaptive_avg_pool3d_out::overload_name)
      .typed<adaptive_avg_pool3d_out::schema>();
}

// aten::adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & adaptive_avg_pool3d_out::call(const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
    
    static auto op = create_adaptive_avg_pool3d_out_typed_handle();
    return op.call(self, output_size, out);
}

// aten::adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & adaptive_avg_pool3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
    
    static auto op = create_adaptive_avg_pool3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d, name, "aten::adaptive_avg_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d, schema_str, "adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor")

// aten::adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_avg_pool3d::schema> create_adaptive_avg_pool3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_avg_pool3d::name, adaptive_avg_pool3d::overload_name)
      .typed<adaptive_avg_pool3d::schema>();
}

// aten::adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor
at::Tensor adaptive_avg_pool3d::call(const at::Tensor & self, c10::SymIntArrayRef output_size) {
    
    static auto op = create_adaptive_avg_pool3d_typed_handle();
    return op.call(self, output_size);
}

// aten::adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor
at::Tensor adaptive_avg_pool3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size) {
    
    static auto op = create_adaptive_avg_pool3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d, name, "aten::_adaptive_avg_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d, schema_str, "_adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor")

// aten::_adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_adaptive_avg_pool3d::schema> create__adaptive_avg_pool3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_adaptive_avg_pool3d::name, _adaptive_avg_pool3d::overload_name)
      .typed<_adaptive_avg_pool3d::schema>();
}

// aten::_adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor
at::Tensor _adaptive_avg_pool3d::call(const at::Tensor & self, c10::SymIntArrayRef output_size) {
    
    static auto op = create__adaptive_avg_pool3d_typed_handle();
    return op.call(self, output_size);
}

// aten::_adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor
at::Tensor _adaptive_avg_pool3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size) {
    
    static auto op = create__adaptive_avg_pool3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_out, name, "aten::adaptive_max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_out, schema_str, "adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))")

// aten::adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool2d_out::schema> create_adaptive_max_pool2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool2d_out::name, adaptive_max_pool2d_out::overload_name)
      .typed<adaptive_max_pool2d_out::schema>();
}

// aten::adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> adaptive_max_pool2d_out::call(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
    
    static auto op = create_adaptive_max_pool2d_out_typed_handle();
    return op.call(self, output_size, out, indices);
}

// aten::adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> adaptive_max_pool2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
    
    static auto op = create_adaptive_max_pool2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, out, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d, name, "aten::adaptive_max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d, schema_str, "adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)")

// aten::adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool2d::schema> create_adaptive_max_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool2d::name, adaptive_max_pool2d::overload_name)
      .typed<adaptive_max_pool2d::schema>();
}

// aten::adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool2d::call(const at::Tensor & self, at::IntArrayRef output_size) {
    
    static auto op = create_adaptive_max_pool2d_typed_handle();
    return op.call(self, output_size);
}

// aten::adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
    
    static auto op = create_adaptive_max_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_out, name, "aten::adaptive_max_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_out, schema_str, "adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))")

// aten::adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool3d_out::schema> create_adaptive_max_pool3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool3d_out::name, adaptive_max_pool3d_out::overload_name)
      .typed<adaptive_max_pool3d_out::schema>();
}

// aten::adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> adaptive_max_pool3d_out::call(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
    
    static auto op = create_adaptive_max_pool3d_out_typed_handle();
    return op.call(self, output_size, out, indices);
}

// aten::adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> adaptive_max_pool3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
    
    static auto op = create_adaptive_max_pool3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, out, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d, name, "aten::adaptive_max_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d, schema_str, "adaptive_max_pool3d(Tensor self, int[3] output_size) -> (Tensor, Tensor)")

// aten::adaptive_max_pool3d(Tensor self, int[3] output_size) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool3d::schema> create_adaptive_max_pool3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool3d::name, adaptive_max_pool3d::overload_name)
      .typed<adaptive_max_pool3d::schema>();
}

// aten::adaptive_max_pool3d(Tensor self, int[3] output_size) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool3d::call(const at::Tensor & self, at::IntArrayRef output_size) {
    
    static auto op = create_adaptive_max_pool3d_typed_handle();
    return op.call(self, output_size);
}

// aten::adaptive_max_pool3d(Tensor self, int[3] output_size) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
    
    static auto op = create_adaptive_max_pool3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_backward_grad_input, name, "aten::avg_pool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_backward_grad_input, schema_str, "avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<avg_pool2d_backward_grad_input::schema> create_avg_pool2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(avg_pool2d_backward_grad_input::name, avg_pool2d_backward_grad_input::overload_name)
      .typed<avg_pool2d_backward_grad_input::schema>();
}

// aten::avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & avg_pool2d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
    
    static auto op = create_avg_pool2d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input);
}

// aten::avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & avg_pool2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
    
    static auto op = create_avg_pool2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_backward, name, "aten::avg_pool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_backward, schema_str, "avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor")

// aten::avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<avg_pool2d_backward::schema> create_avg_pool2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(avg_pool2d_backward::name, avg_pool2d_backward::overload_name)
      .typed<avg_pool2d_backward::schema>();
}

// aten::avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
at::Tensor avg_pool2d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    
    static auto op = create_avg_pool2d_backward_typed_handle();
    return op.call(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

// aten::avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
at::Tensor avg_pool2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    
    static auto op = create_avg_pool2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d_output, name, "aten::fractional_max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d_output, overload_name, "output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d_output, schema_str, "fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))")

// aten::fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<fractional_max_pool2d_output::schema> create_fractional_max_pool2d_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fractional_max_pool2d_output::name, fractional_max_pool2d_output::overload_name)
      .typed<fractional_max_pool2d_output::schema>();
}

// aten::fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> fractional_max_pool2d_output::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
    
    static auto op = create_fractional_max_pool2d_output_typed_handle();
    return op.call(self, kernel_size, output_size, random_samples, output, indices);
}

// aten::fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> fractional_max_pool2d_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
    
    static auto op = create_fractional_max_pool2d_output_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, output_size, random_samples, output, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d, name, "aten::fractional_max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d, schema_str, "fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples) -> (Tensor, Tensor)")

// aten::fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<fractional_max_pool2d::schema> create_fractional_max_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fractional_max_pool2d::name, fractional_max_pool2d::overload_name)
      .typed<fractional_max_pool2d::schema>();
}

// aten::fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> fractional_max_pool2d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples) {
    
    static auto op = create_fractional_max_pool2d_typed_handle();
    return op.call(self, kernel_size, output_size, random_samples);
}

// aten::fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> fractional_max_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples) {
    
    static auto op = create_fractional_max_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, output_size, random_samples);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d_out, name, "aten::max_unpool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d_out, schema_str, "max_unpool2d.out(Tensor self, Tensor indices, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::max_unpool2d.out(Tensor self, Tensor indices, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<max_unpool2d_out::schema> create_max_unpool2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_unpool2d_out::name, max_unpool2d_out::overload_name)
      .typed<max_unpool2d_out::schema>();
}

// aten::max_unpool2d.out(Tensor self, Tensor indices, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & max_unpool2d_out::call(const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size, at::Tensor & out) {
    
    static auto op = create_max_unpool2d_out_typed_handle();
    return op.call(self, indices, output_size, out);
}

// aten::max_unpool2d.out(Tensor self, Tensor indices, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & max_unpool2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size, at::Tensor & out) {
    
    static auto op = create_max_unpool2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, output_size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d, name, "aten::max_unpool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d, schema_str, "max_unpool2d(Tensor self, Tensor indices, SymInt[2] output_size) -> Tensor")

// aten::max_unpool2d(Tensor self, Tensor indices, SymInt[2] output_size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max_unpool2d::schema> create_max_unpool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_unpool2d::name, max_unpool2d::overload_name)
      .typed<max_unpool2d::schema>();
}

// aten::max_unpool2d(Tensor self, Tensor indices, SymInt[2] output_size) -> Tensor
at::Tensor max_unpool2d::call(const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size) {
    
    static auto op = create_max_unpool2d_typed_handle();
    return op.call(self, indices, output_size);
}

// aten::max_unpool2d(Tensor self, Tensor indices, SymInt[2] output_size) -> Tensor
at::Tensor max_unpool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size) {
    
    static auto op = create_max_unpool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d_out, name, "aten::max_unpool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d_out, schema_str, "max_unpool3d.out(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)")

// aten::max_unpool3d.out(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<max_unpool3d_out::schema> create_max_unpool3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_unpool3d_out::name, max_unpool3d_out::overload_name)
      .typed<max_unpool3d_out::schema>();
}

// aten::max_unpool3d.out(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & max_unpool3d_out::call(const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
    
    static auto op = create_max_unpool3d_out_typed_handle();
    return op.call(self, indices, output_size, stride, padding, out);
}

// aten::max_unpool3d.out(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & max_unpool3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
    
    static auto op = create_max_unpool3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, output_size, stride, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d, name, "aten::max_unpool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d, schema_str, "max_unpool3d(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding) -> Tensor")

// aten::max_unpool3d(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max_unpool3d::schema> create_max_unpool3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_unpool3d::name, max_unpool3d::overload_name)
      .typed<max_unpool3d::schema>();
}

// aten::max_unpool3d(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding) -> Tensor
at::Tensor max_unpool3d::call(const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding) {
    
    static auto op = create_max_unpool3d_typed_handle();
    return op.call(self, indices, output_size, stride, padding);
}

// aten::max_unpool3d(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding) -> Tensor
at::Tensor max_unpool3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding) {
    
    static auto op = create_max_unpool3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, output_size, stride, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_backward_grad_input, name, "aten::reflection_pad3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_backward_grad_input, schema_str, "reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad3d_backward_grad_input::schema> create_reflection_pad3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad3d_backward_grad_input::name, reflection_pad3d_backward_grad_input::overload_name)
      .typed<reflection_pad3d_backward_grad_input::schema>();
}

// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & reflection_pad3d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
    
    static auto op = create_reflection_pad3d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, padding, grad_input);
}

// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & reflection_pad3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
    
    static auto op = create_reflection_pad3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_backward, name, "aten::reflection_pad3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_backward, schema_str, "reflection_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor")

// aten::reflection_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad3d_backward::schema> create_reflection_pad3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad3d_backward::name, reflection_pad3d_backward::overload_name)
      .typed<reflection_pad3d_backward::schema>();
}

// aten::reflection_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor
at::Tensor reflection_pad3d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding) {
    
    static auto op = create_reflection_pad3d_backward_typed_handle();
    return op.call(grad_output, self, padding);
}

// aten::reflection_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor
at::Tensor reflection_pad3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding) {
    
    static auto op = create_reflection_pad3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_backward_grad_input, name, "aten::replication_pad2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_backward_grad_input, schema_str, "replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad2d_backward_grad_input::schema> create_replication_pad2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad2d_backward_grad_input::name, replication_pad2d_backward_grad_input::overload_name)
      .typed<replication_pad2d_backward_grad_input::schema>();
}

// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & replication_pad2d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
    
    static auto op = create_replication_pad2d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, padding, grad_input);
}

// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & replication_pad2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
    
    static auto op = create_replication_pad2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_backward, name, "aten::replication_pad2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_backward, schema_str, "replication_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor")

// aten::replication_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad2d_backward::schema> create_replication_pad2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad2d_backward::name, replication_pad2d_backward::overload_name)
      .typed<replication_pad2d_backward::schema>();
}

// aten::replication_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor
at::Tensor replication_pad2d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding) {
    
    static auto op = create_replication_pad2d_backward_typed_handle();
    return op.call(grad_output, self, padding);
}

// aten::replication_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor
at::Tensor replication_pad2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding) {
    
    static auto op = create_replication_pad2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d_out, name, "aten::replication_pad3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d_out, schema_str, "replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)")

// aten::replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad3d_out::schema> create_replication_pad3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad3d_out::name, replication_pad3d_out::overload_name)
      .typed<replication_pad3d_out::schema>();
}

// aten::replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & replication_pad3d_out::call(const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
    
    static auto op = create_replication_pad3d_out_typed_handle();
    return op.call(self, padding, out);
}

// aten::replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & replication_pad3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
    
    static auto op = create_replication_pad3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d, name, "aten::replication_pad3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d, schema_str, "replication_pad3d(Tensor self, SymInt[6] padding) -> Tensor")

// aten::replication_pad3d(Tensor self, SymInt[6] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad3d::schema> create_replication_pad3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad3d::name, replication_pad3d::overload_name)
      .typed<replication_pad3d::schema>();
}

// aten::replication_pad3d(Tensor self, SymInt[6] padding) -> Tensor
at::Tensor replication_pad3d::call(const at::Tensor & self, c10::SymIntArrayRef padding) {
    
    static auto op = create_replication_pad3d_typed_handle();
    return op.call(self, padding);
}

// aten::replication_pad3d(Tensor self, SymInt[6] padding) -> Tensor
at::Tensor replication_pad3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding) {
    
    static auto op = create_replication_pad3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_vec, name, "aten::upsample_linear1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_vec, schema_str, "upsample_linear1d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor")

// aten::upsample_linear1d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_linear1d_vec::schema> create_upsample_linear1d_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_linear1d_vec::name, upsample_linear1d_vec::overload_name)
      .typed<upsample_linear1d_vec::schema>();
}

// aten::upsample_linear1d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_linear1d_vec::call(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    
    static auto op = create_upsample_linear1d_vec_typed_handle();
    return op.call(input, output_size, align_corners, scale_factors);
}

// aten::upsample_linear1d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_linear1d_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    
    static auto op = create_upsample_linear1d_vec_typed_handle();
    return op.redispatch(dispatchKeySet, input, output_size, align_corners, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_vec, name, "aten::upsample_bilinear2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_vec, schema_str, "upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor")

// aten::upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bilinear2d_vec::schema> create_upsample_bilinear2d_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bilinear2d_vec::name, upsample_bilinear2d_vec::overload_name)
      .typed<upsample_bilinear2d_vec::schema>();
}

// aten::upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_bilinear2d_vec::call(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    
    static auto op = create_upsample_bilinear2d_vec_typed_handle();
    return op.call(input, output_size, align_corners, scale_factors);
}

// aten::upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_bilinear2d_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    
    static auto op = create_upsample_bilinear2d_vec_typed_handle();
    return op.redispatch(dispatchKeySet, input, output_size, align_corners, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_vec, name, "aten::upsample_bicubic2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_vec, schema_str, "upsample_bicubic2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor")

// aten::upsample_bicubic2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bicubic2d_vec::schema> create_upsample_bicubic2d_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bicubic2d_vec::name, upsample_bicubic2d_vec::overload_name)
      .typed<upsample_bicubic2d_vec::schema>();
}

// aten::upsample_bicubic2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_bicubic2d_vec::call(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    
    static auto op = create_upsample_bicubic2d_vec_typed_handle();
    return op.call(input, output_size, align_corners, scale_factors);
}

// aten::upsample_bicubic2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_bicubic2d_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    
    static auto op = create_upsample_bicubic2d_vec_typed_handle();
    return op.redispatch(dispatchKeySet, input, output_size, align_corners, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_vec, name, "aten::upsample_nearest2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_vec, schema_str, "upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor")

// aten::upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest2d_vec::schema> create_upsample_nearest2d_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest2d_vec::name, upsample_nearest2d_vec::overload_name)
      .typed<upsample_nearest2d_vec::schema>();
}

// aten::upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest2d_vec::call(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    
    static auto op = create_upsample_nearest2d_vec_typed_handle();
    return op.call(input, output_size, scale_factors);
}

// aten::upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest2d_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    
    static auto op = create_upsample_nearest2d_vec_typed_handle();
    return op.redispatch(dispatchKeySet, input, output_size, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_out, name, "aten::upsample_linear1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_out, schema_str, "upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_linear1d_out::schema> create_upsample_linear1d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_linear1d_out::name, upsample_linear1d_out::overload_name)
      .typed<upsample_linear1d_out::schema>();
}

// aten::upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_linear1d_out::call(const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales, at::Tensor & out) {
    
    static auto op = create_upsample_linear1d_out_typed_handle();
    return op.call(self, output_size, align_corners, scales, out);
}

// aten::upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_linear1d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales, at::Tensor & out) {
    
    static auto op = create_upsample_linear1d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, align_corners, scales, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d, name, "aten::upsample_linear1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d, schema_str, "upsample_linear1d(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None) -> Tensor")

// aten::upsample_linear1d(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_linear1d::schema> create_upsample_linear1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_linear1d::name, upsample_linear1d::overload_name)
      .typed<upsample_linear1d::schema>();
}

// aten::upsample_linear1d(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None) -> Tensor
at::Tensor upsample_linear1d::call(const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales) {
    
    static auto op = create_upsample_linear1d_typed_handle();
    return op.call(self, output_size, align_corners, scales);
}

// aten::upsample_linear1d(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None) -> Tensor
at::Tensor upsample_linear1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales) {
    
    static auto op = create_upsample_linear1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, align_corners, scales);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_out, name, "aten::upsample_bilinear2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_out, schema_str, "upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bilinear2d_out::schema> create_upsample_bilinear2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bilinear2d_out::name, upsample_bilinear2d_out::overload_name)
      .typed<upsample_bilinear2d_out::schema>();
}

// aten::upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_bilinear2d_out::call(const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    
    static auto op = create_upsample_bilinear2d_out_typed_handle();
    return op.call(self, output_size, align_corners, scales_h, scales_w, out);
}

// aten::upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_bilinear2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    
    static auto op = create_upsample_bilinear2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, align_corners, scales_h, scales_w, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d, name, "aten::upsample_bilinear2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d, schema_str, "upsample_bilinear2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_bilinear2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bilinear2d::schema> create_upsample_bilinear2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bilinear2d::name, upsample_bilinear2d::overload_name)
      .typed<upsample_bilinear2d::schema>();
}

// aten::upsample_bilinear2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_bilinear2d::call(const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    
    static auto op = create_upsample_bilinear2d_typed_handle();
    return op.call(self, output_size, align_corners, scales_h, scales_w);
}

// aten::upsample_bilinear2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_bilinear2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    
    static auto op = create_upsample_bilinear2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, align_corners, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_out, name, "aten::upsample_bicubic2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_out, schema_str, "upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bicubic2d_out::schema> create_upsample_bicubic2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bicubic2d_out::name, upsample_bicubic2d_out::overload_name)
      .typed<upsample_bicubic2d_out::schema>();
}

// aten::upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_bicubic2d_out::call(const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    
    static auto op = create_upsample_bicubic2d_out_typed_handle();
    return op.call(self, output_size, align_corners, scales_h, scales_w, out);
}

// aten::upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_bicubic2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    
    static auto op = create_upsample_bicubic2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, align_corners, scales_h, scales_w, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d, name, "aten::upsample_bicubic2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d, schema_str, "upsample_bicubic2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_bicubic2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bicubic2d::schema> create_upsample_bicubic2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bicubic2d::name, upsample_bicubic2d::overload_name)
      .typed<upsample_bicubic2d::schema>();
}

// aten::upsample_bicubic2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_bicubic2d::call(const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    
    static auto op = create_upsample_bicubic2d_typed_handle();
    return op.call(self, output_size, align_corners, scales_h, scales_w);
}

// aten::upsample_bicubic2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_bicubic2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    
    static auto op = create_upsample_bicubic2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, align_corners, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward_grad_input, name, "aten::upsample_bicubic2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward_grad_input, schema_str, "upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bicubic2d_backward_grad_input::schema> create_upsample_bicubic2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bicubic2d_backward_grad_input::name, upsample_bicubic2d_backward_grad_input::overload_name)
      .typed<upsample_bicubic2d_backward_grad_input::schema>();
}

// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_bicubic2d_backward_grad_input::call(const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    
    static auto op = create_upsample_bicubic2d_backward_grad_input_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scales_h, scales_w, grad_input);
}

// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_bicubic2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    
    static auto op = create_upsample_bicubic2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scales_h, scales_w, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward, name, "aten::upsample_bicubic2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward, schema_str, "upsample_bicubic2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_bicubic2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bicubic2d_backward::schema> create_upsample_bicubic2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bicubic2d_backward::name, upsample_bicubic2d_backward::overload_name)
      .typed<upsample_bicubic2d_backward::schema>();
}

// aten::upsample_bicubic2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_bicubic2d_backward::call(const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    
    static auto op = create_upsample_bicubic2d_backward_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scales_h, scales_w);
}

// aten::upsample_bicubic2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_bicubic2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    
    static auto op = create_upsample_bicubic2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward_grad_input, name, "aten::upsample_trilinear3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward_grad_input, schema_str, "upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_trilinear3d_backward_grad_input::schema> create_upsample_trilinear3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_trilinear3d_backward_grad_input::name, upsample_trilinear3d_backward_grad_input::overload_name)
      .typed<upsample_trilinear3d_backward_grad_input::schema>();
}

// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_trilinear3d_backward_grad_input::call(const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    
    static auto op = create_upsample_trilinear3d_backward_grad_input_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w, grad_input);
}

// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_trilinear3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    
    static auto op = create_upsample_trilinear3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward, name, "aten::upsample_trilinear3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward, schema_str, "upsample_trilinear3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_trilinear3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_trilinear3d_backward::schema> create_upsample_trilinear3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_trilinear3d_backward::name, upsample_trilinear3d_backward::overload_name)
      .typed<upsample_trilinear3d_backward::schema>();
}

// aten::upsample_trilinear3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_trilinear3d_backward::call(const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    
    static auto op = create_upsample_trilinear3d_backward_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w);
}

// aten::upsample_trilinear3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_trilinear3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    
    static auto op = create_upsample_trilinear3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_out, name, "aten::upsample_nearest2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_out, schema_str, "upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest2d_out::schema> create_upsample_nearest2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest2d_out::name, upsample_nearest2d_out::overload_name)
      .typed<upsample_nearest2d_out::schema>();
}

// aten::upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_nearest2d_out::call(const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    
    static auto op = create_upsample_nearest2d_out_typed_handle();
    return op.call(self, output_size, scales_h, scales_w, out);
}

// aten::upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_nearest2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    
    static auto op = create_upsample_nearest2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, scales_h, scales_w, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d, name, "aten::upsample_nearest2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d, schema_str, "upsample_nearest2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_nearest2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest2d::schema> create_upsample_nearest2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest2d::name, upsample_nearest2d::overload_name)
      .typed<upsample_nearest2d::schema>();
}

// aten::upsample_nearest2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_nearest2d::call(const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    
    static auto op = create_upsample_nearest2d_typed_handle();
    return op.call(self, output_size, scales_h, scales_w);
}

// aten::upsample_nearest2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_nearest2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    
    static auto op = create_upsample_nearest2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward_grad_input, name, "aten::upsample_nearest3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward_grad_input, schema_str, "upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest3d_backward_grad_input::schema> create_upsample_nearest3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest3d_backward_grad_input::name, upsample_nearest3d_backward_grad_input::overload_name)
      .typed<upsample_nearest3d_backward_grad_input::schema>();
}

// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_nearest3d_backward_grad_input::call(const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    
    static auto op = create_upsample_nearest3d_backward_grad_input_typed_handle();
    return op.call(grad_output, output_size, input_size, scales_d, scales_h, scales_w, grad_input);
}

// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_nearest3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    
    static auto op = create_upsample_nearest3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, scales_d, scales_h, scales_w, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact3d_backward_grad_input, name, "aten::_upsample_nearest_exact3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact3d_backward_grad_input, schema_str, "_upsample_nearest_exact3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::_upsample_nearest_exact3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_upsample_nearest_exact3d_backward_grad_input::schema> create__upsample_nearest_exact3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_upsample_nearest_exact3d_backward_grad_input::name, _upsample_nearest_exact3d_backward_grad_input::overload_name)
      .typed<_upsample_nearest_exact3d_backward_grad_input::schema>();
}

// aten::_upsample_nearest_exact3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & _upsample_nearest_exact3d_backward_grad_input::call(const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    
    static auto op = create__upsample_nearest_exact3d_backward_grad_input_typed_handle();
    return op.call(grad_output, output_size, input_size, scales_d, scales_h, scales_w, grad_input);
}

// aten::_upsample_nearest_exact3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & _upsample_nearest_exact3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    
    static auto op = create__upsample_nearest_exact3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, scales_d, scales_h, scales_w, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward, name, "aten::upsample_nearest3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward, schema_str, "upsample_nearest3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_nearest3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest3d_backward::schema> create_upsample_nearest3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest3d_backward::name, upsample_nearest3d_backward::overload_name)
      .typed<upsample_nearest3d_backward::schema>();
}

// aten::upsample_nearest3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_nearest3d_backward::call(const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    
    static auto op = create_upsample_nearest3d_backward_typed_handle();
    return op.call(grad_output, output_size, input_size, scales_d, scales_h, scales_w);
}

// aten::upsample_nearest3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_nearest3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    
    static auto op = create_upsample_nearest3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, scales_d, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact3d_backward, name, "aten::_upsample_nearest_exact3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact3d_backward, schema_str, "_upsample_nearest_exact3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::_upsample_nearest_exact3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_upsample_nearest_exact3d_backward::schema> create__upsample_nearest_exact3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_upsample_nearest_exact3d_backward::name, _upsample_nearest_exact3d_backward::overload_name)
      .typed<_upsample_nearest_exact3d_backward::schema>();
}

// aten::_upsample_nearest_exact3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor _upsample_nearest_exact3d_backward::call(const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    
    static auto op = create__upsample_nearest_exact3d_backward_typed_handle();
    return op.call(grad_output, output_size, input_size, scales_d, scales_h, scales_w);
}

// aten::_upsample_nearest_exact3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor _upsample_nearest_exact3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    
    static auto op = create__upsample_nearest_exact3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, scales_d, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_backward_grad_input, name, "aten::logit_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_backward_grad_input, schema_str, "logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logit_backward_grad_input::schema> create_logit_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logit_backward_grad_input::name, logit_backward_grad_input::overload_name)
      .typed<logit_backward_grad_input::schema>();
}

// aten::logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & logit_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps, at::Tensor & grad_input) {
    
    static auto op = create_logit_backward_grad_input_typed_handle();
    return op.call(grad_output, self, eps, grad_input);
}

// aten::logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & logit_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps, at::Tensor & grad_input) {
    
    static auto op = create_logit_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, eps, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_backward, name, "aten::logit_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_backward, schema_str, "logit_backward(Tensor grad_output, Tensor self, float? eps=None) -> Tensor")

// aten::logit_backward(Tensor grad_output, Tensor self, float? eps=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logit_backward::schema> create_logit_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logit_backward::name, logit_backward::overload_name)
      .typed<logit_backward::schema>();
}

// aten::logit_backward(Tensor grad_output, Tensor self, float? eps=None) -> Tensor
at::Tensor logit_backward::call(const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps) {
    
    static auto op = create_logit_backward_typed_handle();
    return op.call(grad_output, self, eps);
}

// aten::logit_backward(Tensor grad_output, Tensor self, float? eps=None) -> Tensor
at::Tensor logit_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps) {
    
    static auto op = create_logit_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d_out, name, "aten::slow_conv_transpose2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d_out, schema_str, "slow_conv_transpose2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, SymInt[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, SymInt[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_transpose2d_out::schema> create_slow_conv_transpose2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_transpose2d_out::name, slow_conv_transpose2d_out::overload_name)
      .typed<slow_conv_transpose2d_out::schema>();
}

// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, SymInt[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slow_conv_transpose2d_out::call(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef dilation, at::Tensor & out) {
    
    static auto op = create_slow_conv_transpose2d_out_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, output_padding, dilation, out);
}

// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, SymInt[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slow_conv_transpose2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef dilation, at::Tensor & out) {
    
    static auto op = create_slow_conv_transpose2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, output_padding, dilation, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d, name, "aten::slow_conv_transpose2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d, schema_str, "slow_conv_transpose2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, SymInt[2] dilation=1) -> Tensor")

// aten::slow_conv_transpose2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, SymInt[2] dilation=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_transpose2d::schema> create_slow_conv_transpose2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_transpose2d::name, slow_conv_transpose2d::overload_name)
      .typed<slow_conv_transpose2d::schema>();
}

// aten::slow_conv_transpose2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, SymInt[2] dilation=1) -> Tensor
at::Tensor slow_conv_transpose2d::call(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef dilation) {
    
    static auto op = create_slow_conv_transpose2d_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}

// aten::slow_conv_transpose2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, SymInt[2] dilation=1) -> Tensor
at::Tensor slow_conv_transpose2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef dilation) {
    
    static auto op = create_slow_conv_transpose2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_slow_conv2d_backward_grad_input, name, "aten::_slow_conv2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_slow_conv2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_slow_conv2d_backward_grad_input, schema_str, "_slow_conv2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::_slow_conv2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<_slow_conv2d_backward_grad_input::schema> create__slow_conv2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_slow_conv2d_backward_grad_input::name, _slow_conv2d_backward_grad_input::overload_name)
      .typed<_slow_conv2d_backward_grad_input::schema>();
}

// aten::_slow_conv2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _slow_conv2d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
    
    static auto op = create__slow_conv2d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, grad_input, grad_weight, grad_bias);
}

// aten::_slow_conv2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _slow_conv2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
    
    static auto op = create__slow_conv2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, grad_input, grad_weight, grad_bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_slow_conv2d_backward_output_mask, name, "aten::_slow_conv2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_slow_conv2d_backward_output_mask, overload_name, "output_mask")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_slow_conv2d_backward_output_mask, schema_str, "_slow_conv2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)")

// aten::_slow_conv2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
static C10_NOINLINE c10::TypedOperatorHandle<_slow_conv2d_backward_output_mask::schema> create__slow_conv2d_backward_output_mask_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_slow_conv2d_backward_output_mask::name, _slow_conv2d_backward_output_mask::overload_name)
      .typed<_slow_conv2d_backward_output_mask::schema>();
}

// aten::_slow_conv2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _slow_conv2d_backward_output_mask::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, ::std::array<bool,3> output_mask) {
    
    static auto op = create__slow_conv2d_backward_output_mask_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, output_mask);
}

// aten::_slow_conv2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _slow_conv2d_backward_output_mask::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, ::std::array<bool,3> output_mask) {
    
    static auto op = create__slow_conv2d_backward_output_mask_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d, name, "aten::conv_depthwise3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d, schema_str, "conv_depthwise3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding, SymInt[3] dilation) -> Tensor")

// aten::conv_depthwise3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding, SymInt[3] dilation) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv_depthwise3d::schema> create_conv_depthwise3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv_depthwise3d::name, conv_depthwise3d::overload_name)
      .typed<conv_depthwise3d::schema>();
}

// aten::conv_depthwise3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding, SymInt[3] dilation) -> Tensor
at::Tensor conv_depthwise3d::call(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation) {
    
    static auto op = create_conv_depthwise3d_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, dilation);
}

// aten::conv_depthwise3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding, SymInt[3] dilation) -> Tensor
at::Tensor conv_depthwise3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation) {
    
    static auto op = create_conv_depthwise3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, dilation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated2d, name, "aten::slow_conv_dilated2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated2d, schema_str, "slow_conv_dilated2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] dilation=1) -> Tensor")

// aten::slow_conv_dilated2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] dilation=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_dilated2d::schema> create_slow_conv_dilated2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_dilated2d::name, slow_conv_dilated2d::overload_name)
      .typed<slow_conv_dilated2d::schema>();
}

// aten::slow_conv_dilated2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] dilation=1) -> Tensor
at::Tensor slow_conv_dilated2d::call(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation) {
    
    static auto op = create_slow_conv_dilated2d_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, dilation);
}

// aten::slow_conv_dilated2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] dilation=1) -> Tensor
at::Tensor slow_conv_dilated2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation) {
    
    static auto op = create_slow_conv_dilated2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, dilation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im_out, name, "aten::col2im")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im_out, schema_str, "col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)")

// aten::col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<col2im_out::schema> create_col2im_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(col2im_out::name, col2im_out::overload_name)
      .typed<col2im_out::schema>();
}

// aten::col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & col2im_out::call(const at::Tensor & self, c10::SymIntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & out) {
    
    static auto op = create_col2im_out_typed_handle();
    return op.call(self, output_size, kernel_size, dilation, padding, stride, out);
}

// aten::col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & col2im_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & out) {
    
    static auto op = create_col2im_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, kernel_size, dilation, padding, stride, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im, name, "aten::col2im")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im, schema_str, "col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor")

// aten::col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<col2im::schema> create_col2im_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(col2im::name, col2im::overload_name)
      .typed<col2im::schema>();
}

// aten::col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
at::Tensor col2im::call(const at::Tensor & self, c10::SymIntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
    
    static auto op = create_col2im_typed_handle();
    return op.call(self, output_size, kernel_size, dilation, padding, stride);
}

// aten::col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
at::Tensor col2im::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
    
    static auto op = create_col2im_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, kernel_size, dilation, padding, stride);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isfinite, name, "aten::isfinite")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isfinite, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isfinite, schema_str, "isfinite(Tensor self) -> Tensor")

// aten::isfinite(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<isfinite::schema> create_isfinite_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isfinite::name, isfinite::overload_name)
      .typed<isfinite::schema>();
}

// aten::isfinite(Tensor self) -> Tensor
at::Tensor isfinite::call(const at::Tensor & self) {
    
    static auto op = create_isfinite_typed_handle();
    return op.call(self);
}

// aten::isfinite(Tensor self) -> Tensor
at::Tensor isfinite::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_isfinite_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(record_stream, name, "aten::record_stream")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(record_stream, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(record_stream, schema_str, "record_stream(Tensor(a!) self, Stream s) -> ()")

// aten::record_stream(Tensor(a!) self, Stream s) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<record_stream::schema> create_record_stream_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(record_stream::name, record_stream::overload_name)
      .typed<record_stream::schema>();
}

// aten::record_stream(Tensor(a!) self, Stream s) -> ()
void record_stream::call(at::Tensor & self, at::Stream s) {
    
    static auto op = create_record_stream_typed_handle();
    return op.call(self, s);
}

// aten::record_stream(Tensor(a!) self, Stream s) -> ()
void record_stream::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Stream s) {
    
    static auto op = create_record_stream_typed_handle();
    return op.redispatch(dispatchKeySet, self, s);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isposinf, name, "aten::isposinf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isposinf, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isposinf, schema_str, "isposinf(Tensor self) -> Tensor")

// aten::isposinf(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<isposinf::schema> create_isposinf_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isposinf::name, isposinf::overload_name)
      .typed<isposinf::schema>();
}

// aten::isposinf(Tensor self) -> Tensor
at::Tensor isposinf::call(const at::Tensor & self) {
    
    static auto op = create_isposinf_typed_handle();
    return op.call(self);
}

// aten::isposinf(Tensor self) -> Tensor
at::Tensor isposinf::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_isposinf_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isposinf_out, name, "aten::isposinf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isposinf_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isposinf_out, schema_str, "isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<isposinf_out::schema> create_isposinf_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isposinf_out::name, isposinf_out::overload_name)
      .typed<isposinf_out::schema>();
}

// aten::isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & isposinf_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_isposinf_out_typed_handle();
    return op.call(self, out);
}

// aten::isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & isposinf_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_isposinf_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expm1, name, "aten::special_expm1")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expm1, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expm1, schema_str, "special_expm1(Tensor self) -> Tensor")

// aten::special_expm1(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_expm1::schema> create_special_expm1_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_expm1::name, special_expm1::overload_name)
      .typed<special_expm1::schema>();
}

// aten::special_expm1(Tensor self) -> Tensor
at::Tensor special_expm1::call(const at::Tensor & self) {
    
    static auto op = create_special_expm1_typed_handle();
    return op.call(self);
}

// aten::special_expm1(Tensor self) -> Tensor
at::Tensor special_expm1::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_special_expm1_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expm1_out, name, "aten::special_expm1")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expm1_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expm1_out, schema_str, "special_expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_expm1_out::schema> create_special_expm1_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_expm1_out::name, special_expm1_out::overload_name)
      .typed<special_expm1_out::schema>();
}

// aten::special_expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_expm1_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_expm1_out_typed_handle();
    return op.call(self, out);
}

// aten::special_expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_expm1_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_expm1_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_exp2, name, "aten::special_exp2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_exp2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_exp2, schema_str, "special_exp2(Tensor self) -> Tensor")

// aten::special_exp2(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_exp2::schema> create_special_exp2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_exp2::name, special_exp2::overload_name)
      .typed<special_exp2::schema>();
}

// aten::special_exp2(Tensor self) -> Tensor
at::Tensor special_exp2::call(const at::Tensor & self) {
    
    static auto op = create_special_exp2_typed_handle();
    return op.call(self);
}

// aten::special_exp2(Tensor self) -> Tensor
at::Tensor special_exp2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_special_exp2_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_exp2_out, name, "aten::special_exp2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_exp2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_exp2_out, schema_str, "special_exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_exp2_out::schema> create_special_exp2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_exp2_out::name, special_exp2_out::overload_name)
      .typed<special_exp2_out::schema>();
}

// aten::special_exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_exp2_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_exp2_out_typed_handle();
    return op.call(self, out);
}

// aten::special_exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_exp2_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_exp2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaln, name, "aten::special_gammaln")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaln, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaln, schema_str, "special_gammaln(Tensor self) -> Tensor")

// aten::special_gammaln(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_gammaln::schema> create_special_gammaln_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_gammaln::name, special_gammaln::overload_name)
      .typed<special_gammaln::schema>();
}

// aten::special_gammaln(Tensor self) -> Tensor
at::Tensor special_gammaln::call(const at::Tensor & self) {
    
    static auto op = create_special_gammaln_typed_handle();
    return op.call(self);
}

// aten::special_gammaln(Tensor self) -> Tensor
at::Tensor special_gammaln::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_special_gammaln_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaln_out, name, "aten::special_gammaln")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaln_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaln_out, schema_str, "special_gammaln.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_gammaln.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_gammaln_out::schema> create_special_gammaln_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_gammaln_out::name, special_gammaln_out::overload_name)
      .typed<special_gammaln_out::schema>();
}

// aten::special_gammaln.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_gammaln_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_gammaln_out_typed_handle();
    return op.call(self, out);
}

// aten::special_gammaln.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_gammaln_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_gammaln_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfinv, name, "aten::special_erfinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfinv, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfinv, schema_str, "special_erfinv(Tensor self) -> Tensor")

// aten::special_erfinv(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_erfinv::schema> create_special_erfinv_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_erfinv::name, special_erfinv::overload_name)
      .typed<special_erfinv::schema>();
}

// aten::special_erfinv(Tensor self) -> Tensor
at::Tensor special_erfinv::call(const at::Tensor & self) {
    
    static auto op = create_special_erfinv_typed_handle();
    return op.call(self);
}

// aten::special_erfinv(Tensor self) -> Tensor
at::Tensor special_erfinv::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_special_erfinv_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfinv_out, name, "aten::special_erfinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfinv_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfinv_out, schema_str, "special_erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_erfinv_out::schema> create_special_erfinv_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_erfinv_out::name, special_erfinv_out::overload_name)
      .typed<special_erfinv_out::schema>();
}

// aten::special_erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_erfinv_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_erfinv_out_typed_handle();
    return op.call(self, out);
}

// aten::special_erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_erfinv_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_erfinv_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py, name, "aten::special_xlog1py")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py, schema_str, "special_xlog1py(Tensor self, Tensor other) -> Tensor")

// aten::special_xlog1py(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_xlog1py::schema> create_special_xlog1py_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlog1py::name, special_xlog1py::overload_name)
      .typed<special_xlog1py::schema>();
}

// aten::special_xlog1py(Tensor self, Tensor other) -> Tensor
at::Tensor special_xlog1py::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_special_xlog1py_typed_handle();
    return op.call(self, other);
}

// aten::special_xlog1py(Tensor self, Tensor other) -> Tensor
at::Tensor special_xlog1py::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_special_xlog1py_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_self_scalar, name, "aten::special_xlog1py")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_self_scalar, overload_name, "self_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_self_scalar, schema_str, "special_xlog1py.self_scalar(Scalar self, Tensor other) -> Tensor")

// aten::special_xlog1py.self_scalar(Scalar self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_xlog1py_self_scalar::schema> create_special_xlog1py_self_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlog1py_self_scalar::name, special_xlog1py_self_scalar::overload_name)
      .typed<special_xlog1py_self_scalar::schema>();
}

// aten::special_xlog1py.self_scalar(Scalar self, Tensor other) -> Tensor
at::Tensor special_xlog1py_self_scalar::call(const at::Scalar & self, const at::Tensor & other) {
    
    static auto op = create_special_xlog1py_self_scalar_typed_handle();
    return op.call(self, other);
}

// aten::special_xlog1py.self_scalar(Scalar self, Tensor other) -> Tensor
at::Tensor special_xlog1py_self_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other) {
    
    static auto op = create_special_xlog1py_self_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_other_scalar, name, "aten::special_xlog1py")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_other_scalar, overload_name, "other_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_other_scalar, schema_str, "special_xlog1py.other_scalar(Tensor self, Scalar other) -> Tensor")

// aten::special_xlog1py.other_scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_xlog1py_other_scalar::schema> create_special_xlog1py_other_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlog1py_other_scalar::name, special_xlog1py_other_scalar::overload_name)
      .typed<special_xlog1py_other_scalar::schema>();
}

// aten::special_xlog1py.other_scalar(Tensor self, Scalar other) -> Tensor
at::Tensor special_xlog1py_other_scalar::call(const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_special_xlog1py_other_scalar_typed_handle();
    return op.call(self, other);
}

// aten::special_xlog1py.other_scalar(Tensor self, Scalar other) -> Tensor
at::Tensor special_xlog1py_other_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_special_xlog1py_other_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_out, name, "aten::special_xlog1py")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_out, schema_str, "special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_xlog1py_out::schema> create_special_xlog1py_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlog1py_out::name, special_xlog1py_out::overload_name)
      .typed<special_xlog1py_out::schema>();
}

// aten::special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlog1py_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_special_xlog1py_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlog1py_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_special_xlog1py_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_self_scalar_out, name, "aten::special_xlog1py")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_self_scalar_out, overload_name, "self_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_self_scalar_out, schema_str, "special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_xlog1py_self_scalar_out::schema> create_special_xlog1py_self_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlog1py_self_scalar_out::name, special_xlog1py_self_scalar_out::overload_name)
      .typed<special_xlog1py_self_scalar_out::schema>();
}

// aten::special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlog1py_self_scalar_out::call(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_special_xlog1py_self_scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlog1py_self_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_special_xlog1py_self_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_other_scalar_out, name, "aten::special_xlog1py")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_other_scalar_out, overload_name, "other_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_other_scalar_out, schema_str, "special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_xlog1py_other_scalar_out::schema> create_special_xlog1py_other_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlog1py_other_scalar_out::name, special_xlog1py_other_scalar_out::overload_name)
      .typed<special_xlog1py_other_scalar_out::schema>();
}

// aten::special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlog1py_other_scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_special_xlog1py_other_scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlog1py_other_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_special_xlog1py_other_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0, name, "aten::special_i0")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0, schema_str, "special_i0(Tensor self) -> Tensor")

// aten::special_i0(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_i0::schema> create_special_i0_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_i0::name, special_i0::overload_name)
      .typed<special_i0::schema>();
}

// aten::special_i0(Tensor self) -> Tensor
at::Tensor special_i0::call(const at::Tensor & self) {
    
    static auto op = create_special_i0_typed_handle();
    return op.call(self);
}

// aten::special_i0(Tensor self) -> Tensor
at::Tensor special_i0::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_special_i0_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0_out, name, "aten::special_i0")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0_out, schema_str, "special_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_i0_out::schema> create_special_i0_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_i0_out::name, special_i0_out::overload_name)
      .typed<special_i0_out::schema>();
}

// aten::special_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_i0_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_i0_out_typed_handle();
    return op.call(self, out);
}

// aten::special_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_i0_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_i0_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_polygamma, name, "aten::special_polygamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_polygamma, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_polygamma, schema_str, "special_polygamma(int n, Tensor self) -> Tensor")

// aten::special_polygamma(int n, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_polygamma::schema> create_special_polygamma_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_polygamma::name, special_polygamma::overload_name)
      .typed<special_polygamma::schema>();
}

// aten::special_polygamma(int n, Tensor self) -> Tensor
at::Tensor special_polygamma::call(int64_t n, const at::Tensor & self) {
    
    static auto op = create_special_polygamma_typed_handle();
    return op.call(n, self);
}

// aten::special_polygamma(int n, Tensor self) -> Tensor
at::Tensor special_polygamma::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, const at::Tensor & self) {
    
    static auto op = create_special_polygamma_typed_handle();
    return op.redispatch(dispatchKeySet, n, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_polygamma_out, name, "aten::special_polygamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_polygamma_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_polygamma_out, schema_str, "special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_polygamma_out::schema> create_special_polygamma_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_polygamma_out::name, special_polygamma_out::overload_name)
      .typed<special_polygamma_out::schema>();
}

// aten::special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_polygamma_out::call(int64_t n, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_polygamma_out_typed_handle();
    return op.call(n, self, out);
}

// aten::special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_polygamma_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_polygamma_out_typed_handle();
    return op.redispatch(dispatchKeySet, n, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log1p, name, "aten::special_log1p")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log1p, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log1p, schema_str, "special_log1p(Tensor self) -> Tensor")

// aten::special_log1p(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_log1p::schema> create_special_log1p_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_log1p::name, special_log1p::overload_name)
      .typed<special_log1p::schema>();
}

// aten::special_log1p(Tensor self) -> Tensor
at::Tensor special_log1p::call(const at::Tensor & self) {
    
    static auto op = create_special_log1p_typed_handle();
    return op.call(self);
}

// aten::special_log1p(Tensor self) -> Tensor
at::Tensor special_log1p::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_special_log1p_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log1p_out, name, "aten::special_log1p")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log1p_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log1p_out, schema_str, "special_log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_log1p_out::schema> create_special_log1p_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_log1p_out::name, special_log1p_out::overload_name)
      .typed<special_log1p_out::schema>();
}

// aten::special_log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_log1p_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_log1p_out_typed_handle();
    return op.call(self, out);
}

// aten::special_log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_log1p_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_log1p_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft, name, "aten::fft_irfft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft, schema_str, "fft_irfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor")

// aten::fft_irfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_irfft::schema> create_fft_irfft_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_irfft::name, fft_irfft::overload_name)
      .typed<fft_irfft::schema>();
}

// aten::fft_irfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_irfft::call(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_irfft_typed_handle();
    return op.call(self, n, dim, norm);
}

// aten::fft_irfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_irfft::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_irfft_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft_out, name, "aten::fft_irfft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft_out, schema_str, "fft_irfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_irfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_irfft_out::schema> create_fft_irfft_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_irfft_out::name, fft_irfft_out::overload_name)
      .typed<fft_irfft_out::schema>();
}

// aten::fft_irfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_irfft_out::call(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    
    static auto op = create_fft_irfft_out_typed_handle();
    return op.call(self, n, dim, norm, out);
}

// aten::fft_irfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_irfft_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    
    static auto op = create_fft_irfft_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft2, name, "aten::fft_ifft2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft2, schema_str, "fft_ifft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor")

// aten::fft_ifft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_ifft2::schema> create_fft_ifft2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ifft2::name, fft_ifft2::overload_name)
      .typed<fft_ifft2::schema>();
}

// aten::fft_ifft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
at::Tensor fft_ifft2::call(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_ifft2_typed_handle();
    return op.call(self, s, dim, norm);
}

// aten::fft_ifft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
at::Tensor fft_ifft2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_ifft2_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft2_out, name, "aten::fft_ifft2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft2_out, schema_str, "fft_ifft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_ifft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_ifft2_out::schema> create_fft_ifft2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ifft2_out::name, fft_ifft2_out::overload_name)
      .typed<fft_ifft2_out::schema>();
}

// aten::fft_ifft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_ifft2_out::call(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    
    static auto op = create_fft_ifft2_out_typed_handle();
    return op.call(self, s, dim, norm, out);
}

// aten::fft_ifft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_ifft2_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    
    static auto op = create_fft_ifft2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft2, name, "aten::fft_irfft2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft2, schema_str, "fft_irfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor")

// aten::fft_irfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_irfft2::schema> create_fft_irfft2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_irfft2::name, fft_irfft2::overload_name)
      .typed<fft_irfft2::schema>();
}

// aten::fft_irfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
at::Tensor fft_irfft2::call(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_irfft2_typed_handle();
    return op.call(self, s, dim, norm);
}

// aten::fft_irfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
at::Tensor fft_irfft2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_irfft2_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft2_out, name, "aten::fft_irfft2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft2_out, schema_str, "fft_irfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_irfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_irfft2_out::schema> create_fft_irfft2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_irfft2_out::name, fft_irfft2_out::overload_name)
      .typed<fft_irfft2_out::schema>();
}

// aten::fft_irfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_irfft2_out::call(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    
    static auto op = create_fft_irfft2_out_typed_handle();
    return op.call(self, s, dim, norm, out);
}

// aten::fft_irfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_irfft2_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    
    static auto op = create_fft_irfft2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftn, name, "aten::fft_rfftn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftn, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftn, schema_str, "fft_rfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor")

// aten::fft_rfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_rfftn::schema> create_fft_rfftn_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_rfftn::name, fft_rfftn::overload_name)
      .typed<fft_rfftn::schema>();
}

// aten::fft_rfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
at::Tensor fft_rfftn::call(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_rfftn_typed_handle();
    return op.call(self, s, dim, norm);
}

// aten::fft_rfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
at::Tensor fft_rfftn::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_rfftn_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftn_out, name, "aten::fft_rfftn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftn_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftn_out, schema_str, "fft_rfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_rfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_rfftn_out::schema> create_fft_rfftn_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_rfftn_out::name, fft_rfftn_out::overload_name)
      .typed<fft_rfftn_out::schema>();
}

// aten::fft_rfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_rfftn_out::call(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    
    static auto op = create_fft_rfftn_out_typed_handle();
    return op.call(self, s, dim, norm, out);
}

// aten::fft_rfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_rfftn_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    
    static auto op = create_fft_rfftn_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky, name, "aten::linalg_cholesky")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky, schema_str, "linalg_cholesky(Tensor self, *, bool upper=False) -> Tensor")

// aten::linalg_cholesky(Tensor self, *, bool upper=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cholesky::schema> create_linalg_cholesky_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cholesky::name, linalg_cholesky::overload_name)
      .typed<linalg_cholesky::schema>();
}

// aten::linalg_cholesky(Tensor self, *, bool upper=False) -> Tensor
at::Tensor linalg_cholesky::call(const at::Tensor & self, bool upper) {
    
    static auto op = create_linalg_cholesky_typed_handle();
    return op.call(self, upper);
}

// aten::linalg_cholesky(Tensor self, *, bool upper=False) -> Tensor
at::Tensor linalg_cholesky::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper) {
    
    static auto op = create_linalg_cholesky_typed_handle();
    return op.redispatch(dispatchKeySet, self, upper);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_out, name, "aten::linalg_cholesky")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_out, schema_str, "linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cholesky_out::schema> create_linalg_cholesky_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cholesky_out::name, linalg_cholesky_out::overload_name)
      .typed<linalg_cholesky_out::schema>();
}

// aten::linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_cholesky_out::call(const at::Tensor & self, bool upper, at::Tensor & out) {
    
    static auto op = create_linalg_cholesky_out_typed_handle();
    return op.call(self, upper, out);
}

// aten::linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_cholesky_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper, at::Tensor & out) {
    
    static auto op = create_linalg_cholesky_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, upper, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_det, name, "aten::_linalg_det")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_det, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_det, schema_str, "_linalg_det(Tensor A) -> (Tensor result, Tensor LU, Tensor pivots)")

// aten::_linalg_det(Tensor A) -> (Tensor result, Tensor LU, Tensor pivots)
static C10_NOINLINE c10::TypedOperatorHandle<_linalg_det::schema> create__linalg_det_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_linalg_det::name, _linalg_det::overload_name)
      .typed<_linalg_det::schema>();
}

// aten::_linalg_det(Tensor A) -> (Tensor result, Tensor LU, Tensor pivots)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _linalg_det::call(const at::Tensor & A) {
    
    static auto op = create__linalg_det_typed_handle();
    return op.call(A);
}

// aten::_linalg_det(Tensor A) -> (Tensor result, Tensor LU, Tensor pivots)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _linalg_det::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A) {
    
    static auto op = create__linalg_det_typed_handle();
    return op.redispatch(dispatchKeySet, A);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_det_result, name, "aten::_linalg_det")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_det_result, overload_name, "result")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_det_result, schema_str, "_linalg_det.result(Tensor A, *, Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots) -> (Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots)")

// aten::_linalg_det.result(Tensor A, *, Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots) -> (Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots)
static C10_NOINLINE c10::TypedOperatorHandle<_linalg_det_result::schema> create__linalg_det_result_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_linalg_det_result::name, _linalg_det_result::overload_name)
      .typed<_linalg_det_result::schema>();
}

// aten::_linalg_det.result(Tensor A, *, Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots) -> (Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots)
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _linalg_det_result::call(const at::Tensor & A, at::Tensor & result, at::Tensor & LU, at::Tensor & pivots) {
    
    static auto op = create__linalg_det_result_typed_handle();
    return op.call(A, result, LU, pivots);
}

// aten::_linalg_det.result(Tensor A, *, Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots) -> (Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots)
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _linalg_det_result::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, at::Tensor & result, at::Tensor & LU, at::Tensor & pivots) {
    
    static auto op = create__linalg_det_result_typed_handle();
    return op.redispatch(dispatchKeySet, A, result, LU, pivots);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_ldl_factor, name, "aten::linalg_ldl_factor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_ldl_factor, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_ldl_factor, schema_str, "linalg_ldl_factor(Tensor self, *, bool hermitian=False) -> (Tensor LD, Tensor pivots)")

// aten::linalg_ldl_factor(Tensor self, *, bool hermitian=False) -> (Tensor LD, Tensor pivots)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_ldl_factor::schema> create_linalg_ldl_factor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_ldl_factor::name, linalg_ldl_factor::overload_name)
      .typed<linalg_ldl_factor::schema>();
}

// aten::linalg_ldl_factor(Tensor self, *, bool hermitian=False) -> (Tensor LD, Tensor pivots)
::std::tuple<at::Tensor,at::Tensor> linalg_ldl_factor::call(const at::Tensor & self, bool hermitian) {
    
    static auto op = create_linalg_ldl_factor_typed_handle();
    return op.call(self, hermitian);
}

// aten::linalg_ldl_factor(Tensor self, *, bool hermitian=False) -> (Tensor LD, Tensor pivots)
::std::tuple<at::Tensor,at::Tensor> linalg_ldl_factor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool hermitian) {
    
    static auto op = create_linalg_ldl_factor_typed_handle();
    return op.redispatch(dispatchKeySet, self, hermitian);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_ldl_factor_out, name, "aten::linalg_ldl_factor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_ldl_factor_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_ldl_factor_out, schema_str, "linalg_ldl_factor.out(Tensor self, *, bool hermitian=False, Tensor(a!) LD, Tensor(b!) pivots) -> (Tensor(a!) LD, Tensor(b!) pivots)")

// aten::linalg_ldl_factor.out(Tensor self, *, bool hermitian=False, Tensor(a!) LD, Tensor(b!) pivots) -> (Tensor(a!) LD, Tensor(b!) pivots)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_ldl_factor_out::schema> create_linalg_ldl_factor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_ldl_factor_out::name, linalg_ldl_factor_out::overload_name)
      .typed<linalg_ldl_factor_out::schema>();
}

// aten::linalg_ldl_factor.out(Tensor self, *, bool hermitian=False, Tensor(a!) LD, Tensor(b!) pivots) -> (Tensor(a!) LD, Tensor(b!) pivots)
::std::tuple<at::Tensor &,at::Tensor &> linalg_ldl_factor_out::call(const at::Tensor & self, bool hermitian, at::Tensor & LD, at::Tensor & pivots) {
    
    static auto op = create_linalg_ldl_factor_out_typed_handle();
    return op.call(self, hermitian, LD, pivots);
}

// aten::linalg_ldl_factor.out(Tensor self, *, bool hermitian=False, Tensor(a!) LD, Tensor(b!) pivots) -> (Tensor(a!) LD, Tensor(b!) pivots)
::std::tuple<at::Tensor &,at::Tensor &> linalg_ldl_factor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool hermitian, at::Tensor & LD, at::Tensor & pivots) {
    
    static auto op = create_linalg_ldl_factor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, hermitian, LD, pivots);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matmul, name, "aten::linalg_matmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matmul, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matmul, schema_str, "linalg_matmul(Tensor self, Tensor other) -> Tensor")

// aten::linalg_matmul(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matmul::schema> create_linalg_matmul_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matmul::name, linalg_matmul::overload_name)
      .typed<linalg_matmul::schema>();
}

// aten::linalg_matmul(Tensor self, Tensor other) -> Tensor
at::Tensor linalg_matmul::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_linalg_matmul_typed_handle();
    return op.call(self, other);
}

// aten::linalg_matmul(Tensor self, Tensor other) -> Tensor
at::Tensor linalg_matmul::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_linalg_matmul_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matmul_out, name, "aten::linalg_matmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matmul_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matmul_out, schema_str, "linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matmul_out::schema> create_linalg_matmul_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matmul_out::name, linalg_matmul_out::overload_name)
      .typed<linalg_matmul_out::schema>();
}

// aten::linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matmul_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_linalg_matmul_out_typed_handle();
    return op.call(self, other, out);
}

// aten::linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matmul_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_linalg_matmul_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_slogdet, name, "aten::linalg_slogdet")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_slogdet, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_slogdet, schema_str, "linalg_slogdet(Tensor A) -> (Tensor sign, Tensor logabsdet)")

// aten::linalg_slogdet(Tensor A) -> (Tensor sign, Tensor logabsdet)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_slogdet::schema> create_linalg_slogdet_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_slogdet::name, linalg_slogdet::overload_name)
      .typed<linalg_slogdet::schema>();
}

// aten::linalg_slogdet(Tensor A) -> (Tensor sign, Tensor logabsdet)
::std::tuple<at::Tensor,at::Tensor> linalg_slogdet::call(const at::Tensor & A) {
    
    static auto op = create_linalg_slogdet_typed_handle();
    return op.call(A);
}

// aten::linalg_slogdet(Tensor A) -> (Tensor sign, Tensor logabsdet)
::std::tuple<at::Tensor,at::Tensor> linalg_slogdet::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A) {
    
    static auto op = create_linalg_slogdet_typed_handle();
    return op.redispatch(dispatchKeySet, A);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_slogdet_out, name, "aten::linalg_slogdet")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_slogdet_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_slogdet_out, schema_str, "linalg_slogdet.out(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)")

// aten::linalg_slogdet.out(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_slogdet_out::schema> create_linalg_slogdet_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_slogdet_out::name, linalg_slogdet_out::overload_name)
      .typed<linalg_slogdet_out::schema>();
}

// aten::linalg_slogdet.out(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
::std::tuple<at::Tensor &,at::Tensor &> linalg_slogdet_out::call(const at::Tensor & A, at::Tensor & sign, at::Tensor & logabsdet) {
    
    static auto op = create_linalg_slogdet_out_typed_handle();
    return op.call(A, sign, logabsdet);
}

// aten::linalg_slogdet.out(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
::std::tuple<at::Tensor &,at::Tensor &> linalg_slogdet_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, at::Tensor & sign, at::Tensor & logabsdet) {
    
    static auto op = create_linalg_slogdet_out_typed_handle();
    return op.redispatch(dispatchKeySet, A, sign, logabsdet);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logdet, name, "aten::logdet")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logdet, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logdet, schema_str, "logdet(Tensor self) -> Tensor")

// aten::logdet(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logdet::schema> create_logdet_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logdet::name, logdet::overload_name)
      .typed<logdet::schema>();
}

// aten::logdet(Tensor self) -> Tensor
at::Tensor logdet::call(const at::Tensor & self) {
    
    static auto op = create_logdet_typed_handle();
    return op.call(self);
}

// aten::logdet(Tensor self) -> Tensor
at::Tensor logdet::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_logdet_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvals, name, "aten::linalg_eigvals")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvals, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvals, schema_str, "linalg_eigvals(Tensor self) -> Tensor")

// aten::linalg_eigvals(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_eigvals::schema> create_linalg_eigvals_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_eigvals::name, linalg_eigvals::overload_name)
      .typed<linalg_eigvals::schema>();
}

// aten::linalg_eigvals(Tensor self) -> Tensor
at::Tensor linalg_eigvals::call(const at::Tensor & self) {
    
    static auto op = create_linalg_eigvals_typed_handle();
    return op.call(self);
}

// aten::linalg_eigvals(Tensor self) -> Tensor
at::Tensor linalg_eigvals::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_linalg_eigvals_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvals_out, name, "aten::linalg_eigvals")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvals_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvals_out, schema_str, "linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_eigvals_out::schema> create_linalg_eigvals_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_eigvals_out::name, linalg_eigvals_out::overload_name)
      .typed<linalg_eigvals_out::schema>();
}

// aten::linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_eigvals_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_linalg_eigvals_out_typed_handle();
    return op.call(self, out);
}

// aten::linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_eigvals_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_linalg_eigvals_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_ex, name, "aten::linalg_inv_ex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_ex, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_ex, schema_str, "linalg_inv_ex(Tensor A, *, bool check_errors=False) -> (Tensor inverse, Tensor info)")

// aten::linalg_inv_ex(Tensor A, *, bool check_errors=False) -> (Tensor inverse, Tensor info)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_inv_ex::schema> create_linalg_inv_ex_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_inv_ex::name, linalg_inv_ex::overload_name)
      .typed<linalg_inv_ex::schema>();
}

// aten::linalg_inv_ex(Tensor A, *, bool check_errors=False) -> (Tensor inverse, Tensor info)
::std::tuple<at::Tensor,at::Tensor> linalg_inv_ex::call(const at::Tensor & A, bool check_errors) {
    
    static auto op = create_linalg_inv_ex_typed_handle();
    return op.call(A, check_errors);
}

// aten::linalg_inv_ex(Tensor A, *, bool check_errors=False) -> (Tensor inverse, Tensor info)
::std::tuple<at::Tensor,at::Tensor> linalg_inv_ex::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, bool check_errors) {
    
    static auto op = create_linalg_inv_ex_typed_handle();
    return op.redispatch(dispatchKeySet, A, check_errors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_ex_inverse, name, "aten::linalg_inv_ex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_ex_inverse, overload_name, "inverse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_ex_inverse, schema_str, "linalg_inv_ex.inverse(Tensor A, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)")

// aten::linalg_inv_ex.inverse(Tensor A, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_inv_ex_inverse::schema> create_linalg_inv_ex_inverse_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_inv_ex_inverse::name, linalg_inv_ex_inverse::overload_name)
      .typed<linalg_inv_ex_inverse::schema>();
}

// aten::linalg_inv_ex.inverse(Tensor A, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)
::std::tuple<at::Tensor &,at::Tensor &> linalg_inv_ex_inverse::call(const at::Tensor & A, bool check_errors, at::Tensor & inverse, at::Tensor & info) {
    
    static auto op = create_linalg_inv_ex_inverse_typed_handle();
    return op.call(A, check_errors, inverse, info);
}

// aten::linalg_inv_ex.inverse(Tensor A, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)
::std::tuple<at::Tensor &,at::Tensor &> linalg_inv_ex_inverse::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, bool check_errors, at::Tensor & inverse, at::Tensor & info) {
    
    static auto op = create_linalg_inv_ex_inverse_typed_handle();
    return op.redispatch(dispatchKeySet, A, check_errors, inverse, info);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inner, name, "aten::inner")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inner, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inner, schema_str, "inner(Tensor self, Tensor other) -> Tensor")

// aten::inner(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<inner::schema> create_inner_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(inner::name, inner::overload_name)
      .typed<inner::schema>();
}

// aten::inner(Tensor self, Tensor other) -> Tensor
at::Tensor inner::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_inner_typed_handle();
    return op.call(self, other);
}

// aten::inner(Tensor self, Tensor other) -> Tensor
at::Tensor inner::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_inner_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inner_out, name, "aten::inner")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inner_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inner_out, schema_str, "inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<inner_out::schema> create_inner_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(inner_out::name, inner_out::overload_name)
      .typed<inner_out::schema>();
}

// aten::inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & inner_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_inner_out_typed_handle();
    return op.call(self, other, out);
}

// aten::inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & inner_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_inner_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vector_norm, name, "aten::linalg_vector_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vector_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vector_norm, schema_str, "linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_vector_norm::schema> create_linalg_vector_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_vector_norm::name, linalg_vector_norm::overload_name)
      .typed<linalg_vector_norm::schema>();
}

// aten::linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor linalg_vector_norm::call(const at::Tensor & self, const at::Scalar & ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_linalg_vector_norm_typed_handle();
    return op.call(self, ord, dim, keepdim, dtype);
}

// aten::linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor linalg_vector_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_linalg_vector_norm_typed_handle();
    return op.redispatch(dispatchKeySet, self, ord, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vector_norm_out, name, "aten::linalg_vector_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vector_norm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vector_norm_out, schema_str, "linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_vector_norm_out::schema> create_linalg_vector_norm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_vector_norm_out::name, linalg_vector_norm_out::overload_name)
      .typed<linalg_vector_norm_out::schema>();
}

// aten::linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_vector_norm_out::call(const at::Tensor & self, const at::Scalar & ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create_linalg_vector_norm_out_typed_handle();
    return op.call(self, ord, dim, keepdim, dtype, out);
}

// aten::linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_vector_norm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create_linalg_vector_norm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, ord, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve, name, "aten::linalg_solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve, schema_str, "linalg_solve(Tensor A, Tensor B, *, bool left=True) -> Tensor")

// aten::linalg_solve(Tensor A, Tensor B, *, bool left=True) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_solve::schema> create_linalg_solve_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_solve::name, linalg_solve::overload_name)
      .typed<linalg_solve::schema>();
}

// aten::linalg_solve(Tensor A, Tensor B, *, bool left=True) -> Tensor
at::Tensor linalg_solve::call(const at::Tensor & A, const at::Tensor & B, bool left) {
    
    static auto op = create_linalg_solve_typed_handle();
    return op.call(A, B, left);
}

// aten::linalg_solve(Tensor A, Tensor B, *, bool left=True) -> Tensor
at::Tensor linalg_solve::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, const at::Tensor & B, bool left) {
    
    static auto op = create_linalg_solve_typed_handle();
    return op.redispatch(dispatchKeySet, A, B, left);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_out, name, "aten::linalg_solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_out, schema_str, "linalg_solve.out(Tensor A, Tensor B, *, bool left=True, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_solve.out(Tensor A, Tensor B, *, bool left=True, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_solve_out::schema> create_linalg_solve_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_solve_out::name, linalg_solve_out::overload_name)
      .typed<linalg_solve_out::schema>();
}

// aten::linalg_solve.out(Tensor A, Tensor B, *, bool left=True, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_solve_out::call(const at::Tensor & A, const at::Tensor & B, bool left, at::Tensor & out) {
    
    static auto op = create_linalg_solve_out_typed_handle();
    return op.call(A, B, left, out);
}

// aten::linalg_solve.out(Tensor A, Tensor B, *, bool left=True, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_solve_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, const at::Tensor & B, bool left, at::Tensor & out) {
    
    static auto op = create_linalg_solve_out_typed_handle();
    return op.redispatch(dispatchKeySet, A, B, left, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorinv, name, "aten::linalg_tensorinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorinv, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorinv, schema_str, "linalg_tensorinv(Tensor self, int ind=2) -> Tensor")

// aten::linalg_tensorinv(Tensor self, int ind=2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_tensorinv::schema> create_linalg_tensorinv_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_tensorinv::name, linalg_tensorinv::overload_name)
      .typed<linalg_tensorinv::schema>();
}

// aten::linalg_tensorinv(Tensor self, int ind=2) -> Tensor
at::Tensor linalg_tensorinv::call(const at::Tensor & self, int64_t ind) {
    
    static auto op = create_linalg_tensorinv_typed_handle();
    return op.call(self, ind);
}

// aten::linalg_tensorinv(Tensor self, int ind=2) -> Tensor
at::Tensor linalg_tensorinv::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t ind) {
    
    static auto op = create_linalg_tensorinv_typed_handle();
    return op.redispatch(dispatchKeySet, self, ind);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorinv_out, name, "aten::linalg_tensorinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorinv_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorinv_out, schema_str, "linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_tensorinv_out::schema> create_linalg_tensorinv_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_tensorinv_out::name, linalg_tensorinv_out::overload_name)
      .typed<linalg_tensorinv_out::schema>();
}

// aten::linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_tensorinv_out::call(const at::Tensor & self, int64_t ind, at::Tensor & out) {
    
    static auto op = create_linalg_tensorinv_out_typed_handle();
    return op.call(self, ind, out);
}

// aten::linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_tensorinv_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t ind, at::Tensor & out) {
    
    static auto op = create_linalg_tensorinv_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, ind, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_atol_rtol_tensor, name, "aten::linalg_matrix_rank")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_atol_rtol_tensor, overload_name, "atol_rtol_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_atol_rtol_tensor, schema_str, "linalg_matrix_rank.atol_rtol_tensor(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -> Tensor")

// aten::linalg_matrix_rank.atol_rtol_tensor(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_rank_atol_rtol_tensor::schema> create_linalg_matrix_rank_atol_rtol_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_rank_atol_rtol_tensor::name, linalg_matrix_rank_atol_rtol_tensor::overload_name)
      .typed<linalg_matrix_rank_atol_rtol_tensor::schema>();
}

// aten::linalg_matrix_rank.atol_rtol_tensor(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -> Tensor
at::Tensor linalg_matrix_rank_atol_rtol_tensor::call(const at::Tensor & input, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian) {
    
    static auto op = create_linalg_matrix_rank_atol_rtol_tensor_typed_handle();
    return op.call(input, atol, rtol, hermitian);
}

// aten::linalg_matrix_rank.atol_rtol_tensor(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -> Tensor
at::Tensor linalg_matrix_rank_atol_rtol_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian) {
    
    static auto op = create_linalg_matrix_rank_atol_rtol_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, input, atol, rtol, hermitian);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_atol_rtol_tensor_out, name, "aten::linalg_matrix_rank")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_atol_rtol_tensor_out, overload_name, "atol_rtol_tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_atol_rtol_tensor_out, schema_str, "linalg_matrix_rank.atol_rtol_tensor_out(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_matrix_rank.atol_rtol_tensor_out(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_rank_atol_rtol_tensor_out::schema> create_linalg_matrix_rank_atol_rtol_tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_rank_atol_rtol_tensor_out::name, linalg_matrix_rank_atol_rtol_tensor_out::overload_name)
      .typed<linalg_matrix_rank_atol_rtol_tensor_out::schema>();
}

// aten::linalg_matrix_rank.atol_rtol_tensor_out(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_rank_atol_rtol_tensor_out::call(const at::Tensor & input, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_matrix_rank_atol_rtol_tensor_out_typed_handle();
    return op.call(input, atol, rtol, hermitian, out);
}

// aten::linalg_matrix_rank.atol_rtol_tensor_out(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_rank_atol_rtol_tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_matrix_rank_atol_rtol_tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, atol, rtol, hermitian, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_atol_rtol_float, name, "aten::linalg_matrix_rank")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_atol_rtol_float, overload_name, "atol_rtol_float")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_atol_rtol_float, schema_str, "linalg_matrix_rank.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -> Tensor")

// aten::linalg_matrix_rank.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_rank_atol_rtol_float::schema> create_linalg_matrix_rank_atol_rtol_float_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_rank_atol_rtol_float::name, linalg_matrix_rank_atol_rtol_float::overload_name)
      .typed<linalg_matrix_rank_atol_rtol_float::schema>();
}

// aten::linalg_matrix_rank.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -> Tensor
at::Tensor linalg_matrix_rank_atol_rtol_float::call(const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian) {
    
    static auto op = create_linalg_matrix_rank_atol_rtol_float_typed_handle();
    return op.call(self, atol, rtol, hermitian);
}

// aten::linalg_matrix_rank.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -> Tensor
at::Tensor linalg_matrix_rank_atol_rtol_float::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian) {
    
    static auto op = create_linalg_matrix_rank_atol_rtol_float_typed_handle();
    return op.redispatch(dispatchKeySet, self, atol, rtol, hermitian);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_atol_rtol_float_out, name, "aten::linalg_matrix_rank")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_atol_rtol_float_out, overload_name, "atol_rtol_float_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_atol_rtol_float_out, schema_str, "linalg_matrix_rank.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_matrix_rank.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_rank_atol_rtol_float_out::schema> create_linalg_matrix_rank_atol_rtol_float_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_rank_atol_rtol_float_out::name, linalg_matrix_rank_atol_rtol_float_out::overload_name)
      .typed<linalg_matrix_rank_atol_rtol_float_out::schema>();
}

// aten::linalg_matrix_rank.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_rank_atol_rtol_float_out::call(const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_matrix_rank_atol_rtol_float_out_typed_handle();
    return op.call(self, atol, rtol, hermitian, out);
}

// aten::linalg_matrix_rank.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_rank_atol_rtol_float_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_matrix_rank_atol_rtol_float_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, atol, rtol, hermitian, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank, name, "aten::linalg_matrix_rank")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank, schema_str, "linalg_matrix_rank(Tensor self, float tol, bool hermitian=False) -> Tensor")

// aten::linalg_matrix_rank(Tensor self, float tol, bool hermitian=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_rank::schema> create_linalg_matrix_rank_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_rank::name, linalg_matrix_rank::overload_name)
      .typed<linalg_matrix_rank::schema>();
}

// aten::linalg_matrix_rank(Tensor self, float tol, bool hermitian=False) -> Tensor
at::Tensor linalg_matrix_rank::call(const at::Tensor & self, double tol, bool hermitian) {
    
    static auto op = create_linalg_matrix_rank_typed_handle();
    return op.call(self, tol, hermitian);
}

// aten::linalg_matrix_rank(Tensor self, float tol, bool hermitian=False) -> Tensor
at::Tensor linalg_matrix_rank::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double tol, bool hermitian) {
    
    static auto op = create_linalg_matrix_rank_typed_handle();
    return op.redispatch(dispatchKeySet, self, tol, hermitian);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_out, name, "aten::linalg_matrix_rank")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_out, schema_str, "linalg_matrix_rank.out(Tensor self, float tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_matrix_rank.out(Tensor self, float tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_rank_out::schema> create_linalg_matrix_rank_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_rank_out::name, linalg_matrix_rank_out::overload_name)
      .typed<linalg_matrix_rank_out::schema>();
}

// aten::linalg_matrix_rank.out(Tensor self, float tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_rank_out::call(const at::Tensor & self, double tol, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_matrix_rank_out_typed_handle();
    return op.call(self, tol, hermitian, out);
}

// aten::linalg_matrix_rank.out(Tensor self, float tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_rank_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double tol, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_matrix_rank_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, tol, hermitian, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_tol_tensor, name, "aten::linalg_matrix_rank")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_tol_tensor, overload_name, "tol_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_tol_tensor, schema_str, "linalg_matrix_rank.tol_tensor(Tensor input, Tensor tol, bool hermitian=False) -> Tensor")

// aten::linalg_matrix_rank.tol_tensor(Tensor input, Tensor tol, bool hermitian=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_rank_tol_tensor::schema> create_linalg_matrix_rank_tol_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_rank_tol_tensor::name, linalg_matrix_rank_tol_tensor::overload_name)
      .typed<linalg_matrix_rank_tol_tensor::schema>();
}

// aten::linalg_matrix_rank.tol_tensor(Tensor input, Tensor tol, bool hermitian=False) -> Tensor
at::Tensor linalg_matrix_rank_tol_tensor::call(const at::Tensor & input, const at::Tensor & tol, bool hermitian) {
    
    static auto op = create_linalg_matrix_rank_tol_tensor_typed_handle();
    return op.call(input, tol, hermitian);
}

// aten::linalg_matrix_rank.tol_tensor(Tensor input, Tensor tol, bool hermitian=False) -> Tensor
at::Tensor linalg_matrix_rank_tol_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & tol, bool hermitian) {
    
    static auto op = create_linalg_matrix_rank_tol_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, input, tol, hermitian);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_out_tol_tensor, name, "aten::linalg_matrix_rank")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_out_tol_tensor, overload_name, "out_tol_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_out_tol_tensor, schema_str, "linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_rank_out_tol_tensor::schema> create_linalg_matrix_rank_out_tol_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_rank_out_tol_tensor::name, linalg_matrix_rank_out_tol_tensor::overload_name)
      .typed<linalg_matrix_rank_out_tol_tensor::schema>();
}

// aten::linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_rank_out_tol_tensor::call(const at::Tensor & input, const at::Tensor & tol, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_matrix_rank_out_tol_tensor_typed_handle();
    return op.call(input, tol, hermitian, out);
}

// aten::linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_rank_out_tol_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & tol, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_matrix_rank_out_tol_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, input, tol, hermitian, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_filled_intlist, name, "aten::_test_optional_filled_intlist")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_filled_intlist, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_filled_intlist, schema_str, "_test_optional_filled_intlist(Tensor values, int[2]? addends) -> Tensor")

// aten::_test_optional_filled_intlist(Tensor values, int[2]? addends) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_test_optional_filled_intlist::schema> create__test_optional_filled_intlist_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_test_optional_filled_intlist::name, _test_optional_filled_intlist::overload_name)
      .typed<_test_optional_filled_intlist::schema>();
}

// aten::_test_optional_filled_intlist(Tensor values, int[2]? addends) -> Tensor
at::Tensor _test_optional_filled_intlist::call(const at::Tensor & values, at::OptionalIntArrayRef addends) {
    
    static auto op = create__test_optional_filled_intlist_typed_handle();
    return op.call(values, addends);
}

// aten::_test_optional_filled_intlist(Tensor values, int[2]? addends) -> Tensor
at::Tensor _test_optional_filled_intlist::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & values, at::OptionalIntArrayRef addends) {
    
    static auto op = create__test_optional_filled_intlist_typed_handle();
    return op.redispatch(dispatchKeySet, values, addends);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_autograd_multiple_dispatch_view_copy, name, "aten::_test_autograd_multiple_dispatch_view_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_autograd_multiple_dispatch_view_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_autograd_multiple_dispatch_view_copy, schema_str, "_test_autograd_multiple_dispatch_view_copy(Tensor self) -> Tensor")

// aten::_test_autograd_multiple_dispatch_view_copy(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_test_autograd_multiple_dispatch_view_copy::schema> create__test_autograd_multiple_dispatch_view_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_test_autograd_multiple_dispatch_view_copy::name, _test_autograd_multiple_dispatch_view_copy::overload_name)
      .typed<_test_autograd_multiple_dispatch_view_copy::schema>();
}

// aten::_test_autograd_multiple_dispatch_view_copy(Tensor self) -> Tensor
at::Tensor _test_autograd_multiple_dispatch_view_copy::call(const at::Tensor & self) {
    
    static auto op = create__test_autograd_multiple_dispatch_view_copy_typed_handle();
    return op.call(self);
}

// aten::_test_autograd_multiple_dispatch_view_copy(Tensor self) -> Tensor
at::Tensor _test_autograd_multiple_dispatch_view_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create__test_autograd_multiple_dispatch_view_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pad_sequence, name, "aten::pad_sequence")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pad_sequence, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pad_sequence, schema_str, "pad_sequence(Tensor[] sequences, bool batch_first=False, float padding_value=0.0) -> Tensor")

// aten::pad_sequence(Tensor[] sequences, bool batch_first=False, float padding_value=0.0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<pad_sequence::schema> create_pad_sequence_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pad_sequence::name, pad_sequence::overload_name)
      .typed<pad_sequence::schema>();
}

// aten::pad_sequence(Tensor[] sequences, bool batch_first=False, float padding_value=0.0) -> Tensor
at::Tensor pad_sequence::call(at::TensorList sequences, bool batch_first, double padding_value) {
    
    static auto op = create_pad_sequence_typed_handle();
    return op.call(sequences, batch_first, padding_value);
}

// aten::pad_sequence(Tensor[] sequences, bool batch_first=False, float padding_value=0.0) -> Tensor
at::Tensor pad_sequence::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList sequences, bool batch_first, double padding_value) {
    
    static auto op = create_pad_sequence_typed_handle();
    return op.redispatch(dispatchKeySet, sequences, batch_first, padding_value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fw_primal_copy, name, "aten::_fw_primal_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fw_primal_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fw_primal_copy, schema_str, "_fw_primal_copy(Tensor self, int level) -> Tensor")

// aten::_fw_primal_copy(Tensor self, int level) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_fw_primal_copy::schema> create__fw_primal_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fw_primal_copy::name, _fw_primal_copy::overload_name)
      .typed<_fw_primal_copy::schema>();
}

// aten::_fw_primal_copy(Tensor self, int level) -> Tensor
at::Tensor _fw_primal_copy::call(const at::Tensor & self, int64_t level) {
    
    static auto op = create__fw_primal_copy_typed_handle();
    return op.call(self, level);
}

// aten::_fw_primal_copy(Tensor self, int level) -> Tensor
at::Tensor _fw_primal_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t level) {
    
    static auto op = create__fw_primal_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self, level);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as_real_copy, name, "aten::view_as_real_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as_real_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as_real_copy, schema_str, "view_as_real_copy(Tensor self) -> Tensor")

// aten::view_as_real_copy(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<view_as_real_copy::schema> create_view_as_real_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(view_as_real_copy::name, view_as_real_copy::overload_name)
      .typed<view_as_real_copy::schema>();
}

// aten::view_as_real_copy(Tensor self) -> Tensor
at::Tensor view_as_real_copy::call(const at::Tensor & self) {
    
    static auto op = create_view_as_real_copy_typed_handle();
    return op.call(self);
}

// aten::view_as_real_copy(Tensor self) -> Tensor
at::Tensor view_as_real_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_view_as_real_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(as_strided_copy, name, "aten::as_strided_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(as_strided_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(as_strided_copy, schema_str, "as_strided_copy(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor")

// aten::as_strided_copy(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<as_strided_copy::schema> create_as_strided_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(as_strided_copy::name, as_strided_copy::overload_name)
      .typed<as_strided_copy::schema>();
}

// aten::as_strided_copy(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor
at::Tensor as_strided_copy::call(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset) {
    
    static auto op = create_as_strided_copy_typed_handle();
    return op.call(self, size, stride, storage_offset);
}

// aten::as_strided_copy(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor
at::Tensor as_strided_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset) {
    
    static auto op = create_as_strided_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, stride, storage_offset);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_alias_copy, name, "aten::_reshape_alias_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_alias_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_alias_copy, schema_str, "_reshape_alias_copy(Tensor self, SymInt[] size, SymInt[] stride) -> Tensor")

// aten::_reshape_alias_copy(Tensor self, SymInt[] size, SymInt[] stride) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_reshape_alias_copy::schema> create__reshape_alias_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_reshape_alias_copy::name, _reshape_alias_copy::overload_name)
      .typed<_reshape_alias_copy::schema>();
}

// aten::_reshape_alias_copy(Tensor self, SymInt[] size, SymInt[] stride) -> Tensor
at::Tensor _reshape_alias_copy::call(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
    
    static auto op = create__reshape_alias_copy_typed_handle();
    return op.call(self, size, stride);
}

// aten::_reshape_alias_copy(Tensor self, SymInt[] size, SymInt[] stride) -> Tensor
at::Tensor _reshape_alias_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
    
    static auto op = create__reshape_alias_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, stride);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_copy_Tensor, name, "aten::split_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_copy_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_copy_Tensor, schema_str, "split_copy.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]")

// aten::split_copy.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<split_copy_Tensor::schema> create_split_copy_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(split_copy_Tensor::name, split_copy_Tensor::overload_name)
      .typed<split_copy_Tensor::schema>();
}

// aten::split_copy.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]
::std::vector<at::Tensor> split_copy_Tensor::call(const at::Tensor & self, c10::SymInt split_size, int64_t dim) {
    
    static auto op = create_split_copy_Tensor_typed_handle();
    return op.call(self, split_size, dim);
}

// aten::split_copy.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]
::std::vector<at::Tensor> split_copy_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt split_size, int64_t dim) {
    
    static auto op = create_split_copy_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, split_size, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy, name, "aten::squeeze_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy, schema_str, "squeeze_copy(Tensor self) -> Tensor")

// aten::squeeze_copy(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<squeeze_copy::schema> create_squeeze_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(squeeze_copy::name, squeeze_copy::overload_name)
      .typed<squeeze_copy::schema>();
}

// aten::squeeze_copy(Tensor self) -> Tensor
at::Tensor squeeze_copy::call(const at::Tensor & self) {
    
    static auto op = create_squeeze_copy_typed_handle();
    return op.call(self);
}

// aten::squeeze_copy(Tensor self) -> Tensor
at::Tensor squeeze_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_squeeze_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_dim, name, "aten::squeeze_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_dim, schema_str, "squeeze_copy.dim(Tensor self, int dim) -> Tensor")

// aten::squeeze_copy.dim(Tensor self, int dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<squeeze_copy_dim::schema> create_squeeze_copy_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(squeeze_copy_dim::name, squeeze_copy_dim::overload_name)
      .typed<squeeze_copy_dim::schema>();
}

// aten::squeeze_copy.dim(Tensor self, int dim) -> Tensor
at::Tensor squeeze_copy_dim::call(const at::Tensor & self, int64_t dim) {
    
    static auto op = create_squeeze_copy_dim_typed_handle();
    return op.call(self, dim);
}

// aten::squeeze_copy.dim(Tensor self, int dim) -> Tensor
at::Tensor squeeze_copy_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    
    static auto op = create_squeeze_copy_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_dims, name, "aten::squeeze_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_dims, overload_name, "dims")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_dims, schema_str, "squeeze_copy.dims(Tensor self, int[] dim) -> Tensor")

// aten::squeeze_copy.dims(Tensor self, int[] dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<squeeze_copy_dims::schema> create_squeeze_copy_dims_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(squeeze_copy_dims::name, squeeze_copy_dims::overload_name)
      .typed<squeeze_copy_dims::schema>();
}

// aten::squeeze_copy.dims(Tensor self, int[] dim) -> Tensor
at::Tensor squeeze_copy_dims::call(const at::Tensor & self, at::IntArrayRef dim) {
    
    static auto op = create_squeeze_copy_dims_typed_handle();
    return op.call(self, dim);
}

// aten::squeeze_copy.dims(Tensor self, int[] dim) -> Tensor
at::Tensor squeeze_copy_dims::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim) {
    
    static auto op = create_squeeze_copy_dims_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(indices_copy, name, "aten::indices_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(indices_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(indices_copy, schema_str, "indices_copy(Tensor self) -> Tensor")

// aten::indices_copy(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<indices_copy::schema> create_indices_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(indices_copy::name, indices_copy::overload_name)
      .typed<indices_copy::schema>();
}

// aten::indices_copy(Tensor self) -> Tensor
at::Tensor indices_copy::call(const at::Tensor & self) {
    
    static auto op = create_indices_copy_typed_handle();
    return op.call(self);
}

// aten::indices_copy(Tensor self) -> Tensor
at::Tensor indices_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_indices_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ccol_indices_copy, name, "aten::ccol_indices_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ccol_indices_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ccol_indices_copy, schema_str, "ccol_indices_copy(Tensor self) -> Tensor")

// aten::ccol_indices_copy(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ccol_indices_copy::schema> create_ccol_indices_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ccol_indices_copy::name, ccol_indices_copy::overload_name)
      .typed<ccol_indices_copy::schema>();
}

// aten::ccol_indices_copy(Tensor self) -> Tensor
at::Tensor ccol_indices_copy::call(const at::Tensor & self) {
    
    static auto op = create_ccol_indices_copy_typed_handle();
    return op.call(self);
}

// aten::ccol_indices_copy(Tensor self) -> Tensor
at::Tensor ccol_indices_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_ccol_indices_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_copy_Tensor_out, name, "aten::split_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_copy_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_copy_Tensor_out, schema_str, "split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()")

// aten::split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<split_copy_Tensor_out::schema> create_split_copy_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(split_copy_Tensor_out::name, split_copy_Tensor_out::overload_name)
      .typed<split_copy_Tensor_out::schema>();
}

// aten::split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
void split_copy_Tensor_out::call(const at::Tensor & self, c10::SymInt split_size, int64_t dim, at::TensorList out) {
    
    static auto op = create_split_copy_Tensor_out_typed_handle();
    return op.call(self, split_size, dim, out);
}

// aten::split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
void split_copy_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt split_size, int64_t dim, at::TensorList out) {
    
    static auto op = create_split_copy_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, split_size, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_flash_attention_for_cpu_backward, name, "aten::_scaled_dot_product_flash_attention_for_cpu_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_flash_attention_for_cpu_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_flash_attention_for_cpu_backward, schema_str, "_scaled_dot_product_flash_attention_for_cpu_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, float dropout_p, bool is_causal, *, Tensor? attn_mask=None, float? scale=None) -> (Tensor grad_query, Tensor grad_key, Tensor grad_value)")

// aten::_scaled_dot_product_flash_attention_for_cpu_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, float dropout_p, bool is_causal, *, Tensor? attn_mask=None, float? scale=None) -> (Tensor grad_query, Tensor grad_key, Tensor grad_value)
static C10_NOINLINE c10::TypedOperatorHandle<_scaled_dot_product_flash_attention_for_cpu_backward::schema> create__scaled_dot_product_flash_attention_for_cpu_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_scaled_dot_product_flash_attention_for_cpu_backward::name, _scaled_dot_product_flash_attention_for_cpu_backward::overload_name)
      .typed<_scaled_dot_product_flash_attention_for_cpu_backward::schema>();
}

// aten::_scaled_dot_product_flash_attention_for_cpu_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, float dropout_p, bool is_causal, *, Tensor? attn_mask=None, float? scale=None) -> (Tensor grad_query, Tensor grad_key, Tensor grad_value)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _scaled_dot_product_flash_attention_for_cpu_backward::call(const at::Tensor & grad_out, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const at::Tensor & out, const at::Tensor & logsumexp, double dropout_p, bool is_causal, const c10::optional<at::Tensor> & attn_mask, c10::optional<double> scale) {
    
    static auto op = create__scaled_dot_product_flash_attention_for_cpu_backward_typed_handle();
    return op.call(grad_out, query, key, value, out, logsumexp, dropout_p, is_causal, attn_mask, scale);
}

// aten::_scaled_dot_product_flash_attention_for_cpu_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, float dropout_p, bool is_causal, *, Tensor? attn_mask=None, float? scale=None) -> (Tensor grad_query, Tensor grad_key, Tensor grad_value)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _scaled_dot_product_flash_attention_for_cpu_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const at::Tensor & out, const at::Tensor & logsumexp, double dropout_p, bool is_causal, const c10::optional<at::Tensor> & attn_mask, c10::optional<double> scale) {
    
    static auto op = create__scaled_dot_product_flash_attention_for_cpu_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, query, key, value, out, logsumexp, dropout_p, is_causal, attn_mask, scale);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_efficient_attention, name, "aten::_scaled_dot_product_efficient_attention")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_efficient_attention, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_efficient_attention, schema_str, "_scaled_dot_product_efficient_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_bias, bool compute_log_sumexp, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> (Tensor output, Tensor log_sumexp, Tensor philox_seed, Tensor philox_offset)")

// aten::_scaled_dot_product_efficient_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_bias, bool compute_log_sumexp, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> (Tensor output, Tensor log_sumexp, Tensor philox_seed, Tensor philox_offset)
static C10_NOINLINE c10::TypedOperatorHandle<_scaled_dot_product_efficient_attention::schema> create__scaled_dot_product_efficient_attention_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_scaled_dot_product_efficient_attention::name, _scaled_dot_product_efficient_attention::overload_name)
      .typed<_scaled_dot_product_efficient_attention::schema>();
}

// aten::_scaled_dot_product_efficient_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_bias, bool compute_log_sumexp, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> (Tensor output, Tensor log_sumexp, Tensor philox_seed, Tensor philox_offset)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _scaled_dot_product_efficient_attention::call(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & attn_bias, bool compute_log_sumexp, double dropout_p, bool is_causal, c10::optional<double> scale) {
    
    static auto op = create__scaled_dot_product_efficient_attention_typed_handle();
    return op.call(query, key, value, attn_bias, compute_log_sumexp, dropout_p, is_causal, scale);
}

// aten::_scaled_dot_product_efficient_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_bias, bool compute_log_sumexp, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> (Tensor output, Tensor log_sumexp, Tensor philox_seed, Tensor philox_offset)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _scaled_dot_product_efficient_attention::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & attn_bias, bool compute_log_sumexp, double dropout_p, bool is_causal, c10::optional<double> scale) {
    
    static auto op = create__scaled_dot_product_efficient_attention_typed_handle();
    return op.redispatch(dispatchKeySet, query, key, value, attn_bias, compute_log_sumexp, dropout_p, is_causal, scale);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_efficient_attention_forward, name, "aten::_efficient_attention_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_efficient_attention_forward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_efficient_attention_forward, schema_str, "_efficient_attention_forward(Tensor query, Tensor key, Tensor value, Tensor? bias, Tensor? cu_seqlens_q, Tensor? cu_seqlens_k, int? max_seqlen_q, int? max_seqlen_k, float dropout_p, int custom_mask_type, bool compute_log_sumexp=False, *, float? scale=None, Tensor? causal_diagonal=None, Tensor? seqlen_k=None) -> (Tensor output, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, SymInt max_seqlen_batch_q, SymInt max_seqlen_batch_k)")

// aten::_efficient_attention_forward(Tensor query, Tensor key, Tensor value, Tensor? bias, Tensor? cu_seqlens_q, Tensor? cu_seqlens_k, int? max_seqlen_q, int? max_seqlen_k, float dropout_p, int custom_mask_type, bool compute_log_sumexp=False, *, float? scale=None, Tensor? causal_diagonal=None, Tensor? seqlen_k=None) -> (Tensor output, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, SymInt max_seqlen_batch_q, SymInt max_seqlen_batch_k)
static C10_NOINLINE c10::TypedOperatorHandle<_efficient_attention_forward::schema> create__efficient_attention_forward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_efficient_attention_forward::name, _efficient_attention_forward::overload_name)
      .typed<_efficient_attention_forward::schema>();
}

// aten::_efficient_attention_forward(Tensor query, Tensor key, Tensor value, Tensor? bias, Tensor? cu_seqlens_q, Tensor? cu_seqlens_k, int? max_seqlen_q, int? max_seqlen_k, float dropout_p, int custom_mask_type, bool compute_log_sumexp=False, *, float? scale=None, Tensor? causal_diagonal=None, Tensor? seqlen_k=None) -> (Tensor output, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, SymInt max_seqlen_batch_q, SymInt max_seqlen_batch_k)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,c10::SymInt,c10::SymInt> _efficient_attention_forward::call(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & cu_seqlens_q, const c10::optional<at::Tensor> & cu_seqlens_k, c10::optional<int64_t> max_seqlen_q, c10::optional<int64_t> max_seqlen_k, double dropout_p, int64_t custom_mask_type, bool compute_log_sumexp, c10::optional<double> scale, const c10::optional<at::Tensor> & causal_diagonal, const c10::optional<at::Tensor> & seqlen_k) {
    
    static auto op = create__efficient_attention_forward_typed_handle();
    return op.call(query, key, value, bias, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p, custom_mask_type, compute_log_sumexp, scale, causal_diagonal, seqlen_k);
}

// aten::_efficient_attention_forward(Tensor query, Tensor key, Tensor value, Tensor? bias, Tensor? cu_seqlens_q, Tensor? cu_seqlens_k, int? max_seqlen_q, int? max_seqlen_k, float dropout_p, int custom_mask_type, bool compute_log_sumexp=False, *, float? scale=None, Tensor? causal_diagonal=None, Tensor? seqlen_k=None) -> (Tensor output, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, SymInt max_seqlen_batch_q, SymInt max_seqlen_batch_k)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,c10::SymInt,c10::SymInt> _efficient_attention_forward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & cu_seqlens_q, const c10::optional<at::Tensor> & cu_seqlens_k, c10::optional<int64_t> max_seqlen_q, c10::optional<int64_t> max_seqlen_k, double dropout_p, int64_t custom_mask_type, bool compute_log_sumexp, c10::optional<double> scale, const c10::optional<at::Tensor> & causal_diagonal, const c10::optional<at::Tensor> & seqlen_k) {
    
    static auto op = create__efficient_attention_forward_typed_handle();
    return op.redispatch(dispatchKeySet, query, key, value, bias, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p, custom_mask_type, compute_log_sumexp, scale, causal_diagonal, seqlen_k);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_bessel_j1, name, "aten::special_bessel_j1")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_bessel_j1, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_bessel_j1, schema_str, "special_bessel_j1(Tensor self) -> Tensor")

// aten::special_bessel_j1(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_bessel_j1::schema> create_special_bessel_j1_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_bessel_j1::name, special_bessel_j1::overload_name)
      .typed<special_bessel_j1::schema>();
}

// aten::special_bessel_j1(Tensor self) -> Tensor
at::Tensor special_bessel_j1::call(const at::Tensor & self) {
    
    static auto op = create_special_bessel_j1_typed_handle();
    return op.call(self);
}

// aten::special_bessel_j1(Tensor self) -> Tensor
at::Tensor special_bessel_j1::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_special_bessel_j1_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_bessel_j1_out, name, "aten::special_bessel_j1")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_bessel_j1_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_bessel_j1_out, schema_str, "special_bessel_j1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_bessel_j1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_bessel_j1_out::schema> create_special_bessel_j1_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_bessel_j1_out::name, special_bessel_j1_out::overload_name)
      .typed<special_bessel_j1_out::schema>();
}

// aten::special_bessel_j1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_bessel_j1_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_bessel_j1_out_typed_handle();
    return op.call(self, out);
}

// aten::special_bessel_j1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_bessel_j1_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_bessel_j1_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v, name, "aten::special_chebyshev_polynomial_v")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v, schema_str, "special_chebyshev_polynomial_v(Tensor x, Tensor n) -> Tensor")

// aten::special_chebyshev_polynomial_v(Tensor x, Tensor n) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_chebyshev_polynomial_v::schema> create_special_chebyshev_polynomial_v_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_chebyshev_polynomial_v::name, special_chebyshev_polynomial_v::overload_name)
      .typed<special_chebyshev_polynomial_v::schema>();
}

// aten::special_chebyshev_polynomial_v(Tensor x, Tensor n) -> Tensor
at::Tensor special_chebyshev_polynomial_v::call(const at::Tensor & x, const at::Tensor & n) {
    
    static auto op = create_special_chebyshev_polynomial_v_typed_handle();
    return op.call(x, n);
}

// aten::special_chebyshev_polynomial_v(Tensor x, Tensor n) -> Tensor
at::Tensor special_chebyshev_polynomial_v::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n) {
    
    static auto op = create_special_chebyshev_polynomial_v_typed_handle();
    return op.redispatch(dispatchKeySet, x, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_x_scalar, name, "aten::special_chebyshev_polynomial_v")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_x_scalar, overload_name, "x_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_x_scalar, schema_str, "special_chebyshev_polynomial_v.x_scalar(Scalar x, Tensor n) -> Tensor")

// aten::special_chebyshev_polynomial_v.x_scalar(Scalar x, Tensor n) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_chebyshev_polynomial_v_x_scalar::schema> create_special_chebyshev_polynomial_v_x_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_chebyshev_polynomial_v_x_scalar::name, special_chebyshev_polynomial_v_x_scalar::overload_name)
      .typed<special_chebyshev_polynomial_v_x_scalar::schema>();
}

// aten::special_chebyshev_polynomial_v.x_scalar(Scalar x, Tensor n) -> Tensor
at::Tensor special_chebyshev_polynomial_v_x_scalar::call(const at::Scalar & x, const at::Tensor & n) {
    
    static auto op = create_special_chebyshev_polynomial_v_x_scalar_typed_handle();
    return op.call(x, n);
}

// aten::special_chebyshev_polynomial_v.x_scalar(Scalar x, Tensor n) -> Tensor
at::Tensor special_chebyshev_polynomial_v_x_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n) {
    
    static auto op = create_special_chebyshev_polynomial_v_x_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, x, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_n_scalar, name, "aten::special_chebyshev_polynomial_v")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_n_scalar, overload_name, "n_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_n_scalar, schema_str, "special_chebyshev_polynomial_v.n_scalar(Tensor x, Scalar n) -> Tensor")

// aten::special_chebyshev_polynomial_v.n_scalar(Tensor x, Scalar n) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_chebyshev_polynomial_v_n_scalar::schema> create_special_chebyshev_polynomial_v_n_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_chebyshev_polynomial_v_n_scalar::name, special_chebyshev_polynomial_v_n_scalar::overload_name)
      .typed<special_chebyshev_polynomial_v_n_scalar::schema>();
}

// aten::special_chebyshev_polynomial_v.n_scalar(Tensor x, Scalar n) -> Tensor
at::Tensor special_chebyshev_polynomial_v_n_scalar::call(const at::Tensor & x, const at::Scalar & n) {
    
    static auto op = create_special_chebyshev_polynomial_v_n_scalar_typed_handle();
    return op.call(x, n);
}

// aten::special_chebyshev_polynomial_v.n_scalar(Tensor x, Scalar n) -> Tensor
at::Tensor special_chebyshev_polynomial_v_n_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n) {
    
    static auto op = create_special_chebyshev_polynomial_v_n_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, x, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_out, name, "aten::special_chebyshev_polynomial_v")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_out, schema_str, "special_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_chebyshev_polynomial_v_out::schema> create_special_chebyshev_polynomial_v_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_chebyshev_polynomial_v_out::name, special_chebyshev_polynomial_v_out::overload_name)
      .typed<special_chebyshev_polynomial_v_out::schema>();
}

// aten::special_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_chebyshev_polynomial_v_out::call(const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_chebyshev_polynomial_v_out_typed_handle();
    return op.call(x, n, out);
}

// aten::special_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_chebyshev_polynomial_v_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_chebyshev_polynomial_v_out_typed_handle();
    return op.redispatch(dispatchKeySet, x, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_x_scalar_out, name, "aten::special_chebyshev_polynomial_v")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_x_scalar_out, overload_name, "x_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_x_scalar_out, schema_str, "special_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_chebyshev_polynomial_v_x_scalar_out::schema> create_special_chebyshev_polynomial_v_x_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_chebyshev_polynomial_v_x_scalar_out::name, special_chebyshev_polynomial_v_x_scalar_out::overload_name)
      .typed<special_chebyshev_polynomial_v_x_scalar_out::schema>();
}

// aten::special_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_chebyshev_polynomial_v_x_scalar_out::call(const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_chebyshev_polynomial_v_x_scalar_out_typed_handle();
    return op.call(x, n, out);
}

// aten::special_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_chebyshev_polynomial_v_x_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_chebyshev_polynomial_v_x_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, x, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_n_scalar_out, name, "aten::special_chebyshev_polynomial_v")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_n_scalar_out, overload_name, "n_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_v_n_scalar_out, schema_str, "special_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_chebyshev_polynomial_v_n_scalar_out::schema> create_special_chebyshev_polynomial_v_n_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_chebyshev_polynomial_v_n_scalar_out::name, special_chebyshev_polynomial_v_n_scalar_out::overload_name)
      .typed<special_chebyshev_polynomial_v_n_scalar_out::schema>();
}

// aten::special_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_chebyshev_polynomial_v_n_scalar_out::call(const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
    
    static auto op = create_special_chebyshev_polynomial_v_n_scalar_out_typed_handle();
    return op.call(x, n, out);
}

// aten::special_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_chebyshev_polynomial_v_n_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
    
    static auto op = create_special_chebyshev_polynomial_v_n_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, x, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_backward_out, name, "aten::_cudnn_rnn_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_backward_out, schema_str, "_cudnn_rnn_backward.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!)[] out3) -> ()")

// aten::_cudnn_rnn_backward.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!)[] out3) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_cudnn_rnn_backward_out::schema> create__cudnn_rnn_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cudnn_rnn_backward_out::name, _cudnn_rnn_backward_out::overload_name)
      .typed<_cudnn_rnn_backward_out::schema>();
}

// aten::_cudnn_rnn_backward.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!)[] out3) -> ()
void _cudnn_rnn_backward_out::call(const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, c10::SymIntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, ::std::array<bool,4> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::TensorList out3) {
    
    static auto op = create__cudnn_rnn_backward_out_typed_handle();
    return op.call(input, weight, weight_stride0, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve, output_mask, out0, out1, out2, out3);
}

// aten::_cudnn_rnn_backward.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!)[] out3) -> ()
void _cudnn_rnn_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, c10::SymIntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, ::std::array<bool,4> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::TensorList out3) {
    
    static auto op = create__cudnn_rnn_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, weight_stride0, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve, output_mask, out0, out1, out2, out3);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_dropout_backward_out, name, "aten::native_dropout_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_dropout_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_dropout_backward_out, schema_str, "native_dropout_backward.out(Tensor grad_output, Tensor mask, float scale, *, Tensor(a!) out) -> Tensor(a!)")

// aten::native_dropout_backward.out(Tensor grad_output, Tensor mask, float scale, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<native_dropout_backward_out::schema> create_native_dropout_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_dropout_backward_out::name, native_dropout_backward_out::overload_name)
      .typed<native_dropout_backward_out::schema>();
}

// aten::native_dropout_backward.out(Tensor grad_output, Tensor mask, float scale, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & native_dropout_backward_out::call(const at::Tensor & grad_output, const at::Tensor & mask, double scale, at::Tensor & out) {
    
    static auto op = create_native_dropout_backward_out_typed_handle();
    return op.call(grad_output, mask, scale, out);
}

// aten::native_dropout_backward.out(Tensor grad_output, Tensor mask, float scale, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & native_dropout_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & mask, double scale, at::Tensor & out) {
    
    static auto op = create_native_dropout_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, mask, scale, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Scalar_out, name, "aten::_add_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Scalar_out, schema_str, "_add_relu.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_add_relu.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_add_relu_Scalar_out::schema> create__add_relu_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_add_relu_Scalar_out::name, _add_relu_Scalar_out::overload_name)
      .typed<_add_relu_Scalar_out::schema>();
}

// aten::_add_relu.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _add_relu_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create__add_relu_Scalar_out_typed_handle();
    return op.call(self, other, alpha, out);
}

// aten::_add_relu.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _add_relu_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create__add_relu_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator_out, name, "aten::affine_grid_generator")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator_out, schema_str, "affine_grid_generator.out(Tensor theta, SymInt[] size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)")

// aten::affine_grid_generator.out(Tensor theta, SymInt[] size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<affine_grid_generator_out::schema> create_affine_grid_generator_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(affine_grid_generator_out::name, affine_grid_generator_out::overload_name)
      .typed<affine_grid_generator_out::schema>();
}

// aten::affine_grid_generator.out(Tensor theta, SymInt[] size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & affine_grid_generator_out::call(const at::Tensor & theta, c10::SymIntArrayRef size, bool align_corners, at::Tensor & out) {
    
    static auto op = create_affine_grid_generator_out_typed_handle();
    return op.call(theta, size, align_corners, out);
}

// aten::affine_grid_generator.out(Tensor theta, SymInt[] size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & affine_grid_generator_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & theta, c10::SymIntArrayRef size, bool align_corners, at::Tensor & out) {
    
    static auto op = create_affine_grid_generator_out_typed_handle();
    return op.redispatch(dispatchKeySet, theta, size, align_corners, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_functorch_fallback_out, name, "aten::_test_functorch_fallback")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_functorch_fallback_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_functorch_fallback_out, schema_str, "_test_functorch_fallback.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_test_functorch_fallback.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_test_functorch_fallback_out::schema> create__test_functorch_fallback_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_test_functorch_fallback_out::name, _test_functorch_fallback_out::overload_name)
      .typed<_test_functorch_fallback_out::schema>();
}

// aten::_test_functorch_fallback.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _test_functorch_fallback_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create__test_functorch_fallback_out_typed_handle();
    return op.call(self, other, out);
}

// aten::_test_functorch_fallback.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _test_functorch_fallback_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create__test_functorch_fallback_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window_out, name, "aten::bartlett_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window_out, schema_str, "bartlett_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bartlett_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bartlett_window_out::schema> create_bartlett_window_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bartlett_window_out::name, bartlett_window_out::overload_name)
      .typed<bartlett_window_out::schema>();
}

// aten::bartlett_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bartlett_window_out::call(int64_t window_length, at::Tensor & out) {
    
    static auto op = create_bartlett_window_out_typed_handle();
    return op.call(window_length, out);
}

// aten::bartlett_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bartlett_window_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, at::Tensor & out) {
    
    static auto op = create_bartlett_window_out_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window_periodic_out, name, "aten::bartlett_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window_periodic_out, overload_name, "periodic_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window_periodic_out, schema_str, "bartlett_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bartlett_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bartlett_window_periodic_out::schema> create_bartlett_window_periodic_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bartlett_window_periodic_out::name, bartlett_window_periodic_out::overload_name)
      .typed<bartlett_window_periodic_out::schema>();
}

// aten::bartlett_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bartlett_window_periodic_out::call(int64_t window_length, bool periodic, at::Tensor & out) {
    
    static auto op = create_bartlett_window_periodic_out_typed_handle();
    return op.call(window_length, periodic, out);
}

// aten::bartlett_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bartlett_window_periodic_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, at::Tensor & out) {
    
    static auto op = create_bartlett_window_periodic_out_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy_out, name, "aten::copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy_out, schema_str, "copy.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::copy.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<copy_out::schema> create_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(copy_out::name, copy_out::overload_name)
      .typed<copy_out::schema>();
}

// aten::copy.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & copy_out::call(const at::Tensor & self, const at::Tensor & src, bool non_blocking, at::Tensor & out) {
    
    static auto op = create_copy_out_typed_handle();
    return op.call(self, src, non_blocking, out);
}

// aten::copy.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & src, bool non_blocking, at::Tensor & out) {
    
    static auto op = create_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, src, non_blocking, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_copy_from_and_resize_out, name, "aten::_copy_from_and_resize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_copy_from_and_resize_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_copy_from_and_resize_out, schema_str, "_copy_from_and_resize.out(Tensor self, Tensor dst, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_copy_from_and_resize.out(Tensor self, Tensor dst, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_copy_from_and_resize_out::schema> create__copy_from_and_resize_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_copy_from_and_resize_out::name, _copy_from_and_resize_out::overload_name)
      .typed<_copy_from_and_resize_out::schema>();
}

// aten::_copy_from_and_resize.out(Tensor self, Tensor dst, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _copy_from_and_resize_out::call(const at::Tensor & self, const at::Tensor & dst, at::Tensor & out) {
    
    static auto op = create__copy_from_and_resize_out_typed_handle();
    return op.call(self, dst, out);
}

// aten::_copy_from_and_resize.out(Tensor self, Tensor dst, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _copy_from_and_resize_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & dst, at::Tensor & out) {
    
    static auto op = create__copy_from_and_resize_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dst, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_relu_out, name, "aten::cudnn_convolution_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_relu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_relu_out, schema_str, "cudnn_convolution_relu.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cudnn_convolution_relu.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_relu_out::schema> create_cudnn_convolution_relu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_relu_out::name, cudnn_convolution_relu_out::overload_name)
      .typed<cudnn_convolution_relu_out::schema>();
}

// aten::cudnn_convolution_relu.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cudnn_convolution_relu_out::call(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor & out) {
    
    static auto op = create_cudnn_convolution_relu_out_typed_handle();
    return op.call(self, weight, bias, stride, padding, dilation, groups, out);
}

// aten::cudnn_convolution_relu.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cudnn_convolution_relu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor & out) {
    
    static auto op = create_cudnn_convolution_relu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, stride, padding, dilation, groups, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_embed_out, name, "aten::diag_embed")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_embed_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_embed_out, schema_str, "diag_embed.out(Tensor self, int offset=0, int dim1=-2, int dim2=-1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::diag_embed.out(Tensor self, int offset=0, int dim1=-2, int dim2=-1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<diag_embed_out::schema> create_diag_embed_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(diag_embed_out::name, diag_embed_out::overload_name)
      .typed<diag_embed_out::schema>();
}

// aten::diag_embed.out(Tensor self, int offset=0, int dim1=-2, int dim2=-1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & diag_embed_out::call(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2, at::Tensor & out) {
    
    static auto op = create_diag_embed_out_typed_handle();
    return op.call(self, offset, dim1, dim2, out);
}

// aten::diag_embed.out(Tensor self, int offset=0, int dim1=-2, int dim2=-1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & diag_embed_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2, at::Tensor & out) {
    
    static auto op = create_diag_embed_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, offset, dim1, dim2, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_permuted_out, name, "aten::empty_permuted")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_permuted_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_permuted_out, schema_str, "empty_permuted.out(SymInt[] size, int[] physical_layout, *, Tensor(a!) out) -> Tensor(a!)")

// aten::empty_permuted.out(SymInt[] size, int[] physical_layout, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<empty_permuted_out::schema> create_empty_permuted_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(empty_permuted_out::name, empty_permuted_out::overload_name)
      .typed<empty_permuted_out::schema>();
}

// aten::empty_permuted.out(SymInt[] size, int[] physical_layout, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & empty_permuted_out::call(c10::SymIntArrayRef size, at::IntArrayRef physical_layout, at::Tensor & out) {
    
    static auto op = create_empty_permuted_out_typed_handle();
    return op.call(size, physical_layout, out);
}

// aten::empty_permuted.out(SymInt[] size, int[] physical_layout, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & empty_permuted_out::redispatch(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, at::IntArrayRef physical_layout, at::Tensor & out) {
    
    static auto op = create_empty_permuted_out_typed_handle();
    return op.redispatch(dispatchKeySet, size, physical_layout, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_empty_affine_quantized_out, name, "aten::_empty_affine_quantized")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_empty_affine_quantized_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_empty_affine_quantized_out, schema_str, "_empty_affine_quantized.out(SymInt[] size, *, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format, Tensor(a!) out) -> Tensor(a!)")

// aten::_empty_affine_quantized.out(SymInt[] size, *, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_empty_affine_quantized_out::schema> create__empty_affine_quantized_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_empty_affine_quantized_out::name, _empty_affine_quantized_out::overload_name)
      .typed<_empty_affine_quantized_out::schema>();
}

// aten::_empty_affine_quantized.out(SymInt[] size, *, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _empty_affine_quantized_out::call(c10::SymIntArrayRef size, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    
    static auto op = create__empty_affine_quantized_out_typed_handle();
    return op.call(size, scale, zero_point, memory_format, out);
}

// aten::_empty_affine_quantized.out(SymInt[] size, *, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _empty_affine_quantized_out::redispatch(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    
    static auto op = create__empty_affine_quantized_out_typed_handle();
    return op.redispatch(dispatchKeySet, size, scale, zero_point, memory_format, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_resize_output_out, name, "aten::_resize_output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_resize_output_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_resize_output_out, schema_str, "_resize_output.out(Tensor self, SymInt[] size, Device device, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_resize_output.out(Tensor self, SymInt[] size, Device device, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_resize_output_out::schema> create__resize_output_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_resize_output_out::name, _resize_output_out::overload_name)
      .typed<_resize_output_out::schema>();
}

// aten::_resize_output.out(Tensor self, SymInt[] size, Device device, *, Tensor(a!) out) -> Tensor(a!)
const at::Tensor & _resize_output_out::call(const at::Tensor & self, c10::SymIntArrayRef size, at::Device device, const at::Tensor & out) {
    
    static auto op = create__resize_output_out_typed_handle();
    return op.call(self, size, device, out);
}

// aten::_resize_output.out(Tensor self, SymInt[] size, Device device, *, Tensor(a!) out) -> Tensor(a!)
const at::Tensor & _resize_output_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, at::Device device, const at::Tensor & out) {
    
    static auto op = create__resize_output_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, device, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_resize_output, name, "aten::_resize_output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_resize_output, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_resize_output, schema_str, "_resize_output(Tensor self, SymInt[] size, Device device) -> Tensor")

// aten::_resize_output(Tensor self, SymInt[] size, Device device) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_resize_output::schema> create__resize_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_resize_output::name, _resize_output::overload_name)
      .typed<_resize_output::schema>();
}

// aten::_resize_output(Tensor self, SymInt[] size, Device device) -> Tensor
at::Tensor _resize_output::call(const at::Tensor & self, c10::SymIntArrayRef size, at::Device device) {
    
    static auto op = create__resize_output_typed_handle();
    return op.call(self, size, device);
}

// aten::_resize_output(Tensor self, SymInt[] size, Device device) -> Tensor
at::Tensor _resize_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, at::Device device) {
    
    static auto op = create__resize_output_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, device);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_like_out, name, "aten::empty_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_like_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_like_out, schema_str, "empty_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)")

// aten::empty_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<empty_like_out::schema> create_empty_like_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(empty_like_out::name, empty_like_out::overload_name)
      .typed<empty_like_out::schema>();
}

// aten::empty_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & empty_like_out::call(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    
    static auto op = create_empty_like_out_typed_handle();
    return op.call(self, memory_format, out);
}

// aten::empty_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & empty_like_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    
    static auto op = create_empty_like_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, memory_format, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d_backward_out, name, "aten::grid_sampler_3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d_backward_out, schema_str, "grid_sampler_3d_backward.out(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))")

// aten::grid_sampler_3d_backward.out(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<grid_sampler_3d_backward_out::schema> create_grid_sampler_3d_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(grid_sampler_3d_backward_out::name, grid_sampler_3d_backward_out::overload_name)
      .typed<grid_sampler_3d_backward_out::schema>();
}

// aten::grid_sampler_3d_backward.out(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> grid_sampler_3d_backward_out::call(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, ::std::array<bool,2> output_mask, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create_grid_sampler_3d_backward_out_typed_handle();
    return op.call(grad_output, input, grid, interpolation_mode, padding_mode, align_corners, output_mask, out0, out1);
}

// aten::grid_sampler_3d_backward.out(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> grid_sampler_3d_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, ::std::array<bool,2> output_mask, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create_grid_sampler_3d_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input, grid, interpolation_mode, padding_mode, align_corners, output_mask, out0, out1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm_out, name, "aten::native_group_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm_out, schema_str, "native_group_norm.out(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::native_group_norm.out(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<native_group_norm_out::schema> create_native_group_norm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_group_norm_out::name, native_group_norm_out::overload_name)
      .typed<native_group_norm_out::schema>();
}

// aten::native_group_norm.out(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_group_norm_out::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, double eps, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_native_group_norm_out_typed_handle();
    return op.call(input, weight, bias, N, C, HxW, group, eps, out0, out1, out2);
}

// aten::native_group_norm.out(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_group_norm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, double eps, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_native_group_norm_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, N, C, HxW, group, eps, out0, out1, out2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linear_backward_out, name, "aten::linear_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linear_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linear_backward_out, schema_str, "linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<linear_backward_out::schema> create_linear_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linear_backward_out::name, linear_backward_out::overload_name)
      .typed<linear_backward_out::schema>();
}

// aten::linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linear_backward_out::call(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_linear_backward_out_typed_handle();
    return op.call(self, grad_output, weight, output_mask, out0, out1, out2);
}

// aten::linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linear_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_linear_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, grad_output, weight, output_mask, out0, out1, out2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_input_out, name, "aten::mkldnn_linear_backward_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_input_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_input_out, schema_str, "mkldnn_linear_backward_input.out(int[] input_size, Tensor grad_output, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)")

// aten::mkldnn_linear_backward_input.out(int[] input_size, Tensor grad_output, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_linear_backward_input_out::schema> create_mkldnn_linear_backward_input_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_linear_backward_input_out::name, mkldnn_linear_backward_input_out::overload_name)
      .typed<mkldnn_linear_backward_input_out::schema>();
}

// aten::mkldnn_linear_backward_input.out(int[] input_size, Tensor grad_output, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mkldnn_linear_backward_input_out::call(at::IntArrayRef input_size, const at::Tensor & grad_output, const at::Tensor & weight, at::Tensor & out) {
    
    static auto op = create_mkldnn_linear_backward_input_out_typed_handle();
    return op.call(input_size, grad_output, weight, out);
}

// aten::mkldnn_linear_backward_input.out(int[] input_size, Tensor grad_output, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mkldnn_linear_backward_input_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef input_size, const at::Tensor & grad_output, const at::Tensor & weight, at::Tensor & out) {
    
    static auto op = create_mkldnn_linear_backward_input_out_typed_handle();
    return op.redispatch(dispatchKeySet, input_size, grad_output, weight, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_out, name, "aten::mkldnn_linear_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_out, schema_str, "mkldnn_linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::mkldnn_linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_linear_backward_out::schema> create_mkldnn_linear_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_linear_backward_out::name, mkldnn_linear_backward_out::overload_name)
      .typed<mkldnn_linear_backward_out::schema>();
}

// aten::mkldnn_linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> mkldnn_linear_backward_out::call(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_mkldnn_linear_backward_out_typed_handle();
    return op.call(self, grad_output, weight, output_mask, out0, out1, out2);
}

// aten::mkldnn_linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> mkldnn_linear_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_mkldnn_linear_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, grad_output, weight, output_mask, out0, out1, out2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_native_batch_norm_legit_no_training_out, name, "aten::_native_batch_norm_legit_no_training")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_native_batch_norm_legit_no_training_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_native_batch_norm_legit_no_training_out, schema_str, "_native_batch_norm_legit_no_training.out(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::_native_batch_norm_legit_no_training.out(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<_native_batch_norm_legit_no_training_out::schema> create__native_batch_norm_legit_no_training_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_native_batch_norm_legit_no_training_out::name, _native_batch_norm_legit_no_training_out::overload_name)
      .typed<_native_batch_norm_legit_no_training_out::schema>();
}

// aten::_native_batch_norm_legit_no_training.out(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _native_batch_norm_legit_no_training_out::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & running_mean, const at::Tensor & running_var, double momentum, double eps, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create__native_batch_norm_legit_no_training_out_typed_handle();
    return op.call(input, weight, bias, running_mean, running_var, momentum, eps, out0, out1, out2);
}

// aten::_native_batch_norm_legit_no_training.out(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _native_batch_norm_legit_no_training_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & running_mean, const at::Tensor & running_var, double momentum, double eps, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create__native_batch_norm_legit_no_training_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, running_mean, running_var, momentum, eps, out0, out1, out2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats_with_counts_out, name, "aten::batch_norm_gather_stats_with_counts")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats_with_counts_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats_with_counts_out, schema_str, "batch_norm_gather_stats_with_counts.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))")

// aten::batch_norm_gather_stats_with_counts.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_gather_stats_with_counts_out::schema> create_batch_norm_gather_stats_with_counts_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_gather_stats_with_counts_out::name, batch_norm_gather_stats_with_counts_out::overload_name)
      .typed<batch_norm_gather_stats_with_counts_out::schema>();
}

// aten::batch_norm_gather_stats_with_counts.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> batch_norm_gather_stats_with_counts_out::call(const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, const at::Tensor & counts, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create_batch_norm_gather_stats_with_counts_out_typed_handle();
    return op.call(input, mean, invstd, running_mean, running_var, momentum, eps, counts, out0, out1);
}

// aten::batch_norm_gather_stats_with_counts.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> batch_norm_gather_stats_with_counts_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, const at::Tensor & counts, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create_batch_norm_gather_stats_with_counts_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, mean, invstd, running_mean, running_var, momentum, eps, counts, out0, out1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pdist_backward_out, name, "aten::_pdist_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pdist_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pdist_backward_out, schema_str, "_pdist_backward.out(Tensor grad, Tensor self, float p, Tensor pdist, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_pdist_backward.out(Tensor grad, Tensor self, float p, Tensor pdist, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_pdist_backward_out::schema> create__pdist_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_pdist_backward_out::name, _pdist_backward_out::overload_name)
      .typed<_pdist_backward_out::schema>();
}

// aten::_pdist_backward.out(Tensor grad, Tensor self, float p, Tensor pdist, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _pdist_backward_out::call(const at::Tensor & grad, const at::Tensor & self, double p, const at::Tensor & pdist, at::Tensor & out) {
    
    static auto op = create__pdist_backward_out_typed_handle();
    return op.call(grad, self, p, pdist, out);
}

// aten::_pdist_backward.out(Tensor grad, Tensor self, float p, Tensor pdist, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _pdist_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, double p, const at::Tensor & pdist, at::Tensor & out) {
    
    static auto op = create__pdist_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self, p, pdist, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pixel_shuffle_out, name, "aten::pixel_shuffle")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pixel_shuffle_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pixel_shuffle_out, schema_str, "pixel_shuffle.out(Tensor self, int upscale_factor, *, Tensor(a!) out) -> Tensor(a!)")

// aten::pixel_shuffle.out(Tensor self, int upscale_factor, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<pixel_shuffle_out::schema> create_pixel_shuffle_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pixel_shuffle_out::name, pixel_shuffle_out::overload_name)
      .typed<pixel_shuffle_out::schema>();
}

// aten::pixel_shuffle.out(Tensor self, int upscale_factor, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & pixel_shuffle_out::call(const at::Tensor & self, int64_t upscale_factor, at::Tensor & out) {
    
    static auto op = create_pixel_shuffle_out_typed_handle();
    return op.call(self, upscale_factor, out);
}

// aten::pixel_shuffle.out(Tensor self, int upscale_factor, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & pixel_shuffle_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t upscale_factor, at::Tensor & out) {
    
    static auto op = create_pixel_shuffle_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, upscale_factor, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu_out, name, "aten::celu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu_out, schema_str, "celu.out(Tensor self, Scalar alpha=1.0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::celu.out(Tensor self, Scalar alpha=1.0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<celu_out::schema> create_celu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(celu_out::name, celu_out::overload_name)
      .typed<celu_out::schema>();
}

// aten::celu.out(Tensor self, Scalar alpha=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & celu_out::call(const at::Tensor & self, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create_celu_out_typed_handle();
    return op.call(self, alpha, out);
}

// aten::celu.out(Tensor self, Scalar alpha=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & celu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create_celu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_backward_out, name, "aten::slice_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_backward_out, schema_str, "slice_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step, *, Tensor(a!) out) -> Tensor(a!)")

// aten::slice_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<slice_backward_out::schema> create_slice_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slice_backward_out::name, slice_backward_out::overload_name)
      .typed<slice_backward_out::schema>();
}

// aten::slice_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slice_backward_out::call(const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt start, c10::SymInt end, c10::SymInt step, at::Tensor & out) {
    
    static auto op = create_slice_backward_out_typed_handle();
    return op.call(grad_output, input_sizes, dim, start, end, step, out);
}

// aten::slice_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slice_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt start, c10::SymInt end, c10::SymInt step, at::Tensor & out) {
    
    static auto op = create_slice_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input_sizes, dim, start, end, step, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_split_Tensor_out, name, "aten::unsafe_split")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_split_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_split_Tensor_out, schema_str, "unsafe_split.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()")

// aten::unsafe_split.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<unsafe_split_Tensor_out::schema> create_unsafe_split_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unsafe_split_Tensor_out::name, unsafe_split_Tensor_out::overload_name)
      .typed<unsafe_split_Tensor_out::schema>();
}

// aten::unsafe_split.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
void unsafe_split_Tensor_out::call(const at::Tensor & self, c10::SymInt split_size, int64_t dim, at::TensorList out) {
    
    static auto op = create_unsafe_split_Tensor_out_typed_handle();
    return op.call(self, split_size, dim, out);
}

// aten::unsafe_split.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
void unsafe_split_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt split_size, int64_t dim, at::TensorList out) {
    
    static auto op = create_unsafe_split_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, split_size, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction_out, name, "aten::std_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction_out, overload_name, "correction_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction_out, schema_str, "std_mean.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))")

// aten::std_mean.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<std_mean_correction_out::schema> create_std_mean_correction_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_mean_correction_out::name, std_mean_correction_out::overload_name)
      .typed<std_mean_correction_out::schema>();
}

// aten::std_mean.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> std_mean_correction_out::call(const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create_std_mean_correction_out_typed_handle();
    return op.call(self, dim, correction, keepdim, out0, out1);
}

// aten::std_mean.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> std_mean_correction_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create_std_mean_correction_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim, out0, out1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flip_out, name, "aten::flip")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flip_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flip_out, schema_str, "flip.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)")

// aten::flip.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<flip_out::schema> create_flip_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(flip_out::name, flip_out::overload_name)
      .typed<flip_out::schema>();
}

// aten::flip.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & flip_out::call(const at::Tensor & self, at::IntArrayRef dims, at::Tensor & out) {
    
    static auto op = create_flip_out_typed_handle();
    return op.call(self, dims, out);
}

// aten::flip.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & flip_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dims, at::Tensor & out) {
    
    static auto op = create_flip_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dims, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(roll_out, name, "aten::roll")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(roll_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(roll_out, schema_str, "roll.out(Tensor self, SymInt[1] shifts, int[1] dims=[], *, Tensor(a!) out) -> Tensor(a!)")

// aten::roll.out(Tensor self, SymInt[1] shifts, int[1] dims=[], *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<roll_out::schema> create_roll_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(roll_out::name, roll_out::overload_name)
      .typed<roll_out::schema>();
}

// aten::roll.out(Tensor self, SymInt[1] shifts, int[1] dims=[], *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & roll_out::call(const at::Tensor & self, c10::SymIntArrayRef shifts, at::IntArrayRef dims, at::Tensor & out) {
    
    static auto op = create_roll_out_typed_handle();
    return op.call(self, shifts, dims, out);
}

// aten::roll.out(Tensor self, SymInt[1] shifts, int[1] dims=[], *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & roll_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef shifts, at::IntArrayRef dims, at::Tensor & out) {
    
    static auto op = create_roll_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, shifts, dims, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_from_padded_out, name, "aten::_nested_from_padded")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_from_padded_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_from_padded_out, schema_str, "_nested_from_padded.out(Tensor padded, Tensor cpu_nested_shape_example, bool fuse_transform_0213=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_nested_from_padded.out(Tensor padded, Tensor cpu_nested_shape_example, bool fuse_transform_0213=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_nested_from_padded_out::schema> create__nested_from_padded_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_nested_from_padded_out::name, _nested_from_padded_out::overload_name)
      .typed<_nested_from_padded_out::schema>();
}

// aten::_nested_from_padded.out(Tensor padded, Tensor cpu_nested_shape_example, bool fuse_transform_0213=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _nested_from_padded_out::call(const at::Tensor & padded, const at::Tensor & cpu_nested_shape_example, bool fuse_transform_0213, at::Tensor & out) {
    
    static auto op = create__nested_from_padded_out_typed_handle();
    return op.call(padded, cpu_nested_shape_example, fuse_transform_0213, out);
}

// aten::_nested_from_padded.out(Tensor padded, Tensor cpu_nested_shape_example, bool fuse_transform_0213=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _nested_from_padded_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & padded, const at::Tensor & cpu_nested_shape_example, bool fuse_transform_0213, at::Tensor & out) {
    
    static auto op = create__nested_from_padded_out_typed_handle();
    return op.redispatch(dispatchKeySet, padded, cpu_nested_shape_example, fuse_transform_0213, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_trilinear_out, name, "aten::_trilinear")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_trilinear_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_trilinear_out, schema_str, "_trilinear.out(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_trilinear.out(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_trilinear_out::schema> create__trilinear_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_trilinear_out::name, _trilinear_out::overload_name)
      .typed<_trilinear_out::schema>();
}

// aten::_trilinear.out(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _trilinear_out::call(const at::Tensor & i1, const at::Tensor & i2, const at::Tensor & i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim, at::Tensor & out) {
    
    static auto op = create__trilinear_out_typed_handle();
    return op.call(i1, i2, i3, expand1, expand2, expand3, sumdim, unroll_dim, out);
}

// aten::_trilinear.out(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _trilinear_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & i1, const at::Tensor & i2, const at::Tensor & i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim, at::Tensor & out) {
    
    static auto op = create__trilinear_out_typed_handle();
    return op.redispatch(dispatchKeySet, i1, i2, i3, expand1, expand2, expand3, sumdim, unroll_dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unique2_out, name, "aten::_unique2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unique2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unique2_out, schema_str, "_unique2.out(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::_unique2.out(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<_unique2_out::schema> create__unique2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_unique2_out::name, _unique2_out::overload_name)
      .typed<_unique2_out::schema>();
}

// aten::_unique2.out(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _unique2_out::call(const at::Tensor & self, bool sorted, bool return_inverse, bool return_counts, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create__unique2_out_typed_handle();
    return op.call(self, sorted, return_inverse, return_counts, out0, out1, out2);
}

// aten::_unique2.out(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _unique2_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool sorted, bool return_inverse, bool return_counts, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create__unique2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, sorted, return_inverse, return_counts, out0, out1, out2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_interface_backward_out, name, "aten::_weight_norm_interface_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_interface_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_interface_backward_out, schema_str, "_weight_norm_interface_backward.out(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))")

// aten::_weight_norm_interface_backward.out(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<_weight_norm_interface_backward_out::schema> create__weight_norm_interface_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_weight_norm_interface_backward_out::name, _weight_norm_interface_backward_out::overload_name)
      .typed<_weight_norm_interface_backward_out::schema>();
}

// aten::_weight_norm_interface_backward.out(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> _weight_norm_interface_backward_out::call(const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create__weight_norm_interface_backward_out_typed_handle();
    return op.call(grad_w, saved_v, saved_g, saved_norms, dim, out0, out1);
}

// aten::_weight_norm_interface_backward.out(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> _weight_norm_interface_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create__weight_norm_interface_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_w, saved_v, saved_g, saved_norms, dim, out0, out1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_like_out, name, "aten::zeros_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_like_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_like_out, schema_str, "zeros_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)")

// aten::zeros_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<zeros_like_out::schema> create_zeros_like_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(zeros_like_out::name, zeros_like_out::overload_name)
      .typed<zeros_like_out::schema>();
}

// aten::zeros_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & zeros_like_out::call(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    
    static auto op = create_zeros_like_out_typed_handle();
    return op.call(self, memory_format, out);
}

// aten::zeros_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & zeros_like_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    
    static auto op = create_zeros_like_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, memory_format, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_csr_prod_dim_dtype_out, name, "aten::_sparse_csr_prod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_csr_prod_dim_dtype_out, overload_name, "dim_dtype_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_csr_prod_dim_dtype_out, schema_str, "_sparse_csr_prod.dim_dtype_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::_sparse_csr_prod.dim_dtype_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_csr_prod_dim_dtype_out::schema> create__sparse_csr_prod_dim_dtype_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_csr_prod_dim_dtype_out::name, _sparse_csr_prod_dim_dtype_out::overload_name)
      .typed<_sparse_csr_prod_dim_dtype_out::schema>();
}

// aten::_sparse_csr_prod.dim_dtype_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _sparse_csr_prod_dim_dtype_out::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create__sparse_csr_prod_dim_dtype_out_typed_handle();
    return op.call(self, dim, keepdim, dtype, out);
}

// aten::_sparse_csr_prod.dim_dtype_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _sparse_csr_prod_dim_dtype_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    
    static auto op = create__sparse_csr_prod_dim_dtype_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_backward_data_out, name, "aten::_sparse_softmax_backward_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_backward_data_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_backward_data_out, schema_str, "_sparse_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_sparse_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_softmax_backward_data_out::schema> create__sparse_softmax_backward_data_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_softmax_backward_data_out::name, _sparse_softmax_backward_data_out::overload_name)
      .typed<_sparse_softmax_backward_data_out::schema>();
}

// aten::_sparse_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _sparse_softmax_backward_data_out::call(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create__sparse_softmax_backward_data_out_typed_handle();
    return op.call(grad_output, output, dim, self, out);
}

// aten::_sparse_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _sparse_softmax_backward_data_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create__sparse_softmax_backward_data_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, dim, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_out, name, "aten::_sparse_log_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_out, schema_str, "_sparse_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_sparse_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_log_softmax_out::schema> create__sparse_log_softmax_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_log_softmax_out::name, _sparse_log_softmax_out::overload_name)
      .typed<_sparse_log_softmax_out::schema>();
}

// aten::_sparse_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _sparse_log_softmax_out::call(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
    
    static auto op = create__sparse_log_softmax_out_typed_handle();
    return op.call(self, dim, half_to_float, out);
}

// aten::_sparse_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _sparse_log_softmax_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
    
    static auto op = create__sparse_log_softmax_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, half_to_float, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_backward_data_out, name, "aten::_sparse_log_softmax_backward_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_backward_data_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_backward_data_out, schema_str, "_sparse_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_sparse_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_log_softmax_backward_data_out::schema> create__sparse_log_softmax_backward_data_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_log_softmax_backward_data_out::name, _sparse_log_softmax_backward_data_out::overload_name)
      .typed<_sparse_log_softmax_backward_data_out::schema>();
}

// aten::_sparse_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _sparse_log_softmax_backward_data_out::call(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create__sparse_log_softmax_backward_data_out_typed_handle();
    return op.call(grad_output, output, dim, self, out);
}

// aten::_sparse_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _sparse_log_softmax_backward_data_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create__sparse_log_softmax_backward_data_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, dim, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_spdiags_out, name, "aten::_spdiags")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_spdiags_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_spdiags_out, schema_str, "_spdiags.out(Tensor diagonals, Tensor offsets, int[] shape, Layout? layout=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_spdiags.out(Tensor diagonals, Tensor offsets, int[] shape, Layout? layout=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_spdiags_out::schema> create__spdiags_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_spdiags_out::name, _spdiags_out::overload_name)
      .typed<_spdiags_out::schema>();
}

// aten::_spdiags.out(Tensor diagonals, Tensor offsets, int[] shape, Layout? layout=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _spdiags_out::call(const at::Tensor & diagonals, const at::Tensor & offsets, at::IntArrayRef shape, c10::optional<at::Layout> layout, at::Tensor & out) {
    
    static auto op = create__spdiags_out_typed_handle();
    return op.call(diagonals, offsets, shape, layout, out);
}

// aten::_spdiags.out(Tensor diagonals, Tensor offsets, int[] shape, Layout? layout=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _spdiags_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & diagonals, const at::Tensor & offsets, at::IntArrayRef shape, c10::optional<at::Layout> layout, at::Tensor & out) {
    
    static auto op = create__spdiags_out_typed_handle();
    return op.redispatch(dispatchKeySet, diagonals, offsets, shape, layout, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zero_out, name, "aten::zero")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zero_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zero_out, schema_str, "zero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::zero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<zero_out::schema> create_zero_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(zero_out::name, zero_out::overload_name)
      .typed<zero_out::schema>();
}

// aten::zero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & zero_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_zero_out_typed_handle();
    return op.call(self, out);
}

// aten::zero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & zero_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_zero_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zero, name, "aten::zero")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zero, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zero, schema_str, "zero(Tensor self) -> Tensor")

// aten::zero(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<zero::schema> create_zero_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(zero::name, zero::overload_name)
      .typed<zero::schema>();
}

// aten::zero(Tensor self) -> Tensor
at::Tensor zero::call(const at::Tensor & self) {
    
    static auto op = create_zero_typed_handle();
    return op.call(self);
}

// aten::zero(Tensor self) -> Tensor
at::Tensor zero::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_zero_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Tensor_out, name, "aten::rsub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Tensor_out, schema_str, "rsub.Tensor_out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::rsub.Tensor_out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<rsub_Tensor_out::schema> create_rsub_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rsub_Tensor_out::name, rsub_Tensor_out::overload_name)
      .typed<rsub_Tensor_out::schema>();
}

// aten::rsub.Tensor_out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rsub_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create_rsub_Tensor_out_typed_handle();
    return op.call(self, other, alpha, out);
}

// aten::rsub.Tensor_out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rsub_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create_rsub_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Scalar_out, name, "aten::rsub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Scalar_out, schema_str, "rsub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::rsub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<rsub_Scalar_out::schema> create_rsub_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rsub_Scalar_out::name, rsub_Scalar_out::overload_name)
      .typed<rsub_Scalar_out::schema>();
}

// aten::rsub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rsub_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create_rsub_Scalar_out_typed_handle();
    return op.call(self, other, alpha, out);
}

// aten::rsub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rsub_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create_rsub_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_with_dims_out, name, "aten::_sparse_coo_tensor_with_dims")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_with_dims_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_with_dims_out, schema_str, "_sparse_coo_tensor_with_dims.out(int sparse_dim, int dense_dim, int[] size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_sparse_coo_tensor_with_dims.out(int sparse_dim, int dense_dim, int[] size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_coo_tensor_with_dims_out::schema> create__sparse_coo_tensor_with_dims_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_coo_tensor_with_dims_out::name, _sparse_coo_tensor_with_dims_out::overload_name)
      .typed<_sparse_coo_tensor_with_dims_out::schema>();
}

// aten::_sparse_coo_tensor_with_dims.out(int sparse_dim, int dense_dim, int[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _sparse_coo_tensor_with_dims_out::call(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, at::Tensor & out) {
    
    static auto op = create__sparse_coo_tensor_with_dims_out_typed_handle();
    return op.call(sparse_dim, dense_dim, size, out);
}

// aten::_sparse_coo_tensor_with_dims.out(int sparse_dim, int dense_dim, int[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _sparse_coo_tensor_with_dims_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, at::Tensor & out) {
    
    static auto op = create__sparse_coo_tensor_with_dims_out_typed_handle();
    return op.redispatch(dispatchKeySet, sparse_dim, dense_dim, size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_coalesce_out, name, "aten::_coalesce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_coalesce_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_coalesce_out, schema_str, "_coalesce.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_coalesce.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_coalesce_out::schema> create__coalesce_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_coalesce_out::name, _coalesce_out::overload_name)
      .typed<_coalesce_out::schema>();
}

// aten::_coalesce.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _coalesce_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create__coalesce_out_typed_handle();
    return op.call(self, out);
}

// aten::_coalesce.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _coalesce_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create__coalesce_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_sparse_dim_out, name, "aten::_to_sparse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_sparse_dim_out, overload_name, "sparse_dim_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_sparse_dim_out, schema_str, "_to_sparse.sparse_dim_out(Tensor self, int sparse_dim, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_to_sparse.sparse_dim_out(Tensor self, int sparse_dim, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_to_sparse_sparse_dim_out::schema> create__to_sparse_sparse_dim_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_to_sparse_sparse_dim_out::name, _to_sparse_sparse_dim_out::overload_name)
      .typed<_to_sparse_sparse_dim_out::schema>();
}

// aten::_to_sparse.sparse_dim_out(Tensor self, int sparse_dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _to_sparse_sparse_dim_out::call(const at::Tensor & self, int64_t sparse_dim, at::Tensor & out) {
    
    static auto op = create__to_sparse_sparse_dim_out_typed_handle();
    return op.call(self, sparse_dim, out);
}

// aten::_to_sparse.sparse_dim_out(Tensor self, int sparse_dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _to_sparse_sparse_dim_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sparse_dim, at::Tensor & out) {
    
    static auto op = create__to_sparse_sparse_dim_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, sparse_dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_out, name, "aten::_to_sparse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_out, schema_str, "_to_sparse.out(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None, Tensor(a!) out) -> Tensor(a!)")

// aten::_to_sparse.out(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_to_sparse_out::schema> create__to_sparse_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_to_sparse_out::name, _to_sparse_out::overload_name)
      .typed<_to_sparse_out::schema>();
}

// aten::_to_sparse.out(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _to_sparse_out::call(const at::Tensor & self, c10::optional<at::Layout> layout, at::OptionalIntArrayRef blocksize, c10::optional<int64_t> dense_dim, at::Tensor & out) {
    
    static auto op = create__to_sparse_out_typed_handle();
    return op.call(self, layout, blocksize, dense_dim, out);
}

// aten::_to_sparse.out(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _to_sparse_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Layout> layout, at::OptionalIntArrayRef blocksize, c10::optional<int64_t> dense_dim, at::Tensor & out) {
    
    static auto op = create__to_sparse_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, layout, blocksize, dense_dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_scales_out, name, "aten::q_per_channel_scales")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_scales_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_scales_out, schema_str, "q_per_channel_scales.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::q_per_channel_scales.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<q_per_channel_scales_out::schema> create_q_per_channel_scales_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(q_per_channel_scales_out::name, q_per_channel_scales_out::overload_name)
      .typed<q_per_channel_scales_out::schema>();
}

// aten::q_per_channel_scales.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & q_per_channel_scales_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_q_per_channel_scales_out_typed_handle();
    return op.call(self, out);
}

// aten::q_per_channel_scales.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & q_per_channel_scales_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_q_per_channel_scales_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_mps_backward_out, name, "aten::lstm_mps_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_mps_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_mps_backward_out, schema_str, "lstm_mps_backward.out(Tensor? grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!)[] out1, Tensor(c!)[] out2) -> ()")

// aten::lstm_mps_backward.out(Tensor? grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!)[] out1, Tensor(c!)[] out2) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<lstm_mps_backward_out::schema> create_lstm_mps_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lstm_mps_backward_out::name, lstm_mps_backward_out::overload_name)
      .typed<lstm_mps_backward_out::schema>();
}

// aten::lstm_mps_backward.out(Tensor? grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!)[] out1, Tensor(c!)[] out2) -> ()
void lstm_mps_backward_out::call(const c10::optional<at::Tensor> & grad_y, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & z_state, const at::Tensor & cell_state_fwd, const at::Tensor & input, const at::Tensor & layersOutputs, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first, at::Tensor & out0, at::TensorList out1, at::TensorList out2) {
    
    static auto op = create_lstm_mps_backward_out_typed_handle();
    return op.call(grad_y, grad_hy, grad_cy, z_state, cell_state_fwd, input, layersOutputs, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first, out0, out1, out2);
}

// aten::lstm_mps_backward.out(Tensor? grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!)[] out1, Tensor(c!)[] out2) -> ()
void lstm_mps_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const c10::optional<at::Tensor> & grad_y, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & z_state, const at::Tensor & cell_state_fwd, const at::Tensor & input, const at::Tensor & layersOutputs, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first, at::Tensor & out0, at::TensorList out1, at::TensorList out2) {
    
    static auto op = create_lstm_mps_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_y, grad_hy, grad_cy, z_state, cell_state_fwd, input, layersOutputs, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first, out0, out1, out2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell_backward_impl_out, name, "aten::_thnn_fused_lstm_cell_backward_impl")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell_backward_impl_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell_backward_impl_out, schema_str, "_thnn_fused_lstm_cell_backward_impl.out(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::_thnn_fused_lstm_cell_backward_impl.out(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<_thnn_fused_lstm_cell_backward_impl_out::schema> create__thnn_fused_lstm_cell_backward_impl_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_thnn_fused_lstm_cell_backward_impl_out::name, _thnn_fused_lstm_cell_backward_impl_out::overload_name)
      .typed<_thnn_fused_lstm_cell_backward_impl_out::schema>();
}

// aten::_thnn_fused_lstm_cell_backward_impl.out(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _thnn_fused_lstm_cell_backward_impl_out::call(const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & cx, const at::Tensor & cy, const at::Tensor & workspace, bool has_bias, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create__thnn_fused_lstm_cell_backward_impl_out_typed_handle();
    return op.call(grad_hy, grad_cy, cx, cy, workspace, has_bias, out0, out1, out2);
}

// aten::_thnn_fused_lstm_cell_backward_impl.out(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _thnn_fused_lstm_cell_backward_impl_out::redispatch(c10::DispatchKeySet dispatchKeySet, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & cx, const at::Tensor & cy, const at::Tensor & workspace, bool has_bias, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create__thnn_fused_lstm_cell_backward_impl_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_hy, grad_cy, cx, cy, workspace, has_bias, out0, out1, out2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_gru_cell_out, name, "aten::_thnn_fused_gru_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_gru_cell_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_gru_cell_out, schema_str, "_thnn_fused_gru_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))")

// aten::_thnn_fused_gru_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<_thnn_fused_gru_cell_out::schema> create__thnn_fused_gru_cell_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_thnn_fused_gru_cell_out::name, _thnn_fused_gru_cell_out::overload_name)
      .typed<_thnn_fused_gru_cell_out::schema>();
}

// aten::_thnn_fused_gru_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> _thnn_fused_gru_cell_out::call(const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create__thnn_fused_gru_cell_out_typed_handle();
    return op.call(input_gates, hidden_gates, hx, input_bias, hidden_bias, out0, out1);
}

// aten::_thnn_fused_gru_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> _thnn_fused_gru_cell_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create__thnn_fused_gru_cell_out_typed_handle();
    return op.redispatch(dispatchKeySet, input_gates, hidden_gates, hx, input_bias, hidden_bias, out0, out1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pack_padded_sequence_out, name, "aten::_pack_padded_sequence")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pack_padded_sequence_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pack_padded_sequence_out, schema_str, "_pack_padded_sequence.out(Tensor input, Tensor lengths, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))")

// aten::_pack_padded_sequence.out(Tensor input, Tensor lengths, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<_pack_padded_sequence_out::schema> create__pack_padded_sequence_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_pack_padded_sequence_out::name, _pack_padded_sequence_out::overload_name)
      .typed<_pack_padded_sequence_out::schema>();
}

// aten::_pack_padded_sequence.out(Tensor input, Tensor lengths, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> _pack_padded_sequence_out::call(const at::Tensor & input, const at::Tensor & lengths, bool batch_first, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create__pack_padded_sequence_out_typed_handle();
    return op.call(input, lengths, batch_first, out0, out1);
}

// aten::_pack_padded_sequence.out(Tensor input, Tensor lengths, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> _pack_padded_sequence_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & lengths, bool batch_first, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create__pack_padded_sequence_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, lengths, batch_first, out0, out1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_masked_softmax_out, name, "aten::_masked_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_masked_softmax_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_masked_softmax_out, schema_str, "_masked_softmax.out(Tensor self, Tensor mask, int? dim=None, int? mask_type=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_masked_softmax.out(Tensor self, Tensor mask, int? dim=None, int? mask_type=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_masked_softmax_out::schema> create__masked_softmax_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_masked_softmax_out::name, _masked_softmax_out::overload_name)
      .typed<_masked_softmax_out::schema>();
}

// aten::_masked_softmax.out(Tensor self, Tensor mask, int? dim=None, int? mask_type=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _masked_softmax_out::call(const at::Tensor & self, const at::Tensor & mask, c10::optional<int64_t> dim, c10::optional<int64_t> mask_type, at::Tensor & out) {
    
    static auto op = create__masked_softmax_out_typed_handle();
    return op.call(self, mask, dim, mask_type, out);
}

// aten::_masked_softmax.out(Tensor self, Tensor mask, int? dim=None, int? mask_type=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _masked_softmax_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, c10::optional<int64_t> dim, c10::optional<int64_t> mask_type, at::Tensor & out) {
    
    static auto op = create__masked_softmax_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mask, dim, mask_type, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Scalar_out, name, "aten::_foreach_mul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Scalar_out, schema_str, "_foreach_mul.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_mul.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul_Scalar_out::schema> create__foreach_mul_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul_Scalar_out::name, _foreach_mul_Scalar_out::overload_name)
      .typed<_foreach_mul_Scalar_out::schema>();
}

// aten::_foreach_mul.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
void _foreach_mul_Scalar_out::call(at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
    
    static auto op = create__foreach_mul_Scalar_out_typed_handle();
    return op.call(self, scalar, out);
}

// aten::_foreach_mul.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
void _foreach_mul_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
    
    static auto op = create__foreach_mul_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_List_out, name, "aten::_foreach_mul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_List_out, overload_name, "List_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_List_out, schema_str, "_foreach_mul.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_mul.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul_List_out::schema> create__foreach_mul_List_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul_List_out::name, _foreach_mul_List_out::overload_name)
      .typed<_foreach_mul_List_out::schema>();
}

// aten::_foreach_mul.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
void _foreach_mul_List_out::call(at::TensorList self, at::TensorList other, at::TensorList out) {
    
    static auto op = create__foreach_mul_List_out_typed_handle();
    return op.call(self, other, out);
}

// aten::_foreach_mul.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
void _foreach_mul_List_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, at::TensorList out) {
    
    static auto op = create__foreach_mul_List_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_ScalarList_out, name, "aten::_foreach_mul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_ScalarList_out, overload_name, "ScalarList_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_ScalarList_out, schema_str, "_foreach_mul.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_mul.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul_ScalarList_out::schema> create__foreach_mul_ScalarList_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul_ScalarList_out::name, _foreach_mul_ScalarList_out::overload_name)
      .typed<_foreach_mul_ScalarList_out::schema>();
}

// aten::_foreach_mul.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
void _foreach_mul_ScalarList_out::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
    
    static auto op = create__foreach_mul_ScalarList_out_typed_handle();
    return op.call(self, scalars, out);
}

// aten::_foreach_mul.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
void _foreach_mul_ScalarList_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
    
    static auto op = create__foreach_mul_ScalarList_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Tensor_out, name, "aten::_foreach_mul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Tensor_out, schema_str, "_foreach_mul.Tensor_out(Tensor[] self, Tensor other, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_mul.Tensor_out(Tensor[] self, Tensor other, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul_Tensor_out::schema> create__foreach_mul_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul_Tensor_out::name, _foreach_mul_Tensor_out::overload_name)
      .typed<_foreach_mul_Tensor_out::schema>();
}

// aten::_foreach_mul.Tensor_out(Tensor[] self, Tensor other, *, Tensor(a!)[] out) -> ()
void _foreach_mul_Tensor_out::call(at::TensorList self, const at::Tensor & other, at::TensorList out) {
    
    static auto op = create__foreach_mul_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::_foreach_mul.Tensor_out(Tensor[] self, Tensor other, *, Tensor(a!)[] out) -> ()
void _foreach_mul_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Tensor & other, at::TensorList out) {
    
    static auto op = create__foreach_mul_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Scalar_out, name, "aten::_foreach_div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Scalar_out, schema_str, "_foreach_div.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_div.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div_Scalar_out::schema> create__foreach_div_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div_Scalar_out::name, _foreach_div_Scalar_out::overload_name)
      .typed<_foreach_div_Scalar_out::schema>();
}

// aten::_foreach_div.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
void _foreach_div_Scalar_out::call(at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
    
    static auto op = create__foreach_div_Scalar_out_typed_handle();
    return op.call(self, scalar, out);
}

// aten::_foreach_div.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
void _foreach_div_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
    
    static auto op = create__foreach_div_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_List_out, name, "aten::_foreach_div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_List_out, overload_name, "List_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_List_out, schema_str, "_foreach_div.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_div.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div_List_out::schema> create__foreach_div_List_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div_List_out::name, _foreach_div_List_out::overload_name)
      .typed<_foreach_div_List_out::schema>();
}

// aten::_foreach_div.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
void _foreach_div_List_out::call(at::TensorList self, at::TensorList other, at::TensorList out) {
    
    static auto op = create__foreach_div_List_out_typed_handle();
    return op.call(self, other, out);
}

// aten::_foreach_div.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
void _foreach_div_List_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, at::TensorList out) {
    
    static auto op = create__foreach_div_List_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_ScalarList_out, name, "aten::_foreach_div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_ScalarList_out, overload_name, "ScalarList_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_ScalarList_out, schema_str, "_foreach_div.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_div.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div_ScalarList_out::schema> create__foreach_div_ScalarList_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div_ScalarList_out::name, _foreach_div_ScalarList_out::overload_name)
      .typed<_foreach_div_ScalarList_out::schema>();
}

// aten::_foreach_div.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
void _foreach_div_ScalarList_out::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
    
    static auto op = create__foreach_div_ScalarList_out_typed_handle();
    return op.call(self, scalars, out);
}

// aten::_foreach_div.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
void _foreach_div_ScalarList_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
    
    static auto op = create__foreach_div_ScalarList_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Tensor_out, name, "aten::_foreach_div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Tensor_out, schema_str, "_foreach_div.Tensor_out(Tensor[] self, Tensor other, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_div.Tensor_out(Tensor[] self, Tensor other, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div_Tensor_out::schema> create__foreach_div_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div_Tensor_out::name, _foreach_div_Tensor_out::overload_name)
      .typed<_foreach_div_Tensor_out::schema>();
}

// aten::_foreach_div.Tensor_out(Tensor[] self, Tensor other, *, Tensor(a!)[] out) -> ()
void _foreach_div_Tensor_out::call(at::TensorList self, const at::Tensor & other, at::TensorList out) {
    
    static auto op = create__foreach_div_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::_foreach_div.Tensor_out(Tensor[] self, Tensor other, *, Tensor(a!)[] out) -> ()
void _foreach_div_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Tensor & other, at::TensorList out) {
    
    static auto op = create__foreach_div_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Scalar_out, name, "aten::_foreach_addcmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Scalar_out, schema_str, "_foreach_addcmul.Scalar_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_addcmul.Scalar_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcmul_Scalar_out::schema> create__foreach_addcmul_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcmul_Scalar_out::name, _foreach_addcmul_Scalar_out::overload_name)
      .typed<_foreach_addcmul_Scalar_out::schema>();
}

// aten::_foreach_addcmul.Scalar_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1, *, Tensor(a!)[] out) -> ()
void _foreach_addcmul_Scalar_out::call(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value, at::TensorList out) {
    
    static auto op = create__foreach_addcmul_Scalar_out_typed_handle();
    return op.call(self, tensor1, tensor2, value, out);
}

// aten::_foreach_addcmul.Scalar_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1, *, Tensor(a!)[] out) -> ()
void _foreach_addcmul_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value, at::TensorList out) {
    
    static auto op = create__foreach_addcmul_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, value, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_ScalarList_out, name, "aten::_foreach_addcmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_ScalarList_out, overload_name, "ScalarList_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_ScalarList_out, schema_str, "_foreach_addcmul.ScalarList_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_addcmul.ScalarList_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcmul_ScalarList_out::schema> create__foreach_addcmul_ScalarList_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcmul_ScalarList_out::name, _foreach_addcmul_ScalarList_out::overload_name)
      .typed<_foreach_addcmul_ScalarList_out::schema>();
}

// aten::_foreach_addcmul.ScalarList_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
void _foreach_addcmul_ScalarList_out::call(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
    
    static auto op = create__foreach_addcmul_ScalarList_out_typed_handle();
    return op.call(self, tensor1, tensor2, scalars, out);
}

// aten::_foreach_addcmul.ScalarList_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
void _foreach_addcmul_ScalarList_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
    
    static auto op = create__foreach_addcmul_ScalarList_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, scalars, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Tensor_out, name, "aten::_foreach_addcmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Tensor_out, schema_str, "_foreach_addcmul.Tensor_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_addcmul.Tensor_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcmul_Tensor_out::schema> create__foreach_addcmul_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcmul_Tensor_out::name, _foreach_addcmul_Tensor_out::overload_name)
      .typed<_foreach_addcmul_Tensor_out::schema>();
}

// aten::_foreach_addcmul.Tensor_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars, *, Tensor(a!)[] out) -> ()
void _foreach_addcmul_Tensor_out::call(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Tensor & scalars, at::TensorList out) {
    
    static auto op = create__foreach_addcmul_Tensor_out_typed_handle();
    return op.call(self, tensor1, tensor2, scalars, out);
}

// aten::_foreach_addcmul.Tensor_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars, *, Tensor(a!)[] out) -> ()
void _foreach_addcmul_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Tensor & scalars, at::TensorList out) {
    
    static auto op = create__foreach_addcmul_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, scalars, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin_out, name, "aten::_foreach_asin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin_out, schema_str, "_foreach_asin.out(Tensor[] self, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_asin.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_asin_out::schema> create__foreach_asin_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_asin_out::name, _foreach_asin_out::overload_name)
      .typed<_foreach_asin_out::schema>();
}

// aten::_foreach_asin.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_asin_out::call(at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_asin_out_typed_handle();
    return op.call(self, out);
}

// aten::_foreach_asin.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_asin_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_asin_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos_out, name, "aten::_foreach_cos")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos_out, schema_str, "_foreach_cos.out(Tensor[] self, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_cos.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_cos_out::schema> create__foreach_cos_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_cos_out::name, _foreach_cos_out::overload_name)
      .typed<_foreach_cos_out::schema>();
}

// aten::_foreach_cos.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_cos_out::call(at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_cos_out_typed_handle();
    return op.call(self, out);
}

// aten::_foreach_cos.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_cos_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_cos_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor_out, name, "aten::_foreach_floor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor_out, schema_str, "_foreach_floor.out(Tensor[] self, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_floor.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_floor_out::schema> create__foreach_floor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_floor_out::name, _foreach_floor_out::overload_name)
      .typed<_foreach_floor_out::schema>();
}

// aten::_foreach_floor.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_floor_out::call(at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_floor_out_typed_handle();
    return op.call(self, out);
}

// aten::_foreach_floor.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_floor_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_floor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh_out, name, "aten::_foreach_tanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh_out, schema_str, "_foreach_tanh.out(Tensor[] self, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_tanh.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_tanh_out::schema> create__foreach_tanh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_tanh_out::name, _foreach_tanh_out::overload_name)
      .typed<_foreach_tanh_out::schema>();
}

// aten::_foreach_tanh.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_tanh_out::call(at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_tanh_out_typed_handle();
    return op.call(self, out);
}

// aten::_foreach_tanh.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_tanh_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_tanh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_zero_out, name, "aten::_foreach_zero")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_zero_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_zero_out, schema_str, "_foreach_zero.out(Tensor[] self, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_zero.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_zero_out::schema> create__foreach_zero_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_zero_out::name, _foreach_zero_out::overload_name)
      .typed<_foreach_zero_out::schema>();
}

// aten::_foreach_zero.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_zero_out::call(at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_zero_out_typed_handle();
    return op.call(self, out);
}

// aten::_foreach_zero.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_zero_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_zero_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_zero, name, "aten::_foreach_zero")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_zero, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_zero, schema_str, "_foreach_zero(Tensor[] self) -> Tensor[] self_out")

// aten::_foreach_zero(Tensor[] self) -> Tensor[] self_out
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_zero::schema> create__foreach_zero_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_zero::name, _foreach_zero::overload_name)
      .typed<_foreach_zero::schema>();
}

// aten::_foreach_zero(Tensor[] self) -> Tensor[] self_out
::std::vector<at::Tensor> _foreach_zero::call(at::TensorList self) {
    
    static auto op = create__foreach_zero_typed_handle();
    return op.call(self);
}

// aten::_foreach_zero(Tensor[] self) -> Tensor[] self_out
::std::vector<at::Tensor> _foreach_zero::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_zero_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d_out, name, "aten::_adaptive_avg_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d_out, schema_str, "_adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_adaptive_avg_pool3d_out::schema> create__adaptive_avg_pool3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_adaptive_avg_pool3d_out::name, _adaptive_avg_pool3d_out::overload_name)
      .typed<_adaptive_avg_pool3d_out::schema>();
}

// aten::_adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _adaptive_avg_pool3d_out::call(const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
    
    static auto op = create__adaptive_avg_pool3d_out_typed_handle();
    return op.call(self, output_size, out);
}

// aten::_adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _adaptive_avg_pool3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
    
    static auto op = create__adaptive_avg_pool3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_slow_conv2d_backward_output_mask_out, name, "aten::_slow_conv2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_slow_conv2d_backward_output_mask_out, overload_name, "output_mask_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_slow_conv2d_backward_output_mask_out, schema_str, "_slow_conv2d_backward.output_mask_out(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::_slow_conv2d_backward.output_mask_out(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<_slow_conv2d_backward_output_mask_out::schema> create__slow_conv2d_backward_output_mask_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_slow_conv2d_backward_output_mask_out::name, _slow_conv2d_backward_output_mask_out::overload_name)
      .typed<_slow_conv2d_backward_output_mask_out::schema>();
}

// aten::_slow_conv2d_backward.output_mask_out(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _slow_conv2d_backward_output_mask_out::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create__slow_conv2d_backward_output_mask_out_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, output_mask, out0, out1, out2);
}

// aten::_slow_conv2d_backward.output_mask_out(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _slow_conv2d_backward_output_mask_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create__slow_conv2d_backward_output_mask_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, output_mask, out0, out1, out2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d_out, name, "aten::conv_depthwise3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d_out, schema_str, "conv_depthwise3d.out(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding, SymInt[3] dilation, *, Tensor(a!) out) -> Tensor(a!)")

// aten::conv_depthwise3d.out(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding, SymInt[3] dilation, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<conv_depthwise3d_out::schema> create_conv_depthwise3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv_depthwise3d_out::name, conv_depthwise3d_out::overload_name)
      .typed<conv_depthwise3d_out::schema>();
}

// aten::conv_depthwise3d.out(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding, SymInt[3] dilation, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & conv_depthwise3d_out::call(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, at::Tensor & out) {
    
    static auto op = create_conv_depthwise3d_out_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, dilation, out);
}

// aten::conv_depthwise3d.out(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding, SymInt[3] dilation, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & conv_depthwise3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, at::Tensor & out) {
    
    static auto op = create_conv_depthwise3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, dilation, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated2d_out, name, "aten::slow_conv_dilated2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated2d_out, schema_str, "slow_conv_dilated2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::slow_conv_dilated2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_dilated2d_out::schema> create_slow_conv_dilated2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_dilated2d_out::name, slow_conv_dilated2d_out::overload_name)
      .typed<slow_conv_dilated2d_out::schema>();
}

// aten::slow_conv_dilated2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slow_conv_dilated2d_out::call(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, at::Tensor & out) {
    
    static auto op = create_slow_conv_dilated2d_out_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, dilation, out);
}

// aten::slow_conv_dilated2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slow_conv_dilated2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, at::Tensor & out) {
    
    static auto op = create_slow_conv_dilated2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, dilation, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_filled_intlist_out, name, "aten::_test_optional_filled_intlist")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_filled_intlist_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_filled_intlist_out, schema_str, "_test_optional_filled_intlist.out(Tensor values, int[2]? addends, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_test_optional_filled_intlist.out(Tensor values, int[2]? addends, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_test_optional_filled_intlist_out::schema> create__test_optional_filled_intlist_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_test_optional_filled_intlist_out::name, _test_optional_filled_intlist_out::overload_name)
      .typed<_test_optional_filled_intlist_out::schema>();
}

// aten::_test_optional_filled_intlist.out(Tensor values, int[2]? addends, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _test_optional_filled_intlist_out::call(const at::Tensor & values, at::OptionalIntArrayRef addends, at::Tensor & out) {
    
    static auto op = create__test_optional_filled_intlist_out_typed_handle();
    return op.call(values, addends, out);
}

// aten::_test_optional_filled_intlist.out(Tensor values, int[2]? addends, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _test_optional_filled_intlist_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & values, at::OptionalIntArrayRef addends, at::Tensor & out) {
    
    static auto op = create__test_optional_filled_intlist_out_typed_handle();
    return op.redispatch(dispatchKeySet, values, addends, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_autograd_multiple_dispatch_view_copy_out, name, "aten::_test_autograd_multiple_dispatch_view_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_autograd_multiple_dispatch_view_copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_autograd_multiple_dispatch_view_copy_out, schema_str, "_test_autograd_multiple_dispatch_view_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_test_autograd_multiple_dispatch_view_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_test_autograd_multiple_dispatch_view_copy_out::schema> create__test_autograd_multiple_dispatch_view_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_test_autograd_multiple_dispatch_view_copy_out::name, _test_autograd_multiple_dispatch_view_copy_out::overload_name)
      .typed<_test_autograd_multiple_dispatch_view_copy_out::schema>();
}

// aten::_test_autograd_multiple_dispatch_view_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _test_autograd_multiple_dispatch_view_copy_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create__test_autograd_multiple_dispatch_view_copy_out_typed_handle();
    return op.call(self, out);
}

// aten::_test_autograd_multiple_dispatch_view_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _test_autograd_multiple_dispatch_view_copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create__test_autograd_multiple_dispatch_view_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fw_primal_copy_out, name, "aten::_fw_primal_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fw_primal_copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fw_primal_copy_out, schema_str, "_fw_primal_copy.out(Tensor self, int level, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_fw_primal_copy.out(Tensor self, int level, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_fw_primal_copy_out::schema> create__fw_primal_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fw_primal_copy_out::name, _fw_primal_copy_out::overload_name)
      .typed<_fw_primal_copy_out::schema>();
}

// aten::_fw_primal_copy.out(Tensor self, int level, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _fw_primal_copy_out::call(const at::Tensor & self, int64_t level, at::Tensor & out) {
    
    static auto op = create__fw_primal_copy_out_typed_handle();
    return op.call(self, level, out);
}

// aten::_fw_primal_copy.out(Tensor self, int level, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _fw_primal_copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t level, at::Tensor & out) {
    
    static auto op = create__fw_primal_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, level, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as_real_copy_out, name, "aten::view_as_real_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as_real_copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as_real_copy_out, schema_str, "view_as_real_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::view_as_real_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<view_as_real_copy_out::schema> create_view_as_real_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(view_as_real_copy_out::name, view_as_real_copy_out::overload_name)
      .typed<view_as_real_copy_out::schema>();
}

// aten::view_as_real_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & view_as_real_copy_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_view_as_real_copy_out_typed_handle();
    return op.call(self, out);
}

// aten::view_as_real_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & view_as_real_copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_view_as_real_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(as_strided_copy_out, name, "aten::as_strided_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(as_strided_copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(as_strided_copy_out, schema_str, "as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<as_strided_copy_out::schema> create_as_strided_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(as_strided_copy_out::name, as_strided_copy_out::overload_name)
      .typed<as_strided_copy_out::schema>();
}

// aten::as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & as_strided_copy_out::call(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset, at::Tensor & out) {
    
    static auto op = create_as_strided_copy_out_typed_handle();
    return op.call(self, size, stride, storage_offset, out);
}

// aten::as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & as_strided_copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset, at::Tensor & out) {
    
    static auto op = create_as_strided_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, stride, storage_offset, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_alias_copy_out, name, "aten::_reshape_alias_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_alias_copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_alias_copy_out, schema_str, "_reshape_alias_copy.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_reshape_alias_copy.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_reshape_alias_copy_out::schema> create__reshape_alias_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_reshape_alias_copy_out::name, _reshape_alias_copy_out::overload_name)
      .typed<_reshape_alias_copy_out::schema>();
}

// aten::_reshape_alias_copy.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _reshape_alias_copy_out::call(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::Tensor & out) {
    
    static auto op = create__reshape_alias_copy_out_typed_handle();
    return op.call(self, size, stride, out);
}

// aten::_reshape_alias_copy.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _reshape_alias_copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::Tensor & out) {
    
    static auto op = create__reshape_alias_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, stride, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_out, name, "aten::squeeze_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_out, schema_str, "squeeze_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::squeeze_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<squeeze_copy_out::schema> create_squeeze_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(squeeze_copy_out::name, squeeze_copy_out::overload_name)
      .typed<squeeze_copy_out::schema>();
}

// aten::squeeze_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & squeeze_copy_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_squeeze_copy_out_typed_handle();
    return op.call(self, out);
}

// aten::squeeze_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & squeeze_copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_squeeze_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_dim_out, name, "aten::squeeze_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_dim_out, overload_name, "dim_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_dim_out, schema_str, "squeeze_copy.dim_out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)")

// aten::squeeze_copy.dim_out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<squeeze_copy_dim_out::schema> create_squeeze_copy_dim_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(squeeze_copy_dim_out::name, squeeze_copy_dim_out::overload_name)
      .typed<squeeze_copy_dim_out::schema>();
}

// aten::squeeze_copy.dim_out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & squeeze_copy_dim_out::call(const at::Tensor & self, int64_t dim, at::Tensor & out) {
    
    static auto op = create_squeeze_copy_dim_out_typed_handle();
    return op.call(self, dim, out);
}

// aten::squeeze_copy.dim_out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & squeeze_copy_dim_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
    
    static auto op = create_squeeze_copy_dim_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_dims_out, name, "aten::squeeze_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_dims_out, overload_name, "dims_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_copy_dims_out, schema_str, "squeeze_copy.dims_out(Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)")

// aten::squeeze_copy.dims_out(Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<squeeze_copy_dims_out::schema> create_squeeze_copy_dims_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(squeeze_copy_dims_out::name, squeeze_copy_dims_out::overload_name)
      .typed<squeeze_copy_dims_out::schema>();
}

// aten::squeeze_copy.dims_out(Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & squeeze_copy_dims_out::call(const at::Tensor & self, at::IntArrayRef dim, at::Tensor & out) {
    
    static auto op = create_squeeze_copy_dims_out_typed_handle();
    return op.call(self, dim, out);
}

// aten::squeeze_copy.dims_out(Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & squeeze_copy_dims_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, at::Tensor & out) {
    
    static auto op = create_squeeze_copy_dims_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(indices_copy_out, name, "aten::indices_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(indices_copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(indices_copy_out, schema_str, "indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<indices_copy_out::schema> create_indices_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(indices_copy_out::name, indices_copy_out::overload_name)
      .typed<indices_copy_out::schema>();
}

// aten::indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & indices_copy_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_indices_copy_out_typed_handle();
    return op.call(self, out);
}

// aten::indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & indices_copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_indices_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ccol_indices_copy_out, name, "aten::ccol_indices_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ccol_indices_copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ccol_indices_copy_out, schema_str, "ccol_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ccol_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ccol_indices_copy_out::schema> create_ccol_indices_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ccol_indices_copy_out::name, ccol_indices_copy_out::overload_name)
      .typed<ccol_indices_copy_out::schema>();
}

// aten::ccol_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ccol_indices_copy_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_ccol_indices_copy_out_typed_handle();
    return op.call(self, out);
}

// aten::ccol_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ccol_indices_copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_ccol_indices_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

}} // namespace at::_ops
