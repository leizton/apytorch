#include <ATen/Tensor.h>
#include <ATen/core/dispatch/Dispatcher.h>

// @generated by torchgen/gen.py from Operators.cpp
// NOTE See [Sharded File] comment in VariableType

#ifndef AT_PER_OPERATOR_HEADERS
#include <ATen/Operators.h>
#else
#include <ATen/ops/_cast_Double.h>
#include <ATen/ops/_cast_Float.h>
#include <ATen/ops/_cast_Half.h>
#include <ATen/ops/_version.h>
#include <ATen/ops/_make_dual.h>
#include <ATen/ops/align_as.h>
#include <ATen/ops/_assert_tensor_metadata.h>
#include <ATen/ops/sym_constrain_range_for_size.h>
#include <ATen/ops/refine_names.h>
#include <ATen/ops/_use_cudnn_rnn_flatten_weight.h>
#include <ATen/ops/_cudnn_rnn_flatten_weight.h>
#include <ATen/ops/_sobol_engine_ff.h>
#include <ATen/ops/_sobol_engine_scramble.h>
#include <ATen/ops/feature_alpha_dropout.h>
#include <ATen/ops/feature_alpha_dropout.h>
#include <ATen/ops/abs.h>
#include <ATen/ops/abs.h>
#include <ATen/ops/abs.h>
#include <ATen/ops/imag.h>
#include <ATen/ops/resolve_conj.h>
#include <ATen/ops/resolve_neg.h>
#include <ATen/ops/adaptive_max_pool1d.h>
#include <ATen/ops/addmv.h>
#include <ATen/ops/addmv.h>
#include <ATen/ops/addmv.h>
#include <ATen/ops/addr.h>
#include <ATen/ops/addr.h>
#include <ATen/ops/addr.h>
#include <ATen/ops/affine_grid_generator_backward.h>
#include <ATen/ops/argmin.h>
#include <ATen/ops/argmin.h>
#include <ATen/ops/atan.h>
#include <ATen/ops/atan.h>
#include <ATen/ops/atan.h>
#include <ATen/ops/arctan.h>
#include <ATen/ops/arctan.h>
#include <ATen/ops/arctan.h>
#include <ATen/ops/quantized_batch_norm.h>
#include <ATen/ops/binary_cross_entropy_backward.h>
#include <ATen/ops/binary_cross_entropy_backward.h>
#include <ATen/ops/bitwise_not.h>
#include <ATen/ops/bitwise_not.h>
#include <ATen/ops/bitwise_not.h>
#include <ATen/ops/logical_not.h>
#include <ATen/ops/logical_not.h>
#include <ATen/ops/logical_not.h>
#include <ATen/ops/concatenate.h>
#include <ATen/ops/concatenate.h>
#include <ATen/ops/concatenate.h>
#include <ATen/ops/concatenate.h>
#include <ATen/ops/ceil.h>
#include <ATen/ops/ceil.h>
#include <ATen/ops/ceil.h>
#include <ATen/ops/conv_tbc.h>
#include <ATen/ops/cosh.h>
#include <ATen/ops/cosh.h>
#include <ATen/ops/cosh.h>
#include <ATen/ops/cosine_embedding_loss.h>
#include <ATen/ops/cudnn_affine_grid_generator_backward.h>
#include <ATen/ops/cudnn_grid_sampler.h>
#include <ATen/ops/cummin.h>
#include <ATen/ops/cummin.h>
#include <ATen/ops/cummin.h>
#include <ATen/ops/cummin.h>
#include <ATen/ops/_cummin_helper.h>
#include <ATen/ops/div.h>
#include <ATen/ops/div.h>
#include <ATen/ops/div.h>
#include <ATen/ops/div.h>
#include <ATen/ops/div.h>
#include <ATen/ops/div.h>
#include <ATen/ops/div.h>
#include <ATen/ops/div.h>
#include <ATen/ops/div.h>
#include <ATen/ops/div.h>
#include <ATen/ops/_embedding_bag_forward_only.h>
#include <ATen/ops/embedding_bag.h>
#include <ATen/ops/embedding_bag.h>
#include <ATen/ops/new_zeros.h>
#include <ATen/ops/erf.h>
#include <ATen/ops/erf.h>
#include <ATen/ops/erf.h>
#include <ATen/ops/grid_sampler.h>
#include <ATen/ops/_grid_sampler_2d_cpu_fallback.h>
#include <ATen/ops/grid_sampler_3d.h>
#include <ATen/ops/hann_window.h>
#include <ATen/ops/hann_window.h>
#include <ATen/ops/hamming_window.h>
#include <ATen/ops/hamming_window.h>
#include <ATen/ops/hamming_window.h>
#include <ATen/ops/hamming_window.h>
#include <ATen/ops/native_group_norm_backward.h>
#include <ATen/ops/_fft_c2c.h>
#include <ATen/ops/_fft_c2c.h>
#include <ATen/ops/_validate_compressed_sparse_indices.h>
#include <ATen/ops/_cufft_get_plan_cache_size.h>
#include <ATen/ops/_cufft_get_plan_cache_max_size.h>
#include <ATen/ops/index.h>
#include <ATen/ops/index.h>
#include <ATen/ops/isnan.h>
#include <ATen/ops/kthvalue.h>
#include <ATen/ops/kthvalue.h>
#include <ATen/ops/kthvalue.h>
#include <ATen/ops/kthvalue.h>
#include <ATen/ops/native_layer_norm.h>
#include <ATen/ops/nan_to_num.h>
#include <ATen/ops/nan_to_num.h>
#include <ATen/ops/nan_to_num.h>
#include <ATen/ops/fbgemm_linear_int8_weight_fp32_activation.h>
#include <ATen/ops/fbgemm_linear_int8_weight.h>
#include <ATen/ops/fbgemm_linear_fp16_weight_fp32_activation.h>
#include <ATen/ops/xlogy.h>
#include <ATen/ops/xlogy.h>
#include <ATen/ops/xlogy.h>
#include <ATen/ops/xlogy.h>
#include <ATen/ops/xlogy.h>
#include <ATen/ops/xlogy.h>
#include <ATen/ops/xlogy.h>
#include <ATen/ops/xlogy.h>
#include <ATen/ops/_log_softmax_backward_data.h>
#include <ATen/ops/_log_softmax_backward_data.h>
#include <ATen/ops/logcumsumexp.h>
#include <ATen/ops/logcumsumexp.h>
#include <ATen/ops/logcumsumexp.h>
#include <ATen/ops/logcumsumexp.h>
#include <ATen/ops/matrix_exp_backward.h>
#include <ATen/ops/amax.h>
#include <ATen/ops/amax.h>
#include <ATen/ops/mkldnn_max_pool2d.h>
#include <ATen/ops/quantized_max_pool2d.h>
#include <ATen/ops/amin.h>
#include <ATen/ops/amin.h>
#include <ATen/ops/_mps_convolution.h>
#include <ATen/ops/mkldnn_rnn_layer_backward.h>
#include <ATen/ops/miopen_depthwise_convolution.h>
#include <ATen/ops/_int_mm.h>
#include <ATen/ops/_int_mm.h>
#include <ATen/ops/native_batch_norm.h>
#include <ATen/ops/native_batch_norm.h>
#include <ATen/ops/batch_norm_stats.h>
#include <ATen/ops/batch_norm_gather_stats.h>
#include <ATen/ops/native_batch_norm_backward.h>
#include <ATen/ops/batch_norm_backward_reduce.h>
#include <ATen/ops/is_vulkan_available.h>
#include <ATen/ops/_nnpack_spatial_convolution.h>
#include <ATen/ops/ones.h>
#include <ATen/ops/ones.h>
#include <ATen/ops/ones.h>
#include <ATen/ops/_cdist_forward.h>
#include <ATen/ops/cosine_similarity.h>
#include <ATen/ops/movedim.h>
#include <ATen/ops/movedim.h>
#include <ATen/ops/numpy_T.h>
#include <ATen/ops/mH.h>
#include <ATen/ops/rand_like.h>
#include <ATen/ops/randint_like.h>
#include <ATen/ops/randint_like.h>
#include <ATen/ops/round.h>
#include <ATen/ops/round.h>
#include <ATen/ops/round.h>
#include <ATen/ops/round.h>
#include <ATen/ops/round.h>
#include <ATen/ops/round.h>
#include <ATen/ops/gelu.h>
#include <ATen/ops/gelu.h>
#include <ATen/ops/gelu.h>
#include <ATen/ops/hardshrink.h>
#include <ATen/ops/hardshrink.h>
#include <ATen/ops/select_backward.h>
#include <ATen/ops/mish.h>
#include <ATen/ops/mish.h>
#include <ATen/ops/mish.h>
#include <ATen/ops/sigmoid.h>
#include <ATen/ops/sigmoid.h>
#include <ATen/ops/sigmoid.h>
#include <ATen/ops/detach.h>
#include <ATen/ops/detach.h>
#include <ATen/ops/size.h>
#include <ATen/ops/size.h>
#include <ATen/ops/sym_numel.h>
#include <ATen/ops/slice_scatter.h>
#include <ATen/ops/_softmax_backward_data.h>
#include <ATen/ops/_softmax_backward_data.h>
#include <ATen/ops/split_with_sizes.h>
#include <ATen/ops/hsplit.h>
#include <ATen/ops/hsplit.h>
#include <ATen/ops/stack.h>
#include <ATen/ops/stack.h>
#include <ATen/ops/_stack.h>
#include <ATen/ops/_stack.h>
#include <ATen/ops/square.h>
#include <ATen/ops/square.h>
#include <ATen/ops/square.h>
#include <ATen/ops/tanh.h>
#include <ATen/ops/tanh.h>
#include <ATen/ops/tanh.h>
#include <ATen/ops/tensordot.h>
#include <ATen/ops/tensordot.h>
#include <ATen/ops/tile.h>
#include <ATen/ops/_mkldnn_transpose.h>
#include <ATen/ops/_mkldnn_transpose.h>
#include <ATen/ops/fliplr.h>
#include <ATen/ops/_nested_from_padded_and_nested_example.h>
#include <ATen/ops/fix.h>
#include <ATen/ops/fix.h>
#include <ATen/ops/fix.h>
#include <ATen/ops/unique_dim.h>
#include <ATen/ops/unique_consecutive.h>
#include <ATen/ops/vander.h>
#include <ATen/ops/view_as.h>
#include <ATen/ops/_dirichlet_grad.h>
#include <ATen/ops/frobenius_norm.h>
#include <ATen/ops/frobenius_norm.h>
#include <ATen/ops/clone.h>
#include <ATen/ops/positive.h>
#include <ATen/ops/resize_as.h>
#include <ATen/ops/resize_as_sparse.h>
#include <ATen/ops/sparse_sampled_addmm.h>
#include <ATen/ops/sparse_sampled_addmm.h>
#include <ATen/ops/sparse_csr_tensor.h>
#include <ATen/ops/sparse_csr_tensor.h>
#include <ATen/ops/_sparse_bsc_tensor_unsafe.h>
#include <ATen/ops/dense_dim.h>
#include <ATen/ops/_dimV.h>
#include <ATen/ops/coalesce.h>
#include <ATen/ops/_indices.h>
#include <ATen/ops/to_sparse_csc.h>
#include <ATen/ops/_to_sparse_csc.h>
#include <ATen/ops/_to_sparse_bsc.h>
#include <ATen/ops/mkldnn_reorder_conv2d_weight.h>
#include <ATen/ops/quantize_per_channel.h>
#include <ATen/ops/dequantize.h>
#include <ATen/ops/dequantize.h>
#include <ATen/ops/q_per_channel_zero_points.h>
#include <ATen/ops/fake_quantize_per_tensor_affine.h>
#include <ATen/ops/fake_quantize_per_tensor_affine.h>
#include <ATen/ops/_fake_quantize_learnable_per_channel_affine.h>
#include <ATen/ops/_autocast_to_full_precision.h>
#include <ATen/ops/to.h>
#include <ATen/ops/to.h>
#include <ATen/ops/to.h>
#include <ATen/ops/to.h>
#include <ATen/ops/combinations.h>
#include <ATen/ops/item.h>
#include <ATen/ops/_lstm_mps.h>
#include <ATen/ops/_thnn_fused_lstm_cell.h>
#include <ATen/ops/lstm.h>
#include <ATen/ops/lstm.h>
#include <ATen/ops/gru.h>
#include <ATen/ops/gru.h>
#include <ATen/ops/rnn_tanh.h>
#include <ATen/ops/rnn_tanh.h>
#include <ATen/ops/rnn_relu_cell.h>
#include <ATen/ops/_pad_packed_sequence.h>
#include <ATen/ops/lift_fresh_copy.h>
#include <ATen/ops/index_reduce.h>
#include <ATen/ops/index_reduce.h>
#include <ATen/ops/index_reduce.h>
#include <ATen/ops/index_fill.h>
#include <ATen/ops/index_fill.h>
#include <ATen/ops/index_fill.h>
#include <ATen/ops/index_fill.h>
#include <ATen/ops/index_fill.h>
#include <ATen/ops/index_fill.h>
#include <ATen/ops/index_fill.h>
#include <ATen/ops/index_fill.h>
#include <ATen/ops/scatter_add.h>
#include <ATen/ops/scatter_add.h>
#include <ATen/ops/scatter_add.h>
#include <ATen/ops/scatter_add.h>
#include <ATen/ops/digamma.h>
#include <ATen/ops/random.h>
#include <ATen/ops/random.h>
#include <ATen/ops/random.h>
#include <ATen/ops/cauchy.h>
#include <ATen/ops/log_normal.h>
#include <ATen/ops/cross.h>
#include <ATen/ops/cross.h>
#include <ATen/ops/ne.h>
#include <ATen/ops/ne.h>
#include <ATen/ops/ne.h>
#include <ATen/ops/ne.h>
#include <ATen/ops/ne.h>
#include <ATen/ops/ne.h>
#include <ATen/ops/ge.h>
#include <ATen/ops/ge.h>
#include <ATen/ops/ge.h>
#include <ATen/ops/ge.h>
#include <ATen/ops/ge.h>
#include <ATen/ops/ge.h>
#include <ATen/ops/_gather_sparse_backward.h>
#include <ATen/ops/linalg_vander.h>
#include <ATen/ops/swapaxes.h>
#include <ATen/ops/swapaxes.h>
#include <ATen/ops/cholesky_solve.h>
#include <ATen/ops/cholesky_solve.h>
#include <ATen/ops/qr.h>
#include <ATen/ops/qr.h>
#include <ATen/ops/digamma.h>
#include <ATen/ops/digamma.h>
#include <ATen/ops/polygamma.h>
#include <ATen/ops/polygamma.h>
#include <ATen/ops/polygamma.h>
#include <ATen/ops/histc.h>
#include <ATen/ops/histc.h>
#include <ATen/ops/_histogramdd_bin_edges.h>
#include <ATen/ops/_histogramdd_from_bin_tensors.h>
#include <ATen/ops/nextafter.h>
#include <ATen/ops/nextafter.h>
#include <ATen/ops/nextafter.h>
#include <ATen/ops/maximum.h>
#include <ATen/ops/maximum.h>
#include <ATen/ops/minimum.h>
#include <ATen/ops/minimum.h>
#include <ATen/ops/quantile.h>
#include <ATen/ops/quantile.h>
#include <ATen/ops/quantile.h>
#include <ATen/ops/quantile.h>
#include <ATen/ops/msort.h>
#include <ATen/ops/msort.h>
#include <ATen/ops/argsort.h>
#include <ATen/ops/argsort.h>
#include <ATen/ops/argsort.h>
#include <ATen/ops/topk.h>
#include <ATen/ops/topk.h>
#include <ATen/ops/unfold_backward.h>
#include <ATen/ops/normal.h>
#include <ATen/ops/normal.h>
#include <ATen/ops/normal.h>
#include <ATen/ops/normal.h>
#include <ATen/ops/normal.h>
#include <ATen/ops/normal.h>
#include <ATen/ops/normal.h>
#include <ATen/ops/normal.h>
#include <ATen/ops/normal.h>
#include <ATen/ops/normal.h>
#include <ATen/ops/alias.h>
#include <ATen/ops/_foreach_sub.h>
#include <ATen/ops/_foreach_sub.h>
#include <ATen/ops/_foreach_sub.h>
#include <ATen/ops/_foreach_sub.h>
#include <ATen/ops/_foreach_sub.h>
#include <ATen/ops/_foreach_sub.h>
#include <ATen/ops/_foreach_maximum.h>
#include <ATen/ops/_foreach_maximum.h>
#include <ATen/ops/_foreach_maximum.h>
#include <ATen/ops/_foreach_maximum.h>
#include <ATen/ops/_foreach_maximum.h>
#include <ATen/ops/_foreach_maximum.h>
#include <ATen/ops/_foreach_acos.h>
#include <ATen/ops/_foreach_acos.h>
#include <ATen/ops/_foreach_atan.h>
#include <ATen/ops/_foreach_atan.h>
#include <ATen/ops/_foreach_ceil.h>
#include <ATen/ops/_foreach_ceil.h>
#include <ATen/ops/_foreach_erf.h>
#include <ATen/ops/_foreach_erf.h>
#include <ATen/ops/_foreach_log2.h>
#include <ATen/ops/_foreach_log2.h>
#include <ATen/ops/bucketize.h>
#include <ATen/ops/bucketize.h>
#include <ATen/ops/bucketize.h>
#include <ATen/ops/mse_loss.h>
#include <ATen/ops/mse_loss.h>
#include <ATen/ops/l1_loss.h>
#include <ATen/ops/nll_loss_nd.h>
#include <ATen/ops/nll_loss2d.h>
#include <ATen/ops/nll_loss2d.h>
#include <ATen/ops/nll_loss2d_forward.h>
#include <ATen/ops/nll_loss2d_forward.h>
#include <ATen/ops/nll_loss2d_backward.h>
#include <ATen/ops/nll_loss2d_backward.h>
#include <ATen/ops/soft_margin_loss.h>
#include <ATen/ops/soft_margin_loss.h>
#include <ATen/ops/glu.h>
#include <ATen/ops/glu.h>
#include <ATen/ops/glu_backward_jvp.h>
#include <ATen/ops/hardtanh.h>
#include <ATen/ops/hardtanh.h>
#include <ATen/ops/hardtanh.h>
#include <ATen/ops/hardswish_backward.h>
#include <ATen/ops/leaky_relu.h>
#include <ATen/ops/leaky_relu.h>
#include <ATen/ops/leaky_relu.h>
#include <ATen/ops/log_sigmoid_forward.h>
#include <ATen/ops/log_sigmoid_forward.h>
#include <ATen/ops/log_sigmoid_backward.h>
#include <ATen/ops/log_sigmoid_backward.h>
#include <ATen/ops/softshrink.h>
#include <ATen/ops/softshrink.h>
#include <ATen/ops/adaptive_avg_pool3d_backward.h>
#include <ATen/ops/_adaptive_avg_pool3d_backward.h>
#include <ATen/ops/adaptive_max_pool2d_backward.h>
#include <ATen/ops/adaptive_max_pool2d_backward.h>
#include <ATen/ops/adaptive_max_pool3d_backward.h>
#include <ATen/ops/adaptive_max_pool3d_backward.h>
#include <ATen/ops/fractional_max_pool3d.h>
#include <ATen/ops/fractional_max_pool3d.h>
#include <ATen/ops/reflection_pad3d.h>
#include <ATen/ops/reflection_pad3d.h>
#include <ATen/ops/replication_pad1d.h>
#include <ATen/ops/replication_pad1d.h>
#include <ATen/ops/replication_pad2d.h>
#include <ATen/ops/replication_pad2d.h>
#include <ATen/ops/_pad_circular.h>
#include <ATen/ops/pad.h>
#include <ATen/ops/upsample_nearest1d.h>
#include <ATen/ops/_upsample_nearest_exact1d.h>
#include <ATen/ops/upsample_nearest1d.h>
#include <ATen/ops/_upsample_nearest_exact1d.h>
#include <ATen/ops/upsample_nearest1d.h>
#include <ATen/ops/_upsample_nearest_exact1d.h>
#include <ATen/ops/_conv_depthwise2d.h>
#include <ATen/ops/_conv_depthwise2d.h>
#include <ATen/ops/slow_conv3d.h>
#include <ATen/ops/slow_conv3d.h>
#include <ATen/ops/_remove_batch_dim.h>
#include <ATen/ops/special_log_ndtr.h>
#include <ATen/ops/special_log_ndtr.h>
#include <ATen/ops/special_erf.h>
#include <ATen/ops/special_erf.h>
#include <ATen/ops/special_xlogy.h>
#include <ATen/ops/special_xlogy.h>
#include <ATen/ops/special_xlogy.h>
#include <ATen/ops/special_xlogy.h>
#include <ATen/ops/special_xlogy.h>
#include <ATen/ops/special_xlogy.h>
#include <ATen/ops/special_expit.h>
#include <ATen/ops/special_expit.h>
#include <ATen/ops/special_sinc.h>
#include <ATen/ops/special_sinc.h>
#include <ATen/ops/special_softmax.h>
#include <ATen/ops/fft_fft.h>
#include <ATen/ops/fft_fft.h>
#include <ATen/ops/fft_rfft.h>
#include <ATen/ops/fft_rfft.h>
#include <ATen/ops/fft_hfft2.h>
#include <ATen/ops/fft_hfft2.h>
#include <ATen/ops/fft_ifftn.h>
#include <ATen/ops/fft_ifftn.h>
#include <ATen/ops/fft_ihfftn.h>
#include <ATen/ops/fft_ihfftn.h>
#include <ATen/ops/fft_fftfreq.h>
#include <ATen/ops/fft_fftfreq.h>
#include <ATen/ops/fft_rfftfreq.h>
#include <ATen/ops/fft_rfftfreq.h>
#include <ATen/ops/linalg_cholesky_ex.h>
#include <ATen/ops/linalg_cholesky_ex.h>
#include <ATen/ops/linalg_cross.h>
#include <ATen/ops/linalg_cross.h>
#include <ATen/ops/linalg_lu_factor_ex.h>
#include <ATen/ops/linalg_lu_factor_ex.h>
#include <ATen/ops/det.h>
#include <ATen/ops/inverse.h>
#include <ATen/ops/inverse.h>
#include <ATen/ops/linalg_cond.h>
#include <ATen/ops/linalg_cond.h>
#include <ATen/ops/linalg_cond.h>
#include <ATen/ops/linalg_cond.h>
#include <ATen/ops/linalg_pinv.h>
#include <ATen/ops/linalg_pinv.h>
#include <ATen/ops/linalg_pinv.h>
#include <ATen/ops/linalg_pinv.h>
#include <ATen/ops/linalg_pinv.h>
#include <ATen/ops/linalg_pinv.h>
#include <ATen/ops/linalg_pinv.h>
#include <ATen/ops/linalg_pinv.h>
#include <ATen/ops/linalg_solve_ex.h>
#include <ATen/ops/linalg_solve_ex.h>
#include <ATen/ops/linalg_tensorsolve.h>
#include <ATen/ops/linalg_tensorsolve.h>
#include <ATen/ops/linalg_multi_dot.h>
#include <ATen/ops/linalg_multi_dot.h>
#include <ATen/ops/_test_string_default.h>
#include <ATen/ops/flatten_dense_tensors.h>
#include <ATen/ops/_conj_copy.h>
#include <ATen/ops/detach_copy.h>
#include <ATen/ops/row_indices_copy.h>
#include <ATen/ops/_transformer_encoder_layer_fwd.h>
#include <ATen/ops/_native_multi_head_attention.h>
#include <ATen/ops/_fused_sdp_choice.h>
#include <ATen/ops/_scaled_dot_product_flash_attention.h>
#include <ATen/ops/_scaled_dot_product_flash_attention_for_cpu.h>
#include <ATen/ops/_scaled_dot_product_efficient_attention_backward.h>
#include <ATen/ops/_flash_attention_backward.h>
#include <ATen/ops/_efficient_attention_backward.h>
#include <ATen/ops/_fill_mem_eff_dropout_mask.h>
#include <ATen/ops/_triton_multi_head_attention.h>
#include <ATen/ops/special_airy_ai.h>
#include <ATen/ops/special_airy_ai.h>
#include <ATen/ops/special_chebyshev_polynomial_w.h>
#include <ATen/ops/special_chebyshev_polynomial_w.h>
#include <ATen/ops/special_chebyshev_polynomial_w.h>
#include <ATen/ops/special_chebyshev_polynomial_w.h>
#include <ATen/ops/special_chebyshev_polynomial_w.h>
#include <ATen/ops/special_chebyshev_polynomial_w.h>
#include <ATen/ops/special_hermite_polynomial_h.h>
#include <ATen/ops/special_hermite_polynomial_h.h>
#include <ATen/ops/special_hermite_polynomial_h.h>
#include <ATen/ops/special_hermite_polynomial_h.h>
#include <ATen/ops/special_hermite_polynomial_h.h>
#include <ATen/ops/special_hermite_polynomial_h.h>
#include <ATen/ops/special_modified_bessel_i0.h>
#include <ATen/ops/special_modified_bessel_i0.h>
#include <ATen/ops/special_modified_bessel_k0.h>
#include <ATen/ops/special_modified_bessel_k0.h>
#include <ATen/ops/special_modified_bessel_k1.h>
#include <ATen/ops/special_modified_bessel_k1.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t.h>
#include <ATen/ops/_fused_adamw.h>
#include <ATen/ops/_fused_adamw.h>
#include <ATen/ops/_cudnn_rnn_flatten_weight.h>
#include <ATen/ops/quantized_batch_norm.h>
#include <ATen/ops/conv_tbc.h>
#include <ATen/ops/cudnn_affine_grid_generator_backward.h>
#include <ATen/ops/cudnn_grid_sampler.h>
#include <ATen/ops/div.h>
#include <ATen/ops/div.h>
#include <ATen/ops/_embedding_bag_forward_only.h>
#include <ATen/ops/new_zeros.h>
#include <ATen/ops/_grid_sampler_2d_cpu_fallback.h>
#include <ATen/ops/grid_sampler_3d.h>
#include <ATen/ops/hann_window.h>
#include <ATen/ops/hann_window.h>
#include <ATen/ops/hamming_window.h>
#include <ATen/ops/hamming_window.h>
#include <ATen/ops/hamming_window.h>
#include <ATen/ops/hamming_window.h>
#include <ATen/ops/native_group_norm_backward.h>
#include <ATen/ops/isnan.h>
#include <ATen/ops/native_layer_norm.h>
#include <ATen/ops/mkldnn_max_pool2d.h>
#include <ATen/ops/quantized_max_pool2d.h>
#include <ATen/ops/_mps_convolution.h>
#include <ATen/ops/mkldnn_rnn_layer_backward.h>
#include <ATen/ops/miopen_depthwise_convolution.h>
#include <ATen/ops/batch_norm_stats.h>
#include <ATen/ops/batch_norm_gather_stats.h>
#include <ATen/ops/native_batch_norm_backward.h>
#include <ATen/ops/batch_norm_backward_reduce.h>
#include <ATen/ops/_nnpack_spatial_convolution.h>
#include <ATen/ops/ones.h>
#include <ATen/ops/_cdist_forward.h>
#include <ATen/ops/rand_like.h>
#include <ATen/ops/randint_like.h>
#include <ATen/ops/randint_like.h>
#include <ATen/ops/select_backward.h>
#include <ATen/ops/slice_scatter.h>
#include <ATen/ops/_mkldnn_transpose.h>
#include <ATen/ops/_nested_from_padded_and_nested_example.h>
#include <ATen/ops/unique_dim.h>
#include <ATen/ops/unique_consecutive.h>
#include <ATen/ops/_dirichlet_grad.h>
#include <ATen/ops/clone.h>
#include <ATen/ops/resize_as.h>
#include <ATen/ops/resize_as.h>
#include <ATen/ops/resize_as_sparse.h>
#include <ATen/ops/resize_as_sparse.h>
#include <ATen/ops/_to_sparse_csc.h>
#include <ATen/ops/_to_sparse_bsc.h>
#include <ATen/ops/mkldnn_reorder_conv2d_weight.h>
#include <ATen/ops/quantize_per_channel.h>
#include <ATen/ops/dequantize.h>
#include <ATen/ops/dequantize.h>
#include <ATen/ops/q_per_channel_zero_points.h>
#include <ATen/ops/_fake_quantize_learnable_per_channel_affine.h>
#include <ATen/ops/_lstm_mps.h>
#include <ATen/ops/_thnn_fused_lstm_cell.h>
#include <ATen/ops/lift_fresh_copy.h>
#include <ATen/ops/index_fill.h>
#include <ATen/ops/index_fill.h>
#include <ATen/ops/random.h>
#include <ATen/ops/random.h>
#include <ATen/ops/random.h>
#include <ATen/ops/random.h>
#include <ATen/ops/random.h>
#include <ATen/ops/random.h>
#include <ATen/ops/cauchy.h>
#include <ATen/ops/cauchy.h>
#include <ATen/ops/log_normal.h>
#include <ATen/ops/log_normal.h>
#include <ATen/ops/_histogramdd_bin_edges.h>
#include <ATen/ops/_histogramdd_from_bin_tensors.h>
#include <ATen/ops/argsort.h>
#include <ATen/ops/unfold_backward.h>
#include <ATen/ops/normal.h>
#include <ATen/ops/_foreach_sub.h>
#include <ATen/ops/_foreach_sub.h>
#include <ATen/ops/_foreach_sub.h>
#include <ATen/ops/_foreach_maximum.h>
#include <ATen/ops/_foreach_maximum.h>
#include <ATen/ops/_foreach_maximum.h>
#include <ATen/ops/_foreach_acos.h>
#include <ATen/ops/_foreach_atan.h>
#include <ATen/ops/_foreach_ceil.h>
#include <ATen/ops/_foreach_erf.h>
#include <ATen/ops/_foreach_log2.h>
#include <ATen/ops/bucketize.h>
#include <ATen/ops/glu_backward_jvp.h>
#include <ATen/ops/hardswish_backward.h>
#include <ATen/ops/_adaptive_avg_pool3d_backward.h>
#include <ATen/ops/_conj_copy.h>
#include <ATen/ops/detach_copy.h>
#include <ATen/ops/row_indices_copy.h>
#include <ATen/ops/_transformer_encoder_layer_fwd.h>
#include <ATen/ops/_native_multi_head_attention.h>
#include <ATen/ops/_triton_multi_head_attention.h>
#include <ATen/ops/_fused_adamw.h>
#include <ATen/ops/_fused_adamw.h>
#include <ATen/ops/_fused_adamw.h>
#include <ATen/ops/_fused_adamw.h>
#endif



namespace at { namespace _ops {


STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Double, name, "aten::_cast_Double")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Double, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Double, schema_str, "_cast_Double(Tensor self, bool non_blocking=False) -> Tensor")

// aten::_cast_Double(Tensor self, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cast_Double::schema> create__cast_Double_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cast_Double::name, _cast_Double::overload_name)
      .typed<_cast_Double::schema>();
}

// aten::_cast_Double(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Double::call(const at::Tensor & self, bool non_blocking) {
    
    static auto op = create__cast_Double_typed_handle();
    return op.call(self, non_blocking);
}

// aten::_cast_Double(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Double::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
    
    static auto op = create__cast_Double_typed_handle();
    return op.redispatch(dispatchKeySet, self, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Float, name, "aten::_cast_Float")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Float, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Float, schema_str, "_cast_Float(Tensor self, bool non_blocking=False) -> Tensor")

// aten::_cast_Float(Tensor self, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cast_Float::schema> create__cast_Float_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cast_Float::name, _cast_Float::overload_name)
      .typed<_cast_Float::schema>();
}

// aten::_cast_Float(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Float::call(const at::Tensor & self, bool non_blocking) {
    
    static auto op = create__cast_Float_typed_handle();
    return op.call(self, non_blocking);
}

// aten::_cast_Float(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Float::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
    
    static auto op = create__cast_Float_typed_handle();
    return op.redispatch(dispatchKeySet, self, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Half, name, "aten::_cast_Half")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Half, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Half, schema_str, "_cast_Half(Tensor self, bool non_blocking=False) -> Tensor")

// aten::_cast_Half(Tensor self, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cast_Half::schema> create__cast_Half_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cast_Half::name, _cast_Half::overload_name)
      .typed<_cast_Half::schema>();
}

// aten::_cast_Half(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Half::call(const at::Tensor & self, bool non_blocking) {
    
    static auto op = create__cast_Half_typed_handle();
    return op.call(self, non_blocking);
}

// aten::_cast_Half(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Half::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
    
    static auto op = create__cast_Half_typed_handle();
    return op.redispatch(dispatchKeySet, self, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_version, name, "aten::_version")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_version, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_version, schema_str, "_version(Tensor self) -> int")

// aten::_version(Tensor self) -> int
static C10_NOINLINE c10::TypedOperatorHandle<_version::schema> create__version_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_version::name, _version::overload_name)
      .typed<_version::schema>();
}

// aten::_version(Tensor self) -> int
int64_t _version::call(const at::Tensor & self) {
    
    static auto op = create__version_typed_handle();
    return op.call(self);
}

// aten::_version(Tensor self) -> int
int64_t _version::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create__version_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_make_dual, name, "aten::_make_dual")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_make_dual, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_make_dual, schema_str, "_make_dual(Tensor(a) primal, Tensor tangent, int level) -> Tensor(a)")

// aten::_make_dual(Tensor(a) primal, Tensor tangent, int level) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<_make_dual::schema> create__make_dual_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_make_dual::name, _make_dual::overload_name)
      .typed<_make_dual::schema>();
}

// aten::_make_dual(Tensor(a) primal, Tensor tangent, int level) -> Tensor(a)
at::Tensor _make_dual::call(const at::Tensor & primal, const at::Tensor & tangent, int64_t level) {
    
    static auto op = create__make_dual_typed_handle();
    return op.call(primal, tangent, level);
}

// aten::_make_dual(Tensor(a) primal, Tensor tangent, int level) -> Tensor(a)
at::Tensor _make_dual::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & primal, const at::Tensor & tangent, int64_t level) {
    
    static auto op = create__make_dual_typed_handle();
    return op.redispatch(dispatchKeySet, primal, tangent, level);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_as, name, "aten::align_as")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_as, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_as, schema_str, "align_as(Tensor self, Tensor other) -> Tensor")

// aten::align_as(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<align_as::schema> create_align_as_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(align_as::name, align_as::overload_name)
      .typed<align_as::schema>();
}

// aten::align_as(Tensor self, Tensor other) -> Tensor
at::Tensor align_as::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_align_as_typed_handle();
    return op.call(self, other);
}

// aten::align_as(Tensor self, Tensor other) -> Tensor
at::Tensor align_as::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_align_as_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_assert_tensor_metadata, name, "aten::_assert_tensor_metadata")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_assert_tensor_metadata, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_assert_tensor_metadata, schema_str, "_assert_tensor_metadata(Tensor a, SymInt[]? size=None, SymInt[]? stride=None, ScalarType? dtype=None) -> ()")

// aten::_assert_tensor_metadata(Tensor a, SymInt[]? size=None, SymInt[]? stride=None, ScalarType? dtype=None) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_assert_tensor_metadata::schema> create__assert_tensor_metadata_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_assert_tensor_metadata::name, _assert_tensor_metadata::overload_name)
      .typed<_assert_tensor_metadata::schema>();
}

// aten::_assert_tensor_metadata(Tensor a, SymInt[]? size=None, SymInt[]? stride=None, ScalarType? dtype=None) -> ()
void _assert_tensor_metadata::call(const at::Tensor & a, at::OptionalSymIntArrayRef size, at::OptionalSymIntArrayRef stride, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create__assert_tensor_metadata_typed_handle();
    return op.call(a, size, stride, dtype);
}

// aten::_assert_tensor_metadata(Tensor a, SymInt[]? size=None, SymInt[]? stride=None, ScalarType? dtype=None) -> ()
void _assert_tensor_metadata::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & a, at::OptionalSymIntArrayRef size, at::OptionalSymIntArrayRef stride, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create__assert_tensor_metadata_typed_handle();
    return op.redispatch(dispatchKeySet, a, size, stride, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sym_constrain_range_for_size, name, "aten::sym_constrain_range_for_size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sym_constrain_range_for_size, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sym_constrain_range_for_size, schema_str, "sym_constrain_range_for_size(Scalar size, *, int? min=None, int? max=None) -> ()")

// aten::sym_constrain_range_for_size(Scalar size, *, int? min=None, int? max=None) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<sym_constrain_range_for_size::schema> create_sym_constrain_range_for_size_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sym_constrain_range_for_size::name, sym_constrain_range_for_size::overload_name)
      .typed<sym_constrain_range_for_size::schema>();
}

// aten::sym_constrain_range_for_size(Scalar size, *, int? min=None, int? max=None) -> ()
void sym_constrain_range_for_size::call(const at::Scalar & size, c10::optional<int64_t> min, c10::optional<int64_t> max) {
    
    static auto op = create_sym_constrain_range_for_size_typed_handle();
    return op.call(size, min, max);
}

// aten::sym_constrain_range_for_size(Scalar size, *, int? min=None, int? max=None) -> ()
void sym_constrain_range_for_size::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & size, c10::optional<int64_t> min, c10::optional<int64_t> max) {
    
    static auto op = create_sym_constrain_range_for_size_typed_handle();
    return op.redispatch(dispatchKeySet, size, min, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(refine_names, name, "aten::refine_names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(refine_names, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(refine_names, schema_str, "refine_names(Tensor(a) self, Dimname[] names) -> Tensor(a)")

// aten::refine_names(Tensor(a) self, Dimname[] names) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<refine_names::schema> create_refine_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(refine_names::name, refine_names::overload_name)
      .typed<refine_names::schema>();
}

// aten::refine_names(Tensor(a) self, Dimname[] names) -> Tensor(a)
at::Tensor refine_names::call(const at::Tensor & self, at::DimnameList names) {
    
    static auto op = create_refine_names_typed_handle();
    return op.call(self, names);
}

// aten::refine_names(Tensor(a) self, Dimname[] names) -> Tensor(a)
at::Tensor refine_names::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList names) {
    
    static auto op = create_refine_names_typed_handle();
    return op.redispatch(dispatchKeySet, self, names);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_use_cudnn_rnn_flatten_weight, name, "aten::_use_cudnn_rnn_flatten_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_use_cudnn_rnn_flatten_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_use_cudnn_rnn_flatten_weight, schema_str, "_use_cudnn_rnn_flatten_weight() -> bool")

// aten::_use_cudnn_rnn_flatten_weight() -> bool
static C10_NOINLINE c10::TypedOperatorHandle<_use_cudnn_rnn_flatten_weight::schema> create__use_cudnn_rnn_flatten_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_use_cudnn_rnn_flatten_weight::name, _use_cudnn_rnn_flatten_weight::overload_name)
      .typed<_use_cudnn_rnn_flatten_weight::schema>();
}

// aten::_use_cudnn_rnn_flatten_weight() -> bool
bool _use_cudnn_rnn_flatten_weight::call() {
    
    static auto op = create__use_cudnn_rnn_flatten_weight_typed_handle();
    return op.call();
}

// aten::_use_cudnn_rnn_flatten_weight() -> bool
bool _use_cudnn_rnn_flatten_weight::redispatch(c10::DispatchKeySet dispatchKeySet) {
    
    static auto op = create__use_cudnn_rnn_flatten_weight_typed_handle();
    return op.redispatch(dispatchKeySet);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_flatten_weight, name, "aten::_cudnn_rnn_flatten_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_flatten_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_flatten_weight, schema_str, "_cudnn_rnn_flatten_weight(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional) -> Tensor")

// aten::_cudnn_rnn_flatten_weight(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cudnn_rnn_flatten_weight::schema> create__cudnn_rnn_flatten_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cudnn_rnn_flatten_weight::name, _cudnn_rnn_flatten_weight::overload_name)
      .typed<_cudnn_rnn_flatten_weight::schema>();
}

// aten::_cudnn_rnn_flatten_weight(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional) -> Tensor
at::Tensor _cudnn_rnn_flatten_weight::call(at::TensorList weight_arr, int64_t weight_stride0, c10::SymInt input_size, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, bool batch_first, bool bidirectional) {
    
    static auto op = create__cudnn_rnn_flatten_weight_typed_handle();
    return op.call(weight_arr, weight_stride0, input_size, mode, hidden_size, proj_size, num_layers, batch_first, bidirectional);
}

// aten::_cudnn_rnn_flatten_weight(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional) -> Tensor
at::Tensor _cudnn_rnn_flatten_weight::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList weight_arr, int64_t weight_stride0, c10::SymInt input_size, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, bool batch_first, bool bidirectional) {
    
    static auto op = create__cudnn_rnn_flatten_weight_typed_handle();
    return op.redispatch(dispatchKeySet, weight_arr, weight_stride0, input_size, mode, hidden_size, proj_size, num_layers, batch_first, bidirectional);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_ff_, name, "aten::_sobol_engine_ff_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_ff_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_ff_, schema_str, "_sobol_engine_ff_(Tensor(a!) self, int n, Tensor sobolstate, int dimension, int num_generated) -> Tensor(a!)")

// aten::_sobol_engine_ff_(Tensor(a!) self, int n, Tensor sobolstate, int dimension, int num_generated) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_sobol_engine_ff_::schema> create__sobol_engine_ff__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sobol_engine_ff_::name, _sobol_engine_ff_::overload_name)
      .typed<_sobol_engine_ff_::schema>();
}

// aten::_sobol_engine_ff_(Tensor(a!) self, int n, Tensor sobolstate, int dimension, int num_generated) -> Tensor(a!)
at::Tensor & _sobol_engine_ff_::call(at::Tensor & self, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated) {
    
    static auto op = create__sobol_engine_ff__typed_handle();
    return op.call(self, n, sobolstate, dimension, num_generated);
}

// aten::_sobol_engine_ff_(Tensor(a!) self, int n, Tensor sobolstate, int dimension, int num_generated) -> Tensor(a!)
at::Tensor & _sobol_engine_ff_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated) {
    
    static auto op = create__sobol_engine_ff__typed_handle();
    return op.redispatch(dispatchKeySet, self, n, sobolstate, dimension, num_generated);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_scramble_, name, "aten::_sobol_engine_scramble_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_scramble_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_scramble_, schema_str, "_sobol_engine_scramble_(Tensor(a!) self, Tensor ltm, int dimension) -> Tensor(a!)")

// aten::_sobol_engine_scramble_(Tensor(a!) self, Tensor ltm, int dimension) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_sobol_engine_scramble_::schema> create__sobol_engine_scramble__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sobol_engine_scramble_::name, _sobol_engine_scramble_::overload_name)
      .typed<_sobol_engine_scramble_::schema>();
}

// aten::_sobol_engine_scramble_(Tensor(a!) self, Tensor ltm, int dimension) -> Tensor(a!)
at::Tensor & _sobol_engine_scramble_::call(at::Tensor & self, const at::Tensor & ltm, int64_t dimension) {
    
    static auto op = create__sobol_engine_scramble__typed_handle();
    return op.call(self, ltm, dimension);
}

// aten::_sobol_engine_scramble_(Tensor(a!) self, Tensor ltm, int dimension) -> Tensor(a!)
at::Tensor & _sobol_engine_scramble_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & ltm, int64_t dimension) {
    
    static auto op = create__sobol_engine_scramble__typed_handle();
    return op.redispatch(dispatchKeySet, self, ltm, dimension);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_alpha_dropout, name, "aten::feature_alpha_dropout")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_alpha_dropout, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_alpha_dropout, schema_str, "feature_alpha_dropout(Tensor input, float p, bool train) -> Tensor")

// aten::feature_alpha_dropout(Tensor input, float p, bool train) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<feature_alpha_dropout::schema> create_feature_alpha_dropout_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(feature_alpha_dropout::name, feature_alpha_dropout::overload_name)
      .typed<feature_alpha_dropout::schema>();
}

// aten::feature_alpha_dropout(Tensor input, float p, bool train) -> Tensor
at::Tensor feature_alpha_dropout::call(const at::Tensor & input, double p, bool train) {
    
    static auto op = create_feature_alpha_dropout_typed_handle();
    return op.call(input, p, train);
}

// aten::feature_alpha_dropout(Tensor input, float p, bool train) -> Tensor
at::Tensor feature_alpha_dropout::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double p, bool train) {
    
    static auto op = create_feature_alpha_dropout_typed_handle();
    return op.redispatch(dispatchKeySet, input, p, train);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_alpha_dropout_, name, "aten::feature_alpha_dropout_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_alpha_dropout_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_alpha_dropout_, schema_str, "feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)")

// aten::feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<feature_alpha_dropout_::schema> create_feature_alpha_dropout__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(feature_alpha_dropout_::name, feature_alpha_dropout_::overload_name)
      .typed<feature_alpha_dropout_::schema>();
}

// aten::feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
at::Tensor & feature_alpha_dropout_::call(at::Tensor & self, double p, bool train) {
    
    static auto op = create_feature_alpha_dropout__typed_handle();
    return op.call(self, p, train);
}

// aten::feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
at::Tensor & feature_alpha_dropout_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, bool train) {
    
    static auto op = create_feature_alpha_dropout__typed_handle();
    return op.redispatch(dispatchKeySet, self, p, train);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs, name, "aten::abs")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs, schema_str, "abs(Tensor self) -> Tensor")

// aten::abs(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<abs::schema> create_abs_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(abs::name, abs::overload_name)
      .typed<abs::schema>();
}

// aten::abs(Tensor self) -> Tensor
at::Tensor abs::call(const at::Tensor & self) {
    
    static auto op = create_abs_typed_handle();
    return op.call(self);
}

// aten::abs(Tensor self) -> Tensor
at::Tensor abs::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_abs_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs_, name, "aten::abs_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs_, schema_str, "abs_(Tensor(a!) self) -> Tensor(a!)")

// aten::abs_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<abs_::schema> create_abs__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(abs_::name, abs_::overload_name)
      .typed<abs_::schema>();
}

// aten::abs_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & abs_::call(at::Tensor & self) {
    
    static auto op = create_abs__typed_handle();
    return op.call(self);
}

// aten::abs_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & abs_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_abs__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs_out, name, "aten::abs")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs_out, schema_str, "abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<abs_out::schema> create_abs_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(abs_out::name, abs_out::overload_name)
      .typed<abs_out::schema>();
}

// aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & abs_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_abs_out_typed_handle();
    return op.call(self, out);
}

// aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & abs_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_abs_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(imag, name, "aten::imag")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(imag, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(imag, schema_str, "imag(Tensor(a) self) -> Tensor(a)")

// aten::imag(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<imag::schema> create_imag_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(imag::name, imag::overload_name)
      .typed<imag::schema>();
}

// aten::imag(Tensor(a) self) -> Tensor(a)
at::Tensor imag::call(const at::Tensor & self) {
    
    static auto op = create_imag_typed_handle();
    return op.call(self);
}

// aten::imag(Tensor(a) self) -> Tensor(a)
at::Tensor imag::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_imag_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resolve_conj, name, "aten::resolve_conj")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resolve_conj, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resolve_conj, schema_str, "resolve_conj(Tensor(a) self) -> Tensor(a)")

// aten::resolve_conj(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<resolve_conj::schema> create_resolve_conj_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(resolve_conj::name, resolve_conj::overload_name)
      .typed<resolve_conj::schema>();
}

// aten::resolve_conj(Tensor(a) self) -> Tensor(a)
at::Tensor resolve_conj::call(const at::Tensor & self) {
    
    static auto op = create_resolve_conj_typed_handle();
    return op.call(self);
}

// aten::resolve_conj(Tensor(a) self) -> Tensor(a)
at::Tensor resolve_conj::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_resolve_conj_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resolve_neg, name, "aten::resolve_neg")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resolve_neg, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resolve_neg, schema_str, "resolve_neg(Tensor(a) self) -> Tensor(a)")

// aten::resolve_neg(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<resolve_neg::schema> create_resolve_neg_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(resolve_neg::name, resolve_neg::overload_name)
      .typed<resolve_neg::schema>();
}

// aten::resolve_neg(Tensor(a) self) -> Tensor(a)
at::Tensor resolve_neg::call(const at::Tensor & self) {
    
    static auto op = create_resolve_neg_typed_handle();
    return op.call(self);
}

// aten::resolve_neg(Tensor(a) self) -> Tensor(a)
at::Tensor resolve_neg::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_resolve_neg_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool1d, name, "aten::adaptive_max_pool1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool1d, schema_str, "adaptive_max_pool1d(Tensor self, int[1] output_size) -> (Tensor, Tensor)")

// aten::adaptive_max_pool1d(Tensor self, int[1] output_size) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool1d::schema> create_adaptive_max_pool1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool1d::name, adaptive_max_pool1d::overload_name)
      .typed<adaptive_max_pool1d::schema>();
}

// aten::adaptive_max_pool1d(Tensor self, int[1] output_size) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool1d::call(const at::Tensor & self, at::IntArrayRef output_size) {
    
    static auto op = create_adaptive_max_pool1d_typed_handle();
    return op.call(self, output_size);
}

// aten::adaptive_max_pool1d(Tensor self, int[1] output_size) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
    
    static auto op = create_adaptive_max_pool1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv, name, "aten::addmv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv, schema_str, "addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor")

// aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<addmv::schema> create_addmv_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addmv::name, addmv::overload_name)
      .typed<addmv::schema>();
}

// aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor addmv::call(const at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha) {
    
    static auto op = create_addmv_typed_handle();
    return op.call(self, mat, vec, beta, alpha);
}

// aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor addmv::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha) {
    
    static auto op = create_addmv_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat, vec, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv_, name, "aten::addmv_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv_, schema_str, "addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")

// aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addmv_::schema> create_addmv__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addmv_::name, addmv_::overload_name)
      .typed<addmv_::schema>();
}

// aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & addmv_::call(at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha) {
    
    static auto op = create_addmv__typed_handle();
    return op.call(self, mat, vec, beta, alpha);
}

// aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & addmv_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha) {
    
    static auto op = create_addmv__typed_handle();
    return op.redispatch(dispatchKeySet, self, mat, vec, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv_out, name, "aten::addmv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv_out, schema_str, "addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addmv_out::schema> create_addmv_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addmv_out::name, addmv_out::overload_name)
      .typed<addmv_out::schema>();
}

// aten::addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addmv_out::call(const at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create_addmv_out_typed_handle();
    return op.call(self, mat, vec, beta, alpha, out);
}

// aten::addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addmv_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create_addmv_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat, vec, beta, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr, name, "aten::addr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr, schema_str, "addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor")

// aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<addr::schema> create_addr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addr::name, addr::overload_name)
      .typed<addr::schema>();
}

// aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor addr::call(const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
    
    static auto op = create_addr_typed_handle();
    return op.call(self, vec1, vec2, beta, alpha);
}

// aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor addr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
    
    static auto op = create_addr_typed_handle();
    return op.redispatch(dispatchKeySet, self, vec1, vec2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr_, name, "aten::addr_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr_, schema_str, "addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")

// aten::addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addr_::schema> create_addr__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addr_::name, addr_::overload_name)
      .typed<addr_::schema>();
}

// aten::addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & addr_::call(at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
    
    static auto op = create_addr__typed_handle();
    return op.call(self, vec1, vec2, beta, alpha);
}

// aten::addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & addr_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
    
    static auto op = create_addr__typed_handle();
    return op.redispatch(dispatchKeySet, self, vec1, vec2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr_out, name, "aten::addr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr_out, schema_str, "addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addr_out::schema> create_addr_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addr_out::name, addr_out::overload_name)
      .typed<addr_out::schema>();
}

// aten::addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addr_out::call(const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create_addr_out_typed_handle();
    return op.call(self, vec1, vec2, beta, alpha, out);
}

// aten::addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addr_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create_addr_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, vec1, vec2, beta, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator_backward, name, "aten::affine_grid_generator_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator_backward, schema_str, "affine_grid_generator_backward(Tensor grad, SymInt[] size, bool align_corners) -> Tensor")

// aten::affine_grid_generator_backward(Tensor grad, SymInt[] size, bool align_corners) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<affine_grid_generator_backward::schema> create_affine_grid_generator_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(affine_grid_generator_backward::name, affine_grid_generator_backward::overload_name)
      .typed<affine_grid_generator_backward::schema>();
}

// aten::affine_grid_generator_backward(Tensor grad, SymInt[] size, bool align_corners) -> Tensor
at::Tensor affine_grid_generator_backward::call(const at::Tensor & grad, c10::SymIntArrayRef size, bool align_corners) {
    
    static auto op = create_affine_grid_generator_backward_typed_handle();
    return op.call(grad, size, align_corners);
}

// aten::affine_grid_generator_backward(Tensor grad, SymInt[] size, bool align_corners) -> Tensor
at::Tensor affine_grid_generator_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, c10::SymIntArrayRef size, bool align_corners) {
    
    static auto op = create_affine_grid_generator_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, size, align_corners);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmin, name, "aten::argmin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmin, schema_str, "argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor")

// aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<argmin::schema> create_argmin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(argmin::name, argmin::overload_name)
      .typed<argmin::schema>();
}

// aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
at::Tensor argmin::call(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
    
    static auto op = create_argmin_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
at::Tensor argmin::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
    
    static auto op = create_argmin_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmin_out, name, "aten::argmin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmin_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmin_out, schema_str, "argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<argmin_out::schema> create_argmin_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(argmin_out::name, argmin_out::overload_name)
      .typed<argmin_out::schema>();
}

// aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & argmin_out::call(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    
    static auto op = create_argmin_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & argmin_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    
    static auto op = create_argmin_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan, name, "aten::atan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan, schema_str, "atan(Tensor self) -> Tensor")

// aten::atan(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<atan::schema> create_atan_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atan::name, atan::overload_name)
      .typed<atan::schema>();
}

// aten::atan(Tensor self) -> Tensor
at::Tensor atan::call(const at::Tensor & self) {
    
    static auto op = create_atan_typed_handle();
    return op.call(self);
}

// aten::atan(Tensor self) -> Tensor
at::Tensor atan::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_atan_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan_, name, "aten::atan_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan_, schema_str, "atan_(Tensor(a!) self) -> Tensor(a!)")

// aten::atan_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<atan_::schema> create_atan__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atan_::name, atan_::overload_name)
      .typed<atan_::schema>();
}

// aten::atan_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & atan_::call(at::Tensor & self) {
    
    static auto op = create_atan__typed_handle();
    return op.call(self);
}

// aten::atan_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & atan_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_atan__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan_out, name, "aten::atan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan_out, schema_str, "atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<atan_out::schema> create_atan_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atan_out::name, atan_out::overload_name)
      .typed<atan_out::schema>();
}

// aten::atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & atan_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_atan_out_typed_handle();
    return op.call(self, out);
}

// aten::atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & atan_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_atan_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan, name, "aten::arctan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan, schema_str, "arctan(Tensor self) -> Tensor")

// aten::arctan(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arctan::schema> create_arctan_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arctan::name, arctan::overload_name)
      .typed<arctan::schema>();
}

// aten::arctan(Tensor self) -> Tensor
at::Tensor arctan::call(const at::Tensor & self) {
    
    static auto op = create_arctan_typed_handle();
    return op.call(self);
}

// aten::arctan(Tensor self) -> Tensor
at::Tensor arctan::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_arctan_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan_, name, "aten::arctan_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan_, schema_str, "arctan_(Tensor(a!) self) -> Tensor(a!)")

// aten::arctan_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arctan_::schema> create_arctan__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arctan_::name, arctan_::overload_name)
      .typed<arctan_::schema>();
}

// aten::arctan_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arctan_::call(at::Tensor & self) {
    
    static auto op = create_arctan__typed_handle();
    return op.call(self);
}

// aten::arctan_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arctan_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_arctan__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan_out, name, "aten::arctan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan_out, schema_str, "arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arctan_out::schema> create_arctan_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arctan_out::name, arctan_out::overload_name)
      .typed<arctan_out::schema>();
}

// aten::arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arctan_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_arctan_out_typed_handle();
    return op.call(self, out);
}

// aten::arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arctan_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_arctan_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_batch_norm, name, "aten::quantized_batch_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_batch_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_batch_norm, schema_str, "quantized_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor")

// aten::quantized_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantized_batch_norm::schema> create_quantized_batch_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantized_batch_norm::name, quantized_batch_norm::overload_name)
      .typed<quantized_batch_norm::schema>();
}

// aten::quantized_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor
at::Tensor quantized_batch_norm::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & var, double eps, double output_scale, int64_t output_zero_point) {
    
    static auto op = create_quantized_batch_norm_typed_handle();
    return op.call(input, weight, bias, mean, var, eps, output_scale, output_zero_point);
}

// aten::quantized_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor
at::Tensor quantized_batch_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & var, double eps, double output_scale, int64_t output_zero_point) {
    
    static auto op = create_quantized_batch_norm_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, mean, var, eps, output_scale, output_zero_point);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_backward, name, "aten::binary_cross_entropy_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_backward, schema_str, "binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor")

// aten::binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<binary_cross_entropy_backward::schema> create_binary_cross_entropy_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(binary_cross_entropy_backward::name, binary_cross_entropy_backward::overload_name)
      .typed<binary_cross_entropy_backward::schema>();
}

// aten::binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
at::Tensor binary_cross_entropy_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
    
    static auto op = create_binary_cross_entropy_backward_typed_handle();
    return op.call(grad_output, self, target, weight, reduction);
}

// aten::binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
at::Tensor binary_cross_entropy_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
    
    static auto op = create_binary_cross_entropy_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, weight, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_backward_grad_input, name, "aten::binary_cross_entropy_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_backward_grad_input, schema_str, "binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<binary_cross_entropy_backward_grad_input::schema> create_binary_cross_entropy_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(binary_cross_entropy_backward_grad_input::name, binary_cross_entropy_backward_grad_input::overload_name)
      .typed<binary_cross_entropy_backward_grad_input::schema>();
}

// aten::binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & binary_cross_entropy_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
    
    static auto op = create_binary_cross_entropy_backward_grad_input_typed_handle();
    return op.call(grad_output, self, target, weight, reduction, grad_input);
}

// aten::binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & binary_cross_entropy_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
    
    static auto op = create_binary_cross_entropy_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, weight, reduction, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not, name, "aten::bitwise_not")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not, schema_str, "bitwise_not(Tensor self) -> Tensor")

// aten::bitwise_not(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_not::schema> create_bitwise_not_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_not::name, bitwise_not::overload_name)
      .typed<bitwise_not::schema>();
}

// aten::bitwise_not(Tensor self) -> Tensor
at::Tensor bitwise_not::call(const at::Tensor & self) {
    
    static auto op = create_bitwise_not_typed_handle();
    return op.call(self);
}

// aten::bitwise_not(Tensor self) -> Tensor
at::Tensor bitwise_not::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_bitwise_not_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not_, name, "aten::bitwise_not_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not_, schema_str, "bitwise_not_(Tensor(a!) self) -> Tensor(a!)")

// aten::bitwise_not_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_not_::schema> create_bitwise_not__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_not_::name, bitwise_not_::overload_name)
      .typed<bitwise_not_::schema>();
}

// aten::bitwise_not_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & bitwise_not_::call(at::Tensor & self) {
    
    static auto op = create_bitwise_not__typed_handle();
    return op.call(self);
}

// aten::bitwise_not_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & bitwise_not_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_bitwise_not__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not_out, name, "aten::bitwise_not")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not_out, schema_str, "bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_not_out::schema> create_bitwise_not_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_not_out::name, bitwise_not_out::overload_name)
      .typed<bitwise_not_out::schema>();
}

// aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_not_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_bitwise_not_out_typed_handle();
    return op.call(self, out);
}

// aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_not_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_bitwise_not_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not, name, "aten::logical_not")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not, schema_str, "logical_not(Tensor self) -> Tensor")

// aten::logical_not(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logical_not::schema> create_logical_not_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_not::name, logical_not::overload_name)
      .typed<logical_not::schema>();
}

// aten::logical_not(Tensor self) -> Tensor
at::Tensor logical_not::call(const at::Tensor & self) {
    
    static auto op = create_logical_not_typed_handle();
    return op.call(self);
}

// aten::logical_not(Tensor self) -> Tensor
at::Tensor logical_not::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_logical_not_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not_, name, "aten::logical_not_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not_, schema_str, "logical_not_(Tensor(a!) self) -> Tensor(a!)")

// aten::logical_not_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logical_not_::schema> create_logical_not__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_not_::name, logical_not_::overload_name)
      .typed<logical_not_::schema>();
}

// aten::logical_not_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & logical_not_::call(at::Tensor & self) {
    
    static auto op = create_logical_not__typed_handle();
    return op.call(self);
}

// aten::logical_not_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & logical_not_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_logical_not__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not_out, name, "aten::logical_not")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not_out, schema_str, "logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logical_not_out::schema> create_logical_not_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_not_out::name, logical_not_out::overload_name)
      .typed<logical_not_out::schema>();
}

// aten::logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logical_not_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_logical_not_out_typed_handle();
    return op.call(self, out);
}

// aten::logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logical_not_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_logical_not_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concatenate, name, "aten::concatenate")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concatenate, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concatenate, schema_str, "concatenate(Tensor[] tensors, int dim=0) -> Tensor")

// aten::concatenate(Tensor[] tensors, int dim=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<concatenate::schema> create_concatenate_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(concatenate::name, concatenate::overload_name)
      .typed<concatenate::schema>();
}

// aten::concatenate(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor concatenate::call(at::TensorList tensors, int64_t dim) {
    
    static auto op = create_concatenate_typed_handle();
    return op.call(tensors, dim);
}

// aten::concatenate(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor concatenate::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim) {
    
    static auto op = create_concatenate_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concatenate_out, name, "aten::concatenate")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concatenate_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concatenate_out, schema_str, "concatenate.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::concatenate.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<concatenate_out::schema> create_concatenate_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(concatenate_out::name, concatenate_out::overload_name)
      .typed<concatenate_out::schema>();
}

// aten::concatenate.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & concatenate_out::call(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    
    static auto op = create_concatenate_out_typed_handle();
    return op.call(tensors, dim, out);
}

// aten::concatenate.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & concatenate_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
    
    static auto op = create_concatenate_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concatenate_names, name, "aten::concatenate")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concatenate_names, overload_name, "names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concatenate_names, schema_str, "concatenate.names(Tensor[] tensors, Dimname dim) -> Tensor")

// aten::concatenate.names(Tensor[] tensors, Dimname dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<concatenate_names::schema> create_concatenate_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(concatenate_names::name, concatenate_names::overload_name)
      .typed<concatenate_names::schema>();
}

// aten::concatenate.names(Tensor[] tensors, Dimname dim) -> Tensor
at::Tensor concatenate_names::call(at::TensorList tensors, at::Dimname dim) {
    
    static auto op = create_concatenate_names_typed_handle();
    return op.call(tensors, dim);
}

// aten::concatenate.names(Tensor[] tensors, Dimname dim) -> Tensor
at::Tensor concatenate_names::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Dimname dim) {
    
    static auto op = create_concatenate_names_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concatenate_names_out, name, "aten::concatenate")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concatenate_names_out, overload_name, "names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concatenate_names_out, schema_str, "concatenate.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)")

// aten::concatenate.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<concatenate_names_out::schema> create_concatenate_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(concatenate_names_out::name, concatenate_names_out::overload_name)
      .typed<concatenate_names_out::schema>();
}

// aten::concatenate.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & concatenate_names_out::call(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
    
    static auto op = create_concatenate_names_out_typed_handle();
    return op.call(tensors, dim, out);
}

// aten::concatenate.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & concatenate_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
    
    static auto op = create_concatenate_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil, name, "aten::ceil")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil, schema_str, "ceil(Tensor self) -> Tensor")

// aten::ceil(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ceil::schema> create_ceil_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ceil::name, ceil::overload_name)
      .typed<ceil::schema>();
}

// aten::ceil(Tensor self) -> Tensor
at::Tensor ceil::call(const at::Tensor & self) {
    
    static auto op = create_ceil_typed_handle();
    return op.call(self);
}

// aten::ceil(Tensor self) -> Tensor
at::Tensor ceil::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_ceil_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil_, name, "aten::ceil_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil_, schema_str, "ceil_(Tensor(a!) self) -> Tensor(a!)")

// aten::ceil_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ceil_::schema> create_ceil__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ceil_::name, ceil_::overload_name)
      .typed<ceil_::schema>();
}

// aten::ceil_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & ceil_::call(at::Tensor & self) {
    
    static auto op = create_ceil__typed_handle();
    return op.call(self);
}

// aten::ceil_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & ceil_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_ceil__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil_out, name, "aten::ceil")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil_out, schema_str, "ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ceil_out::schema> create_ceil_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ceil_out::name, ceil_out::overload_name)
      .typed<ceil_out::schema>();
}

// aten::ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ceil_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_ceil_out_typed_handle();
    return op.call(self, out);
}

// aten::ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ceil_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_ceil_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc, name, "aten::conv_tbc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc, schema_str, "conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor")

// aten::conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv_tbc::schema> create_conv_tbc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv_tbc::name, conv_tbc::overload_name)
      .typed<conv_tbc::schema>();
}

// aten::conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor
at::Tensor conv_tbc::call(const at::Tensor & self, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
    
    static auto op = create_conv_tbc_typed_handle();
    return op.call(self, weight, bias, pad);
}

// aten::conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor
at::Tensor conv_tbc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
    
    static auto op = create_conv_tbc_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, pad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh, name, "aten::cosh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh, schema_str, "cosh(Tensor self) -> Tensor")

// aten::cosh(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cosh::schema> create_cosh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cosh::name, cosh::overload_name)
      .typed<cosh::schema>();
}

// aten::cosh(Tensor self) -> Tensor
at::Tensor cosh::call(const at::Tensor & self) {
    
    static auto op = create_cosh_typed_handle();
    return op.call(self);
}

// aten::cosh(Tensor self) -> Tensor
at::Tensor cosh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_cosh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh_, name, "aten::cosh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh_, schema_str, "cosh_(Tensor(a!) self) -> Tensor(a!)")

// aten::cosh_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cosh_::schema> create_cosh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cosh_::name, cosh_::overload_name)
      .typed<cosh_::schema>();
}

// aten::cosh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & cosh_::call(at::Tensor & self) {
    
    static auto op = create_cosh__typed_handle();
    return op.call(self);
}

// aten::cosh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & cosh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_cosh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh_out, name, "aten::cosh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh_out, schema_str, "cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cosh_out::schema> create_cosh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cosh_out::name, cosh_out::overload_name)
      .typed<cosh_out::schema>();
}

// aten::cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cosh_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_cosh_out_typed_handle();
    return op.call(self, out);
}

// aten::cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cosh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_cosh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosine_embedding_loss, name, "aten::cosine_embedding_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosine_embedding_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosine_embedding_loss, schema_str, "cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor")

// aten::cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cosine_embedding_loss::schema> create_cosine_embedding_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cosine_embedding_loss::name, cosine_embedding_loss::overload_name)
      .typed<cosine_embedding_loss::schema>();
}

// aten::cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
at::Tensor cosine_embedding_loss::call(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
    
    static auto op = create_cosine_embedding_loss_typed_handle();
    return op.call(input1, input2, target, margin, reduction);
}

// aten::cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
at::Tensor cosine_embedding_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
    
    static auto op = create_cosine_embedding_loss_typed_handle();
    return op.redispatch(dispatchKeySet, input1, input2, target, margin, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_affine_grid_generator_backward, name, "aten::cudnn_affine_grid_generator_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_affine_grid_generator_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_affine_grid_generator_backward, schema_str, "cudnn_affine_grid_generator_backward(Tensor grad, int N, int C, int H, int W) -> Tensor grad_theta")

// aten::cudnn_affine_grid_generator_backward(Tensor grad, int N, int C, int H, int W) -> Tensor grad_theta
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_affine_grid_generator_backward::schema> create_cudnn_affine_grid_generator_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_affine_grid_generator_backward::name, cudnn_affine_grid_generator_backward::overload_name)
      .typed<cudnn_affine_grid_generator_backward::schema>();
}

// aten::cudnn_affine_grid_generator_backward(Tensor grad, int N, int C, int H, int W) -> Tensor grad_theta
at::Tensor cudnn_affine_grid_generator_backward::call(const at::Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) {
    
    static auto op = create_cudnn_affine_grid_generator_backward_typed_handle();
    return op.call(grad, N, C, H, W);
}

// aten::cudnn_affine_grid_generator_backward(Tensor grad, int N, int C, int H, int W) -> Tensor grad_theta
at::Tensor cudnn_affine_grid_generator_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) {
    
    static auto op = create_cudnn_affine_grid_generator_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, N, C, H, W);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_grid_sampler, name, "aten::cudnn_grid_sampler")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_grid_sampler, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_grid_sampler, schema_str, "cudnn_grid_sampler(Tensor self, Tensor grid) -> Tensor output")

// aten::cudnn_grid_sampler(Tensor self, Tensor grid) -> Tensor output
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_grid_sampler::schema> create_cudnn_grid_sampler_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_grid_sampler::name, cudnn_grid_sampler::overload_name)
      .typed<cudnn_grid_sampler::schema>();
}

// aten::cudnn_grid_sampler(Tensor self, Tensor grid) -> Tensor output
at::Tensor cudnn_grid_sampler::call(const at::Tensor & self, const at::Tensor & grid) {
    
    static auto op = create_cudnn_grid_sampler_typed_handle();
    return op.call(self, grid);
}

// aten::cudnn_grid_sampler(Tensor self, Tensor grid) -> Tensor output
at::Tensor cudnn_grid_sampler::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grid) {
    
    static auto op = create_cudnn_grid_sampler_typed_handle();
    return op.redispatch(dispatchKeySet, self, grid);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin, name, "aten::cummin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin, schema_str, "cummin(Tensor self, int dim) -> (Tensor values, Tensor indices)")

// aten::cummin(Tensor self, int dim) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<cummin::schema> create_cummin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cummin::name, cummin::overload_name)
      .typed<cummin::schema>();
}

// aten::cummin(Tensor self, int dim) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> cummin::call(const at::Tensor & self, int64_t dim) {
    
    static auto op = create_cummin_typed_handle();
    return op.call(self, dim);
}

// aten::cummin(Tensor self, int dim) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> cummin::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    
    static auto op = create_cummin_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_out, name, "aten::cummin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_out, schema_str, "cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<cummin_out::schema> create_cummin_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cummin_out::name, cummin_out::overload_name)
      .typed<cummin_out::schema>();
}

// aten::cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> cummin_out::call(const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
    
    static auto op = create_cummin_out_typed_handle();
    return op.call(self, dim, values, indices);
}

// aten::cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> cummin_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
    
    static auto op = create_cummin_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_dimname, name, "aten::cummin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_dimname, schema_str, "cummin.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)")

// aten::cummin.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<cummin_dimname::schema> create_cummin_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cummin_dimname::name, cummin_dimname::overload_name)
      .typed<cummin_dimname::schema>();
}

// aten::cummin.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> cummin_dimname::call(const at::Tensor & self, at::Dimname dim) {
    
    static auto op = create_cummin_dimname_typed_handle();
    return op.call(self, dim);
}

// aten::cummin.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> cummin_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
    
    static auto op = create_cummin_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_dimname_out, name, "aten::cummin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_dimname_out, schema_str, "cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<cummin_dimname_out::schema> create_cummin_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cummin_dimname_out::name, cummin_dimname_out::overload_name)
      .typed<cummin_dimname_out::schema>();
}

// aten::cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> cummin_dimname_out::call(const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
    
    static auto op = create_cummin_dimname_out_typed_handle();
    return op.call(self, dim, values, indices);
}

// aten::cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> cummin_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
    
    static auto op = create_cummin_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cummin_helper, name, "aten::_cummin_helper")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cummin_helper, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cummin_helper, schema_str, "_cummin_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()")

// aten::_cummin_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_cummin_helper::schema> create__cummin_helper_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cummin_helper::name, _cummin_helper::overload_name)
      .typed<_cummin_helper::schema>();
}

// aten::_cummin_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
void _cummin_helper::call(const at::Tensor & self, at::Tensor & values, at::Tensor & indices, int64_t dim) {
    
    static auto op = create__cummin_helper_typed_handle();
    return op.call(self, values, indices, dim);
}

// aten::_cummin_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
void _cummin_helper::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & values, at::Tensor & indices, int64_t dim) {
    
    static auto op = create__cummin_helper_typed_handle();
    return op.redispatch(dispatchKeySet, self, values, indices, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Tensor, name, "aten::div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Tensor, schema_str, "div.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::div.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<div_Tensor::schema> create_div_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div_Tensor::name, div_Tensor::overload_name)
      .typed<div_Tensor::schema>();
}

// aten::div.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor div_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_div_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::div.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor div_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_div_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Tensor, name, "aten::div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Tensor, schema_str, "div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<div__Tensor::schema> create_div__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div__Tensor::name, div__Tensor::overload_name)
      .typed<div__Tensor::schema>();
}

// aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & div__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_div__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & div__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_div__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_out, name, "aten::div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_out, schema_str, "div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<div_out::schema> create_div_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div_out::name, div_out::overload_name)
      .typed<div_out::schema>();
}

// aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & div_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_div_out_typed_handle();
    return op.call(self, other, out);
}

// aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & div_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_div_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Tensor_mode, name, "aten::div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Tensor_mode, overload_name, "Tensor_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Tensor_mode, schema_str, "div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor")

// aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<div_Tensor_mode::schema> create_div_Tensor_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div_Tensor_mode::name, div_Tensor_mode::overload_name)
      .typed<div_Tensor_mode::schema>();
}

// aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
at::Tensor div_Tensor_mode::call(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_div_Tensor_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
at::Tensor div_Tensor_mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_div_Tensor_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Tensor_mode, name, "aten::div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Tensor_mode, overload_name, "Tensor_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Tensor_mode, schema_str, "div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)")

// aten::div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<div__Tensor_mode::schema> create_div__Tensor_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div__Tensor_mode::name, div__Tensor_mode::overload_name)
      .typed<div__Tensor_mode::schema>();
}

// aten::div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & div__Tensor_mode::call(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_div__Tensor_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & div__Tensor_mode::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_div__Tensor_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_out_mode, name, "aten::div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_out_mode, overload_name, "out_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_out_mode, schema_str, "div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)")

// aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<div_out_mode::schema> create_div_out_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div_out_mode::name, div_out_mode::overload_name)
      .typed<div_out_mode::schema>();
}

// aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
at::Tensor & div_out_mode::call(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    
    static auto op = create_div_out_mode_typed_handle();
    return op.call(self, other, rounding_mode, out);
}

// aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
at::Tensor & div_out_mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    
    static auto op = create_div_out_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar, name, "aten::div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar, schema_str, "div.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::div.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<div_Scalar::schema> create_div_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div_Scalar::name, div_Scalar::overload_name)
      .typed<div_Scalar::schema>();
}

// aten::div.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor div_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_div_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::div.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor div_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_div_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Scalar, name, "aten::div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Scalar, schema_str, "div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<div__Scalar::schema> create_div__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div__Scalar::name, div__Scalar::overload_name)
      .typed<div__Scalar::schema>();
}

// aten::div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & div__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_div__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & div__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_div__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar_mode, name, "aten::div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar_mode, overload_name, "Scalar_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar_mode, schema_str, "div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor")

// aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<div_Scalar_mode::schema> create_div_Scalar_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div_Scalar_mode::name, div_Scalar_mode::overload_name)
      .typed<div_Scalar_mode::schema>();
}

// aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
at::Tensor div_Scalar_mode::call(const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_div_Scalar_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
at::Tensor div_Scalar_mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_div_Scalar_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Scalar_mode, name, "aten::div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Scalar_mode, overload_name, "Scalar_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Scalar_mode, schema_str, "div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)")

// aten::div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<div__Scalar_mode::schema> create_div__Scalar_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div__Scalar_mode::name, div__Scalar_mode::overload_name)
      .typed<div__Scalar_mode::schema>();
}

// aten::div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & div__Scalar_mode::call(at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_div__Scalar_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & div__Scalar_mode::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    
    static auto op = create_div__Scalar_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_forward_only, name, "aten::_embedding_bag_forward_only")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_forward_only, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_forward_only, schema_str, "_embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)")

// aten::_embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_embedding_bag_forward_only::schema> create__embedding_bag_forward_only_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_embedding_bag_forward_only::name, _embedding_bag_forward_only::overload_name)
      .typed<_embedding_bag_forward_only::schema>();
}

// aten::_embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _embedding_bag_forward_only::call(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx) {
    
    static auto op = create__embedding_bag_forward_only_typed_handle();
    return op.call(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

// aten::_embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _embedding_bag_forward_only::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx) {
    
    static auto op = create__embedding_bag_forward_only_typed_handle();
    return op.redispatch(dispatchKeySet, weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_bag, name, "aten::embedding_bag")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_bag, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_bag, schema_str, "embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -> (Tensor, Tensor, Tensor, Tensor)")

// aten::embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -> (Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<embedding_bag::schema> create_embedding_bag_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(embedding_bag::name, embedding_bag::overload_name)
      .typed<embedding_bag::schema>();
}

// aten::embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> embedding_bag::call(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset) {
    
    static auto op = create_embedding_bag_typed_handle();
    return op.call(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset);
}

// aten::embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> embedding_bag::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset) {
    
    static auto op = create_embedding_bag_typed_handle();
    return op.redispatch(dispatchKeySet, weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_bag_padding_idx, name, "aten::embedding_bag")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_bag_padding_idx, overload_name, "padding_idx")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_bag_padding_idx, schema_str, "embedding_bag.padding_idx(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int? padding_idx) -> (Tensor, Tensor, Tensor, Tensor)")

// aten::embedding_bag.padding_idx(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int? padding_idx) -> (Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<embedding_bag_padding_idx::schema> create_embedding_bag_padding_idx_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(embedding_bag_padding_idx::name, embedding_bag_padding_idx::overload_name)
      .typed<embedding_bag_padding_idx::schema>();
}

// aten::embedding_bag.padding_idx(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int? padding_idx) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> embedding_bag_padding_idx::call(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, c10::optional<int64_t> padding_idx) {
    
    static auto op = create_embedding_bag_padding_idx_typed_handle();
    return op.call(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

// aten::embedding_bag.padding_idx(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int? padding_idx) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> embedding_bag_padding_idx::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, c10::optional<int64_t> padding_idx) {
    
    static auto op = create_embedding_bag_padding_idx_typed_handle();
    return op.redispatch(dispatchKeySet, weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_zeros, name, "aten::new_zeros")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_zeros, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_zeros, schema_str, "new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<new_zeros::schema> create_new_zeros_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(new_zeros::name, new_zeros::overload_name)
      .typed<new_zeros::schema>();
}

// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor new_zeros::call(const at::Tensor & self, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_new_zeros_typed_handle();
    return op.call(self, size, dtype, layout, device, pin_memory);
}

// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor new_zeros::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_new_zeros_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf, name, "aten::erf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf, schema_str, "erf(Tensor self) -> Tensor")

// aten::erf(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<erf::schema> create_erf_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(erf::name, erf::overload_name)
      .typed<erf::schema>();
}

// aten::erf(Tensor self) -> Tensor
at::Tensor erf::call(const at::Tensor & self) {
    
    static auto op = create_erf_typed_handle();
    return op.call(self);
}

// aten::erf(Tensor self) -> Tensor
at::Tensor erf::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_erf_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf_, name, "aten::erf_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf_, schema_str, "erf_(Tensor(a!) self) -> Tensor(a!)")

// aten::erf_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<erf_::schema> create_erf__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(erf_::name, erf_::overload_name)
      .typed<erf_::schema>();
}

// aten::erf_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & erf_::call(at::Tensor & self) {
    
    static auto op = create_erf__typed_handle();
    return op.call(self);
}

// aten::erf_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & erf_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_erf__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf_out, name, "aten::erf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf_out, schema_str, "erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<erf_out::schema> create_erf_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(erf_out::name, erf_out::overload_name)
      .typed<erf_out::schema>();
}

// aten::erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & erf_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_erf_out_typed_handle();
    return op.call(self, out);
}

// aten::erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & erf_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_erf_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler, name, "aten::grid_sampler")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler, schema_str, "grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor")

// aten::grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<grid_sampler::schema> create_grid_sampler_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(grid_sampler::name, grid_sampler::overload_name)
      .typed<grid_sampler::schema>();
}

// aten::grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
at::Tensor grid_sampler::call(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    
    static auto op = create_grid_sampler_typed_handle();
    return op.call(input, grid, interpolation_mode, padding_mode, align_corners);
}

// aten::grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
at::Tensor grid_sampler::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    
    static auto op = create_grid_sampler_typed_handle();
    return op.redispatch(dispatchKeySet, input, grid, interpolation_mode, padding_mode, align_corners);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_grid_sampler_2d_cpu_fallback, name, "aten::_grid_sampler_2d_cpu_fallback")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_grid_sampler_2d_cpu_fallback, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_grid_sampler_2d_cpu_fallback, schema_str, "_grid_sampler_2d_cpu_fallback(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor")

// aten::_grid_sampler_2d_cpu_fallback(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_grid_sampler_2d_cpu_fallback::schema> create__grid_sampler_2d_cpu_fallback_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_grid_sampler_2d_cpu_fallback::name, _grid_sampler_2d_cpu_fallback::overload_name)
      .typed<_grid_sampler_2d_cpu_fallback::schema>();
}

// aten::_grid_sampler_2d_cpu_fallback(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
at::Tensor _grid_sampler_2d_cpu_fallback::call(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    
    static auto op = create__grid_sampler_2d_cpu_fallback_typed_handle();
    return op.call(input, grid, interpolation_mode, padding_mode, align_corners);
}

// aten::_grid_sampler_2d_cpu_fallback(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
at::Tensor _grid_sampler_2d_cpu_fallback::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    
    static auto op = create__grid_sampler_2d_cpu_fallback_typed_handle();
    return op.redispatch(dispatchKeySet, input, grid, interpolation_mode, padding_mode, align_corners);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d, name, "aten::grid_sampler_3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d, schema_str, "grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor")

// aten::grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<grid_sampler_3d::schema> create_grid_sampler_3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(grid_sampler_3d::name, grid_sampler_3d::overload_name)
      .typed<grid_sampler_3d::schema>();
}

// aten::grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
at::Tensor grid_sampler_3d::call(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    
    static auto op = create_grid_sampler_3d_typed_handle();
    return op.call(input, grid, interpolation_mode, padding_mode, align_corners);
}

// aten::grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
at::Tensor grid_sampler_3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    
    static auto op = create_grid_sampler_3d_typed_handle();
    return op.redispatch(dispatchKeySet, input, grid, interpolation_mode, padding_mode, align_corners);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window, name, "aten::hann_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window, schema_str, "hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hann_window::schema> create_hann_window_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hann_window::name, hann_window::overload_name)
      .typed<hann_window::schema>();
}

// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hann_window::call(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_hann_window_typed_handle();
    return op.call(window_length, dtype, layout, device, pin_memory);
}

// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hann_window::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_hann_window_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window_periodic, name, "aten::hann_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window_periodic, overload_name, "periodic")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window_periodic, schema_str, "hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hann_window_periodic::schema> create_hann_window_periodic_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hann_window_periodic::name, hann_window_periodic::overload_name)
      .typed<hann_window_periodic::schema>();
}

// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hann_window_periodic::call(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_hann_window_periodic_typed_handle();
    return op.call(window_length, periodic, dtype, layout, device, pin_memory);
}

// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hann_window_periodic::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_hann_window_periodic_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window, name, "aten::hamming_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window, schema_str, "hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hamming_window::schema> create_hamming_window_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hamming_window::name, hamming_window::overload_name)
      .typed<hamming_window::schema>();
}

// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window::call(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_hamming_window_typed_handle();
    return op.call(window_length, dtype, layout, device, pin_memory);
}

// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_hamming_window_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic, name, "aten::hamming_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic, overload_name, "periodic")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic, schema_str, "hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hamming_window_periodic::schema> create_hamming_window_periodic_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hamming_window_periodic::name, hamming_window_periodic::overload_name)
      .typed<hamming_window_periodic::schema>();
}

// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window_periodic::call(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_hamming_window_periodic_typed_handle();
    return op.call(window_length, periodic, dtype, layout, device, pin_memory);
}

// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window_periodic::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_hamming_window_periodic_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha, name, "aten::hamming_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha, overload_name, "periodic_alpha")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha, schema_str, "hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hamming_window_periodic_alpha::schema> create_hamming_window_periodic_alpha_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hamming_window_periodic_alpha::name, hamming_window_periodic_alpha::overload_name)
      .typed<hamming_window_periodic_alpha::schema>();
}

// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window_periodic_alpha::call(int64_t window_length, bool periodic, double alpha, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_hamming_window_periodic_alpha_typed_handle();
    return op.call(window_length, periodic, alpha, dtype, layout, device, pin_memory);
}

// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window_periodic_alpha::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, double alpha, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_hamming_window_periodic_alpha_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, alpha, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha_beta, name, "aten::hamming_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha_beta, overload_name, "periodic_alpha_beta")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha_beta, schema_str, "hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hamming_window_periodic_alpha_beta::schema> create_hamming_window_periodic_alpha_beta_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hamming_window_periodic_alpha_beta::name, hamming_window_periodic_alpha_beta::overload_name)
      .typed<hamming_window_periodic_alpha_beta::schema>();
}

// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window_periodic_alpha_beta::call(int64_t window_length, bool periodic, double alpha, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_hamming_window_periodic_alpha_beta_typed_handle();
    return op.call(window_length, periodic, alpha, beta, dtype, layout, device, pin_memory);
}

// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window_periodic_alpha_beta::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, double alpha, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_hamming_window_periodic_alpha_beta_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, alpha, beta, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm_backward, name, "aten::native_group_norm_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm_backward, schema_str, "native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)")

// aten::native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<native_group_norm_backward::schema> create_native_group_norm_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_group_norm_backward::name, native_group_norm_backward::overload_name)
      .typed<native_group_norm_backward::schema>();
}

// aten::native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_group_norm_backward::call(const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, ::std::array<bool,3> output_mask) {
    
    static auto op = create_native_group_norm_backward_typed_handle();
    return op.call(grad_out, input, mean, rstd, weight, N, C, HxW, group, output_mask);
}

// aten::native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_group_norm_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, ::std::array<bool,3> output_mask) {
    
    static auto op = create_native_group_norm_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, input, mean, rstd, weight, N, C, HxW, group, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2c, name, "aten::_fft_c2c")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2c, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2c, schema_str, "_fft_c2c(Tensor self, SymInt[] dim, int normalization, bool forward) -> Tensor")

// aten::_fft_c2c(Tensor self, SymInt[] dim, int normalization, bool forward) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_fft_c2c::schema> create__fft_c2c_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fft_c2c::name, _fft_c2c::overload_name)
      .typed<_fft_c2c::schema>();
}

// aten::_fft_c2c(Tensor self, SymInt[] dim, int normalization, bool forward) -> Tensor
at::Tensor _fft_c2c::call(const at::Tensor & self, c10::SymIntArrayRef dim, int64_t normalization, bool forward) {
    
    static auto op = create__fft_c2c_typed_handle();
    return op.call(self, dim, normalization, forward);
}

// aten::_fft_c2c(Tensor self, SymInt[] dim, int normalization, bool forward) -> Tensor
at::Tensor _fft_c2c::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef dim, int64_t normalization, bool forward) {
    
    static auto op = create__fft_c2c_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, normalization, forward);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2c_out, name, "aten::_fft_c2c")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2c_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2c_out, schema_str, "_fft_c2c.out(Tensor self, SymInt[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_fft_c2c.out(Tensor self, SymInt[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_fft_c2c_out::schema> create__fft_c2c_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fft_c2c_out::name, _fft_c2c_out::overload_name)
      .typed<_fft_c2c_out::schema>();
}

// aten::_fft_c2c.out(Tensor self, SymInt[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _fft_c2c_out::call(const at::Tensor & self, c10::SymIntArrayRef dim, int64_t normalization, bool forward, at::Tensor & out) {
    
    static auto op = create__fft_c2c_out_typed_handle();
    return op.call(self, dim, normalization, forward, out);
}

// aten::_fft_c2c.out(Tensor self, SymInt[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _fft_c2c_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef dim, int64_t normalization, bool forward, at::Tensor & out) {
    
    static auto op = create__fft_c2c_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, normalization, forward, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_validate_compressed_sparse_indices, name, "aten::_validate_compressed_sparse_indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_validate_compressed_sparse_indices, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_validate_compressed_sparse_indices, schema_str, "_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()")

// aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_validate_compressed_sparse_indices::schema> create__validate_compressed_sparse_indices_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_validate_compressed_sparse_indices::name, _validate_compressed_sparse_indices::overload_name)
      .typed<_validate_compressed_sparse_indices::schema>();
}

// aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()
void _validate_compressed_sparse_indices::call(bool is_crow, const at::Tensor & compressed_idx, const at::Tensor & plain_idx, int64_t cdim, int64_t dim, int64_t nnz) {
    
    static auto op = create__validate_compressed_sparse_indices_typed_handle();
    return op.call(is_crow, compressed_idx, plain_idx, cdim, dim, nnz);
}

// aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()
void _validate_compressed_sparse_indices::redispatch(c10::DispatchKeySet dispatchKeySet, bool is_crow, const at::Tensor & compressed_idx, const at::Tensor & plain_idx, int64_t cdim, int64_t dim, int64_t nnz) {
    
    static auto op = create__validate_compressed_sparse_indices_typed_handle();
    return op.redispatch(dispatchKeySet, is_crow, compressed_idx, plain_idx, cdim, dim, nnz);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_get_plan_cache_size, name, "aten::_cufft_get_plan_cache_size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_get_plan_cache_size, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_get_plan_cache_size, schema_str, "_cufft_get_plan_cache_size(DeviceIndex device_index) -> int")

// aten::_cufft_get_plan_cache_size(DeviceIndex device_index) -> int
static C10_NOINLINE c10::TypedOperatorHandle<_cufft_get_plan_cache_size::schema> create__cufft_get_plan_cache_size_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cufft_get_plan_cache_size::name, _cufft_get_plan_cache_size::overload_name)
      .typed<_cufft_get_plan_cache_size::schema>();
}

// aten::_cufft_get_plan_cache_size(DeviceIndex device_index) -> int
int64_t _cufft_get_plan_cache_size::call(at::DeviceIndex device_index) {
    
    static auto op = create__cufft_get_plan_cache_size_typed_handle();
    return op.call(device_index);
}

// aten::_cufft_get_plan_cache_size(DeviceIndex device_index) -> int
int64_t _cufft_get_plan_cache_size::redispatch(c10::DispatchKeySet dispatchKeySet, at::DeviceIndex device_index) {
    
    static auto op = create__cufft_get_plan_cache_size_typed_handle();
    return op.redispatch(dispatchKeySet, device_index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_get_plan_cache_max_size, name, "aten::_cufft_get_plan_cache_max_size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_get_plan_cache_max_size, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_get_plan_cache_max_size, schema_str, "_cufft_get_plan_cache_max_size(DeviceIndex device_index) -> int")

// aten::_cufft_get_plan_cache_max_size(DeviceIndex device_index) -> int
static C10_NOINLINE c10::TypedOperatorHandle<_cufft_get_plan_cache_max_size::schema> create__cufft_get_plan_cache_max_size_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cufft_get_plan_cache_max_size::name, _cufft_get_plan_cache_max_size::overload_name)
      .typed<_cufft_get_plan_cache_max_size::schema>();
}

// aten::_cufft_get_plan_cache_max_size(DeviceIndex device_index) -> int
int64_t _cufft_get_plan_cache_max_size::call(at::DeviceIndex device_index) {
    
    static auto op = create__cufft_get_plan_cache_max_size_typed_handle();
    return op.call(device_index);
}

// aten::_cufft_get_plan_cache_max_size(DeviceIndex device_index) -> int
int64_t _cufft_get_plan_cache_max_size::redispatch(c10::DispatchKeySet dispatchKeySet, at::DeviceIndex device_index) {
    
    static auto op = create__cufft_get_plan_cache_max_size_typed_handle();
    return op.redispatch(dispatchKeySet, device_index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_Tensor, name, "aten::index")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_Tensor, schema_str, "index.Tensor(Tensor self, Tensor?[] indices) -> Tensor")

// aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_Tensor::schema> create_index_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_Tensor::name, index_Tensor::overload_name)
      .typed<index_Tensor::schema>();
}

// aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor
at::Tensor index_Tensor::call(const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices) {
    
    static auto op = create_index_Tensor_typed_handle();
    return op.call(self, indices);
}

// aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor
at::Tensor index_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices) {
    
    static auto op = create_index_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_Tensor_out, name, "aten::index")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_Tensor_out, schema_str, "index.Tensor_out(Tensor self, Tensor?[] indices, *, Tensor(a!) out) -> Tensor(a!)")

// aten::index.Tensor_out(Tensor self, Tensor?[] indices, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_Tensor_out::schema> create_index_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_Tensor_out::name, index_Tensor_out::overload_name)
      .typed<index_Tensor_out::schema>();
}

// aten::index.Tensor_out(Tensor self, Tensor?[] indices, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & index_Tensor_out::call(const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, at::Tensor & out) {
    
    static auto op = create_index_Tensor_out_typed_handle();
    return op.call(self, indices, out);
}

// aten::index.Tensor_out(Tensor self, Tensor?[] indices, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & index_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, at::Tensor & out) {
    
    static auto op = create_index_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isnan, name, "aten::isnan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isnan, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isnan, schema_str, "isnan(Tensor self) -> Tensor")

// aten::isnan(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<isnan::schema> create_isnan_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isnan::name, isnan::overload_name)
      .typed<isnan::schema>();
}

// aten::isnan(Tensor self) -> Tensor
at::Tensor isnan::call(const at::Tensor & self) {
    
    static auto op = create_isnan_typed_handle();
    return op.call(self);
}

// aten::isnan(Tensor self) -> Tensor
at::Tensor isnan::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_isnan_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue, name, "aten::kthvalue")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue, schema_str, "kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<kthvalue::schema> create_kthvalue_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kthvalue::name, kthvalue::overload_name)
      .typed<kthvalue::schema>();
}

// aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> kthvalue::call(const at::Tensor & self, int64_t k, int64_t dim, bool keepdim) {
    
    static auto op = create_kthvalue_typed_handle();
    return op.call(self, k, dim, keepdim);
}

// aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> kthvalue::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, int64_t dim, bool keepdim) {
    
    static auto op = create_kthvalue_typed_handle();
    return op.redispatch(dispatchKeySet, self, k, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_values, name, "aten::kthvalue")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_values, overload_name, "values")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_values, schema_str, "kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<kthvalue_values::schema> create_kthvalue_values_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kthvalue_values::name, kthvalue_values::overload_name)
      .typed<kthvalue_values::schema>();
}

// aten::kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> kthvalue_values::call(const at::Tensor & self, int64_t k, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    
    static auto op = create_kthvalue_values_typed_handle();
    return op.call(self, k, dim, keepdim, values, indices);
}

// aten::kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> kthvalue_values::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    
    static auto op = create_kthvalue_values_typed_handle();
    return op.redispatch(dispatchKeySet, self, k, dim, keepdim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_dimname, name, "aten::kthvalue")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_dimname, schema_str, "kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<kthvalue_dimname::schema> create_kthvalue_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kthvalue_dimname::name, kthvalue_dimname::overload_name)
      .typed<kthvalue_dimname::schema>();
}

// aten::kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> kthvalue_dimname::call(const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim) {
    
    static auto op = create_kthvalue_dimname_typed_handle();
    return op.call(self, k, dim, keepdim);
}

// aten::kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> kthvalue_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim) {
    
    static auto op = create_kthvalue_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, k, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_dimname_out, name, "aten::kthvalue")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_dimname_out, schema_str, "kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<kthvalue_dimname_out::schema> create_kthvalue_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kthvalue_dimname_out::name, kthvalue_dimname_out::overload_name)
      .typed<kthvalue_dimname_out::schema>();
}

// aten::kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> kthvalue_dimname_out::call(const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    
    static auto op = create_kthvalue_dimname_out_typed_handle();
    return op.call(self, k, dim, keepdim, values, indices);
}

// aten::kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> kthvalue_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    
    static auto op = create_kthvalue_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, k, dim, keepdim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_layer_norm, name, "aten::native_layer_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_layer_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_layer_norm, schema_str, "native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)")

// aten::native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<native_layer_norm::schema> create_native_layer_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_layer_norm::name, native_layer_norm::overload_name)
      .typed<native_layer_norm::schema>();
}

// aten::native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_layer_norm::call(const at::Tensor & input, c10::SymIntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps) {
    
    static auto op = create_native_layer_norm_typed_handle();
    return op.call(input, normalized_shape, weight, bias, eps);
}

// aten::native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_layer_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::SymIntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps) {
    
    static auto op = create_native_layer_norm_typed_handle();
    return op.redispatch(dispatchKeySet, input, normalized_shape, weight, bias, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num, name, "aten::nan_to_num")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num, schema_str, "nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor")

// aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nan_to_num::schema> create_nan_to_num_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nan_to_num::name, nan_to_num::overload_name)
      .typed<nan_to_num::schema>();
}

// aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor
at::Tensor nan_to_num::call(const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
    
    static auto op = create_nan_to_num_typed_handle();
    return op.call(self, nan, posinf, neginf);
}

// aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor
at::Tensor nan_to_num::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
    
    static auto op = create_nan_to_num_typed_handle();
    return op.redispatch(dispatchKeySet, self, nan, posinf, neginf);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num_, name, "aten::nan_to_num_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num_, schema_str, "nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)")

// aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nan_to_num_::schema> create_nan_to_num__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nan_to_num_::name, nan_to_num_::overload_name)
      .typed<nan_to_num_::schema>();
}

// aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)
at::Tensor & nan_to_num_::call(at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
    
    static auto op = create_nan_to_num__typed_handle();
    return op.call(self, nan, posinf, neginf);
}

// aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)
at::Tensor & nan_to_num_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
    
    static auto op = create_nan_to_num__typed_handle();
    return op.redispatch(dispatchKeySet, self, nan, posinf, neginf);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num_out, name, "aten::nan_to_num")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num_out, schema_str, "nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nan_to_num_out::schema> create_nan_to_num_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nan_to_num_out::name, nan_to_num_out::overload_name)
      .typed<nan_to_num_out::schema>();
}

// aten::nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nan_to_num_out::call(const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf, at::Tensor & out) {
    
    static auto op = create_nan_to_num_out_typed_handle();
    return op.call(self, nan, posinf, neginf, out);
}

// aten::nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nan_to_num_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf, at::Tensor & out) {
    
    static auto op = create_nan_to_num_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, nan, posinf, neginf, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_int8_weight_fp32_activation, name, "aten::fbgemm_linear_int8_weight_fp32_activation")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_int8_weight_fp32_activation, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_int8_weight_fp32_activation, schema_str, "fbgemm_linear_int8_weight_fp32_activation(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor")

// aten::fbgemm_linear_int8_weight_fp32_activation(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fbgemm_linear_int8_weight_fp32_activation::schema> create_fbgemm_linear_int8_weight_fp32_activation_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fbgemm_linear_int8_weight_fp32_activation::name, fbgemm_linear_int8_weight_fp32_activation::overload_name)
      .typed<fbgemm_linear_int8_weight_fp32_activation::schema>();
}

// aten::fbgemm_linear_int8_weight_fp32_activation(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
at::Tensor fbgemm_linear_int8_weight_fp32_activation::call(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
    
    static auto op = create_fbgemm_linear_int8_weight_fp32_activation_typed_handle();
    return op.call(input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}

// aten::fbgemm_linear_int8_weight_fp32_activation(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
at::Tensor fbgemm_linear_int8_weight_fp32_activation::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
    
    static auto op = create_fbgemm_linear_int8_weight_fp32_activation_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_int8_weight, name, "aten::fbgemm_linear_int8_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_int8_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_int8_weight, schema_str, "fbgemm_linear_int8_weight(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor")

// aten::fbgemm_linear_int8_weight(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fbgemm_linear_int8_weight::schema> create_fbgemm_linear_int8_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fbgemm_linear_int8_weight::name, fbgemm_linear_int8_weight::overload_name)
      .typed<fbgemm_linear_int8_weight::schema>();
}

// aten::fbgemm_linear_int8_weight(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
at::Tensor fbgemm_linear_int8_weight::call(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
    
    static auto op = create_fbgemm_linear_int8_weight_typed_handle();
    return op.call(input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}

// aten::fbgemm_linear_int8_weight(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
at::Tensor fbgemm_linear_int8_weight::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
    
    static auto op = create_fbgemm_linear_int8_weight_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_fp16_weight_fp32_activation, name, "aten::fbgemm_linear_fp16_weight_fp32_activation")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_fp16_weight_fp32_activation, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_fp16_weight_fp32_activation, schema_str, "fbgemm_linear_fp16_weight_fp32_activation(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor")

// aten::fbgemm_linear_fp16_weight_fp32_activation(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fbgemm_linear_fp16_weight_fp32_activation::schema> create_fbgemm_linear_fp16_weight_fp32_activation_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fbgemm_linear_fp16_weight_fp32_activation::name, fbgemm_linear_fp16_weight_fp32_activation::overload_name)
      .typed<fbgemm_linear_fp16_weight_fp32_activation::schema>();
}

// aten::fbgemm_linear_fp16_weight_fp32_activation(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
at::Tensor fbgemm_linear_fp16_weight_fp32_activation::call(const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
    
    static auto op = create_fbgemm_linear_fp16_weight_fp32_activation_typed_handle();
    return op.call(input, packed_weight, bias);
}

// aten::fbgemm_linear_fp16_weight_fp32_activation(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
at::Tensor fbgemm_linear_fp16_weight_fp32_activation::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
    
    static auto op = create_fbgemm_linear_fp16_weight_fp32_activation_typed_handle();
    return op.redispatch(dispatchKeySet, input, packed_weight, bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Tensor, name, "aten::xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Tensor, schema_str, "xlogy.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::xlogy.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<xlogy_Tensor::schema> create_xlogy_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy_Tensor::name, xlogy_Tensor::overload_name)
      .typed<xlogy_Tensor::schema>();
}

// aten::xlogy.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor xlogy_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_xlogy_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::xlogy.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor xlogy_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_xlogy_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Scalar_Self, name, "aten::xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Scalar_Self, overload_name, "Scalar_Self")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Scalar_Self, schema_str, "xlogy.Scalar_Self(Scalar self, Tensor other) -> Tensor")

// aten::xlogy.Scalar_Self(Scalar self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<xlogy_Scalar_Self::schema> create_xlogy_Scalar_Self_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy_Scalar_Self::name, xlogy_Scalar_Self::overload_name)
      .typed<xlogy_Scalar_Self::schema>();
}

// aten::xlogy.Scalar_Self(Scalar self, Tensor other) -> Tensor
at::Tensor xlogy_Scalar_Self::call(const at::Scalar & self, const at::Tensor & other) {
    
    static auto op = create_xlogy_Scalar_Self_typed_handle();
    return op.call(self, other);
}

// aten::xlogy.Scalar_Self(Scalar self, Tensor other) -> Tensor
at::Tensor xlogy_Scalar_Self::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other) {
    
    static auto op = create_xlogy_Scalar_Self_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Scalar_Other, name, "aten::xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Scalar_Other, overload_name, "Scalar_Other")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Scalar_Other, schema_str, "xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor")

// aten::xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<xlogy_Scalar_Other::schema> create_xlogy_Scalar_Other_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy_Scalar_Other::name, xlogy_Scalar_Other::overload_name)
      .typed<xlogy_Scalar_Other::schema>();
}

// aten::xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor
at::Tensor xlogy_Scalar_Other::call(const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_xlogy_Scalar_Other_typed_handle();
    return op.call(self, other);
}

// aten::xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor
at::Tensor xlogy_Scalar_Other::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_xlogy_Scalar_Other_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy__Tensor, name, "aten::xlogy_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy__Tensor, schema_str, "xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<xlogy__Tensor::schema> create_xlogy__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy__Tensor::name, xlogy__Tensor::overload_name)
      .typed<xlogy__Tensor::schema>();
}

// aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & xlogy__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_xlogy__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & xlogy__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_xlogy__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy__Scalar_Other, name, "aten::xlogy_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy__Scalar_Other, overload_name, "Scalar_Other")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy__Scalar_Other, schema_str, "xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<xlogy__Scalar_Other::schema> create_xlogy__Scalar_Other_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy__Scalar_Other::name, xlogy__Scalar_Other::overload_name)
      .typed<xlogy__Scalar_Other::schema>();
}

// aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & xlogy__Scalar_Other::call(at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_xlogy__Scalar_Other_typed_handle();
    return op.call(self, other);
}

// aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & xlogy__Scalar_Other::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_xlogy__Scalar_Other_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutTensor, name, "aten::xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutTensor, overload_name, "OutTensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutTensor, schema_str, "xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<xlogy_OutTensor::schema> create_xlogy_OutTensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy_OutTensor::name, xlogy_OutTensor::overload_name)
      .typed<xlogy_OutTensor::schema>();
}

// aten::xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & xlogy_OutTensor::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_xlogy_OutTensor_typed_handle();
    return op.call(self, other, out);
}

// aten::xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & xlogy_OutTensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_xlogy_OutTensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutScalar_Self, name, "aten::xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutScalar_Self, overload_name, "OutScalar_Self")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutScalar_Self, schema_str, "xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<xlogy_OutScalar_Self::schema> create_xlogy_OutScalar_Self_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy_OutScalar_Self::name, xlogy_OutScalar_Self::overload_name)
      .typed<xlogy_OutScalar_Self::schema>();
}

// aten::xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & xlogy_OutScalar_Self::call(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_xlogy_OutScalar_Self_typed_handle();
    return op.call(self, other, out);
}

// aten::xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & xlogy_OutScalar_Self::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_xlogy_OutScalar_Self_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutScalar_Other, name, "aten::xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutScalar_Other, overload_name, "OutScalar_Other")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutScalar_Other, schema_str, "xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<xlogy_OutScalar_Other::schema> create_xlogy_OutScalar_Other_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy_OutScalar_Other::name, xlogy_OutScalar_Other::overload_name)
      .typed<xlogy_OutScalar_Other::schema>();
}

// aten::xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & xlogy_OutScalar_Other::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_xlogy_OutScalar_Other_typed_handle();
    return op.call(self, other, out);
}

// aten::xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & xlogy_OutScalar_Other::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_xlogy_OutScalar_Other_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_backward_data, name, "aten::_log_softmax_backward_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_backward_data, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_backward_data, schema_str, "_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor")

// aten::_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_log_softmax_backward_data::schema> create__log_softmax_backward_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_log_softmax_backward_data::name, _log_softmax_backward_data::overload_name)
      .typed<_log_softmax_backward_data::schema>();
}

// aten::_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor
at::Tensor _log_softmax_backward_data::call(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype) {
    
    static auto op = create__log_softmax_backward_data_typed_handle();
    return op.call(grad_output, output, dim, input_dtype);
}

// aten::_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor
at::Tensor _log_softmax_backward_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype) {
    
    static auto op = create__log_softmax_backward_data_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, dim, input_dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_backward_data_out, name, "aten::_log_softmax_backward_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_backward_data_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_backward_data_out, schema_str, "_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_log_softmax_backward_data_out::schema> create__log_softmax_backward_data_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_log_softmax_backward_data_out::name, _log_softmax_backward_data_out::overload_name)
      .typed<_log_softmax_backward_data_out::schema>();
}

// aten::_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _log_softmax_backward_data_out::call(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype, at::Tensor & out) {
    
    static auto op = create__log_softmax_backward_data_out_typed_handle();
    return op.call(grad_output, output, dim, input_dtype, out);
}

// aten::_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _log_softmax_backward_data_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype, at::Tensor & out) {
    
    static auto op = create__log_softmax_backward_data_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, dim, input_dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp, name, "aten::logcumsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp, schema_str, "logcumsumexp(Tensor self, int dim) -> Tensor")

// aten::logcumsumexp(Tensor self, int dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logcumsumexp::schema> create_logcumsumexp_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logcumsumexp::name, logcumsumexp::overload_name)
      .typed<logcumsumexp::schema>();
}

// aten::logcumsumexp(Tensor self, int dim) -> Tensor
at::Tensor logcumsumexp::call(const at::Tensor & self, int64_t dim) {
    
    static auto op = create_logcumsumexp_typed_handle();
    return op.call(self, dim);
}

// aten::logcumsumexp(Tensor self, int dim) -> Tensor
at::Tensor logcumsumexp::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    
    static auto op = create_logcumsumexp_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_out, name, "aten::logcumsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_out, schema_str, "logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logcumsumexp_out::schema> create_logcumsumexp_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logcumsumexp_out::name, logcumsumexp_out::overload_name)
      .typed<logcumsumexp_out::schema>();
}

// aten::logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logcumsumexp_out::call(const at::Tensor & self, int64_t dim, at::Tensor & out) {
    
    static auto op = create_logcumsumexp_out_typed_handle();
    return op.call(self, dim, out);
}

// aten::logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logcumsumexp_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
    
    static auto op = create_logcumsumexp_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_dimname, name, "aten::logcumsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_dimname, schema_str, "logcumsumexp.dimname(Tensor self, Dimname dim) -> Tensor")

// aten::logcumsumexp.dimname(Tensor self, Dimname dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logcumsumexp_dimname::schema> create_logcumsumexp_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logcumsumexp_dimname::name, logcumsumexp_dimname::overload_name)
      .typed<logcumsumexp_dimname::schema>();
}

// aten::logcumsumexp.dimname(Tensor self, Dimname dim) -> Tensor
at::Tensor logcumsumexp_dimname::call(const at::Tensor & self, at::Dimname dim) {
    
    static auto op = create_logcumsumexp_dimname_typed_handle();
    return op.call(self, dim);
}

// aten::logcumsumexp.dimname(Tensor self, Dimname dim) -> Tensor
at::Tensor logcumsumexp_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
    
    static auto op = create_logcumsumexp_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_dimname_out, name, "aten::logcumsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_dimname_out, schema_str, "logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logcumsumexp_dimname_out::schema> create_logcumsumexp_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logcumsumexp_dimname_out::name, logcumsumexp_dimname_out::overload_name)
      .typed<logcumsumexp_dimname_out::schema>();
}

// aten::logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logcumsumexp_dimname_out::call(const at::Tensor & self, at::Dimname dim, at::Tensor & out) {
    
    static auto op = create_logcumsumexp_dimname_out_typed_handle();
    return op.call(self, dim, out);
}

// aten::logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logcumsumexp_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, at::Tensor & out) {
    
    static auto op = create_logcumsumexp_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_exp_backward, name, "aten::matrix_exp_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_exp_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_exp_backward, schema_str, "matrix_exp_backward(Tensor self, Tensor grad) -> Tensor")

// aten::matrix_exp_backward(Tensor self, Tensor grad) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<matrix_exp_backward::schema> create_matrix_exp_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(matrix_exp_backward::name, matrix_exp_backward::overload_name)
      .typed<matrix_exp_backward::schema>();
}

// aten::matrix_exp_backward(Tensor self, Tensor grad) -> Tensor
at::Tensor matrix_exp_backward::call(const at::Tensor & self, const at::Tensor & grad) {
    
    static auto op = create_matrix_exp_backward_typed_handle();
    return op.call(self, grad);
}

// aten::matrix_exp_backward(Tensor self, Tensor grad) -> Tensor
at::Tensor matrix_exp_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad) {
    
    static auto op = create_matrix_exp_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, grad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amax, name, "aten::amax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amax, schema_str, "amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor")

// aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<amax::schema> create_amax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(amax::name, amax::overload_name)
      .typed<amax::schema>();
}

// aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
at::Tensor amax::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    
    static auto op = create_amax_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
at::Tensor amax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    
    static auto op = create_amax_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amax_out, name, "aten::amax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amax_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amax_out, schema_str, "amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<amax_out::schema> create_amax_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(amax_out::name, amax_out::overload_name)
      .typed<amax_out::schema>();
}

// aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & amax_out::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    
    static auto op = create_amax_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & amax_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    
    static auto op = create_amax_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool2d, name, "aten::mkldnn_max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool2d, schema_str, "mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor")

// aten::mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_max_pool2d::schema> create_mkldnn_max_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_max_pool2d::name, mkldnn_max_pool2d::overload_name)
      .typed<mkldnn_max_pool2d::schema>();
}

// aten::mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor mkldnn_max_pool2d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    
    static auto op = create_mkldnn_max_pool2d_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor mkldnn_max_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    
    static auto op = create_mkldnn_max_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_max_pool2d, name, "aten::quantized_max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_max_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_max_pool2d, schema_str, "quantized_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor")

// aten::quantized_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantized_max_pool2d::schema> create_quantized_max_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantized_max_pool2d::name, quantized_max_pool2d::overload_name)
      .typed<quantized_max_pool2d::schema>();
}

// aten::quantized_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor quantized_max_pool2d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    
    static auto op = create_quantized_max_pool2d_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::quantized_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor quantized_max_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    
    static auto op = create_quantized_max_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amin, name, "aten::amin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amin, schema_str, "amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor")

// aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<amin::schema> create_amin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(amin::name, amin::overload_name)
      .typed<amin::schema>();
}

// aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
at::Tensor amin::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    
    static auto op = create_amin_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
at::Tensor amin::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    
    static auto op = create_amin_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amin_out, name, "aten::amin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amin_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amin_out, schema_str, "amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<amin_out::schema> create_amin_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(amin_out::name, amin_out::overload_name)
      .typed<amin_out::schema>();
}

// aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & amin_out::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    
    static auto op = create_amin_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & amin_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    
    static auto op = create_amin_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mps_convolution, name, "aten::_mps_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mps_convolution, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mps_convolution, schema_str, "_mps_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups) -> Tensor")

// aten::_mps_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_mps_convolution::schema> create__mps_convolution_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_mps_convolution::name, _mps_convolution::overload_name)
      .typed<_mps_convolution::schema>();
}

// aten::_mps_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups) -> Tensor
at::Tensor _mps_convolution::call(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    
    static auto op = create__mps_convolution_typed_handle();
    return op.call(self, weight, bias, padding, stride, dilation, groups);
}

// aten::_mps_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups) -> Tensor
at::Tensor _mps_convolution::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    
    static auto op = create__mps_convolution_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, padding, stride, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_rnn_layer_backward, name, "aten::mkldnn_rnn_layer_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_rnn_layer_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_rnn_layer_backward, schema_str, "mkldnn_rnn_layer_backward(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)")

// aten::mkldnn_rnn_layer_backward(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_rnn_layer_backward::schema> create_mkldnn_rnn_layer_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_rnn_layer_backward::name, mkldnn_rnn_layer_backward::overload_name)
      .typed<mkldnn_rnn_layer_backward::schema>();
}

// aten::mkldnn_rnn_layer_backward(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> mkldnn_rnn_layer_backward::call(const at::Tensor & input, const at::Tensor & weight1, const at::Tensor & weight2, const at::Tensor & weight3, const at::Tensor & weight4, const at::Tensor & hx_, const at::Tensor & cx_tmp, const at::Tensor & output, const at::Tensor & hy_, const at::Tensor & cy_, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, bool reverse, int64_t mode, int64_t hidden_size, int64_t num_layers, bool has_biases, bool train, bool bidirectional, at::IntArrayRef batch_sizes, bool batch_first, const at::Tensor & workspace) {
    
    static auto op = create_mkldnn_rnn_layer_backward_typed_handle();
    return op.call(input, weight1, weight2, weight3, weight4, hx_, cx_tmp, output, hy_, cy_, grad_output, grad_hy, grad_cy, reverse, mode, hidden_size, num_layers, has_biases, train, bidirectional, batch_sizes, batch_first, workspace);
}

// aten::mkldnn_rnn_layer_backward(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> mkldnn_rnn_layer_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight1, const at::Tensor & weight2, const at::Tensor & weight3, const at::Tensor & weight4, const at::Tensor & hx_, const at::Tensor & cx_tmp, const at::Tensor & output, const at::Tensor & hy_, const at::Tensor & cy_, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, bool reverse, int64_t mode, int64_t hidden_size, int64_t num_layers, bool has_biases, bool train, bool bidirectional, at::IntArrayRef batch_sizes, bool batch_first, const at::Tensor & workspace) {
    
    static auto op = create_mkldnn_rnn_layer_backward_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight1, weight2, weight3, weight4, hx_, cx_tmp, output, hy_, cy_, grad_output, grad_hy, grad_cy, reverse, mode, hidden_size, num_layers, has_biases, train, bidirectional, batch_sizes, batch_first, workspace);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution, name, "aten::miopen_depthwise_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution, schema_str, "miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic) -> Tensor")

// aten::miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<miopen_depthwise_convolution::schema> create_miopen_depthwise_convolution_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_depthwise_convolution::name, miopen_depthwise_convolution::overload_name)
      .typed<miopen_depthwise_convolution::schema>();
}

// aten::miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_depthwise_convolution::call(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic) {
    
    static auto op = create_miopen_depthwise_convolution_typed_handle();
    return op.call(self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic);
}

// aten::miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_depthwise_convolution::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic) {
    
    static auto op = create_miopen_depthwise_convolution_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_int_mm, name, "aten::_int_mm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_int_mm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_int_mm, schema_str, "_int_mm(Tensor self, Tensor mat2) -> Tensor")

// aten::_int_mm(Tensor self, Tensor mat2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_int_mm::schema> create__int_mm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_int_mm::name, _int_mm::overload_name)
      .typed<_int_mm::schema>();
}

// aten::_int_mm(Tensor self, Tensor mat2) -> Tensor
at::Tensor _int_mm::call(const at::Tensor & self, const at::Tensor & mat2) {
    
    static auto op = create__int_mm_typed_handle();
    return op.call(self, mat2);
}

// aten::_int_mm(Tensor self, Tensor mat2) -> Tensor
at::Tensor _int_mm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2) {
    
    static auto op = create__int_mm_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_int_mm_out, name, "aten::_int_mm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_int_mm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_int_mm_out, schema_str, "_int_mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_int_mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_int_mm_out::schema> create__int_mm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_int_mm_out::name, _int_mm_out::overload_name)
      .typed<_int_mm_out::schema>();
}

// aten::_int_mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _int_mm_out::call(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
    
    static auto op = create__int_mm_out_typed_handle();
    return op.call(self, mat2, out);
}

// aten::_int_mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _int_mm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
    
    static auto op = create__int_mm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat2, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm, name, "aten::native_batch_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm, schema_str, "native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)")

// aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<native_batch_norm::schema> create_native_batch_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_batch_norm::name, native_batch_norm::overload_name)
      .typed<native_batch_norm::schema>();
}

// aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_batch_norm::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps) {
    
    static auto op = create_native_batch_norm_typed_handle();
    return op.call(input, weight, bias, running_mean, running_var, training, momentum, eps);
}

// aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_batch_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps) {
    
    static auto op = create_native_batch_norm_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, running_mean, running_var, training, momentum, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_out, name, "aten::native_batch_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_out, schema_str, "native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<native_batch_norm_out::schema> create_native_batch_norm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_batch_norm_out::name, native_batch_norm_out::overload_name)
      .typed<native_batch_norm_out::schema>();
}

// aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_batch_norm_out::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, at::Tensor & out, at::Tensor & save_mean, at::Tensor & save_invstd) {
    
    static auto op = create_native_batch_norm_out_typed_handle();
    return op.call(input, weight, bias, running_mean, running_var, training, momentum, eps, out, save_mean, save_invstd);
}

// aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_batch_norm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, at::Tensor & out, at::Tensor & save_mean, at::Tensor & save_invstd) {
    
    static auto op = create_native_batch_norm_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, running_mean, running_var, training, momentum, eps, out, save_mean, save_invstd);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_stats, name, "aten::batch_norm_stats")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_stats, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_stats, schema_str, "batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)")

// aten::batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_stats::schema> create_batch_norm_stats_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_stats::name, batch_norm_stats::overload_name)
      .typed<batch_norm_stats::schema>();
}

// aten::batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> batch_norm_stats::call(const at::Tensor & input, double eps) {
    
    static auto op = create_batch_norm_stats_typed_handle();
    return op.call(input, eps);
}

// aten::batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> batch_norm_stats::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double eps) {
    
    static auto op = create_batch_norm_stats_typed_handle();
    return op.redispatch(dispatchKeySet, input, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats, name, "aten::batch_norm_gather_stats")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats, schema_str, "batch_norm_gather_stats(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count) -> (Tensor, Tensor)")

// aten::batch_norm_gather_stats(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_gather_stats::schema> create_batch_norm_gather_stats_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_gather_stats::name, batch_norm_gather_stats::overload_name)
      .typed<batch_norm_gather_stats::schema>();
}

// aten::batch_norm_gather_stats(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> batch_norm_gather_stats::call(const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, int64_t count) {
    
    static auto op = create_batch_norm_gather_stats_typed_handle();
    return op.call(input, mean, invstd, running_mean, running_var, momentum, eps, count);
}

// aten::batch_norm_gather_stats(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> batch_norm_gather_stats::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, int64_t count) {
    
    static auto op = create_batch_norm_gather_stats_typed_handle();
    return op.redispatch(dispatchKeySet, input, mean, invstd, running_mean, running_var, momentum, eps, count);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_backward, name, "aten::native_batch_norm_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_backward, schema_str, "native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)")

// aten::native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<native_batch_norm_backward::schema> create_native_batch_norm_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_batch_norm_backward::name, native_batch_norm_backward::overload_name)
      .typed<native_batch_norm_backward::schema>();
}

// aten::native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_batch_norm_backward::call(const at::Tensor & grad_out, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask) {
    
    static auto op = create_native_batch_norm_backward_typed_handle();
    return op.call(grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
}

// aten::native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_batch_norm_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask) {
    
    static auto op = create_native_batch_norm_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_backward_reduce, name, "aten::batch_norm_backward_reduce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_backward_reduce, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_backward_reduce, schema_str, "batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)")

// aten::batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_backward_reduce::schema> create_batch_norm_backward_reduce_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_backward_reduce::name, batch_norm_backward_reduce::overload_name)
      .typed<batch_norm_backward_reduce::schema>();
}

// aten::batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> batch_norm_backward_reduce::call(const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & weight, bool input_g, bool weight_g, bool bias_g) {
    
    static auto op = create_batch_norm_backward_reduce_typed_handle();
    return op.call(grad_out, input, mean, invstd, weight, input_g, weight_g, bias_g);
}

// aten::batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> batch_norm_backward_reduce::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & weight, bool input_g, bool weight_g, bool bias_g) {
    
    static auto op = create_batch_norm_backward_reduce_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, input, mean, invstd, weight, input_g, weight_g, bias_g);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_vulkan_available, name, "aten::is_vulkan_available")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_vulkan_available, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_vulkan_available, schema_str, "is_vulkan_available() -> bool")

// aten::is_vulkan_available() -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_vulkan_available::schema> create_is_vulkan_available_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_vulkan_available::name, is_vulkan_available::overload_name)
      .typed<is_vulkan_available::schema>();
}

// aten::is_vulkan_available() -> bool
bool is_vulkan_available::call() {
    
    static auto op = create_is_vulkan_available_typed_handle();
    return op.call();
}

// aten::is_vulkan_available() -> bool
bool is_vulkan_available::redispatch(c10::DispatchKeySet dispatchKeySet) {
    
    static auto op = create_is_vulkan_available_typed_handle();
    return op.redispatch(dispatchKeySet);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution, name, "aten::_nnpack_spatial_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution, schema_str, "_nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, SymInt[2] stride=1) -> Tensor")

// aten::_nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, SymInt[2] stride=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_nnpack_spatial_convolution::schema> create__nnpack_spatial_convolution_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_nnpack_spatial_convolution::name, _nnpack_spatial_convolution::overload_name)
      .typed<_nnpack_spatial_convolution::schema>();
}

// aten::_nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, SymInt[2] stride=1) -> Tensor
at::Tensor _nnpack_spatial_convolution::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride) {
    
    static auto op = create__nnpack_spatial_convolution_typed_handle();
    return op.call(input, weight, bias, padding, stride);
}

// aten::_nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, SymInt[2] stride=1) -> Tensor
at::Tensor _nnpack_spatial_convolution::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride) {
    
    static auto op = create__nnpack_spatial_convolution_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, padding, stride);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_names, name, "aten::ones")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_names, overload_name, "names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_names, schema_str, "ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ones_names::schema> create_ones_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ones_names::name, ones_names::overload_name)
      .typed<ones_names::schema>();
}

// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor ones_names::call(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_ones_names_typed_handle();
    return op.call(size, names, dtype, layout, device, pin_memory);
}

// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor ones_names::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_ones_names_typed_handle();
    return op.redispatch(dispatchKeySet, size, names, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones, name, "aten::ones")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones, schema_str, "ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ones::schema> create_ones_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ones::name, ones::overload_name)
      .typed<ones::schema>();
}

// aten::ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor ones::call(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_ones_typed_handle();
    return op.call(size, dtype, layout, device, pin_memory);
}

// aten::ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor ones::redispatch(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_ones_typed_handle();
    return op.redispatch(dispatchKeySet, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_out, name, "aten::ones")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_out, schema_str, "ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ones_out::schema> create_ones_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ones_out::name, ones_out::overload_name)
      .typed<ones_out::schema>();
}

// aten::ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ones_out::call(c10::SymIntArrayRef size, at::Tensor & out) {
    
    static auto op = create_ones_out_typed_handle();
    return op.call(size, out);
}

// aten::ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ones_out::redispatch(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, at::Tensor & out) {
    
    static auto op = create_ones_out_typed_handle();
    return op.redispatch(dispatchKeySet, size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cdist_forward, name, "aten::_cdist_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cdist_forward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cdist_forward, schema_str, "_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor")

// aten::_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cdist_forward::schema> create__cdist_forward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cdist_forward::name, _cdist_forward::overload_name)
      .typed<_cdist_forward::schema>();
}

// aten::_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor
at::Tensor _cdist_forward::call(const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode) {
    
    static auto op = create__cdist_forward_typed_handle();
    return op.call(x1, x2, p, compute_mode);
}

// aten::_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor
at::Tensor _cdist_forward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode) {
    
    static auto op = create__cdist_forward_typed_handle();
    return op.redispatch(dispatchKeySet, x1, x2, p, compute_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosine_similarity, name, "aten::cosine_similarity")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosine_similarity, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosine_similarity, schema_str, "cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -> Tensor")

// aten::cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cosine_similarity::schema> create_cosine_similarity_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cosine_similarity::name, cosine_similarity::overload_name)
      .typed<cosine_similarity::schema>();
}

// aten::cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -> Tensor
at::Tensor cosine_similarity::call(const at::Tensor & x1, const at::Tensor & x2, int64_t dim, double eps) {
    
    static auto op = create_cosine_similarity_typed_handle();
    return op.call(x1, x2, dim, eps);
}

// aten::cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -> Tensor
at::Tensor cosine_similarity::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2, int64_t dim, double eps) {
    
    static auto op = create_cosine_similarity_typed_handle();
    return op.redispatch(dispatchKeySet, x1, x2, dim, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(movedim_intlist, name, "aten::movedim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(movedim_intlist, overload_name, "intlist")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(movedim_intlist, schema_str, "movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)")

// aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<movedim_intlist::schema> create_movedim_intlist_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(movedim_intlist::name, movedim_intlist::overload_name)
      .typed<movedim_intlist::schema>();
}

// aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
at::Tensor movedim_intlist::call(const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
    
    static auto op = create_movedim_intlist_typed_handle();
    return op.call(self, source, destination);
}

// aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
at::Tensor movedim_intlist::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
    
    static auto op = create_movedim_intlist_typed_handle();
    return op.redispatch(dispatchKeySet, self, source, destination);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(movedim_int, name, "aten::movedim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(movedim_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(movedim_int, schema_str, "movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)")

// aten::movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<movedim_int::schema> create_movedim_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(movedim_int::name, movedim_int::overload_name)
      .typed<movedim_int::schema>();
}

// aten::movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)
at::Tensor movedim_int::call(const at::Tensor & self, int64_t source, int64_t destination) {
    
    static auto op = create_movedim_int_typed_handle();
    return op.call(self, source, destination);
}

// aten::movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)
at::Tensor movedim_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t source, int64_t destination) {
    
    static auto op = create_movedim_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, source, destination);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(numpy_T, name, "aten::numpy_T")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(numpy_T, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(numpy_T, schema_str, "numpy_T(Tensor(a) self) -> Tensor(a)")

// aten::numpy_T(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<numpy_T::schema> create_numpy_T_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(numpy_T::name, numpy_T::overload_name)
      .typed<numpy_T::schema>();
}

// aten::numpy_T(Tensor(a) self) -> Tensor(a)
at::Tensor numpy_T::call(const at::Tensor & self) {
    
    static auto op = create_numpy_T_typed_handle();
    return op.call(self);
}

// aten::numpy_T(Tensor(a) self) -> Tensor(a)
at::Tensor numpy_T::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_numpy_T_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mH, name, "aten::mH")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mH, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mH, schema_str, "mH(Tensor(a) self) -> Tensor(a)")

// aten::mH(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<mH::schema> create_mH_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mH::name, mH::overload_name)
      .typed<mH::schema>();
}

// aten::mH(Tensor(a) self) -> Tensor(a)
at::Tensor mH::call(const at::Tensor & self) {
    
    static auto op = create_mH_typed_handle();
    return op.call(self);
}

// aten::mH(Tensor(a) self) -> Tensor(a)
at::Tensor mH::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_mH_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_like, name, "aten::rand_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_like, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_like, schema_str, "rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rand_like::schema> create_rand_like_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rand_like::name, rand_like::overload_name)
      .typed<rand_like::schema>();
}

// aten::rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor rand_like::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_rand_like_typed_handle();
    return op.call(self, dtype, layout, device, pin_memory, memory_format);
}

// aten::rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor rand_like::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_rand_like_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like, name, "aten::randint_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like, schema_str, "randint_like(Tensor self, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::randint_like(Tensor self, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randint_like::schema> create_randint_like_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randint_like::name, randint_like::overload_name)
      .typed<randint_like::schema>();
}

// aten::randint_like(Tensor self, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor randint_like::call(const at::Tensor & self, c10::SymInt high, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_randint_like_typed_handle();
    return op.call(self, high, dtype, layout, device, pin_memory, memory_format);
}

// aten::randint_like(Tensor self, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor randint_like::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt high, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_randint_like_typed_handle();
    return op.redispatch(dispatchKeySet, self, high, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like_low_dtype, name, "aten::randint_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like_low_dtype, overload_name, "low_dtype")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like_low_dtype, schema_str, "randint_like.low_dtype(Tensor self, SymInt low, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::randint_like.low_dtype(Tensor self, SymInt low, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randint_like_low_dtype::schema> create_randint_like_low_dtype_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randint_like_low_dtype::name, randint_like_low_dtype::overload_name)
      .typed<randint_like_low_dtype::schema>();
}

// aten::randint_like.low_dtype(Tensor self, SymInt low, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor randint_like_low_dtype::call(const at::Tensor & self, c10::SymInt low, c10::SymInt high, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_randint_like_low_dtype_typed_handle();
    return op.call(self, low, high, dtype, layout, device, pin_memory, memory_format);
}

// aten::randint_like.low_dtype(Tensor self, SymInt low, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor randint_like_low_dtype::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt low, c10::SymInt high, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_randint_like_low_dtype_typed_handle();
    return op.redispatch(dispatchKeySet, self, low, high, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round, name, "aten::round")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round, schema_str, "round(Tensor self) -> Tensor")

// aten::round(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<round::schema> create_round_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(round::name, round::overload_name)
      .typed<round::schema>();
}

// aten::round(Tensor self) -> Tensor
at::Tensor round::call(const at::Tensor & self) {
    
    static auto op = create_round_typed_handle();
    return op.call(self);
}

// aten::round(Tensor self) -> Tensor
at::Tensor round::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_round_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_, name, "aten::round_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_, schema_str, "round_(Tensor(a!) self) -> Tensor(a!)")

// aten::round_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<round_::schema> create_round__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(round_::name, round_::overload_name)
      .typed<round_::schema>();
}

// aten::round_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & round_::call(at::Tensor & self) {
    
    static auto op = create_round__typed_handle();
    return op.call(self);
}

// aten::round_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & round_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_round__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_out, name, "aten::round")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_out, schema_str, "round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<round_out::schema> create_round_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(round_out::name, round_out::overload_name)
      .typed<round_out::schema>();
}

// aten::round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & round_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_round_out_typed_handle();
    return op.call(self, out);
}

// aten::round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & round_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_round_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_decimals, name, "aten::round")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_decimals, overload_name, "decimals")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_decimals, schema_str, "round.decimals(Tensor self, *, int decimals) -> Tensor")

// aten::round.decimals(Tensor self, *, int decimals) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<round_decimals::schema> create_round_decimals_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(round_decimals::name, round_decimals::overload_name)
      .typed<round_decimals::schema>();
}

// aten::round.decimals(Tensor self, *, int decimals) -> Tensor
at::Tensor round_decimals::call(const at::Tensor & self, int64_t decimals) {
    
    static auto op = create_round_decimals_typed_handle();
    return op.call(self, decimals);
}

// aten::round.decimals(Tensor self, *, int decimals) -> Tensor
at::Tensor round_decimals::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t decimals) {
    
    static auto op = create_round_decimals_typed_handle();
    return op.redispatch(dispatchKeySet, self, decimals);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round__decimals, name, "aten::round_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round__decimals, overload_name, "decimals")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round__decimals, schema_str, "round_.decimals(Tensor(a!) self, *, int decimals) -> Tensor(a!)")

// aten::round_.decimals(Tensor(a!) self, *, int decimals) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<round__decimals::schema> create_round__decimals_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(round__decimals::name, round__decimals::overload_name)
      .typed<round__decimals::schema>();
}

// aten::round_.decimals(Tensor(a!) self, *, int decimals) -> Tensor(a!)
at::Tensor & round__decimals::call(at::Tensor & self, int64_t decimals) {
    
    static auto op = create_round__decimals_typed_handle();
    return op.call(self, decimals);
}

// aten::round_.decimals(Tensor(a!) self, *, int decimals) -> Tensor(a!)
at::Tensor & round__decimals::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t decimals) {
    
    static auto op = create_round__decimals_typed_handle();
    return op.redispatch(dispatchKeySet, self, decimals);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_decimals_out, name, "aten::round")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_decimals_out, overload_name, "decimals_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_decimals_out, schema_str, "round.decimals_out(Tensor self, *, int decimals, Tensor(a!) out) -> Tensor(a!)")

// aten::round.decimals_out(Tensor self, *, int decimals, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<round_decimals_out::schema> create_round_decimals_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(round_decimals_out::name, round_decimals_out::overload_name)
      .typed<round_decimals_out::schema>();
}

// aten::round.decimals_out(Tensor self, *, int decimals, Tensor(a!) out) -> Tensor(a!)
at::Tensor & round_decimals_out::call(const at::Tensor & self, int64_t decimals, at::Tensor & out) {
    
    static auto op = create_round_decimals_out_typed_handle();
    return op.call(self, decimals, out);
}

// aten::round.decimals_out(Tensor self, *, int decimals, Tensor(a!) out) -> Tensor(a!)
at::Tensor & round_decimals_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t decimals, at::Tensor & out) {
    
    static auto op = create_round_decimals_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, decimals, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_out, name, "aten::gelu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_out, schema_str, "gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -> Tensor(a!)")

// aten::gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<gelu_out::schema> create_gelu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gelu_out::name, gelu_out::overload_name)
      .typed<gelu_out::schema>();
}

// aten::gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -> Tensor(a!)
at::Tensor & gelu_out::call(const at::Tensor & self, c10::string_view approximate, at::Tensor & out) {
    
    static auto op = create_gelu_out_typed_handle();
    return op.call(self, approximate, out);
}

// aten::gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -> Tensor(a!)
at::Tensor & gelu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view approximate, at::Tensor & out) {
    
    static auto op = create_gelu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, approximate, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_, name, "aten::gelu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_, schema_str, "gelu_(Tensor(a!) self, *, str approximate='none') -> Tensor(a!)")

// aten::gelu_(Tensor(a!) self, *, str approximate='none') -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<gelu_::schema> create_gelu__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gelu_::name, gelu_::overload_name)
      .typed<gelu_::schema>();
}

// aten::gelu_(Tensor(a!) self, *, str approximate='none') -> Tensor(a!)
at::Tensor & gelu_::call(at::Tensor & self, c10::string_view approximate) {
    
    static auto op = create_gelu__typed_handle();
    return op.call(self, approximate);
}

// aten::gelu_(Tensor(a!) self, *, str approximate='none') -> Tensor(a!)
at::Tensor & gelu_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::string_view approximate) {
    
    static auto op = create_gelu__typed_handle();
    return op.redispatch(dispatchKeySet, self, approximate);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu, name, "aten::gelu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu, schema_str, "gelu(Tensor self, *, str approximate='none') -> Tensor")

// aten::gelu(Tensor self, *, str approximate='none') -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<gelu::schema> create_gelu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gelu::name, gelu::overload_name)
      .typed<gelu::schema>();
}

// aten::gelu(Tensor self, *, str approximate='none') -> Tensor
at::Tensor gelu::call(const at::Tensor & self, c10::string_view approximate) {
    
    static auto op = create_gelu_typed_handle();
    return op.call(self, approximate);
}

// aten::gelu(Tensor self, *, str approximate='none') -> Tensor
at::Tensor gelu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view approximate) {
    
    static auto op = create_gelu_typed_handle();
    return op.redispatch(dispatchKeySet, self, approximate);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink_out, name, "aten::hardshrink")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink_out, schema_str, "hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardshrink_out::schema> create_hardshrink_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardshrink_out::name, hardshrink_out::overload_name)
      .typed<hardshrink_out::schema>();
}

// aten::hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardshrink_out::call(const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
    
    static auto op = create_hardshrink_out_typed_handle();
    return op.call(self, lambd, out);
}

// aten::hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardshrink_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
    
    static auto op = create_hardshrink_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, lambd, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink, name, "aten::hardshrink")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink, schema_str, "hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor")

// aten::hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hardshrink::schema> create_hardshrink_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardshrink::name, hardshrink::overload_name)
      .typed<hardshrink::schema>();
}

// aten::hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor
at::Tensor hardshrink::call(const at::Tensor & self, const at::Scalar & lambd) {
    
    static auto op = create_hardshrink_typed_handle();
    return op.call(self, lambd);
}

// aten::hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor
at::Tensor hardshrink::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & lambd) {
    
    static auto op = create_hardshrink_typed_handle();
    return op.redispatch(dispatchKeySet, self, lambd);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_backward, name, "aten::select_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_backward, schema_str, "select_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index) -> Tensor")

// aten::select_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<select_backward::schema> create_select_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(select_backward::name, select_backward::overload_name)
      .typed<select_backward::schema>();
}

// aten::select_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index) -> Tensor
at::Tensor select_backward::call(const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt index) {
    
    static auto op = create_select_backward_typed_handle();
    return op.call(grad_output, input_sizes, dim, index);
}

// aten::select_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index) -> Tensor
at::Tensor select_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt index) {
    
    static auto op = create_select_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input_sizes, dim, index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish, name, "aten::mish")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish, schema_str, "mish(Tensor self) -> Tensor")

// aten::mish(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mish::schema> create_mish_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mish::name, mish::overload_name)
      .typed<mish::schema>();
}

// aten::mish(Tensor self) -> Tensor
at::Tensor mish::call(const at::Tensor & self) {
    
    static auto op = create_mish_typed_handle();
    return op.call(self);
}

// aten::mish(Tensor self) -> Tensor
at::Tensor mish::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_mish_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_, name, "aten::mish_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_, schema_str, "mish_(Tensor(a!) self) -> Tensor(a!)")

// aten::mish_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mish_::schema> create_mish__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mish_::name, mish_::overload_name)
      .typed<mish_::schema>();
}

// aten::mish_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & mish_::call(at::Tensor & self) {
    
    static auto op = create_mish__typed_handle();
    return op.call(self);
}

// aten::mish_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & mish_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_mish__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_out, name, "aten::mish")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_out, schema_str, "mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mish_out::schema> create_mish_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mish_out::name, mish_out::overload_name)
      .typed<mish_out::schema>();
}

// aten::mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mish_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_mish_out_typed_handle();
    return op.call(self, out);
}

// aten::mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mish_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_mish_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid, name, "aten::sigmoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid, schema_str, "sigmoid(Tensor self) -> Tensor")

// aten::sigmoid(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sigmoid::schema> create_sigmoid_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sigmoid::name, sigmoid::overload_name)
      .typed<sigmoid::schema>();
}

// aten::sigmoid(Tensor self) -> Tensor
at::Tensor sigmoid::call(const at::Tensor & self) {
    
    static auto op = create_sigmoid_typed_handle();
    return op.call(self);
}

// aten::sigmoid(Tensor self) -> Tensor
at::Tensor sigmoid::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_sigmoid_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_, name, "aten::sigmoid_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_, schema_str, "sigmoid_(Tensor(a!) self) -> Tensor(a!)")

// aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sigmoid_::schema> create_sigmoid__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sigmoid_::name, sigmoid_::overload_name)
      .typed<sigmoid_::schema>();
}

// aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sigmoid_::call(at::Tensor & self) {
    
    static auto op = create_sigmoid__typed_handle();
    return op.call(self);
}

// aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sigmoid_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_sigmoid__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_out, name, "aten::sigmoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_out, schema_str, "sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sigmoid_out::schema> create_sigmoid_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sigmoid_out::name, sigmoid_out::overload_name)
      .typed<sigmoid_out::schema>();
}

// aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sigmoid_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_sigmoid_out_typed_handle();
    return op.call(self, out);
}

// aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sigmoid_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_sigmoid_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach, name, "aten::detach")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach, schema_str, "detach(Tensor(a) self) -> Tensor(a)")

// aten::detach(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<detach::schema> create_detach_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(detach::name, detach::overload_name)
      .typed<detach::schema>();
}

// aten::detach(Tensor(a) self) -> Tensor(a)
at::Tensor detach::call(const at::Tensor & self) {
    
    static auto op = create_detach_typed_handle();
    return op.call(self);
}

// aten::detach(Tensor(a) self) -> Tensor(a)
at::Tensor detach::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_detach_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach_, name, "aten::detach_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach_, schema_str, "detach_(Tensor(a!) self) -> Tensor(a!)")

// aten::detach_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<detach_::schema> create_detach__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(detach_::name, detach_::overload_name)
      .typed<detach_::schema>();
}

// aten::detach_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & detach_::call(at::Tensor & self) {
    
    static auto op = create_detach__typed_handle();
    return op.call(self);
}

// aten::detach_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & detach_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_detach__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(size_int, name, "aten::size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(size_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(size_int, schema_str, "size.int(Tensor self, int dim) -> int")

// aten::size.int(Tensor self, int dim) -> int
static C10_NOINLINE c10::TypedOperatorHandle<size_int::schema> create_size_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(size_int::name, size_int::overload_name)
      .typed<size_int::schema>();
}

// aten::size.int(Tensor self, int dim) -> int
int64_t size_int::call(const at::Tensor & self, int64_t dim) {
    
    static auto op = create_size_int_typed_handle();
    return op.call(self, dim);
}

// aten::size.int(Tensor self, int dim) -> int
int64_t size_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    
    static auto op = create_size_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(size_Dimname, name, "aten::size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(size_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(size_Dimname, schema_str, "size.Dimname(Tensor self, Dimname dim) -> int")

// aten::size.Dimname(Tensor self, Dimname dim) -> int
static C10_NOINLINE c10::TypedOperatorHandle<size_Dimname::schema> create_size_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(size_Dimname::name, size_Dimname::overload_name)
      .typed<size_Dimname::schema>();
}

// aten::size.Dimname(Tensor self, Dimname dim) -> int
int64_t size_Dimname::call(const at::Tensor & self, at::Dimname dim) {
    
    static auto op = create_size_Dimname_typed_handle();
    return op.call(self, dim);
}

// aten::size.Dimname(Tensor self, Dimname dim) -> int
int64_t size_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
    
    static auto op = create_size_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sym_numel, name, "aten::sym_numel")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sym_numel, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sym_numel, schema_str, "sym_numel(Tensor self) -> SymInt")

// aten::sym_numel(Tensor self) -> SymInt
static C10_NOINLINE c10::TypedOperatorHandle<sym_numel::schema> create_sym_numel_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sym_numel::name, sym_numel::overload_name)
      .typed<sym_numel::schema>();
}

// aten::sym_numel(Tensor self) -> SymInt
c10::SymInt sym_numel::call(const at::Tensor & self) {
    
    static auto op = create_sym_numel_typed_handle();
    return op.call(self);
}

// aten::sym_numel(Tensor self) -> SymInt
c10::SymInt sym_numel::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_sym_numel_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_scatter, name, "aten::slice_scatter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_scatter, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_scatter, schema_str, "slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor")

// aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<slice_scatter::schema> create_slice_scatter_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slice_scatter::name, slice_scatter::overload_name)
      .typed<slice_scatter::schema>();
}

// aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor
at::Tensor slice_scatter::call(const at::Tensor & self, const at::Tensor & src, int64_t dim, c10::optional<c10::SymInt> start, c10::optional<c10::SymInt> end, c10::SymInt step) {
    
    static auto op = create_slice_scatter_typed_handle();
    return op.call(self, src, dim, start, end, step);
}

// aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor
at::Tensor slice_scatter::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & src, int64_t dim, c10::optional<c10::SymInt> start, c10::optional<c10::SymInt> end, c10::SymInt step) {
    
    static auto op = create_slice_scatter_typed_handle();
    return op.redispatch(dispatchKeySet, self, src, dim, start, end, step);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_backward_data, name, "aten::_softmax_backward_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_backward_data, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_backward_data, schema_str, "_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor")

// aten::_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_softmax_backward_data::schema> create__softmax_backward_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_softmax_backward_data::name, _softmax_backward_data::overload_name)
      .typed<_softmax_backward_data::schema>();
}

// aten::_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor
at::Tensor _softmax_backward_data::call(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype) {
    
    static auto op = create__softmax_backward_data_typed_handle();
    return op.call(grad_output, output, dim, input_dtype);
}

// aten::_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor
at::Tensor _softmax_backward_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype) {
    
    static auto op = create__softmax_backward_data_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, dim, input_dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_backward_data_out, name, "aten::_softmax_backward_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_backward_data_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_backward_data_out, schema_str, "_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_softmax_backward_data_out::schema> create__softmax_backward_data_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_softmax_backward_data_out::name, _softmax_backward_data_out::overload_name)
      .typed<_softmax_backward_data_out::schema>();
}

// aten::_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & _softmax_backward_data_out::call(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype, at::Tensor & grad_input) {
    
    static auto op = create__softmax_backward_data_out_typed_handle();
    return op.call(grad_output, output, dim, input_dtype, grad_input);
}

// aten::_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & _softmax_backward_data_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype, at::Tensor & grad_input) {
    
    static auto op = create__softmax_backward_data_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, dim, input_dtype, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_with_sizes, name, "aten::split_with_sizes")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_with_sizes, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_with_sizes, schema_str, "split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]")

// aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<split_with_sizes::schema> create_split_with_sizes_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(split_with_sizes::name, split_with_sizes::overload_name)
      .typed<split_with_sizes::schema>();
}

// aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> split_with_sizes::call(const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim) {
    
    static auto op = create_split_with_sizes_typed_handle();
    return op.call(self, split_sizes, dim);
}

// aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> split_with_sizes::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim) {
    
    static auto op = create_split_with_sizes_typed_handle();
    return op.redispatch(dispatchKeySet, self, split_sizes, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hsplit_int, name, "aten::hsplit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hsplit_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hsplit_int, schema_str, "hsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]")

// aten::hsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<hsplit_int::schema> create_hsplit_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hsplit_int::name, hsplit_int::overload_name)
      .typed<hsplit_int::schema>();
}

// aten::hsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]
::std::vector<at::Tensor> hsplit_int::call(const at::Tensor & self, int64_t sections) {
    
    static auto op = create_hsplit_int_typed_handle();
    return op.call(self, sections);
}

// aten::hsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]
::std::vector<at::Tensor> hsplit_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sections) {
    
    static auto op = create_hsplit_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, sections);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hsplit_array, name, "aten::hsplit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hsplit_array, overload_name, "array")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hsplit_array, schema_str, "hsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]")

// aten::hsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<hsplit_array::schema> create_hsplit_array_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hsplit_array::name, hsplit_array::overload_name)
      .typed<hsplit_array::schema>();
}

// aten::hsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]
::std::vector<at::Tensor> hsplit_array::call(const at::Tensor & self, at::IntArrayRef indices) {
    
    static auto op = create_hsplit_array_typed_handle();
    return op.call(self, indices);
}

// aten::hsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]
::std::vector<at::Tensor> hsplit_array::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef indices) {
    
    static auto op = create_hsplit_array_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stack, name, "aten::stack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stack, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stack, schema_str, "stack(Tensor[] tensors, int dim=0) -> Tensor")

// aten::stack(Tensor[] tensors, int dim=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<stack::schema> create_stack_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(stack::name, stack::overload_name)
      .typed<stack::schema>();
}

// aten::stack(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor stack::call(at::TensorList tensors, int64_t dim) {
    
    static auto op = create_stack_typed_handle();
    return op.call(tensors, dim);
}

// aten::stack(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor stack::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim) {
    
    static auto op = create_stack_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stack_out, name, "aten::stack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stack_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stack_out, schema_str, "stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<stack_out::schema> create_stack_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(stack_out::name, stack_out::overload_name)
      .typed<stack_out::schema>();
}

// aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & stack_out::call(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    
    static auto op = create_stack_out_typed_handle();
    return op.call(tensors, dim, out);
}

// aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & stack_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
    
    static auto op = create_stack_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_stack, name, "aten::_stack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_stack, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_stack, schema_str, "_stack(Tensor[] tensors, int dim=0) -> Tensor")

// aten::_stack(Tensor[] tensors, int dim=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_stack::schema> create__stack_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_stack::name, _stack::overload_name)
      .typed<_stack::schema>();
}

// aten::_stack(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor _stack::call(at::TensorList tensors, int64_t dim) {
    
    static auto op = create__stack_typed_handle();
    return op.call(tensors, dim);
}

// aten::_stack(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor _stack::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim) {
    
    static auto op = create__stack_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_stack_out, name, "aten::_stack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_stack_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_stack_out, schema_str, "_stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_stack_out::schema> create__stack_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_stack_out::name, _stack_out::overload_name)
      .typed<_stack_out::schema>();
}

// aten::_stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _stack_out::call(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    
    static auto op = create__stack_out_typed_handle();
    return op.call(tensors, dim, out);
}

// aten::_stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _stack_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
    
    static auto op = create__stack_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square, name, "aten::square")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square, schema_str, "square(Tensor self) -> Tensor")

// aten::square(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<square::schema> create_square_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(square::name, square::overload_name)
      .typed<square::schema>();
}

// aten::square(Tensor self) -> Tensor
at::Tensor square::call(const at::Tensor & self) {
    
    static auto op = create_square_typed_handle();
    return op.call(self);
}

// aten::square(Tensor self) -> Tensor
at::Tensor square::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_square_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square_, name, "aten::square_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square_, schema_str, "square_(Tensor(a!) self) -> Tensor(a!)")

// aten::square_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<square_::schema> create_square__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(square_::name, square_::overload_name)
      .typed<square_::schema>();
}

// aten::square_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & square_::call(at::Tensor & self) {
    
    static auto op = create_square__typed_handle();
    return op.call(self);
}

// aten::square_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & square_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_square__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square_out, name, "aten::square")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square_out, schema_str, "square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<square_out::schema> create_square_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(square_out::name, square_out::overload_name)
      .typed<square_out::schema>();
}

// aten::square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & square_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_square_out_typed_handle();
    return op.call(self, out);
}

// aten::square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & square_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_square_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh, name, "aten::tanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh, schema_str, "tanh(Tensor self) -> Tensor")

// aten::tanh(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<tanh::schema> create_tanh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tanh::name, tanh::overload_name)
      .typed<tanh::schema>();
}

// aten::tanh(Tensor self) -> Tensor
at::Tensor tanh::call(const at::Tensor & self) {
    
    static auto op = create_tanh_typed_handle();
    return op.call(self);
}

// aten::tanh(Tensor self) -> Tensor
at::Tensor tanh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_tanh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_, name, "aten::tanh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_, schema_str, "tanh_(Tensor(a!) self) -> Tensor(a!)")

// aten::tanh_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<tanh_::schema> create_tanh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tanh_::name, tanh_::overload_name)
      .typed<tanh_::schema>();
}

// aten::tanh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & tanh_::call(at::Tensor & self) {
    
    static auto op = create_tanh__typed_handle();
    return op.call(self);
}

// aten::tanh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & tanh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_tanh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_out, name, "aten::tanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_out, schema_str, "tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<tanh_out::schema> create_tanh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tanh_out::name, tanh_out::overload_name)
      .typed<tanh_out::schema>();
}

// aten::tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & tanh_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_tanh_out_typed_handle();
    return op.call(self, out);
}

// aten::tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & tanh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_tanh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensordot, name, "aten::tensordot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensordot, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensordot, schema_str, "tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -> Tensor")

// aten::tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<tensordot::schema> create_tensordot_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tensordot::name, tensordot::overload_name)
      .typed<tensordot::schema>();
}

// aten::tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -> Tensor
at::Tensor tensordot::call(const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other) {
    
    static auto op = create_tensordot_typed_handle();
    return op.call(self, other, dims_self, dims_other);
}

// aten::tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -> Tensor
at::Tensor tensordot::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other) {
    
    static auto op = create_tensordot_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, dims_self, dims_other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensordot_out, name, "aten::tensordot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensordot_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensordot_out, schema_str, "tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<tensordot_out::schema> create_tensordot_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tensordot_out::name, tensordot_out::overload_name)
      .typed<tensordot_out::schema>();
}

// aten::tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & tensordot_out::call(const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other, at::Tensor & out) {
    
    static auto op = create_tensordot_out_typed_handle();
    return op.call(self, other, dims_self, dims_other, out);
}

// aten::tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & tensordot_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other, at::Tensor & out) {
    
    static auto op = create_tensordot_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, dims_self, dims_other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tile, name, "aten::tile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tile, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tile, schema_str, "tile(Tensor self, SymInt[] dims) -> Tensor")

// aten::tile(Tensor self, SymInt[] dims) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<tile::schema> create_tile_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tile::name, tile::overload_name)
      .typed<tile::schema>();
}

// aten::tile(Tensor self, SymInt[] dims) -> Tensor
at::Tensor tile::call(const at::Tensor & self, c10::SymIntArrayRef dims) {
    
    static auto op = create_tile_typed_handle();
    return op.call(self, dims);
}

// aten::tile(Tensor self, SymInt[] dims) -> Tensor
at::Tensor tile::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef dims) {
    
    static auto op = create_tile_typed_handle();
    return op.redispatch(dispatchKeySet, self, dims);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose, name, "aten::_mkldnn_transpose")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose, schema_str, "_mkldnn_transpose(Tensor self, int dim0, int dim1) -> Tensor")

// aten::_mkldnn_transpose(Tensor self, int dim0, int dim1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_mkldnn_transpose::schema> create__mkldnn_transpose_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_mkldnn_transpose::name, _mkldnn_transpose::overload_name)
      .typed<_mkldnn_transpose::schema>();
}

// aten::_mkldnn_transpose(Tensor self, int dim0, int dim1) -> Tensor
at::Tensor _mkldnn_transpose::call(const at::Tensor & self, int64_t dim0, int64_t dim1) {
    
    static auto op = create__mkldnn_transpose_typed_handle();
    return op.call(self, dim0, dim1);
}

// aten::_mkldnn_transpose(Tensor self, int dim0, int dim1) -> Tensor
at::Tensor _mkldnn_transpose::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim0, int64_t dim1) {
    
    static auto op = create__mkldnn_transpose_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim0, dim1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose_, name, "aten::_mkldnn_transpose_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose_, schema_str, "_mkldnn_transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)")

// aten::_mkldnn_transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_mkldnn_transpose_::schema> create__mkldnn_transpose__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_mkldnn_transpose_::name, _mkldnn_transpose_::overload_name)
      .typed<_mkldnn_transpose_::schema>();
}

// aten::_mkldnn_transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
at::Tensor & _mkldnn_transpose_::call(at::Tensor & self, int64_t dim0, int64_t dim1) {
    
    static auto op = create__mkldnn_transpose__typed_handle();
    return op.call(self, dim0, dim1);
}

// aten::_mkldnn_transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
at::Tensor & _mkldnn_transpose_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim0, int64_t dim1) {
    
    static auto op = create__mkldnn_transpose__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim0, dim1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fliplr, name, "aten::fliplr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fliplr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fliplr, schema_str, "fliplr(Tensor self) -> Tensor")

// aten::fliplr(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fliplr::schema> create_fliplr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fliplr::name, fliplr::overload_name)
      .typed<fliplr::schema>();
}

// aten::fliplr(Tensor self) -> Tensor
at::Tensor fliplr::call(const at::Tensor & self) {
    
    static auto op = create_fliplr_typed_handle();
    return op.call(self);
}

// aten::fliplr(Tensor self) -> Tensor
at::Tensor fliplr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_fliplr_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_from_padded_and_nested_example, name, "aten::_nested_from_padded_and_nested_example")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_from_padded_and_nested_example, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_from_padded_and_nested_example, schema_str, "_nested_from_padded_and_nested_example(Tensor padded, Tensor nt_example) -> Tensor")

// aten::_nested_from_padded_and_nested_example(Tensor padded, Tensor nt_example) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_nested_from_padded_and_nested_example::schema> create__nested_from_padded_and_nested_example_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_nested_from_padded_and_nested_example::name, _nested_from_padded_and_nested_example::overload_name)
      .typed<_nested_from_padded_and_nested_example::schema>();
}

// aten::_nested_from_padded_and_nested_example(Tensor padded, Tensor nt_example) -> Tensor
at::Tensor _nested_from_padded_and_nested_example::call(const at::Tensor & padded, const at::Tensor & nt_example) {
    
    static auto op = create__nested_from_padded_and_nested_example_typed_handle();
    return op.call(padded, nt_example);
}

// aten::_nested_from_padded_and_nested_example(Tensor padded, Tensor nt_example) -> Tensor
at::Tensor _nested_from_padded_and_nested_example::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & padded, const at::Tensor & nt_example) {
    
    static auto op = create__nested_from_padded_and_nested_example_typed_handle();
    return op.redispatch(dispatchKeySet, padded, nt_example);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix, name, "aten::fix")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix, schema_str, "fix(Tensor self) -> Tensor")

// aten::fix(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fix::schema> create_fix_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fix::name, fix::overload_name)
      .typed<fix::schema>();
}

// aten::fix(Tensor self) -> Tensor
at::Tensor fix::call(const at::Tensor & self) {
    
    static auto op = create_fix_typed_handle();
    return op.call(self);
}

// aten::fix(Tensor self) -> Tensor
at::Tensor fix::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_fix_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix_, name, "aten::fix_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix_, schema_str, "fix_(Tensor(a!) self) -> Tensor(a!)")

// aten::fix_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fix_::schema> create_fix__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fix_::name, fix_::overload_name)
      .typed<fix_::schema>();
}

// aten::fix_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & fix_::call(at::Tensor & self) {
    
    static auto op = create_fix__typed_handle();
    return op.call(self);
}

// aten::fix_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & fix_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_fix__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix_out, name, "aten::fix")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix_out, schema_str, "fix.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fix.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fix_out::schema> create_fix_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fix_out::name, fix_out::overload_name)
      .typed<fix_out::schema>();
}

// aten::fix.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fix_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_fix_out_typed_handle();
    return op.call(self, out);
}

// aten::fix.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fix_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_fix_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_dim, name, "aten::unique_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_dim, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_dim, schema_str, "unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)")

// aten::unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<unique_dim::schema> create_unique_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unique_dim::name, unique_dim::overload_name)
      .typed<unique_dim::schema>();
}

// aten::unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> unique_dim::call(const at::Tensor & self, int64_t dim, bool sorted, bool return_inverse, bool return_counts) {
    
    static auto op = create_unique_dim_typed_handle();
    return op.call(self, dim, sorted, return_inverse, return_counts);
}

// aten::unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> unique_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool sorted, bool return_inverse, bool return_counts) {
    
    static auto op = create_unique_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, sorted, return_inverse, return_counts);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_consecutive, name, "aten::unique_consecutive")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_consecutive, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_consecutive, schema_str, "unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)")

// aten::unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<unique_consecutive::schema> create_unique_consecutive_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unique_consecutive::name, unique_consecutive::overload_name)
      .typed<unique_consecutive::schema>();
}

// aten::unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> unique_consecutive::call(const at::Tensor & self, bool return_inverse, bool return_counts, c10::optional<int64_t> dim) {
    
    static auto op = create_unique_consecutive_typed_handle();
    return op.call(self, return_inverse, return_counts, dim);
}

// aten::unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> unique_consecutive::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool return_inverse, bool return_counts, c10::optional<int64_t> dim) {
    
    static auto op = create_unique_consecutive_typed_handle();
    return op.redispatch(dispatchKeySet, self, return_inverse, return_counts, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vander, name, "aten::vander")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vander, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vander, schema_str, "vander(Tensor x, int? N=None, bool increasing=False) -> Tensor")

// aten::vander(Tensor x, int? N=None, bool increasing=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<vander::schema> create_vander_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(vander::name, vander::overload_name)
      .typed<vander::schema>();
}

// aten::vander(Tensor x, int? N=None, bool increasing=False) -> Tensor
at::Tensor vander::call(const at::Tensor & x, c10::optional<int64_t> N, bool increasing) {
    
    static auto op = create_vander_typed_handle();
    return op.call(x, N, increasing);
}

// aten::vander(Tensor x, int? N=None, bool increasing=False) -> Tensor
at::Tensor vander::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, c10::optional<int64_t> N, bool increasing) {
    
    static auto op = create_vander_typed_handle();
    return op.redispatch(dispatchKeySet, x, N, increasing);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as, name, "aten::view_as")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as, schema_str, "view_as(Tensor(a) self, Tensor other) -> Tensor(a)")

// aten::view_as(Tensor(a) self, Tensor other) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<view_as::schema> create_view_as_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(view_as::name, view_as::overload_name)
      .typed<view_as::schema>();
}

// aten::view_as(Tensor(a) self, Tensor other) -> Tensor(a)
at::Tensor view_as::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_view_as_typed_handle();
    return op.call(self, other);
}

// aten::view_as(Tensor(a) self, Tensor other) -> Tensor(a)
at::Tensor view_as::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_view_as_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dirichlet_grad, name, "aten::_dirichlet_grad")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dirichlet_grad, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dirichlet_grad, schema_str, "_dirichlet_grad(Tensor x, Tensor alpha, Tensor total) -> Tensor")

// aten::_dirichlet_grad(Tensor x, Tensor alpha, Tensor total) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_dirichlet_grad::schema> create__dirichlet_grad_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_dirichlet_grad::name, _dirichlet_grad::overload_name)
      .typed<_dirichlet_grad::schema>();
}

// aten::_dirichlet_grad(Tensor x, Tensor alpha, Tensor total) -> Tensor
at::Tensor _dirichlet_grad::call(const at::Tensor & x, const at::Tensor & alpha, const at::Tensor & total) {
    
    static auto op = create__dirichlet_grad_typed_handle();
    return op.call(x, alpha, total);
}

// aten::_dirichlet_grad(Tensor x, Tensor alpha, Tensor total) -> Tensor
at::Tensor _dirichlet_grad::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & alpha, const at::Tensor & total) {
    
    static auto op = create__dirichlet_grad_typed_handle();
    return op.redispatch(dispatchKeySet, x, alpha, total);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm_dim, name, "aten::frobenius_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm_dim, schema_str, "frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -> Tensor")

// aten::frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<frobenius_norm_dim::schema> create_frobenius_norm_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(frobenius_norm_dim::name, frobenius_norm_dim::overload_name)
      .typed<frobenius_norm_dim::schema>();
}

// aten::frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
at::Tensor frobenius_norm_dim::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    
    static auto op = create_frobenius_norm_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
at::Tensor frobenius_norm_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    
    static auto op = create_frobenius_norm_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm_out, name, "aten::frobenius_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm_out, schema_str, "frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<frobenius_norm_out::schema> create_frobenius_norm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(frobenius_norm_out::name, frobenius_norm_out::overload_name)
      .typed<frobenius_norm_out::schema>();
}

// aten::frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & frobenius_norm_out::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    
    static auto op = create_frobenius_norm_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & frobenius_norm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    
    static auto op = create_frobenius_norm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clone, name, "aten::clone")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clone, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clone, schema_str, "clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor")

// aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<clone::schema> create_clone_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clone::name, clone::overload_name)
      .typed<clone::schema>();
}

// aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor
at::Tensor clone::call(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_clone_typed_handle();
    return op.call(self, memory_format);
}

// aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor
at::Tensor clone::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_clone_typed_handle();
    return op.redispatch(dispatchKeySet, self, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(positive, name, "aten::positive")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(positive, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(positive, schema_str, "positive(Tensor(a) self) -> Tensor(a)")

// aten::positive(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<positive::schema> create_positive_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(positive::name, positive::overload_name)
      .typed<positive::schema>();
}

// aten::positive(Tensor(a) self) -> Tensor(a)
at::Tensor positive::call(const at::Tensor & self) {
    
    static auto op = create_positive_typed_handle();
    return op.call(self);
}

// aten::positive(Tensor(a) self) -> Tensor(a)
at::Tensor positive::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_positive_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_, name, "aten::resize_as_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_, schema_str, "resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)")

// aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<resize_as_::schema> create_resize_as__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(resize_as_::name, resize_as_::overload_name)
      .typed<resize_as_::schema>();
}

// aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)
const at::Tensor & resize_as_::call(const at::Tensor & self, const at::Tensor & the_template, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_resize_as__typed_handle();
    return op.call(self, the_template, memory_format);
}

// aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)
const at::Tensor & resize_as_::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & the_template, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_resize_as__typed_handle();
    return op.redispatch(dispatchKeySet, self, the_template, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_sparse_, name, "aten::resize_as_sparse_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_sparse_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_sparse_, schema_str, "resize_as_sparse_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)")

// aten::resize_as_sparse_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<resize_as_sparse_::schema> create_resize_as_sparse__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(resize_as_sparse_::name, resize_as_sparse_::overload_name)
      .typed<resize_as_sparse_::schema>();
}

// aten::resize_as_sparse_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)
const at::Tensor & resize_as_sparse_::call(const at::Tensor & self, const at::Tensor & the_template) {
    
    static auto op = create_resize_as_sparse__typed_handle();
    return op.call(self, the_template);
}

// aten::resize_as_sparse_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)
const at::Tensor & resize_as_sparse_::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & the_template) {
    
    static auto op = create_resize_as_sparse__typed_handle();
    return op.redispatch(dispatchKeySet, self, the_template);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_sampled_addmm_out, name, "aten::sparse_sampled_addmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_sampled_addmm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_sampled_addmm_out, schema_str, "sparse_sampled_addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::sparse_sampled_addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sparse_sampled_addmm_out::schema> create_sparse_sampled_addmm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sparse_sampled_addmm_out::name, sparse_sampled_addmm_out::overload_name)
      .typed<sparse_sampled_addmm_out::schema>();
}

// aten::sparse_sampled_addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sparse_sampled_addmm_out::call(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create_sparse_sampled_addmm_out_typed_handle();
    return op.call(self, mat1, mat2, beta, alpha, out);
}

// aten::sparse_sampled_addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sparse_sampled_addmm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    
    static auto op = create_sparse_sampled_addmm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat1, mat2, beta, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_sampled_addmm, name, "aten::sparse_sampled_addmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_sampled_addmm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_sampled_addmm, schema_str, "sparse_sampled_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor")

// aten::sparse_sampled_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sparse_sampled_addmm::schema> create_sparse_sampled_addmm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sparse_sampled_addmm::name, sparse_sampled_addmm::overload_name)
      .typed<sparse_sampled_addmm::schema>();
}

// aten::sparse_sampled_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor sparse_sampled_addmm::call(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
    
    static auto op = create_sparse_sampled_addmm_typed_handle();
    return op.call(self, mat1, mat2, beta, alpha);
}

// aten::sparse_sampled_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor sparse_sampled_addmm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
    
    static auto op = create_sparse_sampled_addmm_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat1, mat2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_csr_tensor_crow_col_value_size, name, "aten::sparse_csr_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_csr_tensor_crow_col_value_size, overload_name, "crow_col_value_size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_csr_tensor_crow_col_value_size, schema_str, "sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor")

// aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sparse_csr_tensor_crow_col_value_size::schema> create_sparse_csr_tensor_crow_col_value_size_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sparse_csr_tensor_crow_col_value_size::name, sparse_csr_tensor_crow_col_value_size::overload_name)
      .typed<sparse_csr_tensor_crow_col_value_size::schema>();
}

// aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor sparse_csr_tensor_crow_col_value_size::call(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_sparse_csr_tensor_crow_col_value_size_typed_handle();
    return op.call(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}

// aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor sparse_csr_tensor_crow_col_value_size::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_sparse_csr_tensor_crow_col_value_size_typed_handle();
    return op.redispatch(dispatchKeySet, crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_csr_tensor_crow_col_value, name, "aten::sparse_csr_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_csr_tensor_crow_col_value, overload_name, "crow_col_value")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_csr_tensor_crow_col_value, schema_str, "sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor")

// aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sparse_csr_tensor_crow_col_value::schema> create_sparse_csr_tensor_crow_col_value_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sparse_csr_tensor_crow_col_value::name, sparse_csr_tensor_crow_col_value::overload_name)
      .typed<sparse_csr_tensor_crow_col_value::schema>();
}

// aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor sparse_csr_tensor_crow_col_value::call(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_sparse_csr_tensor_crow_col_value_typed_handle();
    return op.call(crow_indices, col_indices, values, dtype, layout, device, pin_memory);
}

// aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor sparse_csr_tensor_crow_col_value::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_sparse_csr_tensor_crow_col_value_typed_handle();
    return op.redispatch(dispatchKeySet, crow_indices, col_indices, values, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_bsc_tensor_unsafe, name, "aten::_sparse_bsc_tensor_unsafe")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_bsc_tensor_unsafe, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_bsc_tensor_unsafe, schema_str, "_sparse_bsc_tensor_unsafe(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::_sparse_bsc_tensor_unsafe(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_bsc_tensor_unsafe::schema> create__sparse_bsc_tensor_unsafe_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_bsc_tensor_unsafe::name, _sparse_bsc_tensor_unsafe::overload_name)
      .typed<_sparse_bsc_tensor_unsafe::schema>();
}

// aten::_sparse_bsc_tensor_unsafe(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor _sparse_bsc_tensor_unsafe::call(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create__sparse_bsc_tensor_unsafe_typed_handle();
    return op.call(ccol_indices, row_indices, values, size, dtype, layout, device, pin_memory);
}

// aten::_sparse_bsc_tensor_unsafe(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor _sparse_bsc_tensor_unsafe::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create__sparse_bsc_tensor_unsafe_typed_handle();
    return op.redispatch(dispatchKeySet, ccol_indices, row_indices, values, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dense_dim, name, "aten::dense_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dense_dim, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dense_dim, schema_str, "dense_dim(Tensor self) -> int")

// aten::dense_dim(Tensor self) -> int
static C10_NOINLINE c10::TypedOperatorHandle<dense_dim::schema> create_dense_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dense_dim::name, dense_dim::overload_name)
      .typed<dense_dim::schema>();
}

// aten::dense_dim(Tensor self) -> int
int64_t dense_dim::call(const at::Tensor & self) {
    
    static auto op = create_dense_dim_typed_handle();
    return op.call(self);
}

// aten::dense_dim(Tensor self) -> int
int64_t dense_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_dense_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dimV, name, "aten::_dimV")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dimV, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dimV, schema_str, "_dimV(Tensor self) -> int")

// aten::_dimV(Tensor self) -> int
static C10_NOINLINE c10::TypedOperatorHandle<_dimV::schema> create__dimV_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_dimV::name, _dimV::overload_name)
      .typed<_dimV::schema>();
}

// aten::_dimV(Tensor self) -> int
int64_t _dimV::call(const at::Tensor & self) {
    
    static auto op = create__dimV_typed_handle();
    return op.call(self);
}

// aten::_dimV(Tensor self) -> int
int64_t _dimV::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create__dimV_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(coalesce, name, "aten::coalesce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(coalesce, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(coalesce, schema_str, "coalesce(Tensor(a) self) -> Tensor(a)")

// aten::coalesce(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<coalesce::schema> create_coalesce_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(coalesce::name, coalesce::overload_name)
      .typed<coalesce::schema>();
}

// aten::coalesce(Tensor(a) self) -> Tensor(a)
at::Tensor coalesce::call(const at::Tensor & self) {
    
    static auto op = create_coalesce_typed_handle();
    return op.call(self);
}

// aten::coalesce(Tensor(a) self) -> Tensor(a)
at::Tensor coalesce::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_coalesce_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_indices, name, "aten::_indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_indices, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_indices, schema_str, "_indices(Tensor(a) self) -> Tensor(a)")

// aten::_indices(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<_indices::schema> create__indices_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_indices::name, _indices::overload_name)
      .typed<_indices::schema>();
}

// aten::_indices(Tensor(a) self) -> Tensor(a)
at::Tensor _indices::call(const at::Tensor & self) {
    
    static auto op = create__indices_typed_handle();
    return op.call(self);
}

// aten::_indices(Tensor(a) self) -> Tensor(a)
at::Tensor _indices::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create__indices_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_sparse_csc, name, "aten::to_sparse_csc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_sparse_csc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_sparse_csc, schema_str, "to_sparse_csc(Tensor self, int? dense_dim=None) -> Tensor")

// aten::to_sparse_csc(Tensor self, int? dense_dim=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<to_sparse_csc::schema> create_to_sparse_csc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_sparse_csc::name, to_sparse_csc::overload_name)
      .typed<to_sparse_csc::schema>();
}

// aten::to_sparse_csc(Tensor self, int? dense_dim=None) -> Tensor
at::Tensor to_sparse_csc::call(const at::Tensor & self, c10::optional<int64_t> dense_dim) {
    
    static auto op = create_to_sparse_csc_typed_handle();
    return op.call(self, dense_dim);
}

// aten::to_sparse_csc(Tensor self, int? dense_dim=None) -> Tensor
at::Tensor to_sparse_csc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dense_dim) {
    
    static auto op = create_to_sparse_csc_typed_handle();
    return op.redispatch(dispatchKeySet, self, dense_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_csc, name, "aten::_to_sparse_csc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_csc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_csc, schema_str, "_to_sparse_csc(Tensor self, int? dense_dim=None) -> Tensor")

// aten::_to_sparse_csc(Tensor self, int? dense_dim=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_to_sparse_csc::schema> create__to_sparse_csc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_to_sparse_csc::name, _to_sparse_csc::overload_name)
      .typed<_to_sparse_csc::schema>();
}

// aten::_to_sparse_csc(Tensor self, int? dense_dim=None) -> Tensor
at::Tensor _to_sparse_csc::call(const at::Tensor & self, c10::optional<int64_t> dense_dim) {
    
    static auto op = create__to_sparse_csc_typed_handle();
    return op.call(self, dense_dim);
}

// aten::_to_sparse_csc(Tensor self, int? dense_dim=None) -> Tensor
at::Tensor _to_sparse_csc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dense_dim) {
    
    static auto op = create__to_sparse_csc_typed_handle();
    return op.redispatch(dispatchKeySet, self, dense_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_bsc, name, "aten::_to_sparse_bsc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_bsc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_bsc, schema_str, "_to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -> Tensor")

// aten::_to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_to_sparse_bsc::schema> create__to_sparse_bsc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_to_sparse_bsc::name, _to_sparse_bsc::overload_name)
      .typed<_to_sparse_bsc::schema>();
}

// aten::_to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -> Tensor
at::Tensor _to_sparse_bsc::call(const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
    
    static auto op = create__to_sparse_bsc_typed_handle();
    return op.call(self, blocksize, dense_dim);
}

// aten::_to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -> Tensor
at::Tensor _to_sparse_bsc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
    
    static auto op = create__to_sparse_bsc_typed_handle();
    return op.redispatch(dispatchKeySet, self, blocksize, dense_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_reorder_conv2d_weight, name, "aten::mkldnn_reorder_conv2d_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_reorder_conv2d_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_reorder_conv2d_weight, schema_str, "mkldnn_reorder_conv2d_weight(Tensor self, SymInt[2] padding=0, SymInt[2] stride=1, SymInt[2] dilation=1, SymInt groups=1, SymInt[]? input_size=None) -> Tensor")

// aten::mkldnn_reorder_conv2d_weight(Tensor self, SymInt[2] padding=0, SymInt[2] stride=1, SymInt[2] dilation=1, SymInt groups=1, SymInt[]? input_size=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_reorder_conv2d_weight::schema> create_mkldnn_reorder_conv2d_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_reorder_conv2d_weight::name, mkldnn_reorder_conv2d_weight::overload_name)
      .typed<mkldnn_reorder_conv2d_weight::schema>();
}

// aten::mkldnn_reorder_conv2d_weight(Tensor self, SymInt[2] padding=0, SymInt[2] stride=1, SymInt[2] dilation=1, SymInt groups=1, SymInt[]? input_size=None) -> Tensor
at::Tensor mkldnn_reorder_conv2d_weight::call(const at::Tensor & self, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::OptionalSymIntArrayRef input_size) {
    
    static auto op = create_mkldnn_reorder_conv2d_weight_typed_handle();
    return op.call(self, padding, stride, dilation, groups, input_size);
}

// aten::mkldnn_reorder_conv2d_weight(Tensor self, SymInt[2] padding=0, SymInt[2] stride=1, SymInt[2] dilation=1, SymInt groups=1, SymInt[]? input_size=None) -> Tensor
at::Tensor mkldnn_reorder_conv2d_weight::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::OptionalSymIntArrayRef input_size) {
    
    static auto op = create_mkldnn_reorder_conv2d_weight_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding, stride, dilation, groups, input_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_channel, name, "aten::quantize_per_channel")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_channel, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_channel, schema_str, "quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor")

// aten::quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantize_per_channel::schema> create_quantize_per_channel_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantize_per_channel::name, quantize_per_channel::overload_name)
      .typed<quantize_per_channel::schema>();
}

// aten::quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor
at::Tensor quantize_per_channel::call(const at::Tensor & self, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, at::ScalarType dtype) {
    
    static auto op = create_quantize_per_channel_typed_handle();
    return op.call(self, scales, zero_points, axis, dtype);
}

// aten::quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor
at::Tensor quantize_per_channel::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, at::ScalarType dtype) {
    
    static auto op = create_quantize_per_channel_typed_handle();
    return op.redispatch(dispatchKeySet, self, scales, zero_points, axis, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_self, name, "aten::dequantize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_self, overload_name, "self")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_self, schema_str, "dequantize.self(Tensor self) -> Tensor")

// aten::dequantize.self(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<dequantize_self::schema> create_dequantize_self_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dequantize_self::name, dequantize_self::overload_name)
      .typed<dequantize_self::schema>();
}

// aten::dequantize.self(Tensor self) -> Tensor
at::Tensor dequantize_self::call(const at::Tensor & self) {
    
    static auto op = create_dequantize_self_typed_handle();
    return op.call(self);
}

// aten::dequantize.self(Tensor self) -> Tensor
at::Tensor dequantize_self::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_dequantize_self_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_tensors, name, "aten::dequantize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_tensors, overload_name, "tensors")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_tensors, schema_str, "dequantize.tensors(Tensor[] tensors) -> Tensor[]")

// aten::dequantize.tensors(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<dequantize_tensors::schema> create_dequantize_tensors_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dequantize_tensors::name, dequantize_tensors::overload_name)
      .typed<dequantize_tensors::schema>();
}

// aten::dequantize.tensors(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> dequantize_tensors::call(at::TensorList tensors) {
    
    static auto op = create_dequantize_tensors_typed_handle();
    return op.call(tensors);
}

// aten::dequantize.tensors(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> dequantize_tensors::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    
    static auto op = create_dequantize_tensors_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_zero_points, name, "aten::q_per_channel_zero_points")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_zero_points, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_zero_points, schema_str, "q_per_channel_zero_points(Tensor self) -> Tensor")

// aten::q_per_channel_zero_points(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<q_per_channel_zero_points::schema> create_q_per_channel_zero_points_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(q_per_channel_zero_points::name, q_per_channel_zero_points::overload_name)
      .typed<q_per_channel_zero_points::schema>();
}

// aten::q_per_channel_zero_points(Tensor self) -> Tensor
at::Tensor q_per_channel_zero_points::call(const at::Tensor & self) {
    
    static auto op = create_q_per_channel_zero_points_typed_handle();
    return op.call(self);
}

// aten::q_per_channel_zero_points(Tensor self) -> Tensor
at::Tensor q_per_channel_zero_points::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_q_per_channel_zero_points_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine, name, "aten::fake_quantize_per_tensor_affine")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine, schema_str, "fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> Tensor")

// aten::fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fake_quantize_per_tensor_affine::schema> create_fake_quantize_per_tensor_affine_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fake_quantize_per_tensor_affine::name, fake_quantize_per_tensor_affine::overload_name)
      .typed<fake_quantize_per_tensor_affine::schema>();
}

// aten::fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> Tensor
at::Tensor fake_quantize_per_tensor_affine::call(const at::Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
    
    static auto op = create_fake_quantize_per_tensor_affine_typed_handle();
    return op.call(self, scale, zero_point, quant_min, quant_max);
}

// aten::fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> Tensor
at::Tensor fake_quantize_per_tensor_affine::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
    
    static auto op = create_fake_quantize_per_tensor_affine_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, quant_min, quant_max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine_tensor_qparams, name, "aten::fake_quantize_per_tensor_affine")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine_tensor_qparams, overload_name, "tensor_qparams")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine_tensor_qparams, schema_str, "fake_quantize_per_tensor_affine.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -> Tensor")

// aten::fake_quantize_per_tensor_affine.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fake_quantize_per_tensor_affine_tensor_qparams::schema> create_fake_quantize_per_tensor_affine_tensor_qparams_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fake_quantize_per_tensor_affine_tensor_qparams::name, fake_quantize_per_tensor_affine_tensor_qparams::overload_name)
      .typed<fake_quantize_per_tensor_affine_tensor_qparams::schema>();
}

// aten::fake_quantize_per_tensor_affine.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -> Tensor
at::Tensor fake_quantize_per_tensor_affine_tensor_qparams::call(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max) {
    
    static auto op = create_fake_quantize_per_tensor_affine_tensor_qparams_typed_handle();
    return op.call(self, scale, zero_point, quant_min, quant_max);
}

// aten::fake_quantize_per_tensor_affine.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -> Tensor
at::Tensor fake_quantize_per_tensor_affine_tensor_qparams::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max) {
    
    static auto op = create_fake_quantize_per_tensor_affine_tensor_qparams_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, quant_min, quant_max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine, name, "aten::_fake_quantize_learnable_per_channel_affine")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine, schema_str, "_fake_quantize_learnable_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor")

// aten::_fake_quantize_learnable_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_fake_quantize_learnable_per_channel_affine::schema> create__fake_quantize_learnable_per_channel_affine_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fake_quantize_learnable_per_channel_affine::name, _fake_quantize_learnable_per_channel_affine::overload_name)
      .typed<_fake_quantize_learnable_per_channel_affine::schema>();
}

// aten::_fake_quantize_learnable_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor
at::Tensor _fake_quantize_learnable_per_channel_affine::call(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
    
    static auto op = create__fake_quantize_learnable_per_channel_affine_typed_handle();
    return op.call(self, scale, zero_point, axis, quant_min, quant_max, grad_factor);
}

// aten::_fake_quantize_learnable_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor
at::Tensor _fake_quantize_learnable_per_channel_affine::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
    
    static auto op = create__fake_quantize_learnable_per_channel_affine_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, axis, quant_min, quant_max, grad_factor);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_autocast_to_full_precision, name, "aten::_autocast_to_full_precision")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_autocast_to_full_precision, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_autocast_to_full_precision, schema_str, "_autocast_to_full_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled) -> Tensor(a)")

// aten::_autocast_to_full_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<_autocast_to_full_precision::schema> create__autocast_to_full_precision_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_autocast_to_full_precision::name, _autocast_to_full_precision::overload_name)
      .typed<_autocast_to_full_precision::schema>();
}

// aten::_autocast_to_full_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled) -> Tensor(a)
at::Tensor _autocast_to_full_precision::call(const at::Tensor & self, bool cuda_enabled, bool cpu_enabled) {
    
    static auto op = create__autocast_to_full_precision_typed_handle();
    return op.call(self, cuda_enabled, cpu_enabled);
}

// aten::_autocast_to_full_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled) -> Tensor(a)
at::Tensor _autocast_to_full_precision::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool cuda_enabled, bool cpu_enabled) {
    
    static auto op = create__autocast_to_full_precision_typed_handle();
    return op.redispatch(dispatchKeySet, self, cuda_enabled, cpu_enabled);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dtype_layout, name, "aten::to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dtype_layout, overload_name, "dtype_layout")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dtype_layout, schema_str, "to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)")

// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<to_dtype_layout::schema> create_to_dtype_layout_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_dtype_layout::name, to_dtype_layout::overload_name)
      .typed<to_dtype_layout::schema>();
}

// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_dtype_layout::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_to_dtype_layout_typed_handle();
    return op.call(self, dtype, layout, device, pin_memory, non_blocking, copy, memory_format);
}

// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_dtype_layout::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_to_dtype_layout_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype, layout, device, pin_memory, non_blocking, copy, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_device, name, "aten::to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_device, overload_name, "device")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_device, schema_str, "to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)")

// aten::to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<to_device::schema> create_to_device_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_device::name, to_device::overload_name)
      .typed<to_device::schema>();
}

// aten::to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_device::call(const at::Tensor & self, at::Device device, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_to_device_typed_handle();
    return op.call(self, device, dtype, non_blocking, copy, memory_format);
}

// aten::to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_device::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Device device, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_to_device_typed_handle();
    return op.redispatch(dispatchKeySet, self, device, dtype, non_blocking, copy, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dtype, name, "aten::to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dtype, overload_name, "dtype")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dtype, schema_str, "to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)")

// aten::to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<to_dtype::schema> create_to_dtype_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_dtype::name, to_dtype::overload_name)
      .typed<to_dtype::schema>();
}

// aten::to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_dtype::call(const at::Tensor & self, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_to_dtype_typed_handle();
    return op.call(self, dtype, non_blocking, copy, memory_format);
}

// aten::to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_dtype::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_to_dtype_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype, non_blocking, copy, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_other, name, "aten::to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_other, overload_name, "other")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_other, schema_str, "to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)")

// aten::to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<to_other::schema> create_to_other_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_other::name, to_other::overload_name)
      .typed<to_other::schema>();
}

// aten::to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_other::call(const at::Tensor & self, const at::Tensor & other, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_to_other_typed_handle();
    return op.call(self, other, non_blocking, copy, memory_format);
}

// aten::to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_other::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_to_other_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, non_blocking, copy, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(combinations, name, "aten::combinations")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(combinations, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(combinations, schema_str, "combinations(Tensor self, int r=2, bool with_replacement=False) -> Tensor")

// aten::combinations(Tensor self, int r=2, bool with_replacement=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<combinations::schema> create_combinations_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(combinations::name, combinations::overload_name)
      .typed<combinations::schema>();
}

// aten::combinations(Tensor self, int r=2, bool with_replacement=False) -> Tensor
at::Tensor combinations::call(const at::Tensor & self, int64_t r, bool with_replacement) {
    
    static auto op = create_combinations_typed_handle();
    return op.call(self, r, with_replacement);
}

// aten::combinations(Tensor self, int r=2, bool with_replacement=False) -> Tensor
at::Tensor combinations::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t r, bool with_replacement) {
    
    static auto op = create_combinations_typed_handle();
    return op.redispatch(dispatchKeySet, self, r, with_replacement);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(item, name, "aten::item")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(item, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(item, schema_str, "item(Tensor self) -> Scalar")

// aten::item(Tensor self) -> Scalar
static C10_NOINLINE c10::TypedOperatorHandle<item::schema> create_item_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(item::name, item::overload_name)
      .typed<item::schema>();
}

// aten::item(Tensor self) -> Scalar
at::Scalar item::call(const at::Tensor & self) {
    
    static auto op = create_item_typed_handle();
    return op.call(self);
}

// aten::item(Tensor self) -> Scalar
at::Scalar item::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_item_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_lstm_mps, name, "aten::_lstm_mps")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_lstm_mps, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_lstm_mps, schema_str, "_lstm_mps(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)")

// aten::_lstm_mps(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_lstm_mps::schema> create__lstm_mps_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_lstm_mps::name, _lstm_mps::overload_name)
      .typed<_lstm_mps::schema>();
}

// aten::_lstm_mps(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _lstm_mps::call(const at::Tensor & input, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    
    static auto op = create__lstm_mps_typed_handle();
    return op.call(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

// aten::_lstm_mps(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _lstm_mps::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    
    static auto op = create__lstm_mps_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell, name, "aten::_thnn_fused_lstm_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell, schema_str, "_thnn_fused_lstm_cell(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor, Tensor)")

// aten::_thnn_fused_lstm_cell(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_thnn_fused_lstm_cell::schema> create__thnn_fused_lstm_cell_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_thnn_fused_lstm_cell::name, _thnn_fused_lstm_cell::overload_name)
      .typed<_thnn_fused_lstm_cell::schema>();
}

// aten::_thnn_fused_lstm_cell(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _thnn_fused_lstm_cell::call(const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & cx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
    
    static auto op = create__thnn_fused_lstm_cell_typed_handle();
    return op.call(input_gates, hidden_gates, cx, input_bias, hidden_bias);
}

// aten::_thnn_fused_lstm_cell(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _thnn_fused_lstm_cell::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & cx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
    
    static auto op = create__thnn_fused_lstm_cell_typed_handle();
    return op.redispatch(dispatchKeySet, input_gates, hidden_gates, cx, input_bias, hidden_bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_input, name, "aten::lstm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_input, overload_name, "input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_input, schema_str, "lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)")

// aten::lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<lstm_input::schema> create_lstm_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lstm_input::name, lstm_input::overload_name)
      .typed<lstm_input::schema>();
}

// aten::lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> lstm_input::call(const at::Tensor & input, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    
    static auto op = create_lstm_input_typed_handle();
    return op.call(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

// aten::lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> lstm_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    
    static auto op = create_lstm_input_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_data, name, "aten::lstm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_data, overload_name, "data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_data, schema_str, "lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)")

// aten::lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<lstm_data::schema> create_lstm_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lstm_data::name, lstm_data::overload_name)
      .typed<lstm_data::schema>();
}

// aten::lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> lstm_data::call(const at::Tensor & data, const at::Tensor & batch_sizes, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    
    static auto op = create_lstm_data_typed_handle();
    return op.call(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

// aten::lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> lstm_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, const at::Tensor & batch_sizes, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    
    static auto op = create_lstm_data_typed_handle();
    return op.redispatch(dispatchKeySet, data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_input, name, "aten::gru")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_input, overload_name, "input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_input, schema_str, "gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)")

// aten::gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<gru_input::schema> create_gru_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gru_input::name, gru_input::overload_name)
      .typed<gru_input::schema>();
}

// aten::gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> gru_input::call(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    
    static auto op = create_gru_input_typed_handle();
    return op.call(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

// aten::gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> gru_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    
    static auto op = create_gru_input_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_data, name, "aten::gru")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_data, overload_name, "data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_data, schema_str, "gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)")

// aten::gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<gru_data::schema> create_gru_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gru_data::name, gru_data::overload_name)
      .typed<gru_data::schema>();
}

// aten::gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> gru_data::call(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    
    static auto op = create_gru_data_typed_handle();
    return op.call(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

// aten::gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> gru_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    
    static auto op = create_gru_data_typed_handle();
    return op.redispatch(dispatchKeySet, data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_input, name, "aten::rnn_tanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_input, overload_name, "input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_input, schema_str, "rnn_tanh.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)")

// aten::rnn_tanh.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<rnn_tanh_input::schema> create_rnn_tanh_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rnn_tanh_input::name, rnn_tanh_input::overload_name)
      .typed<rnn_tanh_input::schema>();
}

// aten::rnn_tanh.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> rnn_tanh_input::call(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    
    static auto op = create_rnn_tanh_input_typed_handle();
    return op.call(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

// aten::rnn_tanh.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> rnn_tanh_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    
    static auto op = create_rnn_tanh_input_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_data, name, "aten::rnn_tanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_data, overload_name, "data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_data, schema_str, "rnn_tanh.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)")

// aten::rnn_tanh.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<rnn_tanh_data::schema> create_rnn_tanh_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rnn_tanh_data::name, rnn_tanh_data::overload_name)
      .typed<rnn_tanh_data::schema>();
}

// aten::rnn_tanh.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> rnn_tanh_data::call(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    
    static auto op = create_rnn_tanh_data_typed_handle();
    return op.call(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

// aten::rnn_tanh.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> rnn_tanh_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    
    static auto op = create_rnn_tanh_data_typed_handle();
    return op.redispatch(dispatchKeySet, data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_relu_cell, name, "aten::rnn_relu_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_relu_cell, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_relu_cell, schema_str, "rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor")

// aten::rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rnn_relu_cell::schema> create_rnn_relu_cell_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rnn_relu_cell::name, rnn_relu_cell::overload_name)
      .typed<rnn_relu_cell::schema>();
}

// aten::rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
at::Tensor rnn_relu_cell::call(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    
    static auto op = create_rnn_relu_cell_typed_handle();
    return op.call(input, hx, w_ih, w_hh, b_ih, b_hh);
}

// aten::rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
at::Tensor rnn_relu_cell::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    
    static auto op = create_rnn_relu_cell_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pad_packed_sequence, name, "aten::_pad_packed_sequence")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pad_packed_sequence, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pad_packed_sequence, schema_str, "_pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)")

// aten::_pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_pad_packed_sequence::schema> create__pad_packed_sequence_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_pad_packed_sequence::name, _pad_packed_sequence::overload_name)
      .typed<_pad_packed_sequence::schema>();
}

// aten::_pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _pad_packed_sequence::call(const at::Tensor & data, const at::Tensor & batch_sizes, bool batch_first, const at::Scalar & padding_value, int64_t total_length) {
    
    static auto op = create__pad_packed_sequence_typed_handle();
    return op.call(data, batch_sizes, batch_first, padding_value, total_length);
}

// aten::_pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _pad_packed_sequence::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, const at::Tensor & batch_sizes, bool batch_first, const at::Scalar & padding_value, int64_t total_length) {
    
    static auto op = create__pad_packed_sequence_typed_handle();
    return op.redispatch(dispatchKeySet, data, batch_sizes, batch_first, padding_value, total_length);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lift_fresh_copy, name, "aten::lift_fresh_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lift_fresh_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lift_fresh_copy, schema_str, "lift_fresh_copy(Tensor self) -> Tensor")

// aten::lift_fresh_copy(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<lift_fresh_copy::schema> create_lift_fresh_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lift_fresh_copy::name, lift_fresh_copy::overload_name)
      .typed<lift_fresh_copy::schema>();
}

// aten::lift_fresh_copy(Tensor self) -> Tensor
at::Tensor lift_fresh_copy::call(const at::Tensor & self) {
    
    static auto op = create_lift_fresh_copy_typed_handle();
    return op.call(self);
}

// aten::lift_fresh_copy(Tensor self) -> Tensor
at::Tensor lift_fresh_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_lift_fresh_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_reduce_out, name, "aten::index_reduce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_reduce_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_reduce_out, schema_str, "index_reduce.out(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True, Tensor(a!) out) -> Tensor(a!)")

// aten::index_reduce.out(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_reduce_out::schema> create_index_reduce_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_reduce_out::name, index_reduce_out::overload_name)
      .typed<index_reduce_out::schema>();
}

// aten::index_reduce.out(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True, Tensor(a!) out) -> Tensor(a!)
at::Tensor & index_reduce_out::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, c10::string_view reduce, bool include_self, at::Tensor & out) {
    
    static auto op = create_index_reduce_out_typed_handle();
    return op.call(self, dim, index, source, reduce, include_self, out);
}

// aten::index_reduce.out(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True, Tensor(a!) out) -> Tensor(a!)
at::Tensor & index_reduce_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, c10::string_view reduce, bool include_self, at::Tensor & out) {
    
    static auto op = create_index_reduce_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, source, reduce, include_self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_reduce_, name, "aten::index_reduce_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_reduce_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_reduce_, schema_str, "index_reduce_(Tensor(a!) self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor(a!)")

// aten::index_reduce_(Tensor(a!) self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_reduce_::schema> create_index_reduce__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_reduce_::name, index_reduce_::overload_name)
      .typed<index_reduce_::schema>();
}

// aten::index_reduce_(Tensor(a!) self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor(a!)
at::Tensor & index_reduce_::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, c10::string_view reduce, bool include_self) {
    
    static auto op = create_index_reduce__typed_handle();
    return op.call(self, dim, index, source, reduce, include_self);
}

// aten::index_reduce_(Tensor(a!) self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor(a!)
at::Tensor & index_reduce_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, c10::string_view reduce, bool include_self) {
    
    static auto op = create_index_reduce__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, source, reduce, include_self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_reduce, name, "aten::index_reduce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_reduce, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_reduce, schema_str, "index_reduce(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor")

// aten::index_reduce(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_reduce::schema> create_index_reduce_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_reduce::name, index_reduce::overload_name)
      .typed<index_reduce::schema>();
}

// aten::index_reduce(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor
at::Tensor index_reduce::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, c10::string_view reduce, bool include_self) {
    
    static auto op = create_index_reduce_typed_handle();
    return op.call(self, dim, index, source, reduce, include_self);
}

// aten::index_reduce(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor
at::Tensor index_reduce::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, c10::string_view reduce, bool include_self) {
    
    static auto op = create_index_reduce_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, source, reduce, include_self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__int_Scalar, name, "aten::index_fill_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__int_Scalar, overload_name, "int_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__int_Scalar, schema_str, "index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)")

// aten::index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_fill__int_Scalar::schema> create_index_fill__int_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill__int_Scalar::name, index_fill__int_Scalar::overload_name)
      .typed<index_fill__int_Scalar::schema>();
}

// aten::index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)
at::Tensor & index_fill__int_Scalar::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    
    static auto op = create_index_fill__int_Scalar_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)
at::Tensor & index_fill__int_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    
    static auto op = create_index_fill__int_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Scalar, name, "aten::index_fill")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Scalar, overload_name, "int_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Scalar, schema_str, "index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor")

// aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_fill_int_Scalar::schema> create_index_fill_int_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill_int_Scalar::name, index_fill_int_Scalar::overload_name)
      .typed<index_fill_int_Scalar::schema>();
}

// aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
at::Tensor index_fill_int_Scalar::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    
    static auto op = create_index_fill_int_Scalar_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
at::Tensor index_fill_int_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    
    static auto op = create_index_fill_int_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__int_Tensor, name, "aten::index_fill_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__int_Tensor, overload_name, "int_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__int_Tensor, schema_str, "index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)")

// aten::index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_fill__int_Tensor::schema> create_index_fill__int_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill__int_Tensor::name, index_fill__int_Tensor::overload_name)
      .typed<index_fill__int_Tensor::schema>();
}

// aten::index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)
at::Tensor & index_fill__int_Tensor::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
    
    static auto op = create_index_fill__int_Tensor_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)
at::Tensor & index_fill__int_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
    
    static auto op = create_index_fill__int_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Tensor, name, "aten::index_fill")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Tensor, overload_name, "int_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Tensor, schema_str, "index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor")

// aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_fill_int_Tensor::schema> create_index_fill_int_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill_int_Tensor::name, index_fill_int_Tensor::overload_name)
      .typed<index_fill_int_Tensor::schema>();
}

// aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor
at::Tensor index_fill_int_Tensor::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
    
    static auto op = create_index_fill_int_Tensor_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor
at::Tensor index_fill_int_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
    
    static auto op = create_index_fill_int_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__Dimname_Scalar, name, "aten::index_fill_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__Dimname_Scalar, overload_name, "Dimname_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__Dimname_Scalar, schema_str, "index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -> Tensor(a!)")

// aten::index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_fill__Dimname_Scalar::schema> create_index_fill__Dimname_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill__Dimname_Scalar::name, index_fill__Dimname_Scalar::overload_name)
      .typed<index_fill__Dimname_Scalar::schema>();
}

// aten::index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -> Tensor(a!)
at::Tensor & index_fill__Dimname_Scalar::call(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    
    static auto op = create_index_fill__Dimname_Scalar_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -> Tensor(a!)
at::Tensor & index_fill__Dimname_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    
    static auto op = create_index_fill__Dimname_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__Dimname_Tensor, name, "aten::index_fill_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__Dimname_Tensor, overload_name, "Dimname_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__Dimname_Tensor, schema_str, "index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -> Tensor(a!)")

// aten::index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_fill__Dimname_Tensor::schema> create_index_fill__Dimname_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill__Dimname_Tensor::name, index_fill__Dimname_Tensor::overload_name)
      .typed<index_fill__Dimname_Tensor::schema>();
}

// aten::index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -> Tensor(a!)
at::Tensor & index_fill__Dimname_Tensor::call(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
    
    static auto op = create_index_fill__Dimname_Tensor_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -> Tensor(a!)
at::Tensor & index_fill__Dimname_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
    
    static auto op = create_index_fill__Dimname_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_Dimname_Scalar, name, "aten::index_fill")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_Dimname_Scalar, overload_name, "Dimname_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_Dimname_Scalar, schema_str, "index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor")

// aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_fill_Dimname_Scalar::schema> create_index_fill_Dimname_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill_Dimname_Scalar::name, index_fill_Dimname_Scalar::overload_name)
      .typed<index_fill_Dimname_Scalar::schema>();
}

// aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
at::Tensor index_fill_Dimname_Scalar::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    
    static auto op = create_index_fill_Dimname_Scalar_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
at::Tensor index_fill_Dimname_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    
    static auto op = create_index_fill_Dimname_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_Dimname_Tensor, name, "aten::index_fill")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_Dimname_Tensor, overload_name, "Dimname_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_Dimname_Tensor, schema_str, "index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -> Tensor")

// aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_fill_Dimname_Tensor::schema> create_index_fill_Dimname_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill_Dimname_Tensor::name, index_fill_Dimname_Tensor::overload_name)
      .typed<index_fill_Dimname_Tensor::schema>();
}

// aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -> Tensor
at::Tensor index_fill_Dimname_Tensor::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
    
    static auto op = create_index_fill_Dimname_Tensor_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -> Tensor
at::Tensor index_fill_Dimname_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
    
    static auto op = create_index_fill_Dimname_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add, name, "aten::scatter_add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add, schema_str, "scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor")

// aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<scatter_add::schema> create_scatter_add_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_add::name, scatter_add::overload_name)
      .typed<scatter_add::schema>();
}

// aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
at::Tensor scatter_add::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    
    static auto op = create_scatter_add_typed_handle();
    return op.call(self, dim, index, src);
}

// aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
at::Tensor scatter_add::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    
    static auto op = create_scatter_add_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_, name, "aten::scatter_add_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_, schema_str, "scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)")

// aten::scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<scatter_add_::schema> create_scatter_add__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_add_::name, scatter_add_::overload_name)
      .typed<scatter_add_::schema>();
}

// aten::scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)
at::Tensor & scatter_add_::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    
    static auto op = create_scatter_add__typed_handle();
    return op.call(self, dim, index, src);
}

// aten::scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)
at::Tensor & scatter_add_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    
    static auto op = create_scatter_add__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_out, name, "aten::scatter_add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_out, schema_str, "scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)")

// aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<scatter_add_out::schema> create_scatter_add_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_add_out::name, scatter_add_out::overload_name)
      .typed<scatter_add_out::schema>();
}

// aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & scatter_add_out::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, at::Tensor & out) {
    
    static auto op = create_scatter_add_out_typed_handle();
    return op.call(self, dim, index, src, out);
}

// aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & scatter_add_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, at::Tensor & out) {
    
    static auto op = create_scatter_add_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_dimname, name, "aten::scatter_add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_dimname, schema_str, "scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor")

// aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<scatter_add_dimname::schema> create_scatter_add_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_add_dimname::name, scatter_add_dimname::overload_name)
      .typed<scatter_add_dimname::schema>();
}

// aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
at::Tensor scatter_add_dimname::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
    
    static auto op = create_scatter_add_dimname_typed_handle();
    return op.call(self, dim, index, src);
}

// aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
at::Tensor scatter_add_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
    
    static auto op = create_scatter_add_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma_, name, "aten::digamma_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma_, schema_str, "digamma_(Tensor(a!) self) -> Tensor(a!)")

// aten::digamma_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<digamma_::schema> create_digamma__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(digamma_::name, digamma_::overload_name)
      .typed<digamma_::schema>();
}

// aten::digamma_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & digamma_::call(at::Tensor & self) {
    
    static auto op = create_digamma__typed_handle();
    return op.call(self);
}

// aten::digamma_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & digamma_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    
    static auto op = create_digamma__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random__from, name, "aten::random_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random__from, overload_name, "from")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random__from, schema_str, "random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)")

// aten::random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<random__from::schema> create_random__from_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(random__from::name, random__from::overload_name)
      .typed<random__from::schema>();
}

// aten::random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & random__from::call(at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator) {
    
    static auto op = create_random__from_typed_handle();
    return op.call(self, from, to, generator);
}

// aten::random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & random__from::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator) {
    
    static auto op = create_random__from_typed_handle();
    return op.redispatch(dispatchKeySet, self, from, to, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random__to, name, "aten::random_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random__to, overload_name, "to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random__to, schema_str, "random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)")

// aten::random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<random__to::schema> create_random__to_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(random__to::name, random__to::overload_name)
      .typed<random__to::schema>();
}

// aten::random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & random__to::call(at::Tensor & self, int64_t to, c10::optional<at::Generator> generator) {
    
    static auto op = create_random__to_typed_handle();
    return op.call(self, to, generator);
}

// aten::random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & random__to::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t to, c10::optional<at::Generator> generator) {
    
    static auto op = create_random__to_typed_handle();
    return op.redispatch(dispatchKeySet, self, to, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_, name, "aten::random_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_, schema_str, "random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)")

// aten::random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<random_::schema> create_random__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(random_::name, random_::overload_name)
      .typed<random_::schema>();
}

// aten::random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & random_::call(at::Tensor & self, c10::optional<at::Generator> generator) {
    
    static auto op = create_random__typed_handle();
    return op.call(self, generator);
}

// aten::random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & random_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<at::Generator> generator) {
    
    static auto op = create_random__typed_handle();
    return op.redispatch(dispatchKeySet, self, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cauchy_, name, "aten::cauchy_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cauchy_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cauchy_, schema_str, "cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor(a!)")

// aten::cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cauchy_::schema> create_cauchy__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cauchy_::name, cauchy_::overload_name)
      .typed<cauchy_::schema>();
}

// aten::cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & cauchy_::call(at::Tensor & self, double median, double sigma, c10::optional<at::Generator> generator) {
    
    static auto op = create_cauchy__typed_handle();
    return op.call(self, median, sigma, generator);
}

// aten::cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & cauchy_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double median, double sigma, c10::optional<at::Generator> generator) {
    
    static auto op = create_cauchy__typed_handle();
    return op.redispatch(dispatchKeySet, self, median, sigma, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_normal_, name, "aten::log_normal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_normal_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_normal_, schema_str, "log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor(a!)")

// aten::log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log_normal_::schema> create_log_normal__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_normal_::name, log_normal_::overload_name)
      .typed<log_normal_::schema>();
}

// aten::log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & log_normal_::call(at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
    
    static auto op = create_log_normal__typed_handle();
    return op.call(self, mean, std, generator);
}

// aten::log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & log_normal_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
    
    static auto op = create_log_normal__typed_handle();
    return op.redispatch(dispatchKeySet, self, mean, std, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross_out, name, "aten::cross")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross_out, schema_str, "cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cross_out::schema> create_cross_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cross_out::name, cross_out::overload_name)
      .typed<cross_out::schema>();
}

// aten::cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cross_out::call(const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim, at::Tensor & out) {
    
    static auto op = create_cross_out_typed_handle();
    return op.call(self, other, dim, out);
}

// aten::cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cross_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim, at::Tensor & out) {
    
    static auto op = create_cross_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross, name, "aten::cross")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross, schema_str, "cross(Tensor self, Tensor other, int? dim=None) -> Tensor")

// aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cross::schema> create_cross_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cross::name, cross::overload_name)
      .typed<cross::schema>();
}

// aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor
at::Tensor cross::call(const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim) {
    
    static auto op = create_cross_typed_handle();
    return op.call(self, other, dim);
}

// aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor
at::Tensor cross::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim) {
    
    static auto op = create_cross_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Scalar_out, name, "aten::ne")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Scalar_out, schema_str, "ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ne_Scalar_out::schema> create_ne_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ne_Scalar_out::name, ne_Scalar_out::overload_name)
      .typed<ne_Scalar_out::schema>();
}

// aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ne_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_ne_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ne_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_ne_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Scalar, name, "aten::ne")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Scalar, schema_str, "ne.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::ne.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ne_Scalar::schema> create_ne_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ne_Scalar::name, ne_Scalar::overload_name)
      .typed<ne_Scalar::schema>();
}

// aten::ne.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor ne_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_ne_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::ne.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor ne_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_ne_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Tensor_out, name, "aten::ne")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Tensor_out, schema_str, "ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ne_Tensor_out::schema> create_ne_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ne_Tensor_out::name, ne_Tensor_out::overload_name)
      .typed<ne_Tensor_out::schema>();
}

// aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ne_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_ne_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ne_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_ne_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Tensor, name, "aten::ne")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Tensor, schema_str, "ne.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::ne.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ne_Tensor::schema> create_ne_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ne_Tensor::name, ne_Tensor::overload_name)
      .typed<ne_Tensor::schema>();
}

// aten::ne.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor ne_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_ne_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::ne.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor ne_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_ne_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne__Scalar, name, "aten::ne_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne__Scalar, schema_str, "ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ne__Scalar::schema> create_ne__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ne__Scalar::name, ne__Scalar::overload_name)
      .typed<ne__Scalar::schema>();
}

// aten::ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & ne__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_ne__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & ne__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_ne__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne__Tensor, name, "aten::ne_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne__Tensor, schema_str, "ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ne__Tensor::schema> create_ne__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ne__Tensor::name, ne__Tensor::overload_name)
      .typed<ne__Tensor::schema>();
}

// aten::ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & ne__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_ne__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & ne__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_ne__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Scalar_out, name, "aten::ge")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Scalar_out, schema_str, "ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ge_Scalar_out::schema> create_ge_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ge_Scalar_out::name, ge_Scalar_out::overload_name)
      .typed<ge_Scalar_out::schema>();
}

// aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ge_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_ge_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ge_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_ge_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Scalar, name, "aten::ge")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Scalar, schema_str, "ge.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::ge.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ge_Scalar::schema> create_ge_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ge_Scalar::name, ge_Scalar::overload_name)
      .typed<ge_Scalar::schema>();
}

// aten::ge.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor ge_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_ge_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::ge.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor ge_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_ge_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Tensor_out, name, "aten::ge")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Tensor_out, schema_str, "ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ge_Tensor_out::schema> create_ge_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ge_Tensor_out::name, ge_Tensor_out::overload_name)
      .typed<ge_Tensor_out::schema>();
}

// aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ge_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_ge_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ge_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_ge_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Tensor, name, "aten::ge")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Tensor, schema_str, "ge.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::ge.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ge_Tensor::schema> create_ge_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ge_Tensor::name, ge_Tensor::overload_name)
      .typed<ge_Tensor::schema>();
}

// aten::ge.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor ge_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_ge_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::ge.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor ge_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_ge_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge__Scalar, name, "aten::ge_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge__Scalar, schema_str, "ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ge__Scalar::schema> create_ge__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ge__Scalar::name, ge__Scalar::overload_name)
      .typed<ge__Scalar::schema>();
}

// aten::ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & ge__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_ge__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & ge__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_ge__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge__Tensor, name, "aten::ge_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge__Tensor, schema_str, "ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ge__Tensor::schema> create_ge__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ge__Tensor::name, ge__Tensor::overload_name)
      .typed<ge__Tensor::schema>();
}

// aten::ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & ge__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_ge__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & ge__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_ge__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_gather_sparse_backward, name, "aten::_gather_sparse_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_gather_sparse_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_gather_sparse_backward, schema_str, "_gather_sparse_backward(Tensor self, int dim, Tensor index, Tensor grad) -> Tensor")

// aten::_gather_sparse_backward(Tensor self, int dim, Tensor index, Tensor grad) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_gather_sparse_backward::schema> create__gather_sparse_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_gather_sparse_backward::name, _gather_sparse_backward::overload_name)
      .typed<_gather_sparse_backward::schema>();
}

// aten::_gather_sparse_backward(Tensor self, int dim, Tensor index, Tensor grad) -> Tensor
at::Tensor _gather_sparse_backward::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & grad) {
    
    static auto op = create__gather_sparse_backward_typed_handle();
    return op.call(self, dim, index, grad);
}

// aten::_gather_sparse_backward(Tensor self, int dim, Tensor index, Tensor grad) -> Tensor
at::Tensor _gather_sparse_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & grad) {
    
    static auto op = create__gather_sparse_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, grad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vander, name, "aten::linalg_vander")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vander, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vander, schema_str, "linalg_vander(Tensor x, *, SymInt? N=None) -> Tensor")

// aten::linalg_vander(Tensor x, *, SymInt? N=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_vander::schema> create_linalg_vander_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_vander::name, linalg_vander::overload_name)
      .typed<linalg_vander::schema>();
}

// aten::linalg_vander(Tensor x, *, SymInt? N=None) -> Tensor
at::Tensor linalg_vander::call(const at::Tensor & x, c10::optional<c10::SymInt> N) {
    
    static auto op = create_linalg_vander_typed_handle();
    return op.call(x, N);
}

// aten::linalg_vander(Tensor x, *, SymInt? N=None) -> Tensor
at::Tensor linalg_vander::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, c10::optional<c10::SymInt> N) {
    
    static auto op = create_linalg_vander_typed_handle();
    return op.redispatch(dispatchKeySet, x, N);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapaxes, name, "aten::swapaxes")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapaxes, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapaxes, schema_str, "swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)")

// aten::swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<swapaxes::schema> create_swapaxes_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(swapaxes::name, swapaxes::overload_name)
      .typed<swapaxes::schema>();
}

// aten::swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)
at::Tensor swapaxes::call(const at::Tensor & self, int64_t axis0, int64_t axis1) {
    
    static auto op = create_swapaxes_typed_handle();
    return op.call(self, axis0, axis1);
}

// aten::swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)
at::Tensor swapaxes::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t axis0, int64_t axis1) {
    
    static auto op = create_swapaxes_typed_handle();
    return op.redispatch(dispatchKeySet, self, axis0, axis1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapaxes_, name, "aten::swapaxes_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapaxes_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapaxes_, schema_str, "swapaxes_(Tensor(a!) self, int axis0, int axis1) -> Tensor(a!)")

// aten::swapaxes_(Tensor(a!) self, int axis0, int axis1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<swapaxes_::schema> create_swapaxes__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(swapaxes_::name, swapaxes_::overload_name)
      .typed<swapaxes_::schema>();
}

// aten::swapaxes_(Tensor(a!) self, int axis0, int axis1) -> Tensor(a!)
at::Tensor & swapaxes_::call(at::Tensor & self, int64_t axis0, int64_t axis1) {
    
    static auto op = create_swapaxes__typed_handle();
    return op.call(self, axis0, axis1);
}

// aten::swapaxes_(Tensor(a!) self, int axis0, int axis1) -> Tensor(a!)
at::Tensor & swapaxes_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t axis0, int64_t axis1) {
    
    static auto op = create_swapaxes__typed_handle();
    return op.redispatch(dispatchKeySet, self, axis0, axis1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_solve_out, name, "aten::cholesky_solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_solve_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_solve_out, schema_str, "cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cholesky_solve_out::schema> create_cholesky_solve_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cholesky_solve_out::name, cholesky_solve_out::overload_name)
      .typed<cholesky_solve_out::schema>();
}

// aten::cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cholesky_solve_out::call(const at::Tensor & self, const at::Tensor & input2, bool upper, at::Tensor & out) {
    
    static auto op = create_cholesky_solve_out_typed_handle();
    return op.call(self, input2, upper, out);
}

// aten::cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cholesky_solve_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, bool upper, at::Tensor & out) {
    
    static auto op = create_cholesky_solve_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, input2, upper, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_solve, name, "aten::cholesky_solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_solve, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_solve, schema_str, "cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor")

// aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cholesky_solve::schema> create_cholesky_solve_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cholesky_solve::name, cholesky_solve::overload_name)
      .typed<cholesky_solve::schema>();
}

// aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor
at::Tensor cholesky_solve::call(const at::Tensor & self, const at::Tensor & input2, bool upper) {
    
    static auto op = create_cholesky_solve_typed_handle();
    return op.call(self, input2, upper);
}

// aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor
at::Tensor cholesky_solve::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, bool upper) {
    
    static auto op = create_cholesky_solve_typed_handle();
    return op.redispatch(dispatchKeySet, self, input2, upper);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qr_Q, name, "aten::qr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qr_Q, overload_name, "Q")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qr_Q, schema_str, "qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)")

// aten::qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
static C10_NOINLINE c10::TypedOperatorHandle<qr_Q::schema> create_qr_Q_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(qr_Q::name, qr_Q::overload_name)
      .typed<qr_Q::schema>();
}

// aten::qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
::std::tuple<at::Tensor &,at::Tensor &> qr_Q::call(const at::Tensor & self, bool some, at::Tensor & Q, at::Tensor & R) {
    
    static auto op = create_qr_Q_typed_handle();
    return op.call(self, some, Q, R);
}

// aten::qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
::std::tuple<at::Tensor &,at::Tensor &> qr_Q::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool some, at::Tensor & Q, at::Tensor & R) {
    
    static auto op = create_qr_Q_typed_handle();
    return op.redispatch(dispatchKeySet, self, some, Q, R);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qr, name, "aten::qr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qr, schema_str, "qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)")

// aten::qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)
static C10_NOINLINE c10::TypedOperatorHandle<qr::schema> create_qr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(qr::name, qr::overload_name)
      .typed<qr::schema>();
}

// aten::qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)
::std::tuple<at::Tensor,at::Tensor> qr::call(const at::Tensor & self, bool some) {
    
    static auto op = create_qr_typed_handle();
    return op.call(self, some);
}

// aten::qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)
::std::tuple<at::Tensor,at::Tensor> qr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool some) {
    
    static auto op = create_qr_typed_handle();
    return op.redispatch(dispatchKeySet, self, some);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma_out, name, "aten::digamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma_out, schema_str, "digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<digamma_out::schema> create_digamma_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(digamma_out::name, digamma_out::overload_name)
      .typed<digamma_out::schema>();
}

// aten::digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & digamma_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_digamma_out_typed_handle();
    return op.call(self, out);
}

// aten::digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & digamma_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_digamma_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma, name, "aten::digamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma, schema_str, "digamma(Tensor self) -> Tensor")

// aten::digamma(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<digamma::schema> create_digamma_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(digamma::name, digamma::overload_name)
      .typed<digamma::schema>();
}

// aten::digamma(Tensor self) -> Tensor
at::Tensor digamma::call(const at::Tensor & self) {
    
    static auto op = create_digamma_typed_handle();
    return op.call(self);
}

// aten::digamma(Tensor self) -> Tensor
at::Tensor digamma::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_digamma_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma_out, name, "aten::polygamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma_out, schema_str, "polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<polygamma_out::schema> create_polygamma_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(polygamma_out::name, polygamma_out::overload_name)
      .typed<polygamma_out::schema>();
}

// aten::polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & polygamma_out::call(int64_t n, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_polygamma_out_typed_handle();
    return op.call(n, self, out);
}

// aten::polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & polygamma_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_polygamma_out_typed_handle();
    return op.redispatch(dispatchKeySet, n, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma, name, "aten::polygamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma, schema_str, "polygamma(int n, Tensor self) -> Tensor")

// aten::polygamma(int n, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<polygamma::schema> create_polygamma_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(polygamma::name, polygamma::overload_name)
      .typed<polygamma::schema>();
}

// aten::polygamma(int n, Tensor self) -> Tensor
at::Tensor polygamma::call(int64_t n, const at::Tensor & self) {
    
    static auto op = create_polygamma_typed_handle();
    return op.call(n, self);
}

// aten::polygamma(int n, Tensor self) -> Tensor
at::Tensor polygamma::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, const at::Tensor & self) {
    
    static auto op = create_polygamma_typed_handle();
    return op.redispatch(dispatchKeySet, n, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma_, name, "aten::polygamma_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma_, schema_str, "polygamma_(Tensor(a!) self, int n) -> Tensor(a!)")

// aten::polygamma_(Tensor(a!) self, int n) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<polygamma_::schema> create_polygamma__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(polygamma_::name, polygamma_::overload_name)
      .typed<polygamma_::schema>();
}

// aten::polygamma_(Tensor(a!) self, int n) -> Tensor(a!)
at::Tensor & polygamma_::call(at::Tensor & self, int64_t n) {
    
    static auto op = create_polygamma__typed_handle();
    return op.call(self, n);
}

// aten::polygamma_(Tensor(a!) self, int n) -> Tensor(a!)
at::Tensor & polygamma_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t n) {
    
    static auto op = create_polygamma__typed_handle();
    return op.redispatch(dispatchKeySet, self, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histc_out, name, "aten::histc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histc_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histc_out, schema_str, "histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<histc_out::schema> create_histc_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(histc_out::name, histc_out::overload_name)
      .typed<histc_out::schema>();
}

// aten::histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & histc_out::call(const at::Tensor & self, int64_t bins, const at::Scalar & min, const at::Scalar & max, at::Tensor & out) {
    
    static auto op = create_histc_out_typed_handle();
    return op.call(self, bins, min, max, out);
}

// aten::histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & histc_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t bins, const at::Scalar & min, const at::Scalar & max, at::Tensor & out) {
    
    static auto op = create_histc_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, bins, min, max, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histc, name, "aten::histc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histc, schema_str, "histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor")

// aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<histc::schema> create_histc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(histc::name, histc::overload_name)
      .typed<histc::schema>();
}

// aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor
at::Tensor histc::call(const at::Tensor & self, int64_t bins, const at::Scalar & min, const at::Scalar & max) {
    
    static auto op = create_histc_typed_handle();
    return op.call(self, bins, min, max);
}

// aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor
at::Tensor histc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t bins, const at::Scalar & min, const at::Scalar & max) {
    
    static auto op = create_histc_typed_handle();
    return op.redispatch(dispatchKeySet, self, bins, min, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_histogramdd_bin_edges, name, "aten::_histogramdd_bin_edges")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_histogramdd_bin_edges, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_histogramdd_bin_edges, schema_str, "_histogramdd_bin_edges(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False) -> Tensor[]")

// aten::_histogramdd_bin_edges(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_histogramdd_bin_edges::schema> create__histogramdd_bin_edges_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_histogramdd_bin_edges::name, _histogramdd_bin_edges::overload_name)
      .typed<_histogramdd_bin_edges::schema>();
}

// aten::_histogramdd_bin_edges(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False) -> Tensor[]
::std::vector<at::Tensor> _histogramdd_bin_edges::call(const at::Tensor & self, at::IntArrayRef bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density) {
    
    static auto op = create__histogramdd_bin_edges_typed_handle();
    return op.call(self, bins, range, weight, density);
}

// aten::_histogramdd_bin_edges(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False) -> Tensor[]
::std::vector<at::Tensor> _histogramdd_bin_edges::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density) {
    
    static auto op = create__histogramdd_bin_edges_typed_handle();
    return op.redispatch(dispatchKeySet, self, bins, range, weight, density);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_histogramdd_from_bin_tensors, name, "aten::_histogramdd_from_bin_tensors")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_histogramdd_from_bin_tensors, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_histogramdd_from_bin_tensors, schema_str, "_histogramdd_from_bin_tensors(Tensor self, Tensor[] bins, *, Tensor? weight=None, bool density=False) -> Tensor")

// aten::_histogramdd_from_bin_tensors(Tensor self, Tensor[] bins, *, Tensor? weight=None, bool density=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_histogramdd_from_bin_tensors::schema> create__histogramdd_from_bin_tensors_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_histogramdd_from_bin_tensors::name, _histogramdd_from_bin_tensors::overload_name)
      .typed<_histogramdd_from_bin_tensors::schema>();
}

// aten::_histogramdd_from_bin_tensors(Tensor self, Tensor[] bins, *, Tensor? weight=None, bool density=False) -> Tensor
at::Tensor _histogramdd_from_bin_tensors::call(const at::Tensor & self, at::TensorList bins, const c10::optional<at::Tensor> & weight, bool density) {
    
    static auto op = create__histogramdd_from_bin_tensors_typed_handle();
    return op.call(self, bins, weight, density);
}

// aten::_histogramdd_from_bin_tensors(Tensor self, Tensor[] bins, *, Tensor? weight=None, bool density=False) -> Tensor
at::Tensor _histogramdd_from_bin_tensors::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::TensorList bins, const c10::optional<at::Tensor> & weight, bool density) {
    
    static auto op = create__histogramdd_from_bin_tensors_typed_handle();
    return op.redispatch(dispatchKeySet, self, bins, weight, density);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter_out, name, "aten::nextafter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter_out, schema_str, "nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nextafter_out::schema> create_nextafter_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nextafter_out::name, nextafter_out::overload_name)
      .typed<nextafter_out::schema>();
}

// aten::nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nextafter_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_nextafter_out_typed_handle();
    return op.call(self, other, out);
}

// aten::nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nextafter_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_nextafter_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter, name, "aten::nextafter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter, schema_str, "nextafter(Tensor self, Tensor other) -> Tensor")

// aten::nextafter(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nextafter::schema> create_nextafter_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nextafter::name, nextafter::overload_name)
      .typed<nextafter::schema>();
}

// aten::nextafter(Tensor self, Tensor other) -> Tensor
at::Tensor nextafter::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_nextafter_typed_handle();
    return op.call(self, other);
}

// aten::nextafter(Tensor self, Tensor other) -> Tensor
at::Tensor nextafter::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_nextafter_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter_, name, "aten::nextafter_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter_, schema_str, "nextafter_(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::nextafter_(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nextafter_::schema> create_nextafter__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nextafter_::name, nextafter_::overload_name)
      .typed<nextafter_::schema>();
}

// aten::nextafter_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & nextafter_::call(at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_nextafter__typed_handle();
    return op.call(self, other);
}

// aten::nextafter_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & nextafter_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_nextafter__typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(maximum, name, "aten::maximum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(maximum, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(maximum, schema_str, "maximum(Tensor self, Tensor other) -> Tensor")

// aten::maximum(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<maximum::schema> create_maximum_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(maximum::name, maximum::overload_name)
      .typed<maximum::schema>();
}

// aten::maximum(Tensor self, Tensor other) -> Tensor
at::Tensor maximum::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_maximum_typed_handle();
    return op.call(self, other);
}

// aten::maximum(Tensor self, Tensor other) -> Tensor
at::Tensor maximum::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_maximum_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(maximum_out, name, "aten::maximum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(maximum_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(maximum_out, schema_str, "maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<maximum_out::schema> create_maximum_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(maximum_out::name, maximum_out::overload_name)
      .typed<maximum_out::schema>();
}

// aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & maximum_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_maximum_out_typed_handle();
    return op.call(self, other, out);
}

// aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & maximum_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_maximum_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(minimum, name, "aten::minimum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(minimum, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(minimum, schema_str, "minimum(Tensor self, Tensor other) -> Tensor")

// aten::minimum(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<minimum::schema> create_minimum_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(minimum::name, minimum::overload_name)
      .typed<minimum::schema>();
}

// aten::minimum(Tensor self, Tensor other) -> Tensor
at::Tensor minimum::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_minimum_typed_handle();
    return op.call(self, other);
}

// aten::minimum(Tensor self, Tensor other) -> Tensor
at::Tensor minimum::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_minimum_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(minimum_out, name, "aten::minimum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(minimum_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(minimum_out, schema_str, "minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<minimum_out::schema> create_minimum_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(minimum_out::name, minimum_out::overload_name)
      .typed<minimum_out::schema>();
}

// aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & minimum_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_minimum_out_typed_handle();
    return op.call(self, other, out);
}

// aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & minimum_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_minimum_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile, name, "aten::quantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile, schema_str, "quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor")

// aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantile::schema> create_quantile_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantile::name, quantile::overload_name)
      .typed<quantile::schema>();
}

// aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor
at::Tensor quantile::call(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    
    static auto op = create_quantile_typed_handle();
    return op.call(self, q, dim, keepdim, interpolation);
}

// aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor
at::Tensor quantile::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    
    static auto op = create_quantile_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, interpolation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_out, name, "aten::quantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_out, schema_str, "quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)")

// aten::quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<quantile_out::schema> create_quantile_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantile_out::name, quantile_out::overload_name)
      .typed<quantile_out::schema>();
}

// aten::quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantile_out::call(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    
    static auto op = create_quantile_out_typed_handle();
    return op.call(self, q, dim, keepdim, interpolation, out);
}

// aten::quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantile_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    
    static auto op = create_quantile_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, interpolation, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_scalar, name, "aten::quantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_scalar, overload_name, "scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_scalar, schema_str, "quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor")

// aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantile_scalar::schema> create_quantile_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantile_scalar::name, quantile_scalar::overload_name)
      .typed<quantile_scalar::schema>();
}

// aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor
at::Tensor quantile_scalar::call(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    
    static auto op = create_quantile_scalar_typed_handle();
    return op.call(self, q, dim, keepdim, interpolation);
}

// aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor
at::Tensor quantile_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    
    static auto op = create_quantile_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, interpolation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_scalar_out, name, "aten::quantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_scalar_out, overload_name, "scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_scalar_out, schema_str, "quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)")

// aten::quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<quantile_scalar_out::schema> create_quantile_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantile_scalar_out::name, quantile_scalar_out::overload_name)
      .typed<quantile_scalar_out::schema>();
}

// aten::quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantile_scalar_out::call(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    
    static auto op = create_quantile_scalar_out_typed_handle();
    return op.call(self, q, dim, keepdim, interpolation, out);
}

// aten::quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantile_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    
    static auto op = create_quantile_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, interpolation, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(msort_out, name, "aten::msort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(msort_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(msort_out, schema_str, "msort.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::msort.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<msort_out::schema> create_msort_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(msort_out::name, msort_out::overload_name)
      .typed<msort_out::schema>();
}

// aten::msort.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & msort_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_msort_out_typed_handle();
    return op.call(self, out);
}

// aten::msort.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & msort_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_msort_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(msort, name, "aten::msort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(msort, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(msort, schema_str, "msort(Tensor self) -> Tensor")

// aten::msort(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<msort::schema> create_msort_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(msort::name, msort::overload_name)
      .typed<msort::schema>();
}

// aten::msort(Tensor self) -> Tensor
at::Tensor msort::call(const at::Tensor & self) {
    
    static auto op = create_msort_typed_handle();
    return op.call(self);
}

// aten::msort(Tensor self) -> Tensor
at::Tensor msort::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_msort_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort, name, "aten::argsort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort, schema_str, "argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor")

// aten::argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<argsort::schema> create_argsort_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(argsort::name, argsort::overload_name)
      .typed<argsort::schema>();
}

// aten::argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor
at::Tensor argsort::call(const at::Tensor & self, int64_t dim, bool descending) {
    
    static auto op = create_argsort_typed_handle();
    return op.call(self, dim, descending);
}

// aten::argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor
at::Tensor argsort::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool descending) {
    
    static auto op = create_argsort_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, descending);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort_stable, name, "aten::argsort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort_stable, overload_name, "stable")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort_stable, schema_str, "argsort.stable(Tensor self, *, bool stable, int dim=-1, bool descending=False) -> Tensor")

// aten::argsort.stable(Tensor self, *, bool stable, int dim=-1, bool descending=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<argsort_stable::schema> create_argsort_stable_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(argsort_stable::name, argsort_stable::overload_name)
      .typed<argsort_stable::schema>();
}

// aten::argsort.stable(Tensor self, *, bool stable, int dim=-1, bool descending=False) -> Tensor
at::Tensor argsort_stable::call(const at::Tensor & self, bool stable, int64_t dim, bool descending) {
    
    static auto op = create_argsort_stable_typed_handle();
    return op.call(self, stable, dim, descending);
}

// aten::argsort.stable(Tensor self, *, bool stable, int dim=-1, bool descending=False) -> Tensor
at::Tensor argsort_stable::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool stable, int64_t dim, bool descending) {
    
    static auto op = create_argsort_stable_typed_handle();
    return op.redispatch(dispatchKeySet, self, stable, dim, descending);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort_dimname, name, "aten::argsort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort_dimname, schema_str, "argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor")

// aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<argsort_dimname::schema> create_argsort_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(argsort_dimname::name, argsort_dimname::overload_name)
      .typed<argsort_dimname::schema>();
}

// aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor
at::Tensor argsort_dimname::call(const at::Tensor & self, at::Dimname dim, bool descending) {
    
    static auto op = create_argsort_dimname_typed_handle();
    return op.call(self, dim, descending);
}

// aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor
at::Tensor argsort_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool descending) {
    
    static auto op = create_argsort_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, descending);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(topk_values, name, "aten::topk")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(topk_values, overload_name, "values")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(topk_values, schema_str, "topk.values(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::topk.values(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<topk_values::schema> create_topk_values_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(topk_values::name, topk_values::overload_name)
      .typed<topk_values::schema>();
}

// aten::topk.values(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> topk_values::call(const at::Tensor & self, c10::SymInt k, int64_t dim, bool largest, bool sorted, at::Tensor & values, at::Tensor & indices) {
    
    static auto op = create_topk_values_typed_handle();
    return op.call(self, k, dim, largest, sorted, values, indices);
}

// aten::topk.values(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> topk_values::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt k, int64_t dim, bool largest, bool sorted, at::Tensor & values, at::Tensor & indices) {
    
    static auto op = create_topk_values_typed_handle();
    return op.redispatch(dispatchKeySet, self, k, dim, largest, sorted, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(topk, name, "aten::topk")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(topk, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(topk, schema_str, "topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)")

// aten::topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<topk::schema> create_topk_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(topk::name, topk::overload_name)
      .typed<topk::schema>();
}

// aten::topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> topk::call(const at::Tensor & self, c10::SymInt k, int64_t dim, bool largest, bool sorted) {
    
    static auto op = create_topk_typed_handle();
    return op.call(self, k, dim, largest, sorted);
}

// aten::topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> topk::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt k, int64_t dim, bool largest, bool sorted) {
    
    static auto op = create_topk_typed_handle();
    return op.redispatch(dispatchKeySet, self, k, dim, largest, sorted);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unfold_backward, name, "aten::unfold_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unfold_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unfold_backward, schema_str, "unfold_backward(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step) -> Tensor")

// aten::unfold_backward(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<unfold_backward::schema> create_unfold_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unfold_backward::name, unfold_backward::overload_name)
      .typed<unfold_backward::schema>();
}

// aten::unfold_backward(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step) -> Tensor
at::Tensor unfold_backward::call(const at::Tensor & grad_in, c10::SymIntArrayRef input_sizes, int64_t dim, int64_t size, int64_t step) {
    
    static auto op = create_unfold_backward_typed_handle();
    return op.call(grad_in, input_sizes, dim, size, step);
}

// aten::unfold_backward(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step) -> Tensor
at::Tensor unfold_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_in, c10::SymIntArrayRef input_sizes, int64_t dim, int64_t size, int64_t step) {
    
    static auto op = create_unfold_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_in, input_sizes, dim, size, step);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_, name, "aten::normal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_, schema_str, "normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)")

// aten::normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<normal_::schema> create_normal__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_::name, normal_::overload_name)
      .typed<normal_::schema>();
}

// aten::normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & normal_::call(at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
    
    static auto op = create_normal__typed_handle();
    return op.call(self, mean, std, generator);
}

// aten::normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & normal_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
    
    static auto op = create_normal__typed_handle();
    return op.redispatch(dispatchKeySet, self, mean, std, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_functional, name, "aten::normal_functional")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_functional, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_functional, schema_str, "normal_functional(Tensor self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor")

// aten::normal_functional(Tensor self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<normal_functional::schema> create_normal_functional_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_functional::name, normal_functional::overload_name)
      .typed<normal_functional::schema>();
}

// aten::normal_functional(Tensor self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor
at::Tensor normal_functional::call(const at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
    
    static auto op = create_normal_functional_typed_handle();
    return op.call(self, mean, std, generator);
}

// aten::normal_functional(Tensor self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor
at::Tensor normal_functional::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
    
    static auto op = create_normal_functional_typed_handle();
    return op.redispatch(dispatchKeySet, self, mean, std, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_float_out, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_float_out, overload_name, "Tensor_float_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_float_out, schema_str, "normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<normal_Tensor_float_out::schema> create_normal_Tensor_float_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_Tensor_float_out::name, normal_Tensor_float_out::overload_name)
      .typed<normal_Tensor_float_out::schema>();
}

// aten::normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_Tensor_float_out::call(const at::Tensor & mean, double std, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_normal_Tensor_float_out_typed_handle();
    return op.call(mean, std, generator, out);
}

// aten::normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_Tensor_float_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mean, double std, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_normal_Tensor_float_out_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_float, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_float, overload_name, "Tensor_float")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_float, schema_str, "normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor")

// aten::normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<normal_Tensor_float::schema> create_normal_Tensor_float_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_Tensor_float::name, normal_Tensor_float::overload_name)
      .typed<normal_Tensor_float::schema>();
}

// aten::normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor
at::Tensor normal_Tensor_float::call(const at::Tensor & mean, double std, c10::optional<at::Generator> generator) {
    
    static auto op = create_normal_Tensor_float_typed_handle();
    return op.call(mean, std, generator);
}

// aten::normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor
at::Tensor normal_Tensor_float::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mean, double std, c10::optional<at::Generator> generator) {
    
    static auto op = create_normal_Tensor_float_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_Tensor_out, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_Tensor_out, overload_name, "float_Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_Tensor_out, schema_str, "normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<normal_float_Tensor_out::schema> create_normal_float_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_float_Tensor_out::name, normal_float_Tensor_out::overload_name)
      .typed<normal_float_Tensor_out::schema>();
}

// aten::normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_float_Tensor_out::call(double mean, const at::Tensor & std, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_normal_float_Tensor_out_typed_handle();
    return op.call(mean, std, generator, out);
}

// aten::normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_float_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, double mean, const at::Tensor & std, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_normal_float_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_Tensor, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_Tensor, overload_name, "float_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_Tensor, schema_str, "normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor")

// aten::normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<normal_float_Tensor::schema> create_normal_float_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_float_Tensor::name, normal_float_Tensor::overload_name)
      .typed<normal_float_Tensor::schema>();
}

// aten::normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor
at::Tensor normal_float_Tensor::call(double mean, const at::Tensor & std, c10::optional<at::Generator> generator) {
    
    static auto op = create_normal_float_Tensor_typed_handle();
    return op.call(mean, std, generator);
}

// aten::normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor
at::Tensor normal_float_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, double mean, const at::Tensor & std, c10::optional<at::Generator> generator) {
    
    static auto op = create_normal_float_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_Tensor_out, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_Tensor_out, overload_name, "Tensor_Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_Tensor_out, schema_str, "normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<normal_Tensor_Tensor_out::schema> create_normal_Tensor_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_Tensor_Tensor_out::name, normal_Tensor_Tensor_out::overload_name)
      .typed<normal_Tensor_Tensor_out::schema>();
}

// aten::normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_Tensor_Tensor_out::call(const at::Tensor & mean, const at::Tensor & std, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_normal_Tensor_Tensor_out_typed_handle();
    return op.call(mean, std, generator, out);
}

// aten::normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_Tensor_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mean, const at::Tensor & std, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_normal_Tensor_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_Tensor, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_Tensor, overload_name, "Tensor_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_Tensor, schema_str, "normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor")

// aten::normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<normal_Tensor_Tensor::schema> create_normal_Tensor_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_Tensor_Tensor::name, normal_Tensor_Tensor::overload_name)
      .typed<normal_Tensor_Tensor::schema>();
}

// aten::normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor
at::Tensor normal_Tensor_Tensor::call(const at::Tensor & mean, const at::Tensor & std, c10::optional<at::Generator> generator) {
    
    static auto op = create_normal_Tensor_Tensor_typed_handle();
    return op.call(mean, std, generator);
}

// aten::normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor
at::Tensor normal_Tensor_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mean, const at::Tensor & std, c10::optional<at::Generator> generator) {
    
    static auto op = create_normal_Tensor_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_float, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_float, overload_name, "float_float")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_float, schema_str, "normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<normal_float_float::schema> create_normal_float_float_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_float_float::name, normal_float_float::overload_name)
      .typed<normal_float_float::schema>();
}

// aten::normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor normal_float_float::call(double mean, double std, c10::SymIntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_normal_float_float_typed_handle();
    return op.call(mean, std, size, generator, dtype, layout, device, pin_memory);
}

// aten::normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor normal_float_float::redispatch(c10::DispatchKeySet dispatchKeySet, double mean, double std, c10::SymIntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_normal_float_float_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, size, generator, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_float_out, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_float_out, overload_name, "float_float_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_float_out, schema_str, "normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<normal_float_float_out::schema> create_normal_float_float_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_float_float_out::name, normal_float_float_out::overload_name)
      .typed<normal_float_float_out::schema>();
}

// aten::normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_float_float_out::call(double mean, double std, c10::SymIntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_normal_float_float_out_typed_handle();
    return op.call(mean, std, size, generator, out);
}

// aten::normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_float_float_out::redispatch(c10::DispatchKeySet dispatchKeySet, double mean, double std, c10::SymIntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_normal_float_float_out_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, size, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(alias, name, "aten::alias")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(alias, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(alias, schema_str, "alias(Tensor(a) self) -> Tensor(a)")

// aten::alias(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<alias::schema> create_alias_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(alias::name, alias::overload_name)
      .typed<alias::schema>();
}

// aten::alias(Tensor(a) self) -> Tensor(a)
at::Tensor alias::call(const at::Tensor & self) {
    
    static auto op = create_alias_typed_handle();
    return op.call(self);
}

// aten::alias(Tensor(a) self) -> Tensor(a)
at::Tensor alias::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_alias_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_Scalar, name, "aten::_foreach_sub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_Scalar, schema_str, "_foreach_sub.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]")

// aten::_foreach_sub.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub_Scalar::schema> create__foreach_sub_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub_Scalar::name, _foreach_sub_Scalar::overload_name)
      .typed<_foreach_sub_Scalar::schema>();
}

// aten::_foreach_sub.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_sub_Scalar::call(at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_sub_Scalar_typed_handle();
    return op.call(self, scalar);
}

// aten::_foreach_sub.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_sub_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_sub_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__Scalar, name, "aten::_foreach_sub_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__Scalar, schema_str, "_foreach_sub_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()")

// aten::_foreach_sub_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub__Scalar::schema> create__foreach_sub__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub__Scalar::name, _foreach_sub__Scalar::overload_name)
      .typed<_foreach_sub__Scalar::schema>();
}

// aten::_foreach_sub_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_sub__Scalar::call(at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_sub__Scalar_typed_handle();
    return op.call(self, scalar);
}

// aten::_foreach_sub_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_sub__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_sub__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_List, name, "aten::_foreach_sub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_List, schema_str, "_foreach_sub.List(Tensor[] self, Tensor[] other, *, Scalar alpha=1) -> Tensor[]")

// aten::_foreach_sub.List(Tensor[] self, Tensor[] other, *, Scalar alpha=1) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub_List::schema> create__foreach_sub_List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub_List::name, _foreach_sub_List::overload_name)
      .typed<_foreach_sub_List::schema>();
}

// aten::_foreach_sub.List(Tensor[] self, Tensor[] other, *, Scalar alpha=1) -> Tensor[]
::std::vector<at::Tensor> _foreach_sub_List::call(at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
    
    static auto op = create__foreach_sub_List_typed_handle();
    return op.call(self, other, alpha);
}

// aten::_foreach_sub.List(Tensor[] self, Tensor[] other, *, Scalar alpha=1) -> Tensor[]
::std::vector<at::Tensor> _foreach_sub_List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
    
    static auto op = create__foreach_sub_List_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__List, name, "aten::_foreach_sub_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__List, schema_str, "_foreach_sub_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()")

// aten::_foreach_sub_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub__List::schema> create__foreach_sub__List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub__List::name, _foreach_sub__List::overload_name)
      .typed<_foreach_sub__List::schema>();
}

// aten::_foreach_sub_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
void _foreach_sub__List::call(at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
    
    static auto op = create__foreach_sub__List_typed_handle();
    return op.call(self, other, alpha);
}

// aten::_foreach_sub_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
void _foreach_sub__List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
    
    static auto op = create__foreach_sub__List_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_ScalarList, name, "aten::_foreach_sub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_ScalarList, schema_str, "_foreach_sub.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]")

// aten::_foreach_sub.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub_ScalarList::schema> create__foreach_sub_ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub_ScalarList::name, _foreach_sub_ScalarList::overload_name)
      .typed<_foreach_sub_ScalarList::schema>();
}

// aten::_foreach_sub.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_sub_ScalarList::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_sub_ScalarList_typed_handle();
    return op.call(self, scalars);
}

// aten::_foreach_sub.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_sub_ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_sub_ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__ScalarList, name, "aten::_foreach_sub_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__ScalarList, schema_str, "_foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()")

// aten::_foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub__ScalarList::schema> create__foreach_sub__ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub__ScalarList::name, _foreach_sub__ScalarList::overload_name)
      .typed<_foreach_sub__ScalarList::schema>();
}

// aten::_foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_sub__ScalarList::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_sub__ScalarList_typed_handle();
    return op.call(self, scalars);
}

// aten::_foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_sub__ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_sub__ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_Scalar, name, "aten::_foreach_maximum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_Scalar, schema_str, "_foreach_maximum.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]")

// aten::_foreach_maximum.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_maximum_Scalar::schema> create__foreach_maximum_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_maximum_Scalar::name, _foreach_maximum_Scalar::overload_name)
      .typed<_foreach_maximum_Scalar::schema>();
}

// aten::_foreach_maximum.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_maximum_Scalar::call(at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_maximum_Scalar_typed_handle();
    return op.call(self, scalar);
}

// aten::_foreach_maximum.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_maximum_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_maximum_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum__Scalar, name, "aten::_foreach_maximum_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum__Scalar, schema_str, "_foreach_maximum_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()")

// aten::_foreach_maximum_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_maximum__Scalar::schema> create__foreach_maximum__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_maximum__Scalar::name, _foreach_maximum__Scalar::overload_name)
      .typed<_foreach_maximum__Scalar::schema>();
}

// aten::_foreach_maximum_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_maximum__Scalar::call(at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_maximum__Scalar_typed_handle();
    return op.call(self, scalar);
}

// aten::_foreach_maximum_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_maximum__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
    
    static auto op = create__foreach_maximum__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_List, name, "aten::_foreach_maximum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_List, schema_str, "_foreach_maximum.List(Tensor[] self, Tensor[] other) -> Tensor[]")

// aten::_foreach_maximum.List(Tensor[] self, Tensor[] other) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_maximum_List::schema> create__foreach_maximum_List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_maximum_List::name, _foreach_maximum_List::overload_name)
      .typed<_foreach_maximum_List::schema>();
}

// aten::_foreach_maximum.List(Tensor[] self, Tensor[] other) -> Tensor[]
::std::vector<at::Tensor> _foreach_maximum_List::call(at::TensorList self, at::TensorList other) {
    
    static auto op = create__foreach_maximum_List_typed_handle();
    return op.call(self, other);
}

// aten::_foreach_maximum.List(Tensor[] self, Tensor[] other) -> Tensor[]
::std::vector<at::Tensor> _foreach_maximum_List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
    
    static auto op = create__foreach_maximum_List_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum__List, name, "aten::_foreach_maximum_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum__List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum__List, schema_str, "_foreach_maximum_.List(Tensor(a!)[] self, Tensor[] other) -> ()")

// aten::_foreach_maximum_.List(Tensor(a!)[] self, Tensor[] other) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_maximum__List::schema> create__foreach_maximum__List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_maximum__List::name, _foreach_maximum__List::overload_name)
      .typed<_foreach_maximum__List::schema>();
}

// aten::_foreach_maximum_.List(Tensor(a!)[] self, Tensor[] other) -> ()
void _foreach_maximum__List::call(at::TensorList self, at::TensorList other) {
    
    static auto op = create__foreach_maximum__List_typed_handle();
    return op.call(self, other);
}

// aten::_foreach_maximum_.List(Tensor(a!)[] self, Tensor[] other) -> ()
void _foreach_maximum__List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
    
    static auto op = create__foreach_maximum__List_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_ScalarList, name, "aten::_foreach_maximum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_ScalarList, schema_str, "_foreach_maximum.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]")

// aten::_foreach_maximum.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_maximum_ScalarList::schema> create__foreach_maximum_ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_maximum_ScalarList::name, _foreach_maximum_ScalarList::overload_name)
      .typed<_foreach_maximum_ScalarList::schema>();
}

// aten::_foreach_maximum.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_maximum_ScalarList::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_maximum_ScalarList_typed_handle();
    return op.call(self, scalars);
}

// aten::_foreach_maximum.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_maximum_ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_maximum_ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum__ScalarList, name, "aten::_foreach_maximum_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum__ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum__ScalarList, schema_str, "_foreach_maximum_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()")

// aten::_foreach_maximum_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_maximum__ScalarList::schema> create__foreach_maximum__ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_maximum__ScalarList::name, _foreach_maximum__ScalarList::overload_name)
      .typed<_foreach_maximum__ScalarList::schema>();
}

// aten::_foreach_maximum_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_maximum__ScalarList::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_maximum__ScalarList_typed_handle();
    return op.call(self, scalars);
}

// aten::_foreach_maximum_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_maximum__ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    
    static auto op = create__foreach_maximum__ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos, name, "aten::_foreach_acos")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos, schema_str, "_foreach_acos(Tensor[] self) -> Tensor[]")

// aten::_foreach_acos(Tensor[] self) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_acos::schema> create__foreach_acos_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_acos::name, _foreach_acos::overload_name)
      .typed<_foreach_acos::schema>();
}

// aten::_foreach_acos(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_acos::call(at::TensorList self) {
    
    static auto op = create__foreach_acos_typed_handle();
    return op.call(self);
}

// aten::_foreach_acos(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_acos::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_acos_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos_, name, "aten::_foreach_acos_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos_, schema_str, "_foreach_acos_(Tensor(a!)[] self) -> ()")

// aten::_foreach_acos_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_acos_::schema> create__foreach_acos__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_acos_::name, _foreach_acos_::overload_name)
      .typed<_foreach_acos_::schema>();
}

// aten::_foreach_acos_(Tensor(a!)[] self) -> ()
void _foreach_acos_::call(at::TensorList self) {
    
    static auto op = create__foreach_acos__typed_handle();
    return op.call(self);
}

// aten::_foreach_acos_(Tensor(a!)[] self) -> ()
void _foreach_acos_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_acos__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan, name, "aten::_foreach_atan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan, schema_str, "_foreach_atan(Tensor[] self) -> Tensor[]")

// aten::_foreach_atan(Tensor[] self) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_atan::schema> create__foreach_atan_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_atan::name, _foreach_atan::overload_name)
      .typed<_foreach_atan::schema>();
}

// aten::_foreach_atan(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_atan::call(at::TensorList self) {
    
    static auto op = create__foreach_atan_typed_handle();
    return op.call(self);
}

// aten::_foreach_atan(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_atan::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_atan_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan_, name, "aten::_foreach_atan_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan_, schema_str, "_foreach_atan_(Tensor(a!)[] self) -> ()")

// aten::_foreach_atan_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_atan_::schema> create__foreach_atan__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_atan_::name, _foreach_atan_::overload_name)
      .typed<_foreach_atan_::schema>();
}

// aten::_foreach_atan_(Tensor(a!)[] self) -> ()
void _foreach_atan_::call(at::TensorList self) {
    
    static auto op = create__foreach_atan__typed_handle();
    return op.call(self);
}

// aten::_foreach_atan_(Tensor(a!)[] self) -> ()
void _foreach_atan_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_atan__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil, name, "aten::_foreach_ceil")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil, schema_str, "_foreach_ceil(Tensor[] self) -> Tensor[]")

// aten::_foreach_ceil(Tensor[] self) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_ceil::schema> create__foreach_ceil_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_ceil::name, _foreach_ceil::overload_name)
      .typed<_foreach_ceil::schema>();
}

// aten::_foreach_ceil(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_ceil::call(at::TensorList self) {
    
    static auto op = create__foreach_ceil_typed_handle();
    return op.call(self);
}

// aten::_foreach_ceil(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_ceil::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_ceil_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil_, name, "aten::_foreach_ceil_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil_, schema_str, "_foreach_ceil_(Tensor(a!)[] self) -> ()")

// aten::_foreach_ceil_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_ceil_::schema> create__foreach_ceil__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_ceil_::name, _foreach_ceil_::overload_name)
      .typed<_foreach_ceil_::schema>();
}

// aten::_foreach_ceil_(Tensor(a!)[] self) -> ()
void _foreach_ceil_::call(at::TensorList self) {
    
    static auto op = create__foreach_ceil__typed_handle();
    return op.call(self);
}

// aten::_foreach_ceil_(Tensor(a!)[] self) -> ()
void _foreach_ceil_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_ceil__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf, name, "aten::_foreach_erf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf, schema_str, "_foreach_erf(Tensor[] self) -> Tensor[]")

// aten::_foreach_erf(Tensor[] self) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_erf::schema> create__foreach_erf_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_erf::name, _foreach_erf::overload_name)
      .typed<_foreach_erf::schema>();
}

// aten::_foreach_erf(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_erf::call(at::TensorList self) {
    
    static auto op = create__foreach_erf_typed_handle();
    return op.call(self);
}

// aten::_foreach_erf(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_erf::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_erf_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf_, name, "aten::_foreach_erf_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf_, schema_str, "_foreach_erf_(Tensor(a!)[] self) -> ()")

// aten::_foreach_erf_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_erf_::schema> create__foreach_erf__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_erf_::name, _foreach_erf_::overload_name)
      .typed<_foreach_erf_::schema>();
}

// aten::_foreach_erf_(Tensor(a!)[] self) -> ()
void _foreach_erf_::call(at::TensorList self) {
    
    static auto op = create__foreach_erf__typed_handle();
    return op.call(self);
}

// aten::_foreach_erf_(Tensor(a!)[] self) -> ()
void _foreach_erf_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_erf__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2, name, "aten::_foreach_log2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2, schema_str, "_foreach_log2(Tensor[] self) -> Tensor[]")

// aten::_foreach_log2(Tensor[] self) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_log2::schema> create__foreach_log2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_log2::name, _foreach_log2::overload_name)
      .typed<_foreach_log2::schema>();
}

// aten::_foreach_log2(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_log2::call(at::TensorList self) {
    
    static auto op = create__foreach_log2_typed_handle();
    return op.call(self);
}

// aten::_foreach_log2(Tensor[] self) -> Tensor[]
::std::vector<at::Tensor> _foreach_log2::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_log2_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2_, name, "aten::_foreach_log2_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2_, schema_str, "_foreach_log2_(Tensor(a!)[] self) -> ()")

// aten::_foreach_log2_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_log2_::schema> create__foreach_log2__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_log2_::name, _foreach_log2_::overload_name)
      .typed<_foreach_log2_::schema>();
}

// aten::_foreach_log2_(Tensor(a!)[] self) -> ()
void _foreach_log2_::call(at::TensorList self) {
    
    static auto op = create__foreach_log2__typed_handle();
    return op.call(self);
}

// aten::_foreach_log2_(Tensor(a!)[] self) -> ()
void _foreach_log2_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    
    static auto op = create__foreach_log2__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Tensor, name, "aten::bucketize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Tensor, schema_str, "bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor")

// aten::bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bucketize_Tensor::schema> create_bucketize_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bucketize_Tensor::name, bucketize_Tensor::overload_name)
      .typed<bucketize_Tensor::schema>();
}

// aten::bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
at::Tensor bucketize_Tensor::call(const at::Tensor & self, const at::Tensor & boundaries, bool out_int32, bool right) {
    
    static auto op = create_bucketize_Tensor_typed_handle();
    return op.call(self, boundaries, out_int32, right);
}

// aten::bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
at::Tensor bucketize_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & boundaries, bool out_int32, bool right) {
    
    static auto op = create_bucketize_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, boundaries, out_int32, right);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Tensor_out, name, "aten::bucketize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Tensor_out, schema_str, "bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)")

// aten::bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bucketize_Tensor_out::schema> create_bucketize_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bucketize_Tensor_out::name, bucketize_Tensor_out::overload_name)
      .typed<bucketize_Tensor_out::schema>();
}

// aten::bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bucketize_Tensor_out::call(const at::Tensor & self, const at::Tensor & boundaries, bool out_int32, bool right, at::Tensor & out) {
    
    static auto op = create_bucketize_Tensor_out_typed_handle();
    return op.call(self, boundaries, out_int32, right, out);
}

// aten::bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bucketize_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & boundaries, bool out_int32, bool right, at::Tensor & out) {
    
    static auto op = create_bucketize_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, boundaries, out_int32, right, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Scalar, name, "aten::bucketize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Scalar, schema_str, "bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor")

// aten::bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bucketize_Scalar::schema> create_bucketize_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bucketize_Scalar::name, bucketize_Scalar::overload_name)
      .typed<bucketize_Scalar::schema>();
}

// aten::bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
at::Tensor bucketize_Scalar::call(const at::Scalar & self, const at::Tensor & boundaries, bool out_int32, bool right) {
    
    static auto op = create_bucketize_Scalar_typed_handle();
    return op.call(self, boundaries, out_int32, right);
}

// aten::bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
at::Tensor bucketize_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & boundaries, bool out_int32, bool right) {
    
    static auto op = create_bucketize_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, boundaries, out_int32, right);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss_out, name, "aten::mse_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss_out, schema_str, "mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)")

// aten::mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mse_loss_out::schema> create_mse_loss_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mse_loss_out::name, mse_loss_out::overload_name)
      .typed<mse_loss_out::schema>();
}

// aten::mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mse_loss_out::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    
    static auto op = create_mse_loss_out_typed_handle();
    return op.call(self, target, reduction, out);
}

// aten::mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mse_loss_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    
    static auto op = create_mse_loss_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss, name, "aten::mse_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss, schema_str, "mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor")

// aten::mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mse_loss::schema> create_mse_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mse_loss::name, mse_loss::overload_name)
      .typed<mse_loss::schema>();
}

// aten::mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
at::Tensor mse_loss::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    
    static auto op = create_mse_loss_typed_handle();
    return op.call(self, target, reduction);
}

// aten::mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
at::Tensor mse_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    
    static auto op = create_mse_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss, name, "aten::l1_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss, schema_str, "l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor")

// aten::l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<l1_loss::schema> create_l1_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(l1_loss::name, l1_loss::overload_name)
      .typed<l1_loss::schema>();
}

// aten::l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
at::Tensor l1_loss::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    
    static auto op = create_l1_loss_typed_handle();
    return op.call(self, target, reduction);
}

// aten::l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
at::Tensor l1_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    
    static auto op = create_l1_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_nd, name, "aten::nll_loss_nd")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_nd, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_nd, schema_str, "nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor")

// aten::nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss_nd::schema> create_nll_loss_nd_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss_nd::name, nll_loss_nd::overload_name)
      .typed<nll_loss_nd::schema>();
}

// aten::nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
at::Tensor nll_loss_nd::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
    
    static auto op = create_nll_loss_nd_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index);
}

// aten::nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
at::Tensor nll_loss_nd::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
    
    static auto op = create_nll_loss_nd_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_out, name, "aten::nll_loss2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_out, schema_str, "nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)")

// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss2d_out::schema> create_nll_loss2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss2d_out::name, nll_loss2d_out::overload_name)
      .typed<nll_loss2d_out::schema>();
}

// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nll_loss2d_out::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & out) {
    
    static auto op = create_nll_loss2d_out_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index, out);
}

// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nll_loss2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & out) {
    
    static auto op = create_nll_loss2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d, name, "aten::nll_loss2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d, schema_str, "nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor")

// aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss2d::schema> create_nll_loss2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss2d::name, nll_loss2d::overload_name)
      .typed<nll_loss2d::schema>();
}

// aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
at::Tensor nll_loss2d::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
    
    static auto op = create_nll_loss2d_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index);
}

// aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
at::Tensor nll_loss2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
    
    static auto op = create_nll_loss2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_forward_output, name, "aten::nll_loss2d_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_forward_output, overload_name, "output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_forward_output, schema_str, "nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))")

// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss2d_forward_output::schema> create_nll_loss2d_forward_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss2d_forward_output::name, nll_loss2d_forward_output::overload_name)
      .typed<nll_loss2d_forward_output::schema>();
}

// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> nll_loss2d_forward_output::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & output, at::Tensor & total_weight) {
    
    static auto op = create_nll_loss2d_forward_output_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index, output, total_weight);
}

// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> nll_loss2d_forward_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & output, at::Tensor & total_weight) {
    
    static auto op = create_nll_loss2d_forward_output_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index, output, total_weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_forward, name, "aten::nll_loss2d_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_forward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_forward, schema_str, "nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)")

// aten::nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss2d_forward::schema> create_nll_loss2d_forward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss2d_forward::name, nll_loss2d_forward::overload_name)
      .typed<nll_loss2d_forward::schema>();
}

// aten::nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
::std::tuple<at::Tensor,at::Tensor> nll_loss2d_forward::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
    
    static auto op = create_nll_loss2d_forward_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index);
}

// aten::nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
::std::tuple<at::Tensor,at::Tensor> nll_loss2d_forward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
    
    static auto op = create_nll_loss2d_forward_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_backward_grad_input, name, "aten::nll_loss2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_backward_grad_input, schema_str, "nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss2d_backward_grad_input::schema> create_nll_loss2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss2d_backward_grad_input::name, nll_loss2d_backward_grad_input::overload_name)
      .typed<nll_loss2d_backward_grad_input::schema>();
}

// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & nll_loss2d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
    
    static auto op = create_nll_loss2d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, target, weight, reduction, ignore_index, total_weight, grad_input);
}

// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & nll_loss2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
    
    static auto op = create_nll_loss2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, weight, reduction, ignore_index, total_weight, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_backward, name, "aten::nll_loss2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_backward, schema_str, "nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor")

// aten::nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss2d_backward::schema> create_nll_loss2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss2d_backward::name, nll_loss2d_backward::overload_name)
      .typed<nll_loss2d_backward::schema>();
}

// aten::nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
at::Tensor nll_loss2d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight) {
    
    static auto op = create_nll_loss2d_backward_typed_handle();
    return op.call(grad_output, self, target, weight, reduction, ignore_index, total_weight);
}

// aten::nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
at::Tensor nll_loss2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight) {
    
    static auto op = create_nll_loss2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, weight, reduction, ignore_index, total_weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss_out, name, "aten::soft_margin_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss_out, schema_str, "soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)")

// aten::soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<soft_margin_loss_out::schema> create_soft_margin_loss_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(soft_margin_loss_out::name, soft_margin_loss_out::overload_name)
      .typed<soft_margin_loss_out::schema>();
}

// aten::soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & soft_margin_loss_out::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    
    static auto op = create_soft_margin_loss_out_typed_handle();
    return op.call(self, target, reduction, out);
}

// aten::soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & soft_margin_loss_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    
    static auto op = create_soft_margin_loss_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss, name, "aten::soft_margin_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss, schema_str, "soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor")

// aten::soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<soft_margin_loss::schema> create_soft_margin_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(soft_margin_loss::name, soft_margin_loss::overload_name)
      .typed<soft_margin_loss::schema>();
}

// aten::soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
at::Tensor soft_margin_loss::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    
    static auto op = create_soft_margin_loss_typed_handle();
    return op.call(self, target, reduction);
}

// aten::soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
at::Tensor soft_margin_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    
    static auto op = create_soft_margin_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_out, name, "aten::glu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_out, schema_str, "glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<glu_out::schema> create_glu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(glu_out::name, glu_out::overload_name)
      .typed<glu_out::schema>();
}

// aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & glu_out::call(const at::Tensor & self, int64_t dim, at::Tensor & out) {
    
    static auto op = create_glu_out_typed_handle();
    return op.call(self, dim, out);
}

// aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & glu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
    
    static auto op = create_glu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu, name, "aten::glu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu, schema_str, "glu(Tensor self, int dim=-1) -> Tensor")

// aten::glu(Tensor self, int dim=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<glu::schema> create_glu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(glu::name, glu::overload_name)
      .typed<glu::schema>();
}

// aten::glu(Tensor self, int dim=-1) -> Tensor
at::Tensor glu::call(const at::Tensor & self, int64_t dim) {
    
    static auto op = create_glu_typed_handle();
    return op.call(self, dim);
}

// aten::glu(Tensor self, int dim=-1) -> Tensor
at::Tensor glu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    
    static auto op = create_glu_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_backward_jvp, name, "aten::glu_backward_jvp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_backward_jvp, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_backward_jvp, schema_str, "glu_backward_jvp(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim) -> Tensor")

// aten::glu_backward_jvp(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<glu_backward_jvp::schema> create_glu_backward_jvp_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(glu_backward_jvp::name, glu_backward_jvp::overload_name)
      .typed<glu_backward_jvp::schema>();
}

// aten::glu_backward_jvp(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim) -> Tensor
at::Tensor glu_backward_jvp::call(const at::Tensor & grad_x, const at::Tensor & grad_glu, const at::Tensor & x, const at::Tensor & dgrad_glu, const at::Tensor & dx, int64_t dim) {
    
    static auto op = create_glu_backward_jvp_typed_handle();
    return op.call(grad_x, grad_glu, x, dgrad_glu, dx, dim);
}

// aten::glu_backward_jvp(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim) -> Tensor
at::Tensor glu_backward_jvp::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_x, const at::Tensor & grad_glu, const at::Tensor & x, const at::Tensor & dgrad_glu, const at::Tensor & dx, int64_t dim) {
    
    static auto op = create_glu_backward_jvp_typed_handle();
    return op.redispatch(dispatchKeySet, grad_x, grad_glu, x, dgrad_glu, dx, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_out, name, "aten::hardtanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_out, schema_str, "hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardtanh_out::schema> create_hardtanh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardtanh_out::name, hardtanh_out::overload_name)
      .typed<hardtanh_out::schema>();
}

// aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardtanh_out::call(const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & out) {
    
    static auto op = create_hardtanh_out_typed_handle();
    return op.call(self, min_val, max_val, out);
}

// aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardtanh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & out) {
    
    static auto op = create_hardtanh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, min_val, max_val, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh, name, "aten::hardtanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh, schema_str, "hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor")

// aten::hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hardtanh::schema> create_hardtanh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardtanh::name, hardtanh::overload_name)
      .typed<hardtanh::schema>();
}

// aten::hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor
at::Tensor hardtanh::call(const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    
    static auto op = create_hardtanh_typed_handle();
    return op.call(self, min_val, max_val);
}

// aten::hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor
at::Tensor hardtanh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    
    static auto op = create_hardtanh_typed_handle();
    return op.redispatch(dispatchKeySet, self, min_val, max_val);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_, name, "aten::hardtanh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_, schema_str, "hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)")

// aten::hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardtanh_::schema> create_hardtanh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardtanh_::name, hardtanh_::overload_name)
      .typed<hardtanh_::schema>();
}

// aten::hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)
at::Tensor & hardtanh_::call(at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    
    static auto op = create_hardtanh__typed_handle();
    return op.call(self, min_val, max_val);
}

// aten::hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)
at::Tensor & hardtanh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    
    static auto op = create_hardtanh__typed_handle();
    return op.redispatch(dispatchKeySet, self, min_val, max_val);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_backward, name, "aten::hardswish_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_backward, schema_str, "hardswish_backward(Tensor grad_output, Tensor self) -> Tensor")

// aten::hardswish_backward(Tensor grad_output, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hardswish_backward::schema> create_hardswish_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardswish_backward::name, hardswish_backward::overload_name)
      .typed<hardswish_backward::schema>();
}

// aten::hardswish_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor hardswish_backward::call(const at::Tensor & grad_output, const at::Tensor & self) {
    
    static auto op = create_hardswish_backward_typed_handle();
    return op.call(grad_output, self);
}

// aten::hardswish_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor hardswish_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
    
    static auto op = create_hardswish_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_out, name, "aten::leaky_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_out, schema_str, "leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)")

// aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<leaky_relu_out::schema> create_leaky_relu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(leaky_relu_out::name, leaky_relu_out::overload_name)
      .typed<leaky_relu_out::schema>();
}

// aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & leaky_relu_out::call(const at::Tensor & self, const at::Scalar & negative_slope, at::Tensor & out) {
    
    static auto op = create_leaky_relu_out_typed_handle();
    return op.call(self, negative_slope, out);
}

// aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & leaky_relu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & negative_slope, at::Tensor & out) {
    
    static auto op = create_leaky_relu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, negative_slope, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu, name, "aten::leaky_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu, schema_str, "leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor")

// aten::leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<leaky_relu::schema> create_leaky_relu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(leaky_relu::name, leaky_relu::overload_name)
      .typed<leaky_relu::schema>();
}

// aten::leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor
at::Tensor leaky_relu::call(const at::Tensor & self, const at::Scalar & negative_slope) {
    
    static auto op = create_leaky_relu_typed_handle();
    return op.call(self, negative_slope);
}

// aten::leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor
at::Tensor leaky_relu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & negative_slope) {
    
    static auto op = create_leaky_relu_typed_handle();
    return op.redispatch(dispatchKeySet, self, negative_slope);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_, name, "aten::leaky_relu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_, schema_str, "leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)")

// aten::leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<leaky_relu_::schema> create_leaky_relu__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(leaky_relu_::name, leaky_relu_::overload_name)
      .typed<leaky_relu_::schema>();
}

// aten::leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)
at::Tensor & leaky_relu_::call(at::Tensor & self, const at::Scalar & negative_slope) {
    
    static auto op = create_leaky_relu__typed_handle();
    return op.call(self, negative_slope);
}

// aten::leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)
at::Tensor & leaky_relu_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & negative_slope) {
    
    static auto op = create_leaky_relu__typed_handle();
    return op.redispatch(dispatchKeySet, self, negative_slope);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_forward_output, name, "aten::log_sigmoid_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_forward_output, overload_name, "output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_forward_output, schema_str, "log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))")

// aten::log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<log_sigmoid_forward_output::schema> create_log_sigmoid_forward_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_sigmoid_forward_output::name, log_sigmoid_forward_output::overload_name)
      .typed<log_sigmoid_forward_output::schema>();
}

// aten::log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> log_sigmoid_forward_output::call(const at::Tensor & self, at::Tensor & output, at::Tensor & buffer) {
    
    static auto op = create_log_sigmoid_forward_output_typed_handle();
    return op.call(self, output, buffer);
}

// aten::log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> log_sigmoid_forward_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & output, at::Tensor & buffer) {
    
    static auto op = create_log_sigmoid_forward_output_typed_handle();
    return op.redispatch(dispatchKeySet, self, output, buffer);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_forward, name, "aten::log_sigmoid_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_forward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_forward, schema_str, "log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)")

// aten::log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)
static C10_NOINLINE c10::TypedOperatorHandle<log_sigmoid_forward::schema> create_log_sigmoid_forward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_sigmoid_forward::name, log_sigmoid_forward::overload_name)
      .typed<log_sigmoid_forward::schema>();
}

// aten::log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)
::std::tuple<at::Tensor,at::Tensor> log_sigmoid_forward::call(const at::Tensor & self) {
    
    static auto op = create_log_sigmoid_forward_typed_handle();
    return op.call(self);
}

// aten::log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)
::std::tuple<at::Tensor,at::Tensor> log_sigmoid_forward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_log_sigmoid_forward_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_backward_grad_input, name, "aten::log_sigmoid_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_backward_grad_input, schema_str, "log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log_sigmoid_backward_grad_input::schema> create_log_sigmoid_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_sigmoid_backward_grad_input::name, log_sigmoid_backward_grad_input::overload_name)
      .typed<log_sigmoid_backward_grad_input::schema>();
}

// aten::log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & log_sigmoid_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer, at::Tensor & grad_input) {
    
    static auto op = create_log_sigmoid_backward_grad_input_typed_handle();
    return op.call(grad_output, self, buffer, grad_input);
}

// aten::log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & log_sigmoid_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer, at::Tensor & grad_input) {
    
    static auto op = create_log_sigmoid_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, buffer, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_backward, name, "aten::log_sigmoid_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_backward, schema_str, "log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor")

// aten::log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<log_sigmoid_backward::schema> create_log_sigmoid_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_sigmoid_backward::name, log_sigmoid_backward::overload_name)
      .typed<log_sigmoid_backward::schema>();
}

// aten::log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor
at::Tensor log_sigmoid_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer) {
    
    static auto op = create_log_sigmoid_backward_typed_handle();
    return op.call(grad_output, self, buffer);
}

// aten::log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor
at::Tensor log_sigmoid_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer) {
    
    static auto op = create_log_sigmoid_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, buffer);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink_out, name, "aten::softshrink")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink_out, schema_str, "softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)")

// aten::softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<softshrink_out::schema> create_softshrink_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softshrink_out::name, softshrink_out::overload_name)
      .typed<softshrink_out::schema>();
}

// aten::softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & softshrink_out::call(const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
    
    static auto op = create_softshrink_out_typed_handle();
    return op.call(self, lambd, out);
}

// aten::softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & softshrink_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
    
    static auto op = create_softshrink_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, lambd, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink, name, "aten::softshrink")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink, schema_str, "softshrink(Tensor self, Scalar lambd=0.5) -> Tensor")

// aten::softshrink(Tensor self, Scalar lambd=0.5) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<softshrink::schema> create_softshrink_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softshrink::name, softshrink::overload_name)
      .typed<softshrink::schema>();
}

// aten::softshrink(Tensor self, Scalar lambd=0.5) -> Tensor
at::Tensor softshrink::call(const at::Tensor & self, const at::Scalar & lambd) {
    
    static auto op = create_softshrink_typed_handle();
    return op.call(self, lambd);
}

// aten::softshrink(Tensor self, Scalar lambd=0.5) -> Tensor
at::Tensor softshrink::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & lambd) {
    
    static auto op = create_softshrink_typed_handle();
    return op.redispatch(dispatchKeySet, self, lambd);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d_backward_grad_input, name, "aten::adaptive_avg_pool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d_backward_grad_input, schema_str, "adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_avg_pool3d_backward_grad_input::schema> create_adaptive_avg_pool3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_avg_pool3d_backward_grad_input::name, adaptive_avg_pool3d_backward_grad_input::overload_name)
      .typed<adaptive_avg_pool3d_backward_grad_input::schema>();
}

// aten::adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & adaptive_avg_pool3d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
    
    static auto op = create_adaptive_avg_pool3d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, grad_input);
}

// aten::adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & adaptive_avg_pool3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
    
    static auto op = create_adaptive_avg_pool3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d_backward, name, "aten::_adaptive_avg_pool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d_backward, schema_str, "_adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -> Tensor")

// aten::_adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_adaptive_avg_pool3d_backward::schema> create__adaptive_avg_pool3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_adaptive_avg_pool3d_backward::name, _adaptive_avg_pool3d_backward::overload_name)
      .typed<_adaptive_avg_pool3d_backward::schema>();
}

// aten::_adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor _adaptive_avg_pool3d_backward::call(const at::Tensor & grad_output, const at::Tensor & self) {
    
    static auto op = create__adaptive_avg_pool3d_backward_typed_handle();
    return op.call(grad_output, self);
}

// aten::_adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor _adaptive_avg_pool3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
    
    static auto op = create__adaptive_avg_pool3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_backward_grad_input, name, "aten::adaptive_max_pool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_backward_grad_input, schema_str, "adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool2d_backward_grad_input::schema> create_adaptive_max_pool2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool2d_backward_grad_input::name, adaptive_max_pool2d_backward_grad_input::overload_name)
      .typed<adaptive_max_pool2d_backward_grad_input::schema>();
}

// aten::adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & adaptive_max_pool2d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
    
    static auto op = create_adaptive_max_pool2d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, indices, grad_input);
}

// aten::adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & adaptive_max_pool2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
    
    static auto op = create_adaptive_max_pool2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, indices, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_backward, name, "aten::adaptive_max_pool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_backward, schema_str, "adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor")

// aten::adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool2d_backward::schema> create_adaptive_max_pool2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool2d_backward::name, adaptive_max_pool2d_backward::overload_name)
      .typed<adaptive_max_pool2d_backward::schema>();
}

// aten::adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
at::Tensor adaptive_max_pool2d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices) {
    
    static auto op = create_adaptive_max_pool2d_backward_typed_handle();
    return op.call(grad_output, self, indices);
}

// aten::adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
at::Tensor adaptive_max_pool2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices) {
    
    static auto op = create_adaptive_max_pool2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_backward_grad_input, name, "aten::adaptive_max_pool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_backward_grad_input, schema_str, "adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool3d_backward_grad_input::schema> create_adaptive_max_pool3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool3d_backward_grad_input::name, adaptive_max_pool3d_backward_grad_input::overload_name)
      .typed<adaptive_max_pool3d_backward_grad_input::schema>();
}

// aten::adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & adaptive_max_pool3d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
    
    static auto op = create_adaptive_max_pool3d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, indices, grad_input);
}

// aten::adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & adaptive_max_pool3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
    
    static auto op = create_adaptive_max_pool3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, indices, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_backward, name, "aten::adaptive_max_pool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_backward, schema_str, "adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor")

// aten::adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool3d_backward::schema> create_adaptive_max_pool3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool3d_backward::name, adaptive_max_pool3d_backward::overload_name)
      .typed<adaptive_max_pool3d_backward::schema>();
}

// aten::adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
at::Tensor adaptive_max_pool3d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices) {
    
    static auto op = create_adaptive_max_pool3d_backward_typed_handle();
    return op.call(grad_output, self, indices);
}

// aten::adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
at::Tensor adaptive_max_pool3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices) {
    
    static auto op = create_adaptive_max_pool3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d_output, name, "aten::fractional_max_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d_output, overload_name, "output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d_output, schema_str, "fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))")

// aten::fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<fractional_max_pool3d_output::schema> create_fractional_max_pool3d_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fractional_max_pool3d_output::name, fractional_max_pool3d_output::overload_name)
      .typed<fractional_max_pool3d_output::schema>();
}

// aten::fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> fractional_max_pool3d_output::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
    
    static auto op = create_fractional_max_pool3d_output_typed_handle();
    return op.call(self, kernel_size, output_size, random_samples, output, indices);
}

// aten::fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> fractional_max_pool3d_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
    
    static auto op = create_fractional_max_pool3d_output_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, output_size, random_samples, output, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d, name, "aten::fractional_max_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d, schema_str, "fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples) -> (Tensor, Tensor)")

// aten::fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<fractional_max_pool3d::schema> create_fractional_max_pool3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fractional_max_pool3d::name, fractional_max_pool3d::overload_name)
      .typed<fractional_max_pool3d::schema>();
}

// aten::fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> fractional_max_pool3d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples) {
    
    static auto op = create_fractional_max_pool3d_typed_handle();
    return op.call(self, kernel_size, output_size, random_samples);
}

// aten::fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> fractional_max_pool3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples) {
    
    static auto op = create_fractional_max_pool3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, output_size, random_samples);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_out, name, "aten::reflection_pad3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_out, schema_str, "reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)")

// aten::reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad3d_out::schema> create_reflection_pad3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad3d_out::name, reflection_pad3d_out::overload_name)
      .typed<reflection_pad3d_out::schema>();
}

// aten::reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & reflection_pad3d_out::call(const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
    
    static auto op = create_reflection_pad3d_out_typed_handle();
    return op.call(self, padding, out);
}

// aten::reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & reflection_pad3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
    
    static auto op = create_reflection_pad3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d, name, "aten::reflection_pad3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d, schema_str, "reflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor")

// aten::reflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad3d::schema> create_reflection_pad3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad3d::name, reflection_pad3d::overload_name)
      .typed<reflection_pad3d::schema>();
}

// aten::reflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor
at::Tensor reflection_pad3d::call(const at::Tensor & self, c10::SymIntArrayRef padding) {
    
    static auto op = create_reflection_pad3d_typed_handle();
    return op.call(self, padding);
}

// aten::reflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor
at::Tensor reflection_pad3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding) {
    
    static auto op = create_reflection_pad3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d_out, name, "aten::replication_pad1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d_out, schema_str, "replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)")

// aten::replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad1d_out::schema> create_replication_pad1d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad1d_out::name, replication_pad1d_out::overload_name)
      .typed<replication_pad1d_out::schema>();
}

// aten::replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & replication_pad1d_out::call(const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
    
    static auto op = create_replication_pad1d_out_typed_handle();
    return op.call(self, padding, out);
}

// aten::replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & replication_pad1d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
    
    static auto op = create_replication_pad1d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d, name, "aten::replication_pad1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d, schema_str, "replication_pad1d(Tensor self, SymInt[2] padding) -> Tensor")

// aten::replication_pad1d(Tensor self, SymInt[2] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad1d::schema> create_replication_pad1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad1d::name, replication_pad1d::overload_name)
      .typed<replication_pad1d::schema>();
}

// aten::replication_pad1d(Tensor self, SymInt[2] padding) -> Tensor
at::Tensor replication_pad1d::call(const at::Tensor & self, c10::SymIntArrayRef padding) {
    
    static auto op = create_replication_pad1d_typed_handle();
    return op.call(self, padding);
}

// aten::replication_pad1d(Tensor self, SymInt[2] padding) -> Tensor
at::Tensor replication_pad1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding) {
    
    static auto op = create_replication_pad1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_out, name, "aten::replication_pad2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_out, schema_str, "replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)")

// aten::replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad2d_out::schema> create_replication_pad2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad2d_out::name, replication_pad2d_out::overload_name)
      .typed<replication_pad2d_out::schema>();
}

// aten::replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & replication_pad2d_out::call(const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
    
    static auto op = create_replication_pad2d_out_typed_handle();
    return op.call(self, padding, out);
}

// aten::replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & replication_pad2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
    
    static auto op = create_replication_pad2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d, name, "aten::replication_pad2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d, schema_str, "replication_pad2d(Tensor self, SymInt[4] padding) -> Tensor")

// aten::replication_pad2d(Tensor self, SymInt[4] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad2d::schema> create_replication_pad2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad2d::name, replication_pad2d::overload_name)
      .typed<replication_pad2d::schema>();
}

// aten::replication_pad2d(Tensor self, SymInt[4] padding) -> Tensor
at::Tensor replication_pad2d::call(const at::Tensor & self, c10::SymIntArrayRef padding) {
    
    static auto op = create_replication_pad2d_typed_handle();
    return op.call(self, padding);
}

// aten::replication_pad2d(Tensor self, SymInt[4] padding) -> Tensor
at::Tensor replication_pad2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding) {
    
    static auto op = create_replication_pad2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pad_circular, name, "aten::_pad_circular")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pad_circular, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pad_circular, schema_str, "_pad_circular(Tensor self, SymInt[] pad) -> Tensor")

// aten::_pad_circular(Tensor self, SymInt[] pad) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_pad_circular::schema> create__pad_circular_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_pad_circular::name, _pad_circular::overload_name)
      .typed<_pad_circular::schema>();
}

// aten::_pad_circular(Tensor self, SymInt[] pad) -> Tensor
at::Tensor _pad_circular::call(const at::Tensor & self, c10::SymIntArrayRef pad) {
    
    static auto op = create__pad_circular_typed_handle();
    return op.call(self, pad);
}

// aten::_pad_circular(Tensor self, SymInt[] pad) -> Tensor
at::Tensor _pad_circular::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef pad) {
    
    static auto op = create__pad_circular_typed_handle();
    return op.redispatch(dispatchKeySet, self, pad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pad, name, "aten::pad")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pad, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pad, schema_str, "pad(Tensor self, SymInt[] pad, str mode=\"constant\", float? value=None) -> Tensor")

// aten::pad(Tensor self, SymInt[] pad, str mode="constant", float? value=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<pad::schema> create_pad_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pad::name, pad::overload_name)
      .typed<pad::schema>();
}

// aten::pad(Tensor self, SymInt[] pad, str mode="constant", float? value=None) -> Tensor
at::Tensor pad::call(const at::Tensor & self, c10::SymIntArrayRef pad, c10::string_view mode, c10::optional<double> value) {
    
    static auto op = create_pad_typed_handle();
    return op.call(self, pad, mode, value);
}

// aten::pad(Tensor self, SymInt[] pad, str mode="constant", float? value=None) -> Tensor
at::Tensor pad::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef pad, c10::string_view mode, c10::optional<double> value) {
    
    static auto op = create_pad_typed_handle();
    return op.redispatch(dispatchKeySet, self, pad, mode, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_vec, name, "aten::upsample_nearest1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_vec, schema_str, "upsample_nearest1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor")

// aten::upsample_nearest1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest1d_vec::schema> create_upsample_nearest1d_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest1d_vec::name, upsample_nearest1d_vec::overload_name)
      .typed<upsample_nearest1d_vec::schema>();
}

// aten::upsample_nearest1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest1d_vec::call(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    
    static auto op = create_upsample_nearest1d_vec_typed_handle();
    return op.call(input, output_size, scale_factors);
}

// aten::upsample_nearest1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest1d_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    
    static auto op = create_upsample_nearest1d_vec_typed_handle();
    return op.redispatch(dispatchKeySet, input, output_size, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact1d_vec, name, "aten::_upsample_nearest_exact1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact1d_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact1d_vec, schema_str, "_upsample_nearest_exact1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor")

// aten::_upsample_nearest_exact1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_upsample_nearest_exact1d_vec::schema> create__upsample_nearest_exact1d_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_upsample_nearest_exact1d_vec::name, _upsample_nearest_exact1d_vec::overload_name)
      .typed<_upsample_nearest_exact1d_vec::schema>();
}

// aten::_upsample_nearest_exact1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
at::Tensor _upsample_nearest_exact1d_vec::call(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    
    static auto op = create__upsample_nearest_exact1d_vec_typed_handle();
    return op.call(input, output_size, scale_factors);
}

// aten::_upsample_nearest_exact1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
at::Tensor _upsample_nearest_exact1d_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    
    static auto op = create__upsample_nearest_exact1d_vec_typed_handle();
    return op.redispatch(dispatchKeySet, input, output_size, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_out, name, "aten::upsample_nearest1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_out, schema_str, "upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest1d_out::schema> create_upsample_nearest1d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest1d_out::name, upsample_nearest1d_out::overload_name)
      .typed<upsample_nearest1d_out::schema>();
}

// aten::upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_nearest1d_out::call(const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales, at::Tensor & out) {
    
    static auto op = create_upsample_nearest1d_out_typed_handle();
    return op.call(self, output_size, scales, out);
}

// aten::upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_nearest1d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales, at::Tensor & out) {
    
    static auto op = create_upsample_nearest1d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, scales, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact1d_out, name, "aten::_upsample_nearest_exact1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact1d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact1d_out, schema_str, "_upsample_nearest_exact1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_upsample_nearest_exact1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_upsample_nearest_exact1d_out::schema> create__upsample_nearest_exact1d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_upsample_nearest_exact1d_out::name, _upsample_nearest_exact1d_out::overload_name)
      .typed<_upsample_nearest_exact1d_out::schema>();
}

// aten::_upsample_nearest_exact1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _upsample_nearest_exact1d_out::call(const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales, at::Tensor & out) {
    
    static auto op = create__upsample_nearest_exact1d_out_typed_handle();
    return op.call(self, output_size, scales, out);
}

// aten::_upsample_nearest_exact1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _upsample_nearest_exact1d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales, at::Tensor & out) {
    
    static auto op = create__upsample_nearest_exact1d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, scales, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d, name, "aten::upsample_nearest1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d, schema_str, "upsample_nearest1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor")

// aten::upsample_nearest1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest1d::schema> create_upsample_nearest1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest1d::name, upsample_nearest1d::overload_name)
      .typed<upsample_nearest1d::schema>();
}

// aten::upsample_nearest1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor
at::Tensor upsample_nearest1d::call(const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales) {
    
    static auto op = create_upsample_nearest1d_typed_handle();
    return op.call(self, output_size, scales);
}

// aten::upsample_nearest1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor
at::Tensor upsample_nearest1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales) {
    
    static auto op = create_upsample_nearest1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, scales);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact1d, name, "aten::_upsample_nearest_exact1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_upsample_nearest_exact1d, schema_str, "_upsample_nearest_exact1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor")

// aten::_upsample_nearest_exact1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_upsample_nearest_exact1d::schema> create__upsample_nearest_exact1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_upsample_nearest_exact1d::name, _upsample_nearest_exact1d::overload_name)
      .typed<_upsample_nearest_exact1d::schema>();
}

// aten::_upsample_nearest_exact1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor
at::Tensor _upsample_nearest_exact1d::call(const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales) {
    
    static auto op = create__upsample_nearest_exact1d_typed_handle();
    return op.call(self, output_size, scales);
}

// aten::_upsample_nearest_exact1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor
at::Tensor _upsample_nearest_exact1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales) {
    
    static auto op = create__upsample_nearest_exact1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, scales);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d_out, name, "aten::_conv_depthwise2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d_out, schema_str, "_conv_depthwise2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding, SymInt[2] dilation, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_conv_depthwise2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding, SymInt[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_conv_depthwise2d_out::schema> create__conv_depthwise2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_conv_depthwise2d_out::name, _conv_depthwise2d_out::overload_name)
      .typed<_conv_depthwise2d_out::schema>();
}

// aten::_conv_depthwise2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding, SymInt[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
const at::Tensor & _conv_depthwise2d_out::call(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, const at::Tensor & out) {
    
    static auto op = create__conv_depthwise2d_out_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, dilation, out);
}

// aten::_conv_depthwise2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding, SymInt[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
const at::Tensor & _conv_depthwise2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, const at::Tensor & out) {
    
    static auto op = create__conv_depthwise2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, dilation, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d, name, "aten::_conv_depthwise2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d, schema_str, "_conv_depthwise2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding, SymInt[2] dilation) -> Tensor")

// aten::_conv_depthwise2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding, SymInt[2] dilation) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_conv_depthwise2d::schema> create__conv_depthwise2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_conv_depthwise2d::name, _conv_depthwise2d::overload_name)
      .typed<_conv_depthwise2d::schema>();
}

// aten::_conv_depthwise2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding, SymInt[2] dilation) -> Tensor
at::Tensor _conv_depthwise2d::call(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation) {
    
    static auto op = create__conv_depthwise2d_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, dilation);
}

// aten::_conv_depthwise2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding, SymInt[2] dilation) -> Tensor
at::Tensor _conv_depthwise2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation) {
    
    static auto op = create__conv_depthwise2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, dilation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_out, name, "aten::slow_conv3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_out, schema_str, "slow_conv3d.out(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::slow_conv3d.out(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv3d_out::schema> create_slow_conv3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv3d_out::name, slow_conv3d_out::overload_name)
      .typed<slow_conv3d_out::schema>();
}

// aten::slow_conv3d.out(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slow_conv3d_out::call(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor & out) {
    
    static auto op = create_slow_conv3d_out_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, out);
}

// aten::slow_conv3d.out(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slow_conv3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor & out) {
    
    static auto op = create_slow_conv3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d, name, "aten::slow_conv3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d, schema_str, "slow_conv3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0) -> Tensor")

// aten::slow_conv3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv3d::schema> create_slow_conv3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv3d::name, slow_conv3d::overload_name)
      .typed<slow_conv3d::schema>();
}

// aten::slow_conv3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0) -> Tensor
at::Tensor slow_conv3d::call(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding) {
    
    static auto op = create_slow_conv3d_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding);
}

// aten::slow_conv3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0) -> Tensor
at::Tensor slow_conv3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding) {
    
    static auto op = create_slow_conv3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_remove_batch_dim, name, "aten::_remove_batch_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_remove_batch_dim, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_remove_batch_dim, schema_str, "_remove_batch_dim(Tensor self, int level, int batch_size, int out_dim) -> Tensor")

// aten::_remove_batch_dim(Tensor self, int level, int batch_size, int out_dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_remove_batch_dim::schema> create__remove_batch_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_remove_batch_dim::name, _remove_batch_dim::overload_name)
      .typed<_remove_batch_dim::schema>();
}

// aten::_remove_batch_dim(Tensor self, int level, int batch_size, int out_dim) -> Tensor
at::Tensor _remove_batch_dim::call(const at::Tensor & self, int64_t level, int64_t batch_size, int64_t out_dim) {
    
    static auto op = create__remove_batch_dim_typed_handle();
    return op.call(self, level, batch_size, out_dim);
}

// aten::_remove_batch_dim(Tensor self, int level, int batch_size, int out_dim) -> Tensor
at::Tensor _remove_batch_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t level, int64_t batch_size, int64_t out_dim) {
    
    static auto op = create__remove_batch_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, level, batch_size, out_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log_ndtr, name, "aten::special_log_ndtr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log_ndtr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log_ndtr, schema_str, "special_log_ndtr(Tensor self) -> Tensor")

// aten::special_log_ndtr(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_log_ndtr::schema> create_special_log_ndtr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_log_ndtr::name, special_log_ndtr::overload_name)
      .typed<special_log_ndtr::schema>();
}

// aten::special_log_ndtr(Tensor self) -> Tensor
at::Tensor special_log_ndtr::call(const at::Tensor & self) {
    
    static auto op = create_special_log_ndtr_typed_handle();
    return op.call(self);
}

// aten::special_log_ndtr(Tensor self) -> Tensor
at::Tensor special_log_ndtr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_special_log_ndtr_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log_ndtr_out, name, "aten::special_log_ndtr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log_ndtr_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log_ndtr_out, schema_str, "special_log_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_log_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_log_ndtr_out::schema> create_special_log_ndtr_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_log_ndtr_out::name, special_log_ndtr_out::overload_name)
      .typed<special_log_ndtr_out::schema>();
}

// aten::special_log_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_log_ndtr_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_log_ndtr_out_typed_handle();
    return op.call(self, out);
}

// aten::special_log_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_log_ndtr_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_log_ndtr_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erf, name, "aten::special_erf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erf, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erf, schema_str, "special_erf(Tensor self) -> Tensor")

// aten::special_erf(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_erf::schema> create_special_erf_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_erf::name, special_erf::overload_name)
      .typed<special_erf::schema>();
}

// aten::special_erf(Tensor self) -> Tensor
at::Tensor special_erf::call(const at::Tensor & self) {
    
    static auto op = create_special_erf_typed_handle();
    return op.call(self);
}

// aten::special_erf(Tensor self) -> Tensor
at::Tensor special_erf::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_special_erf_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erf_out, name, "aten::special_erf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erf_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erf_out, schema_str, "special_erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_erf_out::schema> create_special_erf_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_erf_out::name, special_erf_out::overload_name)
      .typed<special_erf_out::schema>();
}

// aten::special_erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_erf_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_erf_out_typed_handle();
    return op.call(self, out);
}

// aten::special_erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_erf_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_erf_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy, name, "aten::special_xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy, schema_str, "special_xlogy(Tensor self, Tensor other) -> Tensor")

// aten::special_xlogy(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_xlogy::schema> create_special_xlogy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlogy::name, special_xlogy::overload_name)
      .typed<special_xlogy::schema>();
}

// aten::special_xlogy(Tensor self, Tensor other) -> Tensor
at::Tensor special_xlogy::call(const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_special_xlogy_typed_handle();
    return op.call(self, other);
}

// aten::special_xlogy(Tensor self, Tensor other) -> Tensor
at::Tensor special_xlogy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    
    static auto op = create_special_xlogy_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_self_scalar, name, "aten::special_xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_self_scalar, overload_name, "self_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_self_scalar, schema_str, "special_xlogy.self_scalar(Scalar self, Tensor other) -> Tensor")

// aten::special_xlogy.self_scalar(Scalar self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_xlogy_self_scalar::schema> create_special_xlogy_self_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlogy_self_scalar::name, special_xlogy_self_scalar::overload_name)
      .typed<special_xlogy_self_scalar::schema>();
}

// aten::special_xlogy.self_scalar(Scalar self, Tensor other) -> Tensor
at::Tensor special_xlogy_self_scalar::call(const at::Scalar & self, const at::Tensor & other) {
    
    static auto op = create_special_xlogy_self_scalar_typed_handle();
    return op.call(self, other);
}

// aten::special_xlogy.self_scalar(Scalar self, Tensor other) -> Tensor
at::Tensor special_xlogy_self_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other) {
    
    static auto op = create_special_xlogy_self_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_other_scalar, name, "aten::special_xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_other_scalar, overload_name, "other_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_other_scalar, schema_str, "special_xlogy.other_scalar(Tensor self, Scalar other) -> Tensor")

// aten::special_xlogy.other_scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_xlogy_other_scalar::schema> create_special_xlogy_other_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlogy_other_scalar::name, special_xlogy_other_scalar::overload_name)
      .typed<special_xlogy_other_scalar::schema>();
}

// aten::special_xlogy.other_scalar(Tensor self, Scalar other) -> Tensor
at::Tensor special_xlogy_other_scalar::call(const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_special_xlogy_other_scalar_typed_handle();
    return op.call(self, other);
}

// aten::special_xlogy.other_scalar(Tensor self, Scalar other) -> Tensor
at::Tensor special_xlogy_other_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    
    static auto op = create_special_xlogy_other_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_out, name, "aten::special_xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_out, schema_str, "special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_xlogy_out::schema> create_special_xlogy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlogy_out::name, special_xlogy_out::overload_name)
      .typed<special_xlogy_out::schema>();
}

// aten::special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlogy_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_special_xlogy_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlogy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_special_xlogy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_self_scalar_out, name, "aten::special_xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_self_scalar_out, overload_name, "self_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_self_scalar_out, schema_str, "special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_xlogy_self_scalar_out::schema> create_special_xlogy_self_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlogy_self_scalar_out::name, special_xlogy_self_scalar_out::overload_name)
      .typed<special_xlogy_self_scalar_out::schema>();
}

// aten::special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlogy_self_scalar_out::call(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_special_xlogy_self_scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlogy_self_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    
    static auto op = create_special_xlogy_self_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_other_scalar_out, name, "aten::special_xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_other_scalar_out, overload_name, "other_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_other_scalar_out, schema_str, "special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_xlogy_other_scalar_out::schema> create_special_xlogy_other_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlogy_other_scalar_out::name, special_xlogy_other_scalar_out::overload_name)
      .typed<special_xlogy_other_scalar_out::schema>();
}

// aten::special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlogy_other_scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_special_xlogy_other_scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlogy_other_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_special_xlogy_other_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expit, name, "aten::special_expit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expit, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expit, schema_str, "special_expit(Tensor self) -> Tensor")

// aten::special_expit(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_expit::schema> create_special_expit_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_expit::name, special_expit::overload_name)
      .typed<special_expit::schema>();
}

// aten::special_expit(Tensor self) -> Tensor
at::Tensor special_expit::call(const at::Tensor & self) {
    
    static auto op = create_special_expit_typed_handle();
    return op.call(self);
}

// aten::special_expit(Tensor self) -> Tensor
at::Tensor special_expit::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_special_expit_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expit_out, name, "aten::special_expit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expit_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expit_out, schema_str, "special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_expit_out::schema> create_special_expit_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_expit_out::name, special_expit_out::overload_name)
      .typed<special_expit_out::schema>();
}

// aten::special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_expit_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_expit_out_typed_handle();
    return op.call(self, out);
}

// aten::special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_expit_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_expit_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_sinc, name, "aten::special_sinc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_sinc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_sinc, schema_str, "special_sinc(Tensor self) -> Tensor")

// aten::special_sinc(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_sinc::schema> create_special_sinc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_sinc::name, special_sinc::overload_name)
      .typed<special_sinc::schema>();
}

// aten::special_sinc(Tensor self) -> Tensor
at::Tensor special_sinc::call(const at::Tensor & self) {
    
    static auto op = create_special_sinc_typed_handle();
    return op.call(self);
}

// aten::special_sinc(Tensor self) -> Tensor
at::Tensor special_sinc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_special_sinc_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_sinc_out, name, "aten::special_sinc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_sinc_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_sinc_out, schema_str, "special_sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_sinc_out::schema> create_special_sinc_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_sinc_out::name, special_sinc_out::overload_name)
      .typed<special_sinc_out::schema>();
}

// aten::special_sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_sinc_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_sinc_out_typed_handle();
    return op.call(self, out);
}

// aten::special_sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_sinc_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_sinc_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_softmax, name, "aten::special_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_softmax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_softmax, schema_str, "special_softmax(Tensor self, int dim, ScalarType? dtype=None) -> Tensor")

// aten::special_softmax(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_softmax::schema> create_special_softmax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_softmax::name, special_softmax::overload_name)
      .typed<special_softmax::schema>();
}

// aten::special_softmax(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
at::Tensor special_softmax::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_special_softmax_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::special_softmax(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
at::Tensor special_softmax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    
    static auto op = create_special_softmax_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft, name, "aten::fft_fft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft, schema_str, "fft_fft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor")

// aten::fft_fft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_fft::schema> create_fft_fft_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_fft::name, fft_fft::overload_name)
      .typed<fft_fft::schema>();
}

// aten::fft_fft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_fft::call(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_fft_typed_handle();
    return op.call(self, n, dim, norm);
}

// aten::fft_fft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_fft::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_fft_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft_out, name, "aten::fft_fft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft_out, schema_str, "fft_fft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_fft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_fft_out::schema> create_fft_fft_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_fft_out::name, fft_fft_out::overload_name)
      .typed<fft_fft_out::schema>();
}

// aten::fft_fft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_fft_out::call(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    
    static auto op = create_fft_fft_out_typed_handle();
    return op.call(self, n, dim, norm, out);
}

// aten::fft_fft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_fft_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    
    static auto op = create_fft_fft_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft, name, "aten::fft_rfft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft, schema_str, "fft_rfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor")

// aten::fft_rfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_rfft::schema> create_fft_rfft_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_rfft::name, fft_rfft::overload_name)
      .typed<fft_rfft::schema>();
}

// aten::fft_rfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_rfft::call(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_rfft_typed_handle();
    return op.call(self, n, dim, norm);
}

// aten::fft_rfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_rfft::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_rfft_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft_out, name, "aten::fft_rfft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft_out, schema_str, "fft_rfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_rfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_rfft_out::schema> create_fft_rfft_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_rfft_out::name, fft_rfft_out::overload_name)
      .typed<fft_rfft_out::schema>();
}

// aten::fft_rfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_rfft_out::call(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    
    static auto op = create_fft_rfft_out_typed_handle();
    return op.call(self, n, dim, norm, out);
}

// aten::fft_rfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_rfft_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    
    static auto op = create_fft_rfft_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_hfft2, name, "aten::fft_hfft2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_hfft2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_hfft2, schema_str, "fft_hfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor")

// aten::fft_hfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_hfft2::schema> create_fft_hfft2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_hfft2::name, fft_hfft2::overload_name)
      .typed<fft_hfft2::schema>();
}

// aten::fft_hfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
at::Tensor fft_hfft2::call(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_hfft2_typed_handle();
    return op.call(self, s, dim, norm);
}

// aten::fft_hfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
at::Tensor fft_hfft2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_hfft2_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_hfft2_out, name, "aten::fft_hfft2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_hfft2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_hfft2_out, schema_str, "fft_hfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_hfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_hfft2_out::schema> create_fft_hfft2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_hfft2_out::name, fft_hfft2_out::overload_name)
      .typed<fft_hfft2_out::schema>();
}

// aten::fft_hfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
const at::Tensor & fft_hfft2_out::call(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
    
    static auto op = create_fft_hfft2_out_typed_handle();
    return op.call(self, s, dim, norm, out);
}

// aten::fft_hfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
const at::Tensor & fft_hfft2_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
    
    static auto op = create_fft_hfft2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftn, name, "aten::fft_ifftn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftn, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftn, schema_str, "fft_ifftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor")

// aten::fft_ifftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_ifftn::schema> create_fft_ifftn_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ifftn::name, fft_ifftn::overload_name)
      .typed<fft_ifftn::schema>();
}

// aten::fft_ifftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
at::Tensor fft_ifftn::call(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_ifftn_typed_handle();
    return op.call(self, s, dim, norm);
}

// aten::fft_ifftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
at::Tensor fft_ifftn::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_ifftn_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftn_out, name, "aten::fft_ifftn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftn_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftn_out, schema_str, "fft_ifftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_ifftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_ifftn_out::schema> create_fft_ifftn_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ifftn_out::name, fft_ifftn_out::overload_name)
      .typed<fft_ifftn_out::schema>();
}

// aten::fft_ifftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_ifftn_out::call(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    
    static auto op = create_fft_ifftn_out_typed_handle();
    return op.call(self, s, dim, norm, out);
}

// aten::fft_ifftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_ifftn_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    
    static auto op = create_fft_ifftn_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ihfftn, name, "aten::fft_ihfftn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ihfftn, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ihfftn, schema_str, "fft_ihfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor")

// aten::fft_ihfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_ihfftn::schema> create_fft_ihfftn_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ihfftn::name, fft_ihfftn::overload_name)
      .typed<fft_ihfftn::schema>();
}

// aten::fft_ihfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
at::Tensor fft_ihfftn::call(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_ihfftn_typed_handle();
    return op.call(self, s, dim, norm);
}

// aten::fft_ihfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
at::Tensor fft_ihfftn::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
    
    static auto op = create_fft_ihfftn_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ihfftn_out, name, "aten::fft_ihfftn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ihfftn_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ihfftn_out, schema_str, "fft_ihfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_ihfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_ihfftn_out::schema> create_fft_ihfftn_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ihfftn_out::name, fft_ihfftn_out::overload_name)
      .typed<fft_ihfftn_out::schema>();
}

// aten::fft_ihfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
const at::Tensor & fft_ihfftn_out::call(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
    
    static auto op = create_fft_ihfftn_out_typed_handle();
    return op.call(self, s, dim, norm, out);
}

// aten::fft_ihfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
const at::Tensor & fft_ihfftn_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
    
    static auto op = create_fft_ihfftn_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftfreq, name, "aten::fft_fftfreq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftfreq, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftfreq, schema_str, "fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_fftfreq::schema> create_fft_fftfreq_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_fftfreq::name, fft_fftfreq::overload_name)
      .typed<fft_fftfreq::schema>();
}

// aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor fft_fftfreq::call(int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_fft_fftfreq_typed_handle();
    return op.call(n, d, dtype, layout, device, pin_memory);
}

// aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor fft_fftfreq::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_fft_fftfreq_typed_handle();
    return op.redispatch(dispatchKeySet, n, d, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftfreq_out, name, "aten::fft_fftfreq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftfreq_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftfreq_out, schema_str, "fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_fftfreq_out::schema> create_fft_fftfreq_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_fftfreq_out::name, fft_fftfreq_out::overload_name)
      .typed<fft_fftfreq_out::schema>();
}

// aten::fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_fftfreq_out::call(int64_t n, double d, at::Tensor & out) {
    
    static auto op = create_fft_fftfreq_out_typed_handle();
    return op.call(n, d, out);
}

// aten::fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_fftfreq_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, double d, at::Tensor & out) {
    
    static auto op = create_fft_fftfreq_out_typed_handle();
    return op.redispatch(dispatchKeySet, n, d, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftfreq, name, "aten::fft_rfftfreq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftfreq, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftfreq, schema_str, "fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_rfftfreq::schema> create_fft_rfftfreq_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_rfftfreq::name, fft_rfftfreq::overload_name)
      .typed<fft_rfftfreq::schema>();
}

// aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor fft_rfftfreq::call(int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_fft_rfftfreq_typed_handle();
    return op.call(n, d, dtype, layout, device, pin_memory);
}

// aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor fft_rfftfreq::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    
    static auto op = create_fft_rfftfreq_typed_handle();
    return op.redispatch(dispatchKeySet, n, d, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftfreq_out, name, "aten::fft_rfftfreq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftfreq_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftfreq_out, schema_str, "fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_rfftfreq_out::schema> create_fft_rfftfreq_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_rfftfreq_out::name, fft_rfftfreq_out::overload_name)
      .typed<fft_rfftfreq_out::schema>();
}

// aten::fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_rfftfreq_out::call(int64_t n, double d, at::Tensor & out) {
    
    static auto op = create_fft_rfftfreq_out_typed_handle();
    return op.call(n, d, out);
}

// aten::fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_rfftfreq_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, double d, at::Tensor & out) {
    
    static auto op = create_fft_rfftfreq_out_typed_handle();
    return op.redispatch(dispatchKeySet, n, d, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_ex, name, "aten::linalg_cholesky_ex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_ex, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_ex, schema_str, "linalg_cholesky_ex(Tensor self, *, bool upper=False, bool check_errors=False) -> (Tensor L, Tensor info)")

// aten::linalg_cholesky_ex(Tensor self, *, bool upper=False, bool check_errors=False) -> (Tensor L, Tensor info)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cholesky_ex::schema> create_linalg_cholesky_ex_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cholesky_ex::name, linalg_cholesky_ex::overload_name)
      .typed<linalg_cholesky_ex::schema>();
}

// aten::linalg_cholesky_ex(Tensor self, *, bool upper=False, bool check_errors=False) -> (Tensor L, Tensor info)
::std::tuple<at::Tensor,at::Tensor> linalg_cholesky_ex::call(const at::Tensor & self, bool upper, bool check_errors) {
    
    static auto op = create_linalg_cholesky_ex_typed_handle();
    return op.call(self, upper, check_errors);
}

// aten::linalg_cholesky_ex(Tensor self, *, bool upper=False, bool check_errors=False) -> (Tensor L, Tensor info)
::std::tuple<at::Tensor,at::Tensor> linalg_cholesky_ex::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper, bool check_errors) {
    
    static auto op = create_linalg_cholesky_ex_typed_handle();
    return op.redispatch(dispatchKeySet, self, upper, check_errors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_ex_L, name, "aten::linalg_cholesky_ex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_ex_L, overload_name, "L")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_ex_L, schema_str, "linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -> (Tensor(a!) L, Tensor(b!) info)")

// aten::linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -> (Tensor(a!) L, Tensor(b!) info)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cholesky_ex_L::schema> create_linalg_cholesky_ex_L_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cholesky_ex_L::name, linalg_cholesky_ex_L::overload_name)
      .typed<linalg_cholesky_ex_L::schema>();
}

// aten::linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -> (Tensor(a!) L, Tensor(b!) info)
::std::tuple<at::Tensor &,at::Tensor &> linalg_cholesky_ex_L::call(const at::Tensor & self, bool upper, bool check_errors, at::Tensor & L, at::Tensor & info) {
    
    static auto op = create_linalg_cholesky_ex_L_typed_handle();
    return op.call(self, upper, check_errors, L, info);
}

// aten::linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -> (Tensor(a!) L, Tensor(b!) info)
::std::tuple<at::Tensor &,at::Tensor &> linalg_cholesky_ex_L::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper, bool check_errors, at::Tensor & L, at::Tensor & info) {
    
    static auto op = create_linalg_cholesky_ex_L_typed_handle();
    return op.redispatch(dispatchKeySet, self, upper, check_errors, L, info);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cross, name, "aten::linalg_cross")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cross, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cross, schema_str, "linalg_cross(Tensor self, Tensor other, *, int dim=-1) -> Tensor")

// aten::linalg_cross(Tensor self, Tensor other, *, int dim=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cross::schema> create_linalg_cross_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cross::name, linalg_cross::overload_name)
      .typed<linalg_cross::schema>();
}

// aten::linalg_cross(Tensor self, Tensor other, *, int dim=-1) -> Tensor
at::Tensor linalg_cross::call(const at::Tensor & self, const at::Tensor & other, int64_t dim) {
    
    static auto op = create_linalg_cross_typed_handle();
    return op.call(self, other, dim);
}

// aten::linalg_cross(Tensor self, Tensor other, *, int dim=-1) -> Tensor
at::Tensor linalg_cross::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, int64_t dim) {
    
    static auto op = create_linalg_cross_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cross_out, name, "aten::linalg_cross")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cross_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cross_out, schema_str, "linalg_cross.out(Tensor self, Tensor other, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_cross.out(Tensor self, Tensor other, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cross_out::schema> create_linalg_cross_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cross_out::name, linalg_cross_out::overload_name)
      .typed<linalg_cross_out::schema>();
}

// aten::linalg_cross.out(Tensor self, Tensor other, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_cross_out::call(const at::Tensor & self, const at::Tensor & other, int64_t dim, at::Tensor & out) {
    
    static auto op = create_linalg_cross_out_typed_handle();
    return op.call(self, other, dim, out);
}

// aten::linalg_cross.out(Tensor self, Tensor other, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_cross_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, int64_t dim, at::Tensor & out) {
    
    static auto op = create_linalg_cross_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_lu_factor_ex, name, "aten::linalg_lu_factor_ex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_lu_factor_ex, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_lu_factor_ex, schema_str, "linalg_lu_factor_ex(Tensor A, *, bool pivot=True, bool check_errors=False) -> (Tensor LU, Tensor pivots, Tensor info)")

// aten::linalg_lu_factor_ex(Tensor A, *, bool pivot=True, bool check_errors=False) -> (Tensor LU, Tensor pivots, Tensor info)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_lu_factor_ex::schema> create_linalg_lu_factor_ex_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_lu_factor_ex::name, linalg_lu_factor_ex::overload_name)
      .typed<linalg_lu_factor_ex::schema>();
}

// aten::linalg_lu_factor_ex(Tensor A, *, bool pivot=True, bool check_errors=False) -> (Tensor LU, Tensor pivots, Tensor info)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> linalg_lu_factor_ex::call(const at::Tensor & A, bool pivot, bool check_errors) {
    
    static auto op = create_linalg_lu_factor_ex_typed_handle();
    return op.call(A, pivot, check_errors);
}

// aten::linalg_lu_factor_ex(Tensor A, *, bool pivot=True, bool check_errors=False) -> (Tensor LU, Tensor pivots, Tensor info)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> linalg_lu_factor_ex::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, bool pivot, bool check_errors) {
    
    static auto op = create_linalg_lu_factor_ex_typed_handle();
    return op.redispatch(dispatchKeySet, A, pivot, check_errors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_lu_factor_ex_out, name, "aten::linalg_lu_factor_ex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_lu_factor_ex_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_lu_factor_ex_out, schema_str, "linalg_lu_factor_ex.out(Tensor A, *, bool pivot=True, bool check_errors=False, Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info) -> (Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info)")

// aten::linalg_lu_factor_ex.out(Tensor A, *, bool pivot=True, bool check_errors=False, Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info) -> (Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_lu_factor_ex_out::schema> create_linalg_lu_factor_ex_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_lu_factor_ex_out::name, linalg_lu_factor_ex_out::overload_name)
      .typed<linalg_lu_factor_ex_out::schema>();
}

// aten::linalg_lu_factor_ex.out(Tensor A, *, bool pivot=True, bool check_errors=False, Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info) -> (Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info)
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_lu_factor_ex_out::call(const at::Tensor & A, bool pivot, bool check_errors, at::Tensor & LU, at::Tensor & pivots, at::Tensor & info) {
    
    static auto op = create_linalg_lu_factor_ex_out_typed_handle();
    return op.call(A, pivot, check_errors, LU, pivots, info);
}

// aten::linalg_lu_factor_ex.out(Tensor A, *, bool pivot=True, bool check_errors=False, Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info) -> (Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info)
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_lu_factor_ex_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, bool pivot, bool check_errors, at::Tensor & LU, at::Tensor & pivots, at::Tensor & info) {
    
    static auto op = create_linalg_lu_factor_ex_out_typed_handle();
    return op.redispatch(dispatchKeySet, A, pivot, check_errors, LU, pivots, info);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(det, name, "aten::det")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(det, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(det, schema_str, "det(Tensor self) -> Tensor")

// aten::det(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<det::schema> create_det_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(det::name, det::overload_name)
      .typed<det::schema>();
}

// aten::det(Tensor self) -> Tensor
at::Tensor det::call(const at::Tensor & self) {
    
    static auto op = create_det_typed_handle();
    return op.call(self);
}

// aten::det(Tensor self) -> Tensor
at::Tensor det::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_det_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inverse, name, "aten::inverse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inverse, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inverse, schema_str, "inverse(Tensor self) -> Tensor")

// aten::inverse(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<inverse::schema> create_inverse_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(inverse::name, inverse::overload_name)
      .typed<inverse::schema>();
}

// aten::inverse(Tensor self) -> Tensor
at::Tensor inverse::call(const at::Tensor & self) {
    
    static auto op = create_inverse_typed_handle();
    return op.call(self);
}

// aten::inverse(Tensor self) -> Tensor
at::Tensor inverse::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_inverse_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inverse_out, name, "aten::inverse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inverse_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inverse_out, schema_str, "inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<inverse_out::schema> create_inverse_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(inverse_out::name, inverse_out::overload_name)
      .typed<inverse_out::schema>();
}

// aten::inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & inverse_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_inverse_out_typed_handle();
    return op.call(self, out);
}

// aten::inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & inverse_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_inverse_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond, name, "aten::linalg_cond")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond, schema_str, "linalg_cond(Tensor self, Scalar? p=None) -> Tensor")

// aten::linalg_cond(Tensor self, Scalar? p=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cond::schema> create_linalg_cond_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cond::name, linalg_cond::overload_name)
      .typed<linalg_cond::schema>();
}

// aten::linalg_cond(Tensor self, Scalar? p=None) -> Tensor
at::Tensor linalg_cond::call(const at::Tensor & self, const c10::optional<at::Scalar> & p) {
    
    static auto op = create_linalg_cond_typed_handle();
    return op.call(self, p);
}

// aten::linalg_cond(Tensor self, Scalar? p=None) -> Tensor
at::Tensor linalg_cond::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p) {
    
    static auto op = create_linalg_cond_typed_handle();
    return op.redispatch(dispatchKeySet, self, p);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_out, name, "aten::linalg_cond")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_out, schema_str, "linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cond_out::schema> create_linalg_cond_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cond_out::name, linalg_cond_out::overload_name)
      .typed<linalg_cond_out::schema>();
}

// aten::linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_cond_out::call(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::Tensor & out) {
    
    static auto op = create_linalg_cond_out_typed_handle();
    return op.call(self, p, out);
}

// aten::linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_cond_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::Tensor & out) {
    
    static auto op = create_linalg_cond_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_p_str, name, "aten::linalg_cond")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_p_str, overload_name, "p_str")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_p_str, schema_str, "linalg_cond.p_str(Tensor self, str p) -> Tensor")

// aten::linalg_cond.p_str(Tensor self, str p) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cond_p_str::schema> create_linalg_cond_p_str_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cond_p_str::name, linalg_cond_p_str::overload_name)
      .typed<linalg_cond_p_str::schema>();
}

// aten::linalg_cond.p_str(Tensor self, str p) -> Tensor
at::Tensor linalg_cond_p_str::call(const at::Tensor & self, c10::string_view p) {
    
    static auto op = create_linalg_cond_p_str_typed_handle();
    return op.call(self, p);
}

// aten::linalg_cond.p_str(Tensor self, str p) -> Tensor
at::Tensor linalg_cond_p_str::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view p) {
    
    static auto op = create_linalg_cond_p_str_typed_handle();
    return op.redispatch(dispatchKeySet, self, p);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_p_str_out, name, "aten::linalg_cond")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_p_str_out, overload_name, "p_str_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_p_str_out, schema_str, "linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cond_p_str_out::schema> create_linalg_cond_p_str_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cond_p_str_out::name, linalg_cond_p_str_out::overload_name)
      .typed<linalg_cond_p_str_out::schema>();
}

// aten::linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_cond_p_str_out::call(const at::Tensor & self, c10::string_view p, at::Tensor & out) {
    
    static auto op = create_linalg_cond_p_str_out_typed_handle();
    return op.call(self, p, out);
}

// aten::linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_cond_p_str_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view p, at::Tensor & out) {
    
    static auto op = create_linalg_cond_p_str_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_atol_rtol_tensor, name, "aten::linalg_pinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_atol_rtol_tensor, overload_name, "atol_rtol_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_atol_rtol_tensor, schema_str, "linalg_pinv.atol_rtol_tensor(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -> Tensor")

// aten::linalg_pinv.atol_rtol_tensor(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_pinv_atol_rtol_tensor::schema> create_linalg_pinv_atol_rtol_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_pinv_atol_rtol_tensor::name, linalg_pinv_atol_rtol_tensor::overload_name)
      .typed<linalg_pinv_atol_rtol_tensor::schema>();
}

// aten::linalg_pinv.atol_rtol_tensor(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -> Tensor
at::Tensor linalg_pinv_atol_rtol_tensor::call(const at::Tensor & self, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian) {
    
    static auto op = create_linalg_pinv_atol_rtol_tensor_typed_handle();
    return op.call(self, atol, rtol, hermitian);
}

// aten::linalg_pinv.atol_rtol_tensor(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -> Tensor
at::Tensor linalg_pinv_atol_rtol_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian) {
    
    static auto op = create_linalg_pinv_atol_rtol_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, atol, rtol, hermitian);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_atol_rtol_tensor_out, name, "aten::linalg_pinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_atol_rtol_tensor_out, overload_name, "atol_rtol_tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_atol_rtol_tensor_out, schema_str, "linalg_pinv.atol_rtol_tensor_out(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_pinv.atol_rtol_tensor_out(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_pinv_atol_rtol_tensor_out::schema> create_linalg_pinv_atol_rtol_tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_pinv_atol_rtol_tensor_out::name, linalg_pinv_atol_rtol_tensor_out::overload_name)
      .typed<linalg_pinv_atol_rtol_tensor_out::schema>();
}

// aten::linalg_pinv.atol_rtol_tensor_out(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_pinv_atol_rtol_tensor_out::call(const at::Tensor & self, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_pinv_atol_rtol_tensor_out_typed_handle();
    return op.call(self, atol, rtol, hermitian, out);
}

// aten::linalg_pinv.atol_rtol_tensor_out(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_pinv_atol_rtol_tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_pinv_atol_rtol_tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, atol, rtol, hermitian, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_atol_rtol_float, name, "aten::linalg_pinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_atol_rtol_float, overload_name, "atol_rtol_float")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_atol_rtol_float, schema_str, "linalg_pinv.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -> Tensor")

// aten::linalg_pinv.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_pinv_atol_rtol_float::schema> create_linalg_pinv_atol_rtol_float_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_pinv_atol_rtol_float::name, linalg_pinv_atol_rtol_float::overload_name)
      .typed<linalg_pinv_atol_rtol_float::schema>();
}

// aten::linalg_pinv.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -> Tensor
at::Tensor linalg_pinv_atol_rtol_float::call(const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian) {
    
    static auto op = create_linalg_pinv_atol_rtol_float_typed_handle();
    return op.call(self, atol, rtol, hermitian);
}

// aten::linalg_pinv.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -> Tensor
at::Tensor linalg_pinv_atol_rtol_float::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian) {
    
    static auto op = create_linalg_pinv_atol_rtol_float_typed_handle();
    return op.redispatch(dispatchKeySet, self, atol, rtol, hermitian);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_atol_rtol_float_out, name, "aten::linalg_pinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_atol_rtol_float_out, overload_name, "atol_rtol_float_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_atol_rtol_float_out, schema_str, "linalg_pinv.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_pinv.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_pinv_atol_rtol_float_out::schema> create_linalg_pinv_atol_rtol_float_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_pinv_atol_rtol_float_out::name, linalg_pinv_atol_rtol_float_out::overload_name)
      .typed<linalg_pinv_atol_rtol_float_out::schema>();
}

// aten::linalg_pinv.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_pinv_atol_rtol_float_out::call(const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_pinv_atol_rtol_float_out_typed_handle();
    return op.call(self, atol, rtol, hermitian, out);
}

// aten::linalg_pinv.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_pinv_atol_rtol_float_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_pinv_atol_rtol_float_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, atol, rtol, hermitian, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv, name, "aten::linalg_pinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv, schema_str, "linalg_pinv(Tensor self, float rcond, bool hermitian=False) -> Tensor")

// aten::linalg_pinv(Tensor self, float rcond, bool hermitian=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_pinv::schema> create_linalg_pinv_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_pinv::name, linalg_pinv::overload_name)
      .typed<linalg_pinv::schema>();
}

// aten::linalg_pinv(Tensor self, float rcond, bool hermitian=False) -> Tensor
at::Tensor linalg_pinv::call(const at::Tensor & self, double rcond, bool hermitian) {
    
    static auto op = create_linalg_pinv_typed_handle();
    return op.call(self, rcond, hermitian);
}

// aten::linalg_pinv(Tensor self, float rcond, bool hermitian=False) -> Tensor
at::Tensor linalg_pinv::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double rcond, bool hermitian) {
    
    static auto op = create_linalg_pinv_typed_handle();
    return op.redispatch(dispatchKeySet, self, rcond, hermitian);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_rcond_tensor, name, "aten::linalg_pinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_rcond_tensor, overload_name, "rcond_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_rcond_tensor, schema_str, "linalg_pinv.rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False) -> Tensor")

// aten::linalg_pinv.rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_pinv_rcond_tensor::schema> create_linalg_pinv_rcond_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_pinv_rcond_tensor::name, linalg_pinv_rcond_tensor::overload_name)
      .typed<linalg_pinv_rcond_tensor::schema>();
}

// aten::linalg_pinv.rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False) -> Tensor
at::Tensor linalg_pinv_rcond_tensor::call(const at::Tensor & self, const at::Tensor & rcond, bool hermitian) {
    
    static auto op = create_linalg_pinv_rcond_tensor_typed_handle();
    return op.call(self, rcond, hermitian);
}

// aten::linalg_pinv.rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False) -> Tensor
at::Tensor linalg_pinv_rcond_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & rcond, bool hermitian) {
    
    static auto op = create_linalg_pinv_rcond_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, rcond, hermitian);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_out, name, "aten::linalg_pinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_out, schema_str, "linalg_pinv.out(Tensor self, float rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_pinv.out(Tensor self, float rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_pinv_out::schema> create_linalg_pinv_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_pinv_out::name, linalg_pinv_out::overload_name)
      .typed<linalg_pinv_out::schema>();
}

// aten::linalg_pinv.out(Tensor self, float rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_pinv_out::call(const at::Tensor & self, double rcond, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_pinv_out_typed_handle();
    return op.call(self, rcond, hermitian, out);
}

// aten::linalg_pinv.out(Tensor self, float rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_pinv_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double rcond, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_pinv_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, rcond, hermitian, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_out_rcond_tensor, name, "aten::linalg_pinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_out_rcond_tensor, overload_name, "out_rcond_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_out_rcond_tensor, schema_str, "linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_pinv_out_rcond_tensor::schema> create_linalg_pinv_out_rcond_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_pinv_out_rcond_tensor::name, linalg_pinv_out_rcond_tensor::overload_name)
      .typed<linalg_pinv_out_rcond_tensor::schema>();
}

// aten::linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_pinv_out_rcond_tensor::call(const at::Tensor & self, const at::Tensor & rcond, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_pinv_out_rcond_tensor_typed_handle();
    return op.call(self, rcond, hermitian, out);
}

// aten::linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_pinv_out_rcond_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & rcond, bool hermitian, at::Tensor & out) {
    
    static auto op = create_linalg_pinv_out_rcond_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, rcond, hermitian, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_ex, name, "aten::linalg_solve_ex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_ex, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_ex, schema_str, "linalg_solve_ex(Tensor A, Tensor B, *, bool left=True, bool check_errors=False) -> (Tensor result, Tensor info)")

// aten::linalg_solve_ex(Tensor A, Tensor B, *, bool left=True, bool check_errors=False) -> (Tensor result, Tensor info)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_solve_ex::schema> create_linalg_solve_ex_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_solve_ex::name, linalg_solve_ex::overload_name)
      .typed<linalg_solve_ex::schema>();
}

// aten::linalg_solve_ex(Tensor A, Tensor B, *, bool left=True, bool check_errors=False) -> (Tensor result, Tensor info)
::std::tuple<at::Tensor,at::Tensor> linalg_solve_ex::call(const at::Tensor & A, const at::Tensor & B, bool left, bool check_errors) {
    
    static auto op = create_linalg_solve_ex_typed_handle();
    return op.call(A, B, left, check_errors);
}

// aten::linalg_solve_ex(Tensor A, Tensor B, *, bool left=True, bool check_errors=False) -> (Tensor result, Tensor info)
::std::tuple<at::Tensor,at::Tensor> linalg_solve_ex::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, const at::Tensor & B, bool left, bool check_errors) {
    
    static auto op = create_linalg_solve_ex_typed_handle();
    return op.redispatch(dispatchKeySet, A, B, left, check_errors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_ex_out, name, "aten::linalg_solve_ex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_ex_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_ex_out, schema_str, "linalg_solve_ex.out(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) info) -> (Tensor(a!) result, Tensor(b!) info)")

// aten::linalg_solve_ex.out(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) info) -> (Tensor(a!) result, Tensor(b!) info)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_solve_ex_out::schema> create_linalg_solve_ex_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_solve_ex_out::name, linalg_solve_ex_out::overload_name)
      .typed<linalg_solve_ex_out::schema>();
}

// aten::linalg_solve_ex.out(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) info) -> (Tensor(a!) result, Tensor(b!) info)
::std::tuple<at::Tensor &,at::Tensor &> linalg_solve_ex_out::call(const at::Tensor & A, const at::Tensor & B, bool left, bool check_errors, at::Tensor & result, at::Tensor & info) {
    
    static auto op = create_linalg_solve_ex_out_typed_handle();
    return op.call(A, B, left, check_errors, result, info);
}

// aten::linalg_solve_ex.out(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) info) -> (Tensor(a!) result, Tensor(b!) info)
::std::tuple<at::Tensor &,at::Tensor &> linalg_solve_ex_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, const at::Tensor & B, bool left, bool check_errors, at::Tensor & result, at::Tensor & info) {
    
    static auto op = create_linalg_solve_ex_out_typed_handle();
    return op.redispatch(dispatchKeySet, A, B, left, check_errors, result, info);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorsolve, name, "aten::linalg_tensorsolve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorsolve, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorsolve, schema_str, "linalg_tensorsolve(Tensor self, Tensor other, int[]? dims=None) -> Tensor")

// aten::linalg_tensorsolve(Tensor self, Tensor other, int[]? dims=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_tensorsolve::schema> create_linalg_tensorsolve_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_tensorsolve::name, linalg_tensorsolve::overload_name)
      .typed<linalg_tensorsolve::schema>();
}

// aten::linalg_tensorsolve(Tensor self, Tensor other, int[]? dims=None) -> Tensor
at::Tensor linalg_tensorsolve::call(const at::Tensor & self, const at::Tensor & other, at::OptionalIntArrayRef dims) {
    
    static auto op = create_linalg_tensorsolve_typed_handle();
    return op.call(self, other, dims);
}

// aten::linalg_tensorsolve(Tensor self, Tensor other, int[]? dims=None) -> Tensor
at::Tensor linalg_tensorsolve::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::OptionalIntArrayRef dims) {
    
    static auto op = create_linalg_tensorsolve_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, dims);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorsolve_out, name, "aten::linalg_tensorsolve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorsolve_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorsolve_out, schema_str, "linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_tensorsolve_out::schema> create_linalg_tensorsolve_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_tensorsolve_out::name, linalg_tensorsolve_out::overload_name)
      .typed<linalg_tensorsolve_out::schema>();
}

// aten::linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_tensorsolve_out::call(const at::Tensor & self, const at::Tensor & other, at::OptionalIntArrayRef dims, at::Tensor & out) {
    
    static auto op = create_linalg_tensorsolve_out_typed_handle();
    return op.call(self, other, dims, out);
}

// aten::linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_tensorsolve_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::OptionalIntArrayRef dims, at::Tensor & out) {
    
    static auto op = create_linalg_tensorsolve_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, dims, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_multi_dot, name, "aten::linalg_multi_dot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_multi_dot, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_multi_dot, schema_str, "linalg_multi_dot(Tensor[] tensors) -> Tensor")

// aten::linalg_multi_dot(Tensor[] tensors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_multi_dot::schema> create_linalg_multi_dot_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_multi_dot::name, linalg_multi_dot::overload_name)
      .typed<linalg_multi_dot::schema>();
}

// aten::linalg_multi_dot(Tensor[] tensors) -> Tensor
at::Tensor linalg_multi_dot::call(at::TensorList tensors) {
    
    static auto op = create_linalg_multi_dot_typed_handle();
    return op.call(tensors);
}

// aten::linalg_multi_dot(Tensor[] tensors) -> Tensor
at::Tensor linalg_multi_dot::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    
    static auto op = create_linalg_multi_dot_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_multi_dot_out, name, "aten::linalg_multi_dot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_multi_dot_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_multi_dot_out, schema_str, "linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_multi_dot_out::schema> create_linalg_multi_dot_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_multi_dot_out::name, linalg_multi_dot_out::overload_name)
      .typed<linalg_multi_dot_out::schema>();
}

// aten::linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_multi_dot_out::call(at::TensorList tensors, at::Tensor & out) {
    
    static auto op = create_linalg_multi_dot_out_typed_handle();
    return op.call(tensors, out);
}

// aten::linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_multi_dot_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
    
    static auto op = create_linalg_multi_dot_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_string_default, name, "aten::_test_string_default")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_string_default, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_string_default, schema_str, "_test_string_default(Tensor dummy, str a=\"\\\"'\\\\\", str b='\"\\'\\\\') -> Tensor")

// aten::_test_string_default(Tensor dummy, str a="\"'\\", str b='"\'\\') -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_test_string_default::schema> create__test_string_default_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_test_string_default::name, _test_string_default::overload_name)
      .typed<_test_string_default::schema>();
}

// aten::_test_string_default(Tensor dummy, str a="\"'\\", str b='"\'\\') -> Tensor
at::Tensor _test_string_default::call(const at::Tensor & dummy, c10::string_view a, c10::string_view b) {
    
    static auto op = create__test_string_default_typed_handle();
    return op.call(dummy, a, b);
}

// aten::_test_string_default(Tensor dummy, str a="\"'\\", str b='"\'\\') -> Tensor
at::Tensor _test_string_default::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & dummy, c10::string_view a, c10::string_view b) {
    
    static auto op = create__test_string_default_typed_handle();
    return op.redispatch(dispatchKeySet, dummy, a, b);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_dense_tensors, name, "aten::flatten_dense_tensors")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_dense_tensors, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_dense_tensors, schema_str, "flatten_dense_tensors(Tensor[] tensors) -> Tensor")

// aten::flatten_dense_tensors(Tensor[] tensors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<flatten_dense_tensors::schema> create_flatten_dense_tensors_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(flatten_dense_tensors::name, flatten_dense_tensors::overload_name)
      .typed<flatten_dense_tensors::schema>();
}

// aten::flatten_dense_tensors(Tensor[] tensors) -> Tensor
at::Tensor flatten_dense_tensors::call(at::TensorList tensors) {
    
    static auto op = create_flatten_dense_tensors_typed_handle();
    return op.call(tensors);
}

// aten::flatten_dense_tensors(Tensor[] tensors) -> Tensor
at::Tensor flatten_dense_tensors::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    
    static auto op = create_flatten_dense_tensors_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conj_copy, name, "aten::_conj_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conj_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conj_copy, schema_str, "_conj_copy(Tensor self) -> Tensor")

// aten::_conj_copy(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_conj_copy::schema> create__conj_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_conj_copy::name, _conj_copy::overload_name)
      .typed<_conj_copy::schema>();
}

// aten::_conj_copy(Tensor self) -> Tensor
at::Tensor _conj_copy::call(const at::Tensor & self) {
    
    static auto op = create__conj_copy_typed_handle();
    return op.call(self);
}

// aten::_conj_copy(Tensor self) -> Tensor
at::Tensor _conj_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create__conj_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach_copy, name, "aten::detach_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach_copy, schema_str, "detach_copy(Tensor self) -> Tensor")

// aten::detach_copy(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<detach_copy::schema> create_detach_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(detach_copy::name, detach_copy::overload_name)
      .typed<detach_copy::schema>();
}

// aten::detach_copy(Tensor self) -> Tensor
at::Tensor detach_copy::call(const at::Tensor & self) {
    
    static auto op = create_detach_copy_typed_handle();
    return op.call(self);
}

// aten::detach_copy(Tensor self) -> Tensor
at::Tensor detach_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_detach_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(row_indices_copy, name, "aten::row_indices_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(row_indices_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(row_indices_copy, schema_str, "row_indices_copy(Tensor self) -> Tensor")

// aten::row_indices_copy(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<row_indices_copy::schema> create_row_indices_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(row_indices_copy::name, row_indices_copy::overload_name)
      .typed<row_indices_copy::schema>();
}

// aten::row_indices_copy(Tensor self) -> Tensor
at::Tensor row_indices_copy::call(const at::Tensor & self) {
    
    static auto op = create_row_indices_copy_typed_handle();
    return op.call(self);
}

// aten::row_indices_copy(Tensor self) -> Tensor
at::Tensor row_indices_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_row_indices_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_transformer_encoder_layer_fwd, name, "aten::_transformer_encoder_layer_fwd")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_transformer_encoder_layer_fwd, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_transformer_encoder_layer_fwd, schema_str, "_transformer_encoder_layer_fwd(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None) -> Tensor")

// aten::_transformer_encoder_layer_fwd(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_transformer_encoder_layer_fwd::schema> create__transformer_encoder_layer_fwd_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_transformer_encoder_layer_fwd::name, _transformer_encoder_layer_fwd::overload_name)
      .typed<_transformer_encoder_layer_fwd::schema>();
}

// aten::_transformer_encoder_layer_fwd(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None) -> Tensor
at::Tensor _transformer_encoder_layer_fwd::call(const at::Tensor & src, int64_t embed_dim, int64_t num_heads, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, bool use_gelu, bool norm_first, double eps, const at::Tensor & norm_weight_1, const at::Tensor & norm_bias_1, const at::Tensor & norm_weight_2, const at::Tensor & norm_bias_2, const at::Tensor & ffn_weight_1, const at::Tensor & ffn_bias_1, const at::Tensor & ffn_weight_2, const at::Tensor & ffn_bias_2, const c10::optional<at::Tensor> & mask, c10::optional<int64_t> mask_type) {
    
    static auto op = create__transformer_encoder_layer_fwd_typed_handle();
    return op.call(src, embed_dim, num_heads, qkv_weight, qkv_bias, proj_weight, proj_bias, use_gelu, norm_first, eps, norm_weight_1, norm_bias_1, norm_weight_2, norm_bias_2, ffn_weight_1, ffn_bias_1, ffn_weight_2, ffn_bias_2, mask, mask_type);
}

// aten::_transformer_encoder_layer_fwd(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None) -> Tensor
at::Tensor _transformer_encoder_layer_fwd::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & src, int64_t embed_dim, int64_t num_heads, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, bool use_gelu, bool norm_first, double eps, const at::Tensor & norm_weight_1, const at::Tensor & norm_bias_1, const at::Tensor & norm_weight_2, const at::Tensor & norm_bias_2, const at::Tensor & ffn_weight_1, const at::Tensor & ffn_bias_1, const at::Tensor & ffn_weight_2, const at::Tensor & ffn_bias_2, const c10::optional<at::Tensor> & mask, c10::optional<int64_t> mask_type) {
    
    static auto op = create__transformer_encoder_layer_fwd_typed_handle();
    return op.redispatch(dispatchKeySet, src, embed_dim, num_heads, qkv_weight, qkv_bias, proj_weight, proj_bias, use_gelu, norm_first, eps, norm_weight_1, norm_bias_1, norm_weight_2, norm_bias_2, ffn_weight_1, ffn_bias_1, ffn_weight_2, ffn_bias_2, mask, mask_type);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_native_multi_head_attention, name, "aten::_native_multi_head_attention")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_native_multi_head_attention, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_native_multi_head_attention, schema_str, "_native_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None) -> (Tensor, Tensor)")

// aten::_native_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_native_multi_head_attention::schema> create__native_multi_head_attention_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_native_multi_head_attention::name, _native_multi_head_attention::overload_name)
      .typed<_native_multi_head_attention::schema>();
}

// aten::_native_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _native_multi_head_attention::call(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, int64_t embed_dim, int64_t num_head, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, const c10::optional<at::Tensor> & mask, bool need_weights, bool average_attn_weights, c10::optional<int64_t> mask_type) {
    
    static auto op = create__native_multi_head_attention_typed_handle();
    return op.call(query, key, value, embed_dim, num_head, qkv_weight, qkv_bias, proj_weight, proj_bias, mask, need_weights, average_attn_weights, mask_type);
}

// aten::_native_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _native_multi_head_attention::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, int64_t embed_dim, int64_t num_head, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, const c10::optional<at::Tensor> & mask, bool need_weights, bool average_attn_weights, c10::optional<int64_t> mask_type) {
    
    static auto op = create__native_multi_head_attention_typed_handle();
    return op.redispatch(dispatchKeySet, query, key, value, embed_dim, num_head, qkv_weight, qkv_bias, proj_weight, proj_bias, mask, need_weights, average_attn_weights, mask_type);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_sdp_choice, name, "aten::_fused_sdp_choice")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_sdp_choice, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_sdp_choice, schema_str, "_fused_sdp_choice(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> int")

// aten::_fused_sdp_choice(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> int
static C10_NOINLINE c10::TypedOperatorHandle<_fused_sdp_choice::schema> create__fused_sdp_choice_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fused_sdp_choice::name, _fused_sdp_choice::overload_name)
      .typed<_fused_sdp_choice::schema>();
}

// aten::_fused_sdp_choice(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> int
int64_t _fused_sdp_choice::call(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & attn_mask, double dropout_p, bool is_causal, c10::optional<double> scale) {
    
    static auto op = create__fused_sdp_choice_typed_handle();
    return op.call(query, key, value, attn_mask, dropout_p, is_causal, scale);
}

// aten::_fused_sdp_choice(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> int
int64_t _fused_sdp_choice::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & attn_mask, double dropout_p, bool is_causal, c10::optional<double> scale) {
    
    static auto op = create__fused_sdp_choice_typed_handle();
    return op.redispatch(dispatchKeySet, query, key, value, attn_mask, dropout_p, is_causal, scale);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_flash_attention, name, "aten::_scaled_dot_product_flash_attention")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_flash_attention, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_flash_attention, schema_str, "_scaled_dot_product_flash_attention(Tensor query, Tensor key, Tensor value, float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False, *, float? scale=None) -> (Tensor output, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, Tensor philox_seed, Tensor philox_offset, Tensor debug_attn_mask)")

// aten::_scaled_dot_product_flash_attention(Tensor query, Tensor key, Tensor value, float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False, *, float? scale=None) -> (Tensor output, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, Tensor philox_seed, Tensor philox_offset, Tensor debug_attn_mask)
static C10_NOINLINE c10::TypedOperatorHandle<_scaled_dot_product_flash_attention::schema> create__scaled_dot_product_flash_attention_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_scaled_dot_product_flash_attention::name, _scaled_dot_product_flash_attention::overload_name)
      .typed<_scaled_dot_product_flash_attention::schema>();
}

// aten::_scaled_dot_product_flash_attention(Tensor query, Tensor key, Tensor value, float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False, *, float? scale=None) -> (Tensor output, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, Tensor philox_seed, Tensor philox_offset, Tensor debug_attn_mask)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,c10::SymInt,c10::SymInt,at::Tensor,at::Tensor,at::Tensor> _scaled_dot_product_flash_attention::call(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, double dropout_p, bool is_causal, bool return_debug_mask, c10::optional<double> scale) {
    
    static auto op = create__scaled_dot_product_flash_attention_typed_handle();
    return op.call(query, key, value, dropout_p, is_causal, return_debug_mask, scale);
}

// aten::_scaled_dot_product_flash_attention(Tensor query, Tensor key, Tensor value, float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False, *, float? scale=None) -> (Tensor output, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, Tensor philox_seed, Tensor philox_offset, Tensor debug_attn_mask)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,c10::SymInt,c10::SymInt,at::Tensor,at::Tensor,at::Tensor> _scaled_dot_product_flash_attention::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, double dropout_p, bool is_causal, bool return_debug_mask, c10::optional<double> scale) {
    
    static auto op = create__scaled_dot_product_flash_attention_typed_handle();
    return op.redispatch(dispatchKeySet, query, key, value, dropout_p, is_causal, return_debug_mask, scale);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_flash_attention_for_cpu, name, "aten::_scaled_dot_product_flash_attention_for_cpu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_flash_attention_for_cpu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_flash_attention_for_cpu, schema_str, "_scaled_dot_product_flash_attention_for_cpu(Tensor query, Tensor key, Tensor value, float dropout_p=0.0, bool is_causal=False, *, Tensor? attn_mask=None, float? scale=None) -> (Tensor output, Tensor logsumexp)")

// aten::_scaled_dot_product_flash_attention_for_cpu(Tensor query, Tensor key, Tensor value, float dropout_p=0.0, bool is_causal=False, *, Tensor? attn_mask=None, float? scale=None) -> (Tensor output, Tensor logsumexp)
static C10_NOINLINE c10::TypedOperatorHandle<_scaled_dot_product_flash_attention_for_cpu::schema> create__scaled_dot_product_flash_attention_for_cpu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_scaled_dot_product_flash_attention_for_cpu::name, _scaled_dot_product_flash_attention_for_cpu::overload_name)
      .typed<_scaled_dot_product_flash_attention_for_cpu::schema>();
}

// aten::_scaled_dot_product_flash_attention_for_cpu(Tensor query, Tensor key, Tensor value, float dropout_p=0.0, bool is_causal=False, *, Tensor? attn_mask=None, float? scale=None) -> (Tensor output, Tensor logsumexp)
::std::tuple<at::Tensor,at::Tensor> _scaled_dot_product_flash_attention_for_cpu::call(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, double dropout_p, bool is_causal, const c10::optional<at::Tensor> & attn_mask, c10::optional<double> scale) {
    
    static auto op = create__scaled_dot_product_flash_attention_for_cpu_typed_handle();
    return op.call(query, key, value, dropout_p, is_causal, attn_mask, scale);
}

// aten::_scaled_dot_product_flash_attention_for_cpu(Tensor query, Tensor key, Tensor value, float dropout_p=0.0, bool is_causal=False, *, Tensor? attn_mask=None, float? scale=None) -> (Tensor output, Tensor logsumexp)
::std::tuple<at::Tensor,at::Tensor> _scaled_dot_product_flash_attention_for_cpu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, double dropout_p, bool is_causal, const c10::optional<at::Tensor> & attn_mask, c10::optional<double> scale) {
    
    static auto op = create__scaled_dot_product_flash_attention_for_cpu_typed_handle();
    return op.redispatch(dispatchKeySet, query, key, value, dropout_p, is_causal, attn_mask, scale);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_efficient_attention_backward, name, "aten::_scaled_dot_product_efficient_attention_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_efficient_attention_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_scaled_dot_product_efficient_attention_backward, schema_str, "_scaled_dot_product_efficient_attention_backward(Tensor grad_out_, Tensor query, Tensor key, Tensor value, Tensor attn_bias, Tensor out, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, float dropout_p, bool[4] grad_input_mask, bool is_causal=False, *, float? scale=None) -> (Tensor, Tensor, Tensor, Tensor)")

// aten::_scaled_dot_product_efficient_attention_backward(Tensor grad_out_, Tensor query, Tensor key, Tensor value, Tensor attn_bias, Tensor out, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, float dropout_p, bool[4] grad_input_mask, bool is_causal=False, *, float? scale=None) -> (Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_scaled_dot_product_efficient_attention_backward::schema> create__scaled_dot_product_efficient_attention_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_scaled_dot_product_efficient_attention_backward::name, _scaled_dot_product_efficient_attention_backward::overload_name)
      .typed<_scaled_dot_product_efficient_attention_backward::schema>();
}

// aten::_scaled_dot_product_efficient_attention_backward(Tensor grad_out_, Tensor query, Tensor key, Tensor value, Tensor attn_bias, Tensor out, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, float dropout_p, bool[4] grad_input_mask, bool is_causal=False, *, float? scale=None) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _scaled_dot_product_efficient_attention_backward::call(const at::Tensor & grad_out_, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const at::Tensor & attn_bias, const at::Tensor & out, const at::Tensor & logsumexp, const at::Tensor & philox_seed, const at::Tensor & philox_offset, double dropout_p, ::std::array<bool,4> grad_input_mask, bool is_causal, c10::optional<double> scale) {
    
    static auto op = create__scaled_dot_product_efficient_attention_backward_typed_handle();
    return op.call(grad_out_, query, key, value, attn_bias, out, logsumexp, philox_seed, philox_offset, dropout_p, grad_input_mask, is_causal, scale);
}

// aten::_scaled_dot_product_efficient_attention_backward(Tensor grad_out_, Tensor query, Tensor key, Tensor value, Tensor attn_bias, Tensor out, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, float dropout_p, bool[4] grad_input_mask, bool is_causal=False, *, float? scale=None) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _scaled_dot_product_efficient_attention_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out_, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const at::Tensor & attn_bias, const at::Tensor & out, const at::Tensor & logsumexp, const at::Tensor & philox_seed, const at::Tensor & philox_offset, double dropout_p, ::std::array<bool,4> grad_input_mask, bool is_causal, c10::optional<double> scale) {
    
    static auto op = create__scaled_dot_product_efficient_attention_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out_, query, key, value, attn_bias, out, logsumexp, philox_seed, philox_offset, dropout_p, grad_input_mask, is_causal, scale);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_flash_attention_backward, name, "aten::_flash_attention_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_flash_attention_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_flash_attention_backward, schema_str, "_flash_attention_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, Tensor philox_seed, Tensor philox_offset, *, float? scale=None) -> (Tensor, Tensor, Tensor)")

// aten::_flash_attention_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, Tensor philox_seed, Tensor philox_offset, *, float? scale=None) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_flash_attention_backward::schema> create__flash_attention_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_flash_attention_backward::name, _flash_attention_backward::overload_name)
      .typed<_flash_attention_backward::schema>();
}

// aten::_flash_attention_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, Tensor philox_seed, Tensor philox_offset, *, float? scale=None) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _flash_attention_backward::call(const at::Tensor & grad_out, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const at::Tensor & out, const at::Tensor & logsumexp, const at::Tensor & cum_seq_q, const at::Tensor & cum_seq_k, c10::SymInt max_q, c10::SymInt max_k, double dropout_p, bool is_causal, const at::Tensor & philox_seed, const at::Tensor & philox_offset, c10::optional<double> scale) {
    
    static auto op = create__flash_attention_backward_typed_handle();
    return op.call(grad_out, query, key, value, out, logsumexp, cum_seq_q, cum_seq_k, max_q, max_k, dropout_p, is_causal, philox_seed, philox_offset, scale);
}

// aten::_flash_attention_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, Tensor philox_seed, Tensor philox_offset, *, float? scale=None) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _flash_attention_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const at::Tensor & out, const at::Tensor & logsumexp, const at::Tensor & cum_seq_q, const at::Tensor & cum_seq_k, c10::SymInt max_q, c10::SymInt max_k, double dropout_p, bool is_causal, const at::Tensor & philox_seed, const at::Tensor & philox_offset, c10::optional<double> scale) {
    
    static auto op = create__flash_attention_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, query, key, value, out, logsumexp, cum_seq_q, cum_seq_k, max_q, max_k, dropout_p, is_causal, philox_seed, philox_offset, scale);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_efficient_attention_backward, name, "aten::_efficient_attention_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_efficient_attention_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_efficient_attention_backward, schema_str, "_efficient_attention_backward(Tensor grad_out_, Tensor query, Tensor key, Tensor value, Tensor? bias, Tensor out, Tensor? cu_seqlens_q, Tensor? cu_seqlens_k, SymInt max_seqlen_q, SymInt max_seqlen_k, Tensor logsumexp, float dropout_p, Tensor philox_seed, Tensor philox_offset, int custom_mask_type, bool bias_requires_grad, *, float? scale=None, int? num_splits_key=None) -> (Tensor, Tensor, Tensor, Tensor)")

// aten::_efficient_attention_backward(Tensor grad_out_, Tensor query, Tensor key, Tensor value, Tensor? bias, Tensor out, Tensor? cu_seqlens_q, Tensor? cu_seqlens_k, SymInt max_seqlen_q, SymInt max_seqlen_k, Tensor logsumexp, float dropout_p, Tensor philox_seed, Tensor philox_offset, int custom_mask_type, bool bias_requires_grad, *, float? scale=None, int? num_splits_key=None) -> (Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_efficient_attention_backward::schema> create__efficient_attention_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_efficient_attention_backward::name, _efficient_attention_backward::overload_name)
      .typed<_efficient_attention_backward::schema>();
}

// aten::_efficient_attention_backward(Tensor grad_out_, Tensor query, Tensor key, Tensor value, Tensor? bias, Tensor out, Tensor? cu_seqlens_q, Tensor? cu_seqlens_k, SymInt max_seqlen_q, SymInt max_seqlen_k, Tensor logsumexp, float dropout_p, Tensor philox_seed, Tensor philox_offset, int custom_mask_type, bool bias_requires_grad, *, float? scale=None, int? num_splits_key=None) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _efficient_attention_backward::call(const at::Tensor & grad_out_, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & bias, const at::Tensor & out, const c10::optional<at::Tensor> & cu_seqlens_q, const c10::optional<at::Tensor> & cu_seqlens_k, c10::SymInt max_seqlen_q, c10::SymInt max_seqlen_k, const at::Tensor & logsumexp, double dropout_p, const at::Tensor & philox_seed, const at::Tensor & philox_offset, int64_t custom_mask_type, bool bias_requires_grad, c10::optional<double> scale, c10::optional<int64_t> num_splits_key) {
    
    static auto op = create__efficient_attention_backward_typed_handle();
    return op.call(grad_out_, query, key, value, bias, out, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, logsumexp, dropout_p, philox_seed, philox_offset, custom_mask_type, bias_requires_grad, scale, num_splits_key);
}

// aten::_efficient_attention_backward(Tensor grad_out_, Tensor query, Tensor key, Tensor value, Tensor? bias, Tensor out, Tensor? cu_seqlens_q, Tensor? cu_seqlens_k, SymInt max_seqlen_q, SymInt max_seqlen_k, Tensor logsumexp, float dropout_p, Tensor philox_seed, Tensor philox_offset, int custom_mask_type, bool bias_requires_grad, *, float? scale=None, int? num_splits_key=None) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _efficient_attention_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out_, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & bias, const at::Tensor & out, const c10::optional<at::Tensor> & cu_seqlens_q, const c10::optional<at::Tensor> & cu_seqlens_k, c10::SymInt max_seqlen_q, c10::SymInt max_seqlen_k, const at::Tensor & logsumexp, double dropout_p, const at::Tensor & philox_seed, const at::Tensor & philox_offset, int64_t custom_mask_type, bool bias_requires_grad, c10::optional<double> scale, c10::optional<int64_t> num_splits_key) {
    
    static auto op = create__efficient_attention_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out_, query, key, value, bias, out, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, logsumexp, dropout_p, philox_seed, philox_offset, custom_mask_type, bias_requires_grad, scale, num_splits_key);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fill_mem_eff_dropout_mask_, name, "aten::_fill_mem_eff_dropout_mask_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fill_mem_eff_dropout_mask_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fill_mem_eff_dropout_mask_, schema_str, "_fill_mem_eff_dropout_mask_(Tensor(a!) self, float dropout_p, int seed, int offset) -> Tensor(a!)")

// aten::_fill_mem_eff_dropout_mask_(Tensor(a!) self, float dropout_p, int seed, int offset) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_fill_mem_eff_dropout_mask_::schema> create__fill_mem_eff_dropout_mask__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fill_mem_eff_dropout_mask_::name, _fill_mem_eff_dropout_mask_::overload_name)
      .typed<_fill_mem_eff_dropout_mask_::schema>();
}

// aten::_fill_mem_eff_dropout_mask_(Tensor(a!) self, float dropout_p, int seed, int offset) -> Tensor(a!)
at::Tensor & _fill_mem_eff_dropout_mask_::call(at::Tensor & self, double dropout_p, int64_t seed, int64_t offset) {
    
    static auto op = create__fill_mem_eff_dropout_mask__typed_handle();
    return op.call(self, dropout_p, seed, offset);
}

// aten::_fill_mem_eff_dropout_mask_(Tensor(a!) self, float dropout_p, int seed, int offset) -> Tensor(a!)
at::Tensor & _fill_mem_eff_dropout_mask_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double dropout_p, int64_t seed, int64_t offset) {
    
    static auto op = create__fill_mem_eff_dropout_mask__typed_handle();
    return op.redispatch(dispatchKeySet, self, dropout_p, seed, offset);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_triton_multi_head_attention, name, "aten::_triton_multi_head_attention")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_triton_multi_head_attention, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_triton_multi_head_attention, schema_str, "_triton_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None) -> Tensor")

// aten::_triton_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_triton_multi_head_attention::schema> create__triton_multi_head_attention_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_triton_multi_head_attention::name, _triton_multi_head_attention::overload_name)
      .typed<_triton_multi_head_attention::schema>();
}

// aten::_triton_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None) -> Tensor
at::Tensor _triton_multi_head_attention::call(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, int64_t embed_dim, int64_t num_head, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, const c10::optional<at::Tensor> & mask) {
    
    static auto op = create__triton_multi_head_attention_typed_handle();
    return op.call(query, key, value, embed_dim, num_head, qkv_weight, qkv_bias, proj_weight, proj_bias, mask);
}

// aten::_triton_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None) -> Tensor
at::Tensor _triton_multi_head_attention::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, int64_t embed_dim, int64_t num_head, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, const c10::optional<at::Tensor> & mask) {
    
    static auto op = create__triton_multi_head_attention_typed_handle();
    return op.redispatch(dispatchKeySet, query, key, value, embed_dim, num_head, qkv_weight, qkv_bias, proj_weight, proj_bias, mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_airy_ai, name, "aten::special_airy_ai")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_airy_ai, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_airy_ai, schema_str, "special_airy_ai(Tensor x) -> Tensor")

// aten::special_airy_ai(Tensor x) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_airy_ai::schema> create_special_airy_ai_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_airy_ai::name, special_airy_ai::overload_name)
      .typed<special_airy_ai::schema>();
}

// aten::special_airy_ai(Tensor x) -> Tensor
at::Tensor special_airy_ai::call(const at::Tensor & x) {
    
    static auto op = create_special_airy_ai_typed_handle();
    return op.call(x);
}

// aten::special_airy_ai(Tensor x) -> Tensor
at::Tensor special_airy_ai::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x) {
    
    static auto op = create_special_airy_ai_typed_handle();
    return op.redispatch(dispatchKeySet, x);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_airy_ai_out, name, "aten::special_airy_ai")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_airy_ai_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_airy_ai_out, schema_str, "special_airy_ai.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_airy_ai.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_airy_ai_out::schema> create_special_airy_ai_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_airy_ai_out::name, special_airy_ai_out::overload_name)
      .typed<special_airy_ai_out::schema>();
}

// aten::special_airy_ai.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_airy_ai_out::call(const at::Tensor & x, at::Tensor & out) {
    
    static auto op = create_special_airy_ai_out_typed_handle();
    return op.call(x, out);
}

// aten::special_airy_ai.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_airy_ai_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, at::Tensor & out) {
    
    static auto op = create_special_airy_ai_out_typed_handle();
    return op.redispatch(dispatchKeySet, x, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w, name, "aten::special_chebyshev_polynomial_w")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w, schema_str, "special_chebyshev_polynomial_w(Tensor x, Tensor n) -> Tensor")

// aten::special_chebyshev_polynomial_w(Tensor x, Tensor n) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_chebyshev_polynomial_w::schema> create_special_chebyshev_polynomial_w_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_chebyshev_polynomial_w::name, special_chebyshev_polynomial_w::overload_name)
      .typed<special_chebyshev_polynomial_w::schema>();
}

// aten::special_chebyshev_polynomial_w(Tensor x, Tensor n) -> Tensor
at::Tensor special_chebyshev_polynomial_w::call(const at::Tensor & x, const at::Tensor & n) {
    
    static auto op = create_special_chebyshev_polynomial_w_typed_handle();
    return op.call(x, n);
}

// aten::special_chebyshev_polynomial_w(Tensor x, Tensor n) -> Tensor
at::Tensor special_chebyshev_polynomial_w::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n) {
    
    static auto op = create_special_chebyshev_polynomial_w_typed_handle();
    return op.redispatch(dispatchKeySet, x, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_x_scalar, name, "aten::special_chebyshev_polynomial_w")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_x_scalar, overload_name, "x_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_x_scalar, schema_str, "special_chebyshev_polynomial_w.x_scalar(Scalar x, Tensor n) -> Tensor")

// aten::special_chebyshev_polynomial_w.x_scalar(Scalar x, Tensor n) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_chebyshev_polynomial_w_x_scalar::schema> create_special_chebyshev_polynomial_w_x_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_chebyshev_polynomial_w_x_scalar::name, special_chebyshev_polynomial_w_x_scalar::overload_name)
      .typed<special_chebyshev_polynomial_w_x_scalar::schema>();
}

// aten::special_chebyshev_polynomial_w.x_scalar(Scalar x, Tensor n) -> Tensor
at::Tensor special_chebyshev_polynomial_w_x_scalar::call(const at::Scalar & x, const at::Tensor & n) {
    
    static auto op = create_special_chebyshev_polynomial_w_x_scalar_typed_handle();
    return op.call(x, n);
}

// aten::special_chebyshev_polynomial_w.x_scalar(Scalar x, Tensor n) -> Tensor
at::Tensor special_chebyshev_polynomial_w_x_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n) {
    
    static auto op = create_special_chebyshev_polynomial_w_x_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, x, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_n_scalar, name, "aten::special_chebyshev_polynomial_w")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_n_scalar, overload_name, "n_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_n_scalar, schema_str, "special_chebyshev_polynomial_w.n_scalar(Tensor x, Scalar n) -> Tensor")

// aten::special_chebyshev_polynomial_w.n_scalar(Tensor x, Scalar n) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_chebyshev_polynomial_w_n_scalar::schema> create_special_chebyshev_polynomial_w_n_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_chebyshev_polynomial_w_n_scalar::name, special_chebyshev_polynomial_w_n_scalar::overload_name)
      .typed<special_chebyshev_polynomial_w_n_scalar::schema>();
}

// aten::special_chebyshev_polynomial_w.n_scalar(Tensor x, Scalar n) -> Tensor
at::Tensor special_chebyshev_polynomial_w_n_scalar::call(const at::Tensor & x, const at::Scalar & n) {
    
    static auto op = create_special_chebyshev_polynomial_w_n_scalar_typed_handle();
    return op.call(x, n);
}

// aten::special_chebyshev_polynomial_w.n_scalar(Tensor x, Scalar n) -> Tensor
at::Tensor special_chebyshev_polynomial_w_n_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n) {
    
    static auto op = create_special_chebyshev_polynomial_w_n_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, x, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_out, name, "aten::special_chebyshev_polynomial_w")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_out, schema_str, "special_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_chebyshev_polynomial_w_out::schema> create_special_chebyshev_polynomial_w_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_chebyshev_polynomial_w_out::name, special_chebyshev_polynomial_w_out::overload_name)
      .typed<special_chebyshev_polynomial_w_out::schema>();
}

// aten::special_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_chebyshev_polynomial_w_out::call(const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_chebyshev_polynomial_w_out_typed_handle();
    return op.call(x, n, out);
}

// aten::special_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_chebyshev_polynomial_w_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_chebyshev_polynomial_w_out_typed_handle();
    return op.redispatch(dispatchKeySet, x, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_x_scalar_out, name, "aten::special_chebyshev_polynomial_w")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_x_scalar_out, overload_name, "x_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_x_scalar_out, schema_str, "special_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_chebyshev_polynomial_w_x_scalar_out::schema> create_special_chebyshev_polynomial_w_x_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_chebyshev_polynomial_w_x_scalar_out::name, special_chebyshev_polynomial_w_x_scalar_out::overload_name)
      .typed<special_chebyshev_polynomial_w_x_scalar_out::schema>();
}

// aten::special_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_chebyshev_polynomial_w_x_scalar_out::call(const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_chebyshev_polynomial_w_x_scalar_out_typed_handle();
    return op.call(x, n, out);
}

// aten::special_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_chebyshev_polynomial_w_x_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_chebyshev_polynomial_w_x_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, x, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_n_scalar_out, name, "aten::special_chebyshev_polynomial_w")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_n_scalar_out, overload_name, "n_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_chebyshev_polynomial_w_n_scalar_out, schema_str, "special_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_chebyshev_polynomial_w_n_scalar_out::schema> create_special_chebyshev_polynomial_w_n_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_chebyshev_polynomial_w_n_scalar_out::name, special_chebyshev_polynomial_w_n_scalar_out::overload_name)
      .typed<special_chebyshev_polynomial_w_n_scalar_out::schema>();
}

// aten::special_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_chebyshev_polynomial_w_n_scalar_out::call(const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
    
    static auto op = create_special_chebyshev_polynomial_w_n_scalar_out_typed_handle();
    return op.call(x, n, out);
}

// aten::special_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_chebyshev_polynomial_w_n_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
    
    static auto op = create_special_chebyshev_polynomial_w_n_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, x, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h, name, "aten::special_hermite_polynomial_h")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h, schema_str, "special_hermite_polynomial_h(Tensor x, Tensor n) -> Tensor")

// aten::special_hermite_polynomial_h(Tensor x, Tensor n) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_hermite_polynomial_h::schema> create_special_hermite_polynomial_h_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_hermite_polynomial_h::name, special_hermite_polynomial_h::overload_name)
      .typed<special_hermite_polynomial_h::schema>();
}

// aten::special_hermite_polynomial_h(Tensor x, Tensor n) -> Tensor
at::Tensor special_hermite_polynomial_h::call(const at::Tensor & x, const at::Tensor & n) {
    
    static auto op = create_special_hermite_polynomial_h_typed_handle();
    return op.call(x, n);
}

// aten::special_hermite_polynomial_h(Tensor x, Tensor n) -> Tensor
at::Tensor special_hermite_polynomial_h::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n) {
    
    static auto op = create_special_hermite_polynomial_h_typed_handle();
    return op.redispatch(dispatchKeySet, x, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_x_scalar, name, "aten::special_hermite_polynomial_h")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_x_scalar, overload_name, "x_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_x_scalar, schema_str, "special_hermite_polynomial_h.x_scalar(Scalar x, Tensor n) -> Tensor")

// aten::special_hermite_polynomial_h.x_scalar(Scalar x, Tensor n) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_hermite_polynomial_h_x_scalar::schema> create_special_hermite_polynomial_h_x_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_hermite_polynomial_h_x_scalar::name, special_hermite_polynomial_h_x_scalar::overload_name)
      .typed<special_hermite_polynomial_h_x_scalar::schema>();
}

// aten::special_hermite_polynomial_h.x_scalar(Scalar x, Tensor n) -> Tensor
at::Tensor special_hermite_polynomial_h_x_scalar::call(const at::Scalar & x, const at::Tensor & n) {
    
    static auto op = create_special_hermite_polynomial_h_x_scalar_typed_handle();
    return op.call(x, n);
}

// aten::special_hermite_polynomial_h.x_scalar(Scalar x, Tensor n) -> Tensor
at::Tensor special_hermite_polynomial_h_x_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n) {
    
    static auto op = create_special_hermite_polynomial_h_x_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, x, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_n_scalar, name, "aten::special_hermite_polynomial_h")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_n_scalar, overload_name, "n_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_n_scalar, schema_str, "special_hermite_polynomial_h.n_scalar(Tensor x, Scalar n) -> Tensor")

// aten::special_hermite_polynomial_h.n_scalar(Tensor x, Scalar n) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_hermite_polynomial_h_n_scalar::schema> create_special_hermite_polynomial_h_n_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_hermite_polynomial_h_n_scalar::name, special_hermite_polynomial_h_n_scalar::overload_name)
      .typed<special_hermite_polynomial_h_n_scalar::schema>();
}

// aten::special_hermite_polynomial_h.n_scalar(Tensor x, Scalar n) -> Tensor
at::Tensor special_hermite_polynomial_h_n_scalar::call(const at::Tensor & x, const at::Scalar & n) {
    
    static auto op = create_special_hermite_polynomial_h_n_scalar_typed_handle();
    return op.call(x, n);
}

// aten::special_hermite_polynomial_h.n_scalar(Tensor x, Scalar n) -> Tensor
at::Tensor special_hermite_polynomial_h_n_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n) {
    
    static auto op = create_special_hermite_polynomial_h_n_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, x, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_out, name, "aten::special_hermite_polynomial_h")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_out, schema_str, "special_hermite_polynomial_h.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_hermite_polynomial_h.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_hermite_polynomial_h_out::schema> create_special_hermite_polynomial_h_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_hermite_polynomial_h_out::name, special_hermite_polynomial_h_out::overload_name)
      .typed<special_hermite_polynomial_h_out::schema>();
}

// aten::special_hermite_polynomial_h.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_hermite_polynomial_h_out::call(const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_hermite_polynomial_h_out_typed_handle();
    return op.call(x, n, out);
}

// aten::special_hermite_polynomial_h.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_hermite_polynomial_h_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_hermite_polynomial_h_out_typed_handle();
    return op.redispatch(dispatchKeySet, x, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_x_scalar_out, name, "aten::special_hermite_polynomial_h")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_x_scalar_out, overload_name, "x_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_x_scalar_out, schema_str, "special_hermite_polynomial_h.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_hermite_polynomial_h.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_hermite_polynomial_h_x_scalar_out::schema> create_special_hermite_polynomial_h_x_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_hermite_polynomial_h_x_scalar_out::name, special_hermite_polynomial_h_x_scalar_out::overload_name)
      .typed<special_hermite_polynomial_h_x_scalar_out::schema>();
}

// aten::special_hermite_polynomial_h.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_hermite_polynomial_h_x_scalar_out::call(const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_hermite_polynomial_h_x_scalar_out_typed_handle();
    return op.call(x, n, out);
}

// aten::special_hermite_polynomial_h.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_hermite_polynomial_h_x_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_hermite_polynomial_h_x_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, x, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_n_scalar_out, name, "aten::special_hermite_polynomial_h")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_n_scalar_out, overload_name, "n_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_hermite_polynomial_h_n_scalar_out, schema_str, "special_hermite_polynomial_h.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_hermite_polynomial_h.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_hermite_polynomial_h_n_scalar_out::schema> create_special_hermite_polynomial_h_n_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_hermite_polynomial_h_n_scalar_out::name, special_hermite_polynomial_h_n_scalar_out::overload_name)
      .typed<special_hermite_polynomial_h_n_scalar_out::schema>();
}

// aten::special_hermite_polynomial_h.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_hermite_polynomial_h_n_scalar_out::call(const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
    
    static auto op = create_special_hermite_polynomial_h_n_scalar_out_typed_handle();
    return op.call(x, n, out);
}

// aten::special_hermite_polynomial_h.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_hermite_polynomial_h_n_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
    
    static auto op = create_special_hermite_polynomial_h_n_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, x, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_i0, name, "aten::special_modified_bessel_i0")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_i0, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_i0, schema_str, "special_modified_bessel_i0(Tensor self) -> Tensor")

// aten::special_modified_bessel_i0(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_modified_bessel_i0::schema> create_special_modified_bessel_i0_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_modified_bessel_i0::name, special_modified_bessel_i0::overload_name)
      .typed<special_modified_bessel_i0::schema>();
}

// aten::special_modified_bessel_i0(Tensor self) -> Tensor
at::Tensor special_modified_bessel_i0::call(const at::Tensor & self) {
    
    static auto op = create_special_modified_bessel_i0_typed_handle();
    return op.call(self);
}

// aten::special_modified_bessel_i0(Tensor self) -> Tensor
at::Tensor special_modified_bessel_i0::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_special_modified_bessel_i0_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_i0_out, name, "aten::special_modified_bessel_i0")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_i0_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_i0_out, schema_str, "special_modified_bessel_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_modified_bessel_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_modified_bessel_i0_out::schema> create_special_modified_bessel_i0_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_modified_bessel_i0_out::name, special_modified_bessel_i0_out::overload_name)
      .typed<special_modified_bessel_i0_out::schema>();
}

// aten::special_modified_bessel_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_modified_bessel_i0_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_modified_bessel_i0_out_typed_handle();
    return op.call(self, out);
}

// aten::special_modified_bessel_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_modified_bessel_i0_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_modified_bessel_i0_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_k0, name, "aten::special_modified_bessel_k0")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_k0, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_k0, schema_str, "special_modified_bessel_k0(Tensor self) -> Tensor")

// aten::special_modified_bessel_k0(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_modified_bessel_k0::schema> create_special_modified_bessel_k0_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_modified_bessel_k0::name, special_modified_bessel_k0::overload_name)
      .typed<special_modified_bessel_k0::schema>();
}

// aten::special_modified_bessel_k0(Tensor self) -> Tensor
at::Tensor special_modified_bessel_k0::call(const at::Tensor & self) {
    
    static auto op = create_special_modified_bessel_k0_typed_handle();
    return op.call(self);
}

// aten::special_modified_bessel_k0(Tensor self) -> Tensor
at::Tensor special_modified_bessel_k0::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_special_modified_bessel_k0_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_k0_out, name, "aten::special_modified_bessel_k0")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_k0_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_k0_out, schema_str, "special_modified_bessel_k0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_modified_bessel_k0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_modified_bessel_k0_out::schema> create_special_modified_bessel_k0_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_modified_bessel_k0_out::name, special_modified_bessel_k0_out::overload_name)
      .typed<special_modified_bessel_k0_out::schema>();
}

// aten::special_modified_bessel_k0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_modified_bessel_k0_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_modified_bessel_k0_out_typed_handle();
    return op.call(self, out);
}

// aten::special_modified_bessel_k0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_modified_bessel_k0_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_modified_bessel_k0_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_k1, name, "aten::special_modified_bessel_k1")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_k1, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_k1, schema_str, "special_modified_bessel_k1(Tensor self) -> Tensor")

// aten::special_modified_bessel_k1(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_modified_bessel_k1::schema> create_special_modified_bessel_k1_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_modified_bessel_k1::name, special_modified_bessel_k1::overload_name)
      .typed<special_modified_bessel_k1::schema>();
}

// aten::special_modified_bessel_k1(Tensor self) -> Tensor
at::Tensor special_modified_bessel_k1::call(const at::Tensor & self) {
    
    static auto op = create_special_modified_bessel_k1_typed_handle();
    return op.call(self);
}

// aten::special_modified_bessel_k1(Tensor self) -> Tensor
at::Tensor special_modified_bessel_k1::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    
    static auto op = create_special_modified_bessel_k1_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_k1_out, name, "aten::special_modified_bessel_k1")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_k1_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_modified_bessel_k1_out, schema_str, "special_modified_bessel_k1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_modified_bessel_k1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_modified_bessel_k1_out::schema> create_special_modified_bessel_k1_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_modified_bessel_k1_out::name, special_modified_bessel_k1_out::overload_name)
      .typed<special_modified_bessel_k1_out::schema>();
}

// aten::special_modified_bessel_k1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_modified_bessel_k1_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_modified_bessel_k1_out_typed_handle();
    return op.call(self, out);
}

// aten::special_modified_bessel_k1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_modified_bessel_k1_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_special_modified_bessel_k1_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t, name, "aten::special_shifted_chebyshev_polynomial_t")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t, schema_str, "special_shifted_chebyshev_polynomial_t(Tensor x, Tensor n) -> Tensor")

// aten::special_shifted_chebyshev_polynomial_t(Tensor x, Tensor n) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_shifted_chebyshev_polynomial_t::schema> create_special_shifted_chebyshev_polynomial_t_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_shifted_chebyshev_polynomial_t::name, special_shifted_chebyshev_polynomial_t::overload_name)
      .typed<special_shifted_chebyshev_polynomial_t::schema>();
}

// aten::special_shifted_chebyshev_polynomial_t(Tensor x, Tensor n) -> Tensor
at::Tensor special_shifted_chebyshev_polynomial_t::call(const at::Tensor & x, const at::Tensor & n) {
    
    static auto op = create_special_shifted_chebyshev_polynomial_t_typed_handle();
    return op.call(x, n);
}

// aten::special_shifted_chebyshev_polynomial_t(Tensor x, Tensor n) -> Tensor
at::Tensor special_shifted_chebyshev_polynomial_t::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n) {
    
    static auto op = create_special_shifted_chebyshev_polynomial_t_typed_handle();
    return op.redispatch(dispatchKeySet, x, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_x_scalar, name, "aten::special_shifted_chebyshev_polynomial_t")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_x_scalar, overload_name, "x_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_x_scalar, schema_str, "special_shifted_chebyshev_polynomial_t.x_scalar(Scalar x, Tensor n) -> Tensor")

// aten::special_shifted_chebyshev_polynomial_t.x_scalar(Scalar x, Tensor n) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_shifted_chebyshev_polynomial_t_x_scalar::schema> create_special_shifted_chebyshev_polynomial_t_x_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_shifted_chebyshev_polynomial_t_x_scalar::name, special_shifted_chebyshev_polynomial_t_x_scalar::overload_name)
      .typed<special_shifted_chebyshev_polynomial_t_x_scalar::schema>();
}

// aten::special_shifted_chebyshev_polynomial_t.x_scalar(Scalar x, Tensor n) -> Tensor
at::Tensor special_shifted_chebyshev_polynomial_t_x_scalar::call(const at::Scalar & x, const at::Tensor & n) {
    
    static auto op = create_special_shifted_chebyshev_polynomial_t_x_scalar_typed_handle();
    return op.call(x, n);
}

// aten::special_shifted_chebyshev_polynomial_t.x_scalar(Scalar x, Tensor n) -> Tensor
at::Tensor special_shifted_chebyshev_polynomial_t_x_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n) {
    
    static auto op = create_special_shifted_chebyshev_polynomial_t_x_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, x, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_n_scalar, name, "aten::special_shifted_chebyshev_polynomial_t")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_n_scalar, overload_name, "n_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_n_scalar, schema_str, "special_shifted_chebyshev_polynomial_t.n_scalar(Tensor x, Scalar n) -> Tensor")

// aten::special_shifted_chebyshev_polynomial_t.n_scalar(Tensor x, Scalar n) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_shifted_chebyshev_polynomial_t_n_scalar::schema> create_special_shifted_chebyshev_polynomial_t_n_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_shifted_chebyshev_polynomial_t_n_scalar::name, special_shifted_chebyshev_polynomial_t_n_scalar::overload_name)
      .typed<special_shifted_chebyshev_polynomial_t_n_scalar::schema>();
}

// aten::special_shifted_chebyshev_polynomial_t.n_scalar(Tensor x, Scalar n) -> Tensor
at::Tensor special_shifted_chebyshev_polynomial_t_n_scalar::call(const at::Tensor & x, const at::Scalar & n) {
    
    static auto op = create_special_shifted_chebyshev_polynomial_t_n_scalar_typed_handle();
    return op.call(x, n);
}

// aten::special_shifted_chebyshev_polynomial_t.n_scalar(Tensor x, Scalar n) -> Tensor
at::Tensor special_shifted_chebyshev_polynomial_t_n_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n) {
    
    static auto op = create_special_shifted_chebyshev_polynomial_t_n_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, x, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_out, name, "aten::special_shifted_chebyshev_polynomial_t")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_out, schema_str, "special_shifted_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_shifted_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_shifted_chebyshev_polynomial_t_out::schema> create_special_shifted_chebyshev_polynomial_t_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_shifted_chebyshev_polynomial_t_out::name, special_shifted_chebyshev_polynomial_t_out::overload_name)
      .typed<special_shifted_chebyshev_polynomial_t_out::schema>();
}

// aten::special_shifted_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_shifted_chebyshev_polynomial_t_out::call(const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_shifted_chebyshev_polynomial_t_out_typed_handle();
    return op.call(x, n, out);
}

// aten::special_shifted_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_shifted_chebyshev_polynomial_t_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_shifted_chebyshev_polynomial_t_out_typed_handle();
    return op.redispatch(dispatchKeySet, x, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_x_scalar_out, name, "aten::special_shifted_chebyshev_polynomial_t")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_x_scalar_out, overload_name, "x_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_x_scalar_out, schema_str, "special_shifted_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_shifted_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_shifted_chebyshev_polynomial_t_x_scalar_out::schema> create_special_shifted_chebyshev_polynomial_t_x_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_shifted_chebyshev_polynomial_t_x_scalar_out::name, special_shifted_chebyshev_polynomial_t_x_scalar_out::overload_name)
      .typed<special_shifted_chebyshev_polynomial_t_x_scalar_out::schema>();
}

// aten::special_shifted_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_shifted_chebyshev_polynomial_t_x_scalar_out::call(const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_shifted_chebyshev_polynomial_t_x_scalar_out_typed_handle();
    return op.call(x, n, out);
}

// aten::special_shifted_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_shifted_chebyshev_polynomial_t_x_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
    
    static auto op = create_special_shifted_chebyshev_polynomial_t_x_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, x, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_n_scalar_out, name, "aten::special_shifted_chebyshev_polynomial_t")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_n_scalar_out, overload_name, "n_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_shifted_chebyshev_polynomial_t_n_scalar_out, schema_str, "special_shifted_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_shifted_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_shifted_chebyshev_polynomial_t_n_scalar_out::schema> create_special_shifted_chebyshev_polynomial_t_n_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_shifted_chebyshev_polynomial_t_n_scalar_out::name, special_shifted_chebyshev_polynomial_t_n_scalar_out::overload_name)
      .typed<special_shifted_chebyshev_polynomial_t_n_scalar_out::schema>();
}

// aten::special_shifted_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_shifted_chebyshev_polynomial_t_n_scalar_out::call(const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
    
    static auto op = create_special_shifted_chebyshev_polynomial_t_n_scalar_out_typed_handle();
    return op.call(x, n, out);
}

// aten::special_shifted_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_shifted_chebyshev_polynomial_t_n_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
    
    static auto op = create_special_shifted_chebyshev_polynomial_t_n_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, x, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw_, name, "aten::_fused_adamw_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw_, schema_str, "_fused_adamw_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()")

// aten::_fused_adamw_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_fused_adamw_::schema> create__fused_adamw__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fused_adamw_::name, _fused_adamw_::overload_name)
      .typed<_fused_adamw_::schema>();
}

// aten::_fused_adamw_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()
void _fused_adamw_::call(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
    
    static auto op = create__fused_adamw__typed_handle();
    return op.call(self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf);
}

// aten::_fused_adamw_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()
void _fused_adamw_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
    
    static auto op = create__fused_adamw__typed_handle();
    return op.redispatch(dispatchKeySet, self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw__tensor_lr, name, "aten::_fused_adamw_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw__tensor_lr, overload_name, "tensor_lr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw__tensor_lr, schema_str, "_fused_adamw_.tensor_lr(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()")

// aten::_fused_adamw_.tensor_lr(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_fused_adamw__tensor_lr::schema> create__fused_adamw__tensor_lr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fused_adamw__tensor_lr::name, _fused_adamw__tensor_lr::overload_name)
      .typed<_fused_adamw__tensor_lr::schema>();
}

// aten::_fused_adamw_.tensor_lr(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()
void _fused_adamw__tensor_lr::call(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, const at::Tensor & lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
    
    static auto op = create__fused_adamw__tensor_lr_typed_handle();
    return op.call(self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf);
}

// aten::_fused_adamw_.tensor_lr(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()
void _fused_adamw__tensor_lr::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, const at::Tensor & lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
    
    static auto op = create__fused_adamw__tensor_lr_typed_handle();
    return op.redispatch(dispatchKeySet, self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_flatten_weight_out, name, "aten::_cudnn_rnn_flatten_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_flatten_weight_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_flatten_weight_out, schema_str, "_cudnn_rnn_flatten_weight.out(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_cudnn_rnn_flatten_weight.out(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_cudnn_rnn_flatten_weight_out::schema> create__cudnn_rnn_flatten_weight_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cudnn_rnn_flatten_weight_out::name, _cudnn_rnn_flatten_weight_out::overload_name)
      .typed<_cudnn_rnn_flatten_weight_out::schema>();
}

// aten::_cudnn_rnn_flatten_weight.out(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _cudnn_rnn_flatten_weight_out::call(at::TensorList weight_arr, int64_t weight_stride0, c10::SymInt input_size, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, bool batch_first, bool bidirectional, at::Tensor & out) {
    
    static auto op = create__cudnn_rnn_flatten_weight_out_typed_handle();
    return op.call(weight_arr, weight_stride0, input_size, mode, hidden_size, proj_size, num_layers, batch_first, bidirectional, out);
}

// aten::_cudnn_rnn_flatten_weight.out(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _cudnn_rnn_flatten_weight_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList weight_arr, int64_t weight_stride0, c10::SymInt input_size, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, bool batch_first, bool bidirectional, at::Tensor & out) {
    
    static auto op = create__cudnn_rnn_flatten_weight_out_typed_handle();
    return op.redispatch(dispatchKeySet, weight_arr, weight_stride0, input_size, mode, hidden_size, proj_size, num_layers, batch_first, bidirectional, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_batch_norm_out, name, "aten::quantized_batch_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_batch_norm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_batch_norm_out, schema_str, "quantized_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point, *, Tensor(a!) out) -> Tensor(a!)")

// aten::quantized_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<quantized_batch_norm_out::schema> create_quantized_batch_norm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantized_batch_norm_out::name, quantized_batch_norm_out::overload_name)
      .typed<quantized_batch_norm_out::schema>();
}

// aten::quantized_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantized_batch_norm_out::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & var, double eps, double output_scale, int64_t output_zero_point, at::Tensor & out) {
    
    static auto op = create_quantized_batch_norm_out_typed_handle();
    return op.call(input, weight, bias, mean, var, eps, output_scale, output_zero_point, out);
}

// aten::quantized_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantized_batch_norm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & var, double eps, double output_scale, int64_t output_zero_point, at::Tensor & out) {
    
    static auto op = create_quantized_batch_norm_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, mean, var, eps, output_scale, output_zero_point, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc_out, name, "aten::conv_tbc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc_out, schema_str, "conv_tbc.out(Tensor self, Tensor weight, Tensor bias, int pad=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::conv_tbc.out(Tensor self, Tensor weight, Tensor bias, int pad=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<conv_tbc_out::schema> create_conv_tbc_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv_tbc_out::name, conv_tbc_out::overload_name)
      .typed<conv_tbc_out::schema>();
}

// aten::conv_tbc.out(Tensor self, Tensor weight, Tensor bias, int pad=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & conv_tbc_out::call(const at::Tensor & self, const at::Tensor & weight, const at::Tensor & bias, int64_t pad, at::Tensor & out) {
    
    static auto op = create_conv_tbc_out_typed_handle();
    return op.call(self, weight, bias, pad, out);
}

// aten::conv_tbc.out(Tensor self, Tensor weight, Tensor bias, int pad=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & conv_tbc_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const at::Tensor & bias, int64_t pad, at::Tensor & out) {
    
    static auto op = create_conv_tbc_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, pad, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_affine_grid_generator_backward_out, name, "aten::cudnn_affine_grid_generator_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_affine_grid_generator_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_affine_grid_generator_backward_out, schema_str, "cudnn_affine_grid_generator_backward.out(Tensor grad, int N, int C, int H, int W, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cudnn_affine_grid_generator_backward.out(Tensor grad, int N, int C, int H, int W, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_affine_grid_generator_backward_out::schema> create_cudnn_affine_grid_generator_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_affine_grid_generator_backward_out::name, cudnn_affine_grid_generator_backward_out::overload_name)
      .typed<cudnn_affine_grid_generator_backward_out::schema>();
}

// aten::cudnn_affine_grid_generator_backward.out(Tensor grad, int N, int C, int H, int W, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cudnn_affine_grid_generator_backward_out::call(const at::Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W, at::Tensor & out) {
    
    static auto op = create_cudnn_affine_grid_generator_backward_out_typed_handle();
    return op.call(grad, N, C, H, W, out);
}

// aten::cudnn_affine_grid_generator_backward.out(Tensor grad, int N, int C, int H, int W, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cudnn_affine_grid_generator_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W, at::Tensor & out) {
    
    static auto op = create_cudnn_affine_grid_generator_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad, N, C, H, W, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_grid_sampler_out, name, "aten::cudnn_grid_sampler")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_grid_sampler_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_grid_sampler_out, schema_str, "cudnn_grid_sampler.out(Tensor self, Tensor grid, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cudnn_grid_sampler.out(Tensor self, Tensor grid, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_grid_sampler_out::schema> create_cudnn_grid_sampler_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_grid_sampler_out::name, cudnn_grid_sampler_out::overload_name)
      .typed<cudnn_grid_sampler_out::schema>();
}

// aten::cudnn_grid_sampler.out(Tensor self, Tensor grid, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cudnn_grid_sampler_out::call(const at::Tensor & self, const at::Tensor & grid, at::Tensor & out) {
    
    static auto op = create_cudnn_grid_sampler_out_typed_handle();
    return op.call(self, grid, out);
}

// aten::cudnn_grid_sampler.out(Tensor self, Tensor grid, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cudnn_grid_sampler_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grid, at::Tensor & out) {
    
    static auto op = create_cudnn_grid_sampler_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, grid, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar_out, name, "aten::div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar_out, schema_str, "div.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::div.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<div_Scalar_out::schema> create_div_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div_Scalar_out::name, div_Scalar_out::overload_name)
      .typed<div_Scalar_out::schema>();
}

// aten::div.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & div_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_div_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::div.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & div_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    
    static auto op = create_div_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar_mode_out, name, "aten::div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar_mode_out, overload_name, "Scalar_mode_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar_mode_out, schema_str, "div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)")

// aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<div_Scalar_mode_out::schema> create_div_Scalar_mode_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div_Scalar_mode_out::name, div_Scalar_mode_out::overload_name)
      .typed<div_Scalar_mode_out::schema>();
}

// aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
at::Tensor & div_Scalar_mode_out::call(const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    
    static auto op = create_div_Scalar_mode_out_typed_handle();
    return op.call(self, other, rounding_mode, out);
}

// aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
at::Tensor & div_Scalar_mode_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    
    static auto op = create_div_Scalar_mode_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_forward_only_out, name, "aten::_embedding_bag_forward_only")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_forward_only_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_forward_only_out, schema_str, "_embedding_bag_forward_only.out(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))")

// aten::_embedding_bag_forward_only.out(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
static C10_NOINLINE c10::TypedOperatorHandle<_embedding_bag_forward_only_out::schema> create__embedding_bag_forward_only_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_embedding_bag_forward_only_out::name, _embedding_bag_forward_only_out::overload_name)
      .typed<_embedding_bag_forward_only_out::schema>();
}

// aten::_embedding_bag_forward_only.out(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> _embedding_bag_forward_only_out::call(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3) {
    
    static auto op = create__embedding_bag_forward_only_out_typed_handle();
    return op.call(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx, out0, out1, out2, out3);
}

// aten::_embedding_bag_forward_only.out(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> _embedding_bag_forward_only_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3) {
    
    static auto op = create__embedding_bag_forward_only_out_typed_handle();
    return op.redispatch(dispatchKeySet, weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx, out0, out1, out2, out3);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_zeros_out, name, "aten::new_zeros")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_zeros_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_zeros_out, schema_str, "new_zeros.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::new_zeros.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<new_zeros_out::schema> create_new_zeros_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(new_zeros_out::name, new_zeros_out::overload_name)
      .typed<new_zeros_out::schema>();
}

// aten::new_zeros.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & new_zeros_out::call(const at::Tensor & self, c10::SymIntArrayRef size, at::Tensor & out) {
    
    static auto op = create_new_zeros_out_typed_handle();
    return op.call(self, size, out);
}

// aten::new_zeros.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & new_zeros_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, at::Tensor & out) {
    
    static auto op = create_new_zeros_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_grid_sampler_2d_cpu_fallback_out, name, "aten::_grid_sampler_2d_cpu_fallback")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_grid_sampler_2d_cpu_fallback_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_grid_sampler_2d_cpu_fallback_out, schema_str, "_grid_sampler_2d_cpu_fallback.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_grid_sampler_2d_cpu_fallback.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_grid_sampler_2d_cpu_fallback_out::schema> create__grid_sampler_2d_cpu_fallback_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_grid_sampler_2d_cpu_fallback_out::name, _grid_sampler_2d_cpu_fallback_out::overload_name)
      .typed<_grid_sampler_2d_cpu_fallback_out::schema>();
}

// aten::_grid_sampler_2d_cpu_fallback.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _grid_sampler_2d_cpu_fallback_out::call(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, at::Tensor & out) {
    
    static auto op = create__grid_sampler_2d_cpu_fallback_out_typed_handle();
    return op.call(input, grid, interpolation_mode, padding_mode, align_corners, out);
}

// aten::_grid_sampler_2d_cpu_fallback.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _grid_sampler_2d_cpu_fallback_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, at::Tensor & out) {
    
    static auto op = create__grid_sampler_2d_cpu_fallback_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, grid, interpolation_mode, padding_mode, align_corners, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d_out, name, "aten::grid_sampler_3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d_out, schema_str, "grid_sampler_3d.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)")

// aten::grid_sampler_3d.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<grid_sampler_3d_out::schema> create_grid_sampler_3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(grid_sampler_3d_out::name, grid_sampler_3d_out::overload_name)
      .typed<grid_sampler_3d_out::schema>();
}

// aten::grid_sampler_3d.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & grid_sampler_3d_out::call(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, at::Tensor & out) {
    
    static auto op = create_grid_sampler_3d_out_typed_handle();
    return op.call(input, grid, interpolation_mode, padding_mode, align_corners, out);
}

// aten::grid_sampler_3d.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & grid_sampler_3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, at::Tensor & out) {
    
    static auto op = create_grid_sampler_3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, grid, interpolation_mode, padding_mode, align_corners, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window_out, name, "aten::hann_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window_out, schema_str, "hann_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hann_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hann_window_out::schema> create_hann_window_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hann_window_out::name, hann_window_out::overload_name)
      .typed<hann_window_out::schema>();
}

// aten::hann_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hann_window_out::call(int64_t window_length, at::Tensor & out) {
    
    static auto op = create_hann_window_out_typed_handle();
    return op.call(window_length, out);
}

// aten::hann_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hann_window_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, at::Tensor & out) {
    
    static auto op = create_hann_window_out_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window_periodic_out, name, "aten::hann_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window_periodic_out, overload_name, "periodic_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window_periodic_out, schema_str, "hann_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hann_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hann_window_periodic_out::schema> create_hann_window_periodic_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hann_window_periodic_out::name, hann_window_periodic_out::overload_name)
      .typed<hann_window_periodic_out::schema>();
}

// aten::hann_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hann_window_periodic_out::call(int64_t window_length, bool periodic, at::Tensor & out) {
    
    static auto op = create_hann_window_periodic_out_typed_handle();
    return op.call(window_length, periodic, out);
}

// aten::hann_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hann_window_periodic_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, at::Tensor & out) {
    
    static auto op = create_hann_window_periodic_out_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_out, name, "aten::hamming_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_out, schema_str, "hamming_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hamming_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hamming_window_out::schema> create_hamming_window_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hamming_window_out::name, hamming_window_out::overload_name)
      .typed<hamming_window_out::schema>();
}

// aten::hamming_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hamming_window_out::call(int64_t window_length, at::Tensor & out) {
    
    static auto op = create_hamming_window_out_typed_handle();
    return op.call(window_length, out);
}

// aten::hamming_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hamming_window_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, at::Tensor & out) {
    
    static auto op = create_hamming_window_out_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_out, name, "aten::hamming_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_out, overload_name, "periodic_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_out, schema_str, "hamming_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hamming_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hamming_window_periodic_out::schema> create_hamming_window_periodic_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hamming_window_periodic_out::name, hamming_window_periodic_out::overload_name)
      .typed<hamming_window_periodic_out::schema>();
}

// aten::hamming_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hamming_window_periodic_out::call(int64_t window_length, bool periodic, at::Tensor & out) {
    
    static auto op = create_hamming_window_periodic_out_typed_handle();
    return op.call(window_length, periodic, out);
}

// aten::hamming_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hamming_window_periodic_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, at::Tensor & out) {
    
    static auto op = create_hamming_window_periodic_out_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha_out, name, "aten::hamming_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha_out, overload_name, "periodic_alpha_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha_out, schema_str, "hamming_window.periodic_alpha_out(int window_length, bool periodic, float alpha, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hamming_window.periodic_alpha_out(int window_length, bool periodic, float alpha, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hamming_window_periodic_alpha_out::schema> create_hamming_window_periodic_alpha_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hamming_window_periodic_alpha_out::name, hamming_window_periodic_alpha_out::overload_name)
      .typed<hamming_window_periodic_alpha_out::schema>();
}

// aten::hamming_window.periodic_alpha_out(int window_length, bool periodic, float alpha, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hamming_window_periodic_alpha_out::call(int64_t window_length, bool periodic, double alpha, at::Tensor & out) {
    
    static auto op = create_hamming_window_periodic_alpha_out_typed_handle();
    return op.call(window_length, periodic, alpha, out);
}

// aten::hamming_window.periodic_alpha_out(int window_length, bool periodic, float alpha, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hamming_window_periodic_alpha_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, double alpha, at::Tensor & out) {
    
    static auto op = create_hamming_window_periodic_alpha_out_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha_beta_out, name, "aten::hamming_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha_beta_out, overload_name, "periodic_alpha_beta_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha_beta_out, schema_str, "hamming_window.periodic_alpha_beta_out(int window_length, bool periodic, float alpha, float beta, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hamming_window.periodic_alpha_beta_out(int window_length, bool periodic, float alpha, float beta, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hamming_window_periodic_alpha_beta_out::schema> create_hamming_window_periodic_alpha_beta_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hamming_window_periodic_alpha_beta_out::name, hamming_window_periodic_alpha_beta_out::overload_name)
      .typed<hamming_window_periodic_alpha_beta_out::schema>();
}

// aten::hamming_window.periodic_alpha_beta_out(int window_length, bool periodic, float alpha, float beta, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hamming_window_periodic_alpha_beta_out::call(int64_t window_length, bool periodic, double alpha, double beta, at::Tensor & out) {
    
    static auto op = create_hamming_window_periodic_alpha_beta_out_typed_handle();
    return op.call(window_length, periodic, alpha, beta, out);
}

// aten::hamming_window.periodic_alpha_beta_out(int window_length, bool periodic, float alpha, float beta, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hamming_window_periodic_alpha_beta_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, double alpha, double beta, at::Tensor & out) {
    
    static auto op = create_hamming_window_periodic_alpha_beta_out_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, alpha, beta, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm_backward_out, name, "aten::native_group_norm_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm_backward_out, schema_str, "native_group_norm_backward.out(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::native_group_norm_backward.out(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<native_group_norm_backward_out::schema> create_native_group_norm_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_group_norm_backward_out::name, native_group_norm_backward_out::overload_name)
      .typed<native_group_norm_backward_out::schema>();
}

// aten::native_group_norm_backward.out(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_group_norm_backward_out::call(const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_native_group_norm_backward_out_typed_handle();
    return op.call(grad_out, input, mean, rstd, weight, N, C, HxW, group, output_mask, out0, out1, out2);
}

// aten::native_group_norm_backward.out(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_group_norm_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_native_group_norm_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, input, mean, rstd, weight, N, C, HxW, group, output_mask, out0, out1, out2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isnan_out, name, "aten::isnan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isnan_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isnan_out, schema_str, "isnan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::isnan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<isnan_out::schema> create_isnan_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isnan_out::name, isnan_out::overload_name)
      .typed<isnan_out::schema>();
}

// aten::isnan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & isnan_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_isnan_out_typed_handle();
    return op.call(self, out);
}

// aten::isnan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & isnan_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_isnan_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_layer_norm_out, name, "aten::native_layer_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_layer_norm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_layer_norm_out, schema_str, "native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<native_layer_norm_out::schema> create_native_layer_norm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_layer_norm_out::name, native_layer_norm_out::overload_name)
      .typed<native_layer_norm_out::schema>();
}

// aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_layer_norm_out::call(const at::Tensor & input, c10::SymIntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_native_layer_norm_out_typed_handle();
    return op.call(input, normalized_shape, weight, bias, eps, out0, out1, out2);
}

// aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_layer_norm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::SymIntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_native_layer_norm_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, normalized_shape, weight, bias, eps, out0, out1, out2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool2d_out, name, "aten::mkldnn_max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool2d_out, schema_str, "mkldnn_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::mkldnn_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_max_pool2d_out::schema> create_mkldnn_max_pool2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_max_pool2d_out::name, mkldnn_max_pool2d_out::overload_name)
      .typed<mkldnn_max_pool2d_out::schema>();
}

// aten::mkldnn_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mkldnn_max_pool2d_out::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out) {
    
    static auto op = create_mkldnn_max_pool2d_out_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode, out);
}

// aten::mkldnn_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mkldnn_max_pool2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out) {
    
    static auto op = create_mkldnn_max_pool2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_max_pool2d_out, name, "aten::quantized_max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_max_pool2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_max_pool2d_out, schema_str, "quantized_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::quantized_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<quantized_max_pool2d_out::schema> create_quantized_max_pool2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantized_max_pool2d_out::name, quantized_max_pool2d_out::overload_name)
      .typed<quantized_max_pool2d_out::schema>();
}

// aten::quantized_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantized_max_pool2d_out::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out) {
    
    static auto op = create_quantized_max_pool2d_out_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode, out);
}

// aten::quantized_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantized_max_pool2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out) {
    
    static auto op = create_quantized_max_pool2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mps_convolution_out, name, "aten::_mps_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mps_convolution_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mps_convolution_out, schema_str, "_mps_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_mps_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_mps_convolution_out::schema> create__mps_convolution_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_mps_convolution_out::name, _mps_convolution_out::overload_name)
      .typed<_mps_convolution_out::schema>();
}

// aten::_mps_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _mps_convolution_out::call(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor & out) {
    
    static auto op = create__mps_convolution_out_typed_handle();
    return op.call(self, weight, bias, padding, stride, dilation, groups, out);
}

// aten::_mps_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _mps_convolution_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor & out) {
    
    static auto op = create__mps_convolution_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, padding, stride, dilation, groups, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_rnn_layer_backward_out, name, "aten::mkldnn_rnn_layer_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_rnn_layer_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_rnn_layer_backward_out, schema_str, "mkldnn_rnn_layer_backward.out(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5, Tensor(g!) out6) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!), Tensor(g!))")

// aten::mkldnn_rnn_layer_backward.out(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5, Tensor(g!) out6) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!), Tensor(g!))
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_rnn_layer_backward_out::schema> create_mkldnn_rnn_layer_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_rnn_layer_backward_out::name, mkldnn_rnn_layer_backward_out::overload_name)
      .typed<mkldnn_rnn_layer_backward_out::schema>();
}

// aten::mkldnn_rnn_layer_backward.out(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5, Tensor(g!) out6) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!), Tensor(g!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> mkldnn_rnn_layer_backward_out::call(const at::Tensor & input, const at::Tensor & weight1, const at::Tensor & weight2, const at::Tensor & weight3, const at::Tensor & weight4, const at::Tensor & hx_, const at::Tensor & cx_tmp, const at::Tensor & output, const at::Tensor & hy_, const at::Tensor & cy_, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, bool reverse, int64_t mode, int64_t hidden_size, int64_t num_layers, bool has_biases, bool train, bool bidirectional, at::IntArrayRef batch_sizes, bool batch_first, const at::Tensor & workspace, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3, at::Tensor & out4, at::Tensor & out5, at::Tensor & out6) {
    
    static auto op = create_mkldnn_rnn_layer_backward_out_typed_handle();
    return op.call(input, weight1, weight2, weight3, weight4, hx_, cx_tmp, output, hy_, cy_, grad_output, grad_hy, grad_cy, reverse, mode, hidden_size, num_layers, has_biases, train, bidirectional, batch_sizes, batch_first, workspace, out0, out1, out2, out3, out4, out5, out6);
}

// aten::mkldnn_rnn_layer_backward.out(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5, Tensor(g!) out6) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!), Tensor(g!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> mkldnn_rnn_layer_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight1, const at::Tensor & weight2, const at::Tensor & weight3, const at::Tensor & weight4, const at::Tensor & hx_, const at::Tensor & cx_tmp, const at::Tensor & output, const at::Tensor & hy_, const at::Tensor & cy_, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, bool reverse, int64_t mode, int64_t hidden_size, int64_t num_layers, bool has_biases, bool train, bool bidirectional, at::IntArrayRef batch_sizes, bool batch_first, const at::Tensor & workspace, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3, at::Tensor & out4, at::Tensor & out5, at::Tensor & out6) {
    
    static auto op = create_mkldnn_rnn_layer_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight1, weight2, weight3, weight4, hx_, cx_tmp, output, hy_, cy_, grad_output, grad_hy, grad_cy, reverse, mode, hidden_size, num_layers, has_biases, train, bidirectional, batch_sizes, batch_first, workspace, out0, out1, out2, out3, out4, out5, out6);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution_out, name, "aten::miopen_depthwise_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution_out, schema_str, "miopen_depthwise_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)")

// aten::miopen_depthwise_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<miopen_depthwise_convolution_out::schema> create_miopen_depthwise_convolution_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_depthwise_convolution_out::name, miopen_depthwise_convolution_out::overload_name)
      .typed<miopen_depthwise_convolution_out::schema>();
}

// aten::miopen_depthwise_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & miopen_depthwise_convolution_out::call(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic, at::Tensor & out) {
    
    static auto op = create_miopen_depthwise_convolution_out_typed_handle();
    return op.call(self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic, out);
}

// aten::miopen_depthwise_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & miopen_depthwise_convolution_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic, at::Tensor & out) {
    
    static auto op = create_miopen_depthwise_convolution_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_stats_out, name, "aten::batch_norm_stats")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_stats_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_stats_out, schema_str, "batch_norm_stats.out(Tensor input, float eps, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))")

// aten::batch_norm_stats.out(Tensor input, float eps, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_stats_out::schema> create_batch_norm_stats_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_stats_out::name, batch_norm_stats_out::overload_name)
      .typed<batch_norm_stats_out::schema>();
}

// aten::batch_norm_stats.out(Tensor input, float eps, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> batch_norm_stats_out::call(const at::Tensor & input, double eps, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create_batch_norm_stats_out_typed_handle();
    return op.call(input, eps, out0, out1);
}

// aten::batch_norm_stats.out(Tensor input, float eps, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> batch_norm_stats_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double eps, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create_batch_norm_stats_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, eps, out0, out1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats_out, name, "aten::batch_norm_gather_stats")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats_out, schema_str, "batch_norm_gather_stats.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))")

// aten::batch_norm_gather_stats.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_gather_stats_out::schema> create_batch_norm_gather_stats_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_gather_stats_out::name, batch_norm_gather_stats_out::overload_name)
      .typed<batch_norm_gather_stats_out::schema>();
}

// aten::batch_norm_gather_stats.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> batch_norm_gather_stats_out::call(const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, int64_t count, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create_batch_norm_gather_stats_out_typed_handle();
    return op.call(input, mean, invstd, running_mean, running_var, momentum, eps, count, out0, out1);
}

// aten::batch_norm_gather_stats.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> batch_norm_gather_stats_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, int64_t count, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create_batch_norm_gather_stats_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, mean, invstd, running_mean, running_var, momentum, eps, count, out0, out1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_backward_out, name, "aten::native_batch_norm_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_backward_out, schema_str, "native_batch_norm_backward.out(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::native_batch_norm_backward.out(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<native_batch_norm_backward_out::schema> create_native_batch_norm_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_batch_norm_backward_out::name, native_batch_norm_backward_out::overload_name)
      .typed<native_batch_norm_backward_out::schema>();
}

// aten::native_batch_norm_backward.out(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_batch_norm_backward_out::call(const at::Tensor & grad_out, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_native_batch_norm_backward_out_typed_handle();
    return op.call(grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask, out0, out1, out2);
}

// aten::native_batch_norm_backward.out(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_batch_norm_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_native_batch_norm_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask, out0, out1, out2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_backward_reduce_out, name, "aten::batch_norm_backward_reduce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_backward_reduce_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_backward_reduce_out, schema_str, "batch_norm_backward_reduce.out(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))")

// aten::batch_norm_backward_reduce.out(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_backward_reduce_out::schema> create_batch_norm_backward_reduce_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_backward_reduce_out::name, batch_norm_backward_reduce_out::overload_name)
      .typed<batch_norm_backward_reduce_out::schema>();
}

// aten::batch_norm_backward_reduce.out(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> batch_norm_backward_reduce_out::call(const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & weight, bool input_g, bool weight_g, bool bias_g, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3) {
    
    static auto op = create_batch_norm_backward_reduce_out_typed_handle();
    return op.call(grad_out, input, mean, invstd, weight, input_g, weight_g, bias_g, out0, out1, out2, out3);
}

// aten::batch_norm_backward_reduce.out(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> batch_norm_backward_reduce_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & weight, bool input_g, bool weight_g, bool bias_g, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3) {
    
    static auto op = create_batch_norm_backward_reduce_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, input, mean, invstd, weight, input_g, weight_g, bias_g, out0, out1, out2, out3);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution_out, name, "aten::_nnpack_spatial_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution_out, schema_str, "_nnpack_spatial_convolution.out(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, SymInt[2] stride=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_nnpack_spatial_convolution.out(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, SymInt[2] stride=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_nnpack_spatial_convolution_out::schema> create__nnpack_spatial_convolution_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_nnpack_spatial_convolution_out::name, _nnpack_spatial_convolution_out::overload_name)
      .typed<_nnpack_spatial_convolution_out::schema>();
}

// aten::_nnpack_spatial_convolution.out(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, SymInt[2] stride=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _nnpack_spatial_convolution_out::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, at::Tensor & out) {
    
    static auto op = create__nnpack_spatial_convolution_out_typed_handle();
    return op.call(input, weight, bias, padding, stride, out);
}

// aten::_nnpack_spatial_convolution.out(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, SymInt[2] stride=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _nnpack_spatial_convolution_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, at::Tensor & out) {
    
    static auto op = create__nnpack_spatial_convolution_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, padding, stride, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_names_out, name, "aten::ones")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_names_out, overload_name, "names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_names_out, schema_str, "ones.names_out(int[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)")

// aten::ones.names_out(int[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ones_names_out::schema> create_ones_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ones_names_out::name, ones_names_out::overload_name)
      .typed<ones_names_out::schema>();
}

// aten::ones.names_out(int[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ones_names_out::call(at::IntArrayRef size, c10::optional<at::DimnameList> names, at::Tensor & out) {
    
    static auto op = create_ones_names_out_typed_handle();
    return op.call(size, names, out);
}

// aten::ones.names_out(int[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ones_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, at::Tensor & out) {
    
    static auto op = create_ones_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, size, names, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cdist_forward_out, name, "aten::_cdist_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cdist_forward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cdist_forward_out, schema_str, "_cdist_forward.out(Tensor x1, Tensor x2, float p, int? compute_mode, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_cdist_forward.out(Tensor x1, Tensor x2, float p, int? compute_mode, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_cdist_forward_out::schema> create__cdist_forward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cdist_forward_out::name, _cdist_forward_out::overload_name)
      .typed<_cdist_forward_out::schema>();
}

// aten::_cdist_forward.out(Tensor x1, Tensor x2, float p, int? compute_mode, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _cdist_forward_out::call(const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode, at::Tensor & out) {
    
    static auto op = create__cdist_forward_out_typed_handle();
    return op.call(x1, x2, p, compute_mode, out);
}

// aten::_cdist_forward.out(Tensor x1, Tensor x2, float p, int? compute_mode, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _cdist_forward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode, at::Tensor & out) {
    
    static auto op = create__cdist_forward_out_typed_handle();
    return op.redispatch(dispatchKeySet, x1, x2, p, compute_mode, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_like_out, name, "aten::rand_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_like_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_like_out, schema_str, "rand_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)")

// aten::rand_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<rand_like_out::schema> create_rand_like_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rand_like_out::name, rand_like_out::overload_name)
      .typed<rand_like_out::schema>();
}

// aten::rand_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rand_like_out::call(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    
    static auto op = create_rand_like_out_typed_handle();
    return op.call(self, memory_format, out);
}

// aten::rand_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rand_like_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    
    static auto op = create_rand_like_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, memory_format, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like_out, name, "aten::randint_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like_out, schema_str, "randint_like.out(Tensor self, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)")

// aten::randint_like.out(Tensor self, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<randint_like_out::schema> create_randint_like_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randint_like_out::name, randint_like_out::overload_name)
      .typed<randint_like_out::schema>();
}

// aten::randint_like.out(Tensor self, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randint_like_out::call(const at::Tensor & self, c10::SymInt high, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    
    static auto op = create_randint_like_out_typed_handle();
    return op.call(self, high, memory_format, out);
}

// aten::randint_like.out(Tensor self, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randint_like_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt high, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    
    static auto op = create_randint_like_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, high, memory_format, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like_low_dtype_out, name, "aten::randint_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like_low_dtype_out, overload_name, "low_dtype_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like_low_dtype_out, schema_str, "randint_like.low_dtype_out(Tensor self, SymInt low, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)")

// aten::randint_like.low_dtype_out(Tensor self, SymInt low, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<randint_like_low_dtype_out::schema> create_randint_like_low_dtype_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randint_like_low_dtype_out::name, randint_like_low_dtype_out::overload_name)
      .typed<randint_like_low_dtype_out::schema>();
}

// aten::randint_like.low_dtype_out(Tensor self, SymInt low, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randint_like_low_dtype_out::call(const at::Tensor & self, c10::SymInt low, c10::SymInt high, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    
    static auto op = create_randint_like_low_dtype_out_typed_handle();
    return op.call(self, low, high, memory_format, out);
}

// aten::randint_like.low_dtype_out(Tensor self, SymInt low, SymInt high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randint_like_low_dtype_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt low, c10::SymInt high, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    
    static auto op = create_randint_like_low_dtype_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, low, high, memory_format, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_backward_out, name, "aten::select_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_backward_out, schema_str, "select_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)")

// aten::select_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<select_backward_out::schema> create_select_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(select_backward_out::name, select_backward_out::overload_name)
      .typed<select_backward_out::schema>();
}

// aten::select_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & select_backward_out::call(const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt index, at::Tensor & out) {
    
    static auto op = create_select_backward_out_typed_handle();
    return op.call(grad_output, input_sizes, dim, index, out);
}

// aten::select_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & select_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt index, at::Tensor & out) {
    
    static auto op = create_select_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input_sizes, dim, index, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_scatter_out, name, "aten::slice_scatter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_scatter_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_scatter_out, schema_str, "slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<slice_scatter_out::schema> create_slice_scatter_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slice_scatter_out::name, slice_scatter_out::overload_name)
      .typed<slice_scatter_out::schema>();
}

// aten::slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slice_scatter_out::call(const at::Tensor & self, const at::Tensor & src, int64_t dim, c10::optional<c10::SymInt> start, c10::optional<c10::SymInt> end, c10::SymInt step, at::Tensor & out) {
    
    static auto op = create_slice_scatter_out_typed_handle();
    return op.call(self, src, dim, start, end, step, out);
}

// aten::slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slice_scatter_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & src, int64_t dim, c10::optional<c10::SymInt> start, c10::optional<c10::SymInt> end, c10::SymInt step, at::Tensor & out) {
    
    static auto op = create_slice_scatter_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, src, dim, start, end, step, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose_out, name, "aten::_mkldnn_transpose")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose_out, schema_str, "_mkldnn_transpose.out(Tensor self, int dim0, int dim1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_mkldnn_transpose.out(Tensor self, int dim0, int dim1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_mkldnn_transpose_out::schema> create__mkldnn_transpose_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_mkldnn_transpose_out::name, _mkldnn_transpose_out::overload_name)
      .typed<_mkldnn_transpose_out::schema>();
}

// aten::_mkldnn_transpose.out(Tensor self, int dim0, int dim1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _mkldnn_transpose_out::call(const at::Tensor & self, int64_t dim0, int64_t dim1, at::Tensor & out) {
    
    static auto op = create__mkldnn_transpose_out_typed_handle();
    return op.call(self, dim0, dim1, out);
}

// aten::_mkldnn_transpose.out(Tensor self, int dim0, int dim1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _mkldnn_transpose_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim0, int64_t dim1, at::Tensor & out) {
    
    static auto op = create__mkldnn_transpose_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim0, dim1, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_from_padded_and_nested_example_out, name, "aten::_nested_from_padded_and_nested_example")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_from_padded_and_nested_example_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nested_from_padded_and_nested_example_out, schema_str, "_nested_from_padded_and_nested_example.out(Tensor padded, Tensor nt_example, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_nested_from_padded_and_nested_example.out(Tensor padded, Tensor nt_example, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_nested_from_padded_and_nested_example_out::schema> create__nested_from_padded_and_nested_example_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_nested_from_padded_and_nested_example_out::name, _nested_from_padded_and_nested_example_out::overload_name)
      .typed<_nested_from_padded_and_nested_example_out::schema>();
}

// aten::_nested_from_padded_and_nested_example.out(Tensor padded, Tensor nt_example, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _nested_from_padded_and_nested_example_out::call(const at::Tensor & padded, const at::Tensor & nt_example, at::Tensor & out) {
    
    static auto op = create__nested_from_padded_and_nested_example_out_typed_handle();
    return op.call(padded, nt_example, out);
}

// aten::_nested_from_padded_and_nested_example.out(Tensor padded, Tensor nt_example, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _nested_from_padded_and_nested_example_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & padded, const at::Tensor & nt_example, at::Tensor & out) {
    
    static auto op = create__nested_from_padded_and_nested_example_out_typed_handle();
    return op.redispatch(dispatchKeySet, padded, nt_example, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_dim_out, name, "aten::unique_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_dim_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_dim_out, schema_str, "unique_dim.out(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::unique_dim.out(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<unique_dim_out::schema> create_unique_dim_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unique_dim_out::name, unique_dim_out::overload_name)
      .typed<unique_dim_out::schema>();
}

// aten::unique_dim.out(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> unique_dim_out::call(const at::Tensor & self, int64_t dim, bool sorted, bool return_inverse, bool return_counts, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_unique_dim_out_typed_handle();
    return op.call(self, dim, sorted, return_inverse, return_counts, out0, out1, out2);
}

// aten::unique_dim.out(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> unique_dim_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool sorted, bool return_inverse, bool return_counts, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_unique_dim_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, sorted, return_inverse, return_counts, out0, out1, out2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_consecutive_out, name, "aten::unique_consecutive")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_consecutive_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_consecutive_out, schema_str, "unique_consecutive.out(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::unique_consecutive.out(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<unique_consecutive_out::schema> create_unique_consecutive_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unique_consecutive_out::name, unique_consecutive_out::overload_name)
      .typed<unique_consecutive_out::schema>();
}

// aten::unique_consecutive.out(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> unique_consecutive_out::call(const at::Tensor & self, bool return_inverse, bool return_counts, c10::optional<int64_t> dim, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_unique_consecutive_out_typed_handle();
    return op.call(self, return_inverse, return_counts, dim, out0, out1, out2);
}

// aten::unique_consecutive.out(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> unique_consecutive_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool return_inverse, bool return_counts, c10::optional<int64_t> dim, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create_unique_consecutive_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, return_inverse, return_counts, dim, out0, out1, out2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dirichlet_grad_out, name, "aten::_dirichlet_grad")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dirichlet_grad_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dirichlet_grad_out, schema_str, "_dirichlet_grad.out(Tensor x, Tensor alpha, Tensor total, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_dirichlet_grad.out(Tensor x, Tensor alpha, Tensor total, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_dirichlet_grad_out::schema> create__dirichlet_grad_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_dirichlet_grad_out::name, _dirichlet_grad_out::overload_name)
      .typed<_dirichlet_grad_out::schema>();
}

// aten::_dirichlet_grad.out(Tensor x, Tensor alpha, Tensor total, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _dirichlet_grad_out::call(const at::Tensor & x, const at::Tensor & alpha, const at::Tensor & total, at::Tensor & out) {
    
    static auto op = create__dirichlet_grad_out_typed_handle();
    return op.call(x, alpha, total, out);
}

// aten::_dirichlet_grad.out(Tensor x, Tensor alpha, Tensor total, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _dirichlet_grad_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & alpha, const at::Tensor & total, at::Tensor & out) {
    
    static auto op = create__dirichlet_grad_out_typed_handle();
    return op.redispatch(dispatchKeySet, x, alpha, total, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clone_out, name, "aten::clone")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clone_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clone_out, schema_str, "clone.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)")

// aten::clone.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clone_out::schema> create_clone_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clone_out::name, clone_out::overload_name)
      .typed<clone_out::schema>();
}

// aten::clone.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clone_out::call(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    
    static auto op = create_clone_out_typed_handle();
    return op.call(self, memory_format, out);
}

// aten::clone.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clone_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    
    static auto op = create_clone_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, memory_format, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_out, name, "aten::resize_as")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_out, schema_str, "resize_as.out(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)")

// aten::resize_as.out(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<resize_as_out::schema> create_resize_as_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(resize_as_out::name, resize_as_out::overload_name)
      .typed<resize_as_out::schema>();
}

// aten::resize_as.out(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
const at::Tensor & resize_as_out::call(const at::Tensor & self, const at::Tensor & the_template, c10::optional<at::MemoryFormat> memory_format, const at::Tensor & out) {
    
    static auto op = create_resize_as_out_typed_handle();
    return op.call(self, the_template, memory_format, out);
}

// aten::resize_as.out(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
const at::Tensor & resize_as_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & the_template, c10::optional<at::MemoryFormat> memory_format, const at::Tensor & out) {
    
    static auto op = create_resize_as_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, the_template, memory_format, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as, name, "aten::resize_as")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as, schema_str, "resize_as(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor")

// aten::resize_as(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<resize_as::schema> create_resize_as_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(resize_as::name, resize_as::overload_name)
      .typed<resize_as::schema>();
}

// aten::resize_as(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor
at::Tensor resize_as::call(const at::Tensor & self, const at::Tensor & the_template, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_resize_as_typed_handle();
    return op.call(self, the_template, memory_format);
}

// aten::resize_as(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor
at::Tensor resize_as::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & the_template, c10::optional<at::MemoryFormat> memory_format) {
    
    static auto op = create_resize_as_typed_handle();
    return op.redispatch(dispatchKeySet, self, the_template, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_sparse_out, name, "aten::resize_as_sparse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_sparse_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_sparse_out, schema_str, "resize_as_sparse.out(Tensor self, Tensor the_template, *, Tensor(a!) out) -> Tensor(a!)")

// aten::resize_as_sparse.out(Tensor self, Tensor the_template, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<resize_as_sparse_out::schema> create_resize_as_sparse_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(resize_as_sparse_out::name, resize_as_sparse_out::overload_name)
      .typed<resize_as_sparse_out::schema>();
}

// aten::resize_as_sparse.out(Tensor self, Tensor the_template, *, Tensor(a!) out) -> Tensor(a!)
const at::Tensor & resize_as_sparse_out::call(const at::Tensor & self, const at::Tensor & the_template, const at::Tensor & out) {
    
    static auto op = create_resize_as_sparse_out_typed_handle();
    return op.call(self, the_template, out);
}

// aten::resize_as_sparse.out(Tensor self, Tensor the_template, *, Tensor(a!) out) -> Tensor(a!)
const at::Tensor & resize_as_sparse_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & the_template, const at::Tensor & out) {
    
    static auto op = create_resize_as_sparse_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, the_template, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_sparse, name, "aten::resize_as_sparse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_sparse, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_sparse, schema_str, "resize_as_sparse(Tensor self, Tensor the_template) -> Tensor")

// aten::resize_as_sparse(Tensor self, Tensor the_template) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<resize_as_sparse::schema> create_resize_as_sparse_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(resize_as_sparse::name, resize_as_sparse::overload_name)
      .typed<resize_as_sparse::schema>();
}

// aten::resize_as_sparse(Tensor self, Tensor the_template) -> Tensor
at::Tensor resize_as_sparse::call(const at::Tensor & self, const at::Tensor & the_template) {
    
    static auto op = create_resize_as_sparse_typed_handle();
    return op.call(self, the_template);
}

// aten::resize_as_sparse(Tensor self, Tensor the_template) -> Tensor
at::Tensor resize_as_sparse::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & the_template) {
    
    static auto op = create_resize_as_sparse_typed_handle();
    return op.redispatch(dispatchKeySet, self, the_template);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_csc_out, name, "aten::_to_sparse_csc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_csc_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_csc_out, schema_str, "_to_sparse_csc.out(Tensor self, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_to_sparse_csc.out(Tensor self, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_to_sparse_csc_out::schema> create__to_sparse_csc_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_to_sparse_csc_out::name, _to_sparse_csc_out::overload_name)
      .typed<_to_sparse_csc_out::schema>();
}

// aten::_to_sparse_csc.out(Tensor self, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _to_sparse_csc_out::call(const at::Tensor & self, c10::optional<int64_t> dense_dim, at::Tensor & out) {
    
    static auto op = create__to_sparse_csc_out_typed_handle();
    return op.call(self, dense_dim, out);
}

// aten::_to_sparse_csc.out(Tensor self, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _to_sparse_csc_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dense_dim, at::Tensor & out) {
    
    static auto op = create__to_sparse_csc_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dense_dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_bsc_out, name, "aten::_to_sparse_bsc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_bsc_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_sparse_bsc_out, schema_str, "_to_sparse_bsc.out(Tensor self, int[2] blocksize, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_to_sparse_bsc.out(Tensor self, int[2] blocksize, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_to_sparse_bsc_out::schema> create__to_sparse_bsc_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_to_sparse_bsc_out::name, _to_sparse_bsc_out::overload_name)
      .typed<_to_sparse_bsc_out::schema>();
}

// aten::_to_sparse_bsc.out(Tensor self, int[2] blocksize, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _to_sparse_bsc_out::call(const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim, at::Tensor & out) {
    
    static auto op = create__to_sparse_bsc_out_typed_handle();
    return op.call(self, blocksize, dense_dim, out);
}

// aten::_to_sparse_bsc.out(Tensor self, int[2] blocksize, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _to_sparse_bsc_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim, at::Tensor & out) {
    
    static auto op = create__to_sparse_bsc_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, blocksize, dense_dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_reorder_conv2d_weight_out, name, "aten::mkldnn_reorder_conv2d_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_reorder_conv2d_weight_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_reorder_conv2d_weight_out, schema_str, "mkldnn_reorder_conv2d_weight.out(Tensor self, SymInt[2] padding=0, SymInt[2] stride=1, SymInt[2] dilation=1, SymInt groups=1, SymInt[]? input_size=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::mkldnn_reorder_conv2d_weight.out(Tensor self, SymInt[2] padding=0, SymInt[2] stride=1, SymInt[2] dilation=1, SymInt groups=1, SymInt[]? input_size=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_reorder_conv2d_weight_out::schema> create_mkldnn_reorder_conv2d_weight_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_reorder_conv2d_weight_out::name, mkldnn_reorder_conv2d_weight_out::overload_name)
      .typed<mkldnn_reorder_conv2d_weight_out::schema>();
}

// aten::mkldnn_reorder_conv2d_weight.out(Tensor self, SymInt[2] padding=0, SymInt[2] stride=1, SymInt[2] dilation=1, SymInt groups=1, SymInt[]? input_size=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mkldnn_reorder_conv2d_weight_out::call(const at::Tensor & self, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::OptionalSymIntArrayRef input_size, at::Tensor & out) {
    
    static auto op = create_mkldnn_reorder_conv2d_weight_out_typed_handle();
    return op.call(self, padding, stride, dilation, groups, input_size, out);
}

// aten::mkldnn_reorder_conv2d_weight.out(Tensor self, SymInt[2] padding=0, SymInt[2] stride=1, SymInt[2] dilation=1, SymInt groups=1, SymInt[]? input_size=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mkldnn_reorder_conv2d_weight_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::OptionalSymIntArrayRef input_size, at::Tensor & out) {
    
    static auto op = create_mkldnn_reorder_conv2d_weight_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding, stride, dilation, groups, input_size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_channel_out, name, "aten::quantize_per_channel")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_channel_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_channel_out, schema_str, "quantize_per_channel.out(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)")

// aten::quantize_per_channel.out(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<quantize_per_channel_out::schema> create_quantize_per_channel_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantize_per_channel_out::name, quantize_per_channel_out::overload_name)
      .typed<quantize_per_channel_out::schema>();
}

// aten::quantize_per_channel.out(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantize_per_channel_out::call(const at::Tensor & self, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, at::ScalarType dtype, at::Tensor & out) {
    
    static auto op = create_quantize_per_channel_out_typed_handle();
    return op.call(self, scales, zero_points, axis, dtype, out);
}

// aten::quantize_per_channel.out(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantize_per_channel_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, at::ScalarType dtype, at::Tensor & out) {
    
    static auto op = create_quantize_per_channel_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, scales, zero_points, axis, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_self_out, name, "aten::dequantize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_self_out, overload_name, "self_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_self_out, schema_str, "dequantize.self_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::dequantize.self_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<dequantize_self_out::schema> create_dequantize_self_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dequantize_self_out::name, dequantize_self_out::overload_name)
      .typed<dequantize_self_out::schema>();
}

// aten::dequantize.self_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & dequantize_self_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_dequantize_self_out_typed_handle();
    return op.call(self, out);
}

// aten::dequantize.self_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & dequantize_self_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_dequantize_self_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_tensors_out, name, "aten::dequantize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_tensors_out, overload_name, "tensors_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_tensors_out, schema_str, "dequantize.tensors_out(Tensor[] tensors, *, Tensor(a!)[] out) -> ()")

// aten::dequantize.tensors_out(Tensor[] tensors, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<dequantize_tensors_out::schema> create_dequantize_tensors_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dequantize_tensors_out::name, dequantize_tensors_out::overload_name)
      .typed<dequantize_tensors_out::schema>();
}

// aten::dequantize.tensors_out(Tensor[] tensors, *, Tensor(a!)[] out) -> ()
void dequantize_tensors_out::call(at::TensorList tensors, at::TensorList out) {
    
    static auto op = create_dequantize_tensors_out_typed_handle();
    return op.call(tensors, out);
}

// aten::dequantize.tensors_out(Tensor[] tensors, *, Tensor(a!)[] out) -> ()
void dequantize_tensors_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::TensorList out) {
    
    static auto op = create_dequantize_tensors_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_zero_points_out, name, "aten::q_per_channel_zero_points")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_zero_points_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_zero_points_out, schema_str, "q_per_channel_zero_points.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::q_per_channel_zero_points.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<q_per_channel_zero_points_out::schema> create_q_per_channel_zero_points_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(q_per_channel_zero_points_out::name, q_per_channel_zero_points_out::overload_name)
      .typed<q_per_channel_zero_points_out::schema>();
}

// aten::q_per_channel_zero_points.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & q_per_channel_zero_points_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_q_per_channel_zero_points_out_typed_handle();
    return op.call(self, out);
}

// aten::q_per_channel_zero_points.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & q_per_channel_zero_points_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_q_per_channel_zero_points_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine_out, name, "aten::_fake_quantize_learnable_per_channel_affine")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine_out, schema_str, "_fake_quantize_learnable_per_channel_affine.out(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_fake_quantize_learnable_per_channel_affine.out(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_fake_quantize_learnable_per_channel_affine_out::schema> create__fake_quantize_learnable_per_channel_affine_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fake_quantize_learnable_per_channel_affine_out::name, _fake_quantize_learnable_per_channel_affine_out::overload_name)
      .typed<_fake_quantize_learnable_per_channel_affine_out::schema>();
}

// aten::_fake_quantize_learnable_per_channel_affine.out(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _fake_quantize_learnable_per_channel_affine_out::call(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor, at::Tensor & out) {
    
    static auto op = create__fake_quantize_learnable_per_channel_affine_out_typed_handle();
    return op.call(self, scale, zero_point, axis, quant_min, quant_max, grad_factor, out);
}

// aten::_fake_quantize_learnable_per_channel_affine.out(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _fake_quantize_learnable_per_channel_affine_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor, at::Tensor & out) {
    
    static auto op = create__fake_quantize_learnable_per_channel_affine_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, axis, quant_min, quant_max, grad_factor, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_lstm_mps_out, name, "aten::_lstm_mps")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_lstm_mps_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_lstm_mps_out, schema_str, "_lstm_mps.out(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!))")

// aten::_lstm_mps.out(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!))
static C10_NOINLINE c10::TypedOperatorHandle<_lstm_mps_out::schema> create__lstm_mps_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_lstm_mps_out::name, _lstm_mps_out::overload_name)
      .typed<_lstm_mps_out::schema>();
}

// aten::_lstm_mps.out(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> _lstm_mps_out::call(const at::Tensor & input, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3, at::Tensor & out4, at::Tensor & out5) {
    
    static auto op = create__lstm_mps_out_typed_handle();
    return op.call(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first, out0, out1, out2, out3, out4, out5);
}

// aten::_lstm_mps.out(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> _lstm_mps_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3, at::Tensor & out4, at::Tensor & out5) {
    
    static auto op = create__lstm_mps_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first, out0, out1, out2, out3, out4, out5);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell_out, name, "aten::_thnn_fused_lstm_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell_out, schema_str, "_thnn_fused_lstm_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::_thnn_fused_lstm_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<_thnn_fused_lstm_cell_out::schema> create__thnn_fused_lstm_cell_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_thnn_fused_lstm_cell_out::name, _thnn_fused_lstm_cell_out::overload_name)
      .typed<_thnn_fused_lstm_cell_out::schema>();
}

// aten::_thnn_fused_lstm_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _thnn_fused_lstm_cell_out::call(const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & cx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create__thnn_fused_lstm_cell_out_typed_handle();
    return op.call(input_gates, hidden_gates, cx, input_bias, hidden_bias, out0, out1, out2);
}

// aten::_thnn_fused_lstm_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _thnn_fused_lstm_cell_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & cx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
    
    static auto op = create__thnn_fused_lstm_cell_out_typed_handle();
    return op.redispatch(dispatchKeySet, input_gates, hidden_gates, cx, input_bias, hidden_bias, out0, out1, out2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lift_fresh_copy_out, name, "aten::lift_fresh_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lift_fresh_copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lift_fresh_copy_out, schema_str, "lift_fresh_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::lift_fresh_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lift_fresh_copy_out::schema> create_lift_fresh_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lift_fresh_copy_out::name, lift_fresh_copy_out::overload_name)
      .typed<lift_fresh_copy_out::schema>();
}

// aten::lift_fresh_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lift_fresh_copy_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_lift_fresh_copy_out_typed_handle();
    return op.call(self, out);
}

// aten::lift_fresh_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lift_fresh_copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_lift_fresh_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Scalar_out, name, "aten::index_fill")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Scalar_out, overload_name, "int_Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Scalar_out, schema_str, "index_fill.int_Scalar_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)")

// aten::index_fill.int_Scalar_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_fill_int_Scalar_out::schema> create_index_fill_int_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill_int_Scalar_out::name, index_fill_int_Scalar_out::overload_name)
      .typed<index_fill_int_Scalar_out::schema>();
}

// aten::index_fill.int_Scalar_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & index_fill_int_Scalar_out::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, at::Tensor & out) {
    
    static auto op = create_index_fill_int_Scalar_out_typed_handle();
    return op.call(self, dim, index, value, out);
}

// aten::index_fill.int_Scalar_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & index_fill_int_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, at::Tensor & out) {
    
    static auto op = create_index_fill_int_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Tensor_out, name, "aten::index_fill")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Tensor_out, overload_name, "int_Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Tensor_out, schema_str, "index_fill.int_Tensor_out(Tensor self, int dim, Tensor index, Tensor value, *, Tensor(a!) out) -> Tensor(a!)")

// aten::index_fill.int_Tensor_out(Tensor self, int dim, Tensor index, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_fill_int_Tensor_out::schema> create_index_fill_int_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill_int_Tensor_out::name, index_fill_int_Tensor_out::overload_name)
      .typed<index_fill_int_Tensor_out::schema>();
}

// aten::index_fill.int_Tensor_out(Tensor self, int dim, Tensor index, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & index_fill_int_Tensor_out::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value, at::Tensor & out) {
    
    static auto op = create_index_fill_int_Tensor_out_typed_handle();
    return op.call(self, dim, index, value, out);
}

// aten::index_fill.int_Tensor_out(Tensor self, int dim, Tensor index, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & index_fill_int_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value, at::Tensor & out) {
    
    static auto op = create_index_fill_int_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_from_out, name, "aten::random")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_from_out, overload_name, "from_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_from_out, schema_str, "random.from_out(Tensor self, int from, int? to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::random.from_out(Tensor self, int from, int? to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<random_from_out::schema> create_random_from_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(random_from_out::name, random_from_out::overload_name)
      .typed<random_from_out::schema>();
}

// aten::random.from_out(Tensor self, int from, int? to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & random_from_out::call(const at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_random_from_out_typed_handle();
    return op.call(self, from, to, generator, out);
}

// aten::random.from_out(Tensor self, int from, int? to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & random_from_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_random_from_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, from, to, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_from, name, "aten::random")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_from, overload_name, "from")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_from, schema_str, "random.from(Tensor self, int from, int? to, *, Generator? generator=None) -> Tensor")

// aten::random.from(Tensor self, int from, int? to, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<random_from::schema> create_random_from_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(random_from::name, random_from::overload_name)
      .typed<random_from::schema>();
}

// aten::random.from(Tensor self, int from, int? to, *, Generator? generator=None) -> Tensor
at::Tensor random_from::call(const at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator) {
    
    static auto op = create_random_from_typed_handle();
    return op.call(self, from, to, generator);
}

// aten::random.from(Tensor self, int from, int? to, *, Generator? generator=None) -> Tensor
at::Tensor random_from::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator) {
    
    static auto op = create_random_from_typed_handle();
    return op.redispatch(dispatchKeySet, self, from, to, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_to_out, name, "aten::random")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_to_out, overload_name, "to_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_to_out, schema_str, "random.to_out(Tensor self, int to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::random.to_out(Tensor self, int to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<random_to_out::schema> create_random_to_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(random_to_out::name, random_to_out::overload_name)
      .typed<random_to_out::schema>();
}

// aten::random.to_out(Tensor self, int to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & random_to_out::call(const at::Tensor & self, int64_t to, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_random_to_out_typed_handle();
    return op.call(self, to, generator, out);
}

// aten::random.to_out(Tensor self, int to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & random_to_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t to, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_random_to_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, to, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_to, name, "aten::random")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_to, overload_name, "to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_to, schema_str, "random.to(Tensor self, int to, *, Generator? generator=None) -> Tensor")

// aten::random.to(Tensor self, int to, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<random_to::schema> create_random_to_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(random_to::name, random_to::overload_name)
      .typed<random_to::schema>();
}

// aten::random.to(Tensor self, int to, *, Generator? generator=None) -> Tensor
at::Tensor random_to::call(const at::Tensor & self, int64_t to, c10::optional<at::Generator> generator) {
    
    static auto op = create_random_to_typed_handle();
    return op.call(self, to, generator);
}

// aten::random.to(Tensor self, int to, *, Generator? generator=None) -> Tensor
at::Tensor random_to::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t to, c10::optional<at::Generator> generator) {
    
    static auto op = create_random_to_typed_handle();
    return op.redispatch(dispatchKeySet, self, to, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_out, name, "aten::random")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_out, schema_str, "random.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::random.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<random_out::schema> create_random_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(random_out::name, random_out::overload_name)
      .typed<random_out::schema>();
}

// aten::random.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & random_out::call(const at::Tensor & self, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_random_out_typed_handle();
    return op.call(self, generator, out);
}

// aten::random.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & random_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_random_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random, name, "aten::random")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random, schema_str, "random(Tensor self, *, Generator? generator=None) -> Tensor")

// aten::random(Tensor self, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<random::schema> create_random_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(random::name, random::overload_name)
      .typed<random::schema>();
}

// aten::random(Tensor self, *, Generator? generator=None) -> Tensor
at::Tensor random::call(const at::Tensor & self, c10::optional<at::Generator> generator) {
    
    static auto op = create_random_typed_handle();
    return op.call(self, generator);
}

// aten::random(Tensor self, *, Generator? generator=None) -> Tensor
at::Tensor random::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator) {
    
    static auto op = create_random_typed_handle();
    return op.redispatch(dispatchKeySet, self, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cauchy_out, name, "aten::cauchy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cauchy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cauchy_out, schema_str, "cauchy.out(Tensor self, float median=0, float sigma=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::cauchy.out(Tensor self, float median=0, float sigma=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cauchy_out::schema> create_cauchy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cauchy_out::name, cauchy_out::overload_name)
      .typed<cauchy_out::schema>();
}

// aten::cauchy.out(Tensor self, float median=0, float sigma=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cauchy_out::call(const at::Tensor & self, double median, double sigma, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_cauchy_out_typed_handle();
    return op.call(self, median, sigma, generator, out);
}

// aten::cauchy.out(Tensor self, float median=0, float sigma=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cauchy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double median, double sigma, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_cauchy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, median, sigma, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cauchy, name, "aten::cauchy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cauchy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cauchy, schema_str, "cauchy(Tensor self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor")

// aten::cauchy(Tensor self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cauchy::schema> create_cauchy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cauchy::name, cauchy::overload_name)
      .typed<cauchy::schema>();
}

// aten::cauchy(Tensor self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor
at::Tensor cauchy::call(const at::Tensor & self, double median, double sigma, c10::optional<at::Generator> generator) {
    
    static auto op = create_cauchy_typed_handle();
    return op.call(self, median, sigma, generator);
}

// aten::cauchy(Tensor self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor
at::Tensor cauchy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double median, double sigma, c10::optional<at::Generator> generator) {
    
    static auto op = create_cauchy_typed_handle();
    return op.redispatch(dispatchKeySet, self, median, sigma, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_normal_out, name, "aten::log_normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_normal_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_normal_out, schema_str, "log_normal.out(Tensor self, float mean=1, float std=2, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::log_normal.out(Tensor self, float mean=1, float std=2, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log_normal_out::schema> create_log_normal_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_normal_out::name, log_normal_out::overload_name)
      .typed<log_normal_out::schema>();
}

// aten::log_normal.out(Tensor self, float mean=1, float std=2, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & log_normal_out::call(const at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_log_normal_out_typed_handle();
    return op.call(self, mean, std, generator, out);
}

// aten::log_normal.out(Tensor self, float mean=1, float std=2, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & log_normal_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_log_normal_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mean, std, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_normal, name, "aten::log_normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_normal, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_normal, schema_str, "log_normal(Tensor self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor")

// aten::log_normal(Tensor self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<log_normal::schema> create_log_normal_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_normal::name, log_normal::overload_name)
      .typed<log_normal::schema>();
}

// aten::log_normal(Tensor self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor
at::Tensor log_normal::call(const at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
    
    static auto op = create_log_normal_typed_handle();
    return op.call(self, mean, std, generator);
}

// aten::log_normal(Tensor self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor
at::Tensor log_normal::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
    
    static auto op = create_log_normal_typed_handle();
    return op.redispatch(dispatchKeySet, self, mean, std, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_histogramdd_bin_edges_out, name, "aten::_histogramdd_bin_edges")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_histogramdd_bin_edges_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_histogramdd_bin_edges_out, schema_str, "_histogramdd_bin_edges.out(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!)[] out) -> ()")

// aten::_histogramdd_bin_edges.out(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_histogramdd_bin_edges_out::schema> create__histogramdd_bin_edges_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_histogramdd_bin_edges_out::name, _histogramdd_bin_edges_out::overload_name)
      .typed<_histogramdd_bin_edges_out::schema>();
}

// aten::_histogramdd_bin_edges.out(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!)[] out) -> ()
void _histogramdd_bin_edges_out::call(const at::Tensor & self, at::IntArrayRef bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density, at::TensorList out) {
    
    static auto op = create__histogramdd_bin_edges_out_typed_handle();
    return op.call(self, bins, range, weight, density, out);
}

// aten::_histogramdd_bin_edges.out(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!)[] out) -> ()
void _histogramdd_bin_edges_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density, at::TensorList out) {
    
    static auto op = create__histogramdd_bin_edges_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, bins, range, weight, density, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_histogramdd_from_bin_tensors_out, name, "aten::_histogramdd_from_bin_tensors")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_histogramdd_from_bin_tensors_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_histogramdd_from_bin_tensors_out, schema_str, "_histogramdd_from_bin_tensors.out(Tensor self, Tensor[] bins, *, Tensor? weight=None, bool density=False, Tensor(a!) out) -> Tensor(a!)")

// aten::_histogramdd_from_bin_tensors.out(Tensor self, Tensor[] bins, *, Tensor? weight=None, bool density=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_histogramdd_from_bin_tensors_out::schema> create__histogramdd_from_bin_tensors_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_histogramdd_from_bin_tensors_out::name, _histogramdd_from_bin_tensors_out::overload_name)
      .typed<_histogramdd_from_bin_tensors_out::schema>();
}

// aten::_histogramdd_from_bin_tensors.out(Tensor self, Tensor[] bins, *, Tensor? weight=None, bool density=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _histogramdd_from_bin_tensors_out::call(const at::Tensor & self, at::TensorList bins, const c10::optional<at::Tensor> & weight, bool density, at::Tensor & out) {
    
    static auto op = create__histogramdd_from_bin_tensors_out_typed_handle();
    return op.call(self, bins, weight, density, out);
}

// aten::_histogramdd_from_bin_tensors.out(Tensor self, Tensor[] bins, *, Tensor? weight=None, bool density=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _histogramdd_from_bin_tensors_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::TensorList bins, const c10::optional<at::Tensor> & weight, bool density, at::Tensor & out) {
    
    static auto op = create__histogramdd_from_bin_tensors_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, bins, weight, density, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort_stable_out, name, "aten::argsort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort_stable_out, overload_name, "stable_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort_stable_out, schema_str, "argsort.stable_out(Tensor self, *, bool stable, int dim=-1, bool descending=False, Tensor(a!) out) -> Tensor(a!)")

// aten::argsort.stable_out(Tensor self, *, bool stable, int dim=-1, bool descending=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<argsort_stable_out::schema> create_argsort_stable_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(argsort_stable_out::name, argsort_stable_out::overload_name)
      .typed<argsort_stable_out::schema>();
}

// aten::argsort.stable_out(Tensor self, *, bool stable, int dim=-1, bool descending=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & argsort_stable_out::call(const at::Tensor & self, bool stable, int64_t dim, bool descending, at::Tensor & out) {
    
    static auto op = create_argsort_stable_out_typed_handle();
    return op.call(self, stable, dim, descending, out);
}

// aten::argsort.stable_out(Tensor self, *, bool stable, int dim=-1, bool descending=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & argsort_stable_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool stable, int64_t dim, bool descending, at::Tensor & out) {
    
    static auto op = create_argsort_stable_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, stable, dim, descending, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unfold_backward_out, name, "aten::unfold_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unfold_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unfold_backward_out, schema_str, "unfold_backward.out(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step, *, Tensor(a!) out) -> Tensor(a!)")

// aten::unfold_backward.out(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<unfold_backward_out::schema> create_unfold_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unfold_backward_out::name, unfold_backward_out::overload_name)
      .typed<unfold_backward_out::schema>();
}

// aten::unfold_backward.out(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & unfold_backward_out::call(const at::Tensor & grad_in, c10::SymIntArrayRef input_sizes, int64_t dim, int64_t size, int64_t step, at::Tensor & out) {
    
    static auto op = create_unfold_backward_out_typed_handle();
    return op.call(grad_in, input_sizes, dim, size, step, out);
}

// aten::unfold_backward.out(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & unfold_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_in, c10::SymIntArrayRef input_sizes, int64_t dim, int64_t size, int64_t step, at::Tensor & out) {
    
    static auto op = create_unfold_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_in, input_sizes, dim, size, step, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_out, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_out, schema_str, "normal.out(Tensor self, float mean=0, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::normal.out(Tensor self, float mean=0, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<normal_out::schema> create_normal_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_out::name, normal_out::overload_name)
      .typed<normal_out::schema>();
}

// aten::normal.out(Tensor self, float mean=0, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_out::call(const at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_normal_out_typed_handle();
    return op.call(self, mean, std, generator, out);
}

// aten::normal.out(Tensor self, float mean=0, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator, at::Tensor & out) {
    
    static auto op = create_normal_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mean, std, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_Scalar_out, name, "aten::_foreach_sub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_Scalar_out, schema_str, "_foreach_sub.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_sub.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub_Scalar_out::schema> create__foreach_sub_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub_Scalar_out::name, _foreach_sub_Scalar_out::overload_name)
      .typed<_foreach_sub_Scalar_out::schema>();
}

// aten::_foreach_sub.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
void _foreach_sub_Scalar_out::call(at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
    
    static auto op = create__foreach_sub_Scalar_out_typed_handle();
    return op.call(self, scalar, out);
}

// aten::_foreach_sub.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
void _foreach_sub_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
    
    static auto op = create__foreach_sub_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_List_out, name, "aten::_foreach_sub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_List_out, overload_name, "List_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_List_out, schema_str, "_foreach_sub.List_out(Tensor[] self, Tensor[] other, *, Scalar alpha=1, Tensor(a!)[] out) -> ()")

// aten::_foreach_sub.List_out(Tensor[] self, Tensor[] other, *, Scalar alpha=1, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub_List_out::schema> create__foreach_sub_List_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub_List_out::name, _foreach_sub_List_out::overload_name)
      .typed<_foreach_sub_List_out::schema>();
}

// aten::_foreach_sub.List_out(Tensor[] self, Tensor[] other, *, Scalar alpha=1, Tensor(a!)[] out) -> ()
void _foreach_sub_List_out::call(at::TensorList self, at::TensorList other, const at::Scalar & alpha, at::TensorList out) {
    
    static auto op = create__foreach_sub_List_out_typed_handle();
    return op.call(self, other, alpha, out);
}

// aten::_foreach_sub.List_out(Tensor[] self, Tensor[] other, *, Scalar alpha=1, Tensor(a!)[] out) -> ()
void _foreach_sub_List_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, const at::Scalar & alpha, at::TensorList out) {
    
    static auto op = create__foreach_sub_List_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_ScalarList_out, name, "aten::_foreach_sub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_ScalarList_out, overload_name, "ScalarList_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_ScalarList_out, schema_str, "_foreach_sub.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_sub.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub_ScalarList_out::schema> create__foreach_sub_ScalarList_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub_ScalarList_out::name, _foreach_sub_ScalarList_out::overload_name)
      .typed<_foreach_sub_ScalarList_out::schema>();
}

// aten::_foreach_sub.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
void _foreach_sub_ScalarList_out::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
    
    static auto op = create__foreach_sub_ScalarList_out_typed_handle();
    return op.call(self, scalars, out);
}

// aten::_foreach_sub.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
void _foreach_sub_ScalarList_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
    
    static auto op = create__foreach_sub_ScalarList_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_Scalar_out, name, "aten::_foreach_maximum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_Scalar_out, schema_str, "_foreach_maximum.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_maximum.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_maximum_Scalar_out::schema> create__foreach_maximum_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_maximum_Scalar_out::name, _foreach_maximum_Scalar_out::overload_name)
      .typed<_foreach_maximum_Scalar_out::schema>();
}

// aten::_foreach_maximum.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
void _foreach_maximum_Scalar_out::call(at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
    
    static auto op = create__foreach_maximum_Scalar_out_typed_handle();
    return op.call(self, scalar, out);
}

// aten::_foreach_maximum.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
void _foreach_maximum_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
    
    static auto op = create__foreach_maximum_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_List_out, name, "aten::_foreach_maximum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_List_out, overload_name, "List_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_List_out, schema_str, "_foreach_maximum.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_maximum.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_maximum_List_out::schema> create__foreach_maximum_List_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_maximum_List_out::name, _foreach_maximum_List_out::overload_name)
      .typed<_foreach_maximum_List_out::schema>();
}

// aten::_foreach_maximum.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
void _foreach_maximum_List_out::call(at::TensorList self, at::TensorList other, at::TensorList out) {
    
    static auto op = create__foreach_maximum_List_out_typed_handle();
    return op.call(self, other, out);
}

// aten::_foreach_maximum.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
void _foreach_maximum_List_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, at::TensorList out) {
    
    static auto op = create__foreach_maximum_List_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_ScalarList_out, name, "aten::_foreach_maximum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_ScalarList_out, overload_name, "ScalarList_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_ScalarList_out, schema_str, "_foreach_maximum.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_maximum.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_maximum_ScalarList_out::schema> create__foreach_maximum_ScalarList_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_maximum_ScalarList_out::name, _foreach_maximum_ScalarList_out::overload_name)
      .typed<_foreach_maximum_ScalarList_out::schema>();
}

// aten::_foreach_maximum.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
void _foreach_maximum_ScalarList_out::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
    
    static auto op = create__foreach_maximum_ScalarList_out_typed_handle();
    return op.call(self, scalars, out);
}

// aten::_foreach_maximum.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
void _foreach_maximum_ScalarList_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
    
    static auto op = create__foreach_maximum_ScalarList_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos_out, name, "aten::_foreach_acos")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos_out, schema_str, "_foreach_acos.out(Tensor[] self, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_acos.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_acos_out::schema> create__foreach_acos_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_acos_out::name, _foreach_acos_out::overload_name)
      .typed<_foreach_acos_out::schema>();
}

// aten::_foreach_acos.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_acos_out::call(at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_acos_out_typed_handle();
    return op.call(self, out);
}

// aten::_foreach_acos.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_acos_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_acos_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan_out, name, "aten::_foreach_atan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan_out, schema_str, "_foreach_atan.out(Tensor[] self, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_atan.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_atan_out::schema> create__foreach_atan_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_atan_out::name, _foreach_atan_out::overload_name)
      .typed<_foreach_atan_out::schema>();
}

// aten::_foreach_atan.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_atan_out::call(at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_atan_out_typed_handle();
    return op.call(self, out);
}

// aten::_foreach_atan.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_atan_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_atan_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil_out, name, "aten::_foreach_ceil")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil_out, schema_str, "_foreach_ceil.out(Tensor[] self, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_ceil.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_ceil_out::schema> create__foreach_ceil_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_ceil_out::name, _foreach_ceil_out::overload_name)
      .typed<_foreach_ceil_out::schema>();
}

// aten::_foreach_ceil.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_ceil_out::call(at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_ceil_out_typed_handle();
    return op.call(self, out);
}

// aten::_foreach_ceil.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_ceil_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_ceil_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf_out, name, "aten::_foreach_erf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf_out, schema_str, "_foreach_erf.out(Tensor[] self, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_erf.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_erf_out::schema> create__foreach_erf_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_erf_out::name, _foreach_erf_out::overload_name)
      .typed<_foreach_erf_out::schema>();
}

// aten::_foreach_erf.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_erf_out::call(at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_erf_out_typed_handle();
    return op.call(self, out);
}

// aten::_foreach_erf.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_erf_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_erf_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2_out, name, "aten::_foreach_log2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2_out, schema_str, "_foreach_log2.out(Tensor[] self, *, Tensor(a!)[] out) -> ()")

// aten::_foreach_log2.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_log2_out::schema> create__foreach_log2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_log2_out::name, _foreach_log2_out::overload_name)
      .typed<_foreach_log2_out::schema>();
}

// aten::_foreach_log2.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_log2_out::call(at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_log2_out_typed_handle();
    return op.call(self, out);
}

// aten::_foreach_log2.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
void _foreach_log2_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
    
    static auto op = create__foreach_log2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Scalar_out, name, "aten::bucketize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Scalar_out, schema_str, "bucketize.Scalar_out(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)")

// aten::bucketize.Scalar_out(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bucketize_Scalar_out::schema> create_bucketize_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bucketize_Scalar_out::name, bucketize_Scalar_out::overload_name)
      .typed<bucketize_Scalar_out::schema>();
}

// aten::bucketize.Scalar_out(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bucketize_Scalar_out::call(const at::Scalar & self, const at::Tensor & boundaries, bool out_int32, bool right, at::Tensor & out) {
    
    static auto op = create_bucketize_Scalar_out_typed_handle();
    return op.call(self, boundaries, out_int32, right, out);
}

// aten::bucketize.Scalar_out(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bucketize_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & boundaries, bool out_int32, bool right, at::Tensor & out) {
    
    static auto op = create_bucketize_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, boundaries, out_int32, right, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_backward_jvp_out, name, "aten::glu_backward_jvp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_backward_jvp_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_backward_jvp_out, schema_str, "glu_backward_jvp.out(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim, *, Tensor(a!) out) -> Tensor(a!)")

// aten::glu_backward_jvp.out(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<glu_backward_jvp_out::schema> create_glu_backward_jvp_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(glu_backward_jvp_out::name, glu_backward_jvp_out::overload_name)
      .typed<glu_backward_jvp_out::schema>();
}

// aten::glu_backward_jvp.out(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & glu_backward_jvp_out::call(const at::Tensor & grad_x, const at::Tensor & grad_glu, const at::Tensor & x, const at::Tensor & dgrad_glu, const at::Tensor & dx, int64_t dim, at::Tensor & out) {
    
    static auto op = create_glu_backward_jvp_out_typed_handle();
    return op.call(grad_x, grad_glu, x, dgrad_glu, dx, dim, out);
}

// aten::glu_backward_jvp.out(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & glu_backward_jvp_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_x, const at::Tensor & grad_glu, const at::Tensor & x, const at::Tensor & dgrad_glu, const at::Tensor & dx, int64_t dim, at::Tensor & out) {
    
    static auto op = create_glu_backward_jvp_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_x, grad_glu, x, dgrad_glu, dx, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_backward_out, name, "aten::hardswish_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_backward_out, schema_str, "hardswish_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hardswish_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardswish_backward_out::schema> create_hardswish_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardswish_backward_out::name, hardswish_backward_out::overload_name)
      .typed<hardswish_backward_out::schema>();
}

// aten::hardswish_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardswish_backward_out::call(const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_hardswish_backward_out_typed_handle();
    return op.call(grad_output, self, out);
}

// aten::hardswish_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardswish_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_hardswish_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d_backward_out, name, "aten::_adaptive_avg_pool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d_backward_out, schema_str, "_adaptive_avg_pool3d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_adaptive_avg_pool3d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_adaptive_avg_pool3d_backward_out::schema> create__adaptive_avg_pool3d_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_adaptive_avg_pool3d_backward_out::name, _adaptive_avg_pool3d_backward_out::overload_name)
      .typed<_adaptive_avg_pool3d_backward_out::schema>();
}

// aten::_adaptive_avg_pool3d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _adaptive_avg_pool3d_backward_out::call(const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create__adaptive_avg_pool3d_backward_out_typed_handle();
    return op.call(grad_output, self, out);
}

// aten::_adaptive_avg_pool3d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _adaptive_avg_pool3d_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create__adaptive_avg_pool3d_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conj_copy_out, name, "aten::_conj_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conj_copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conj_copy_out, schema_str, "_conj_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_conj_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_conj_copy_out::schema> create__conj_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_conj_copy_out::name, _conj_copy_out::overload_name)
      .typed<_conj_copy_out::schema>();
}

// aten::_conj_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _conj_copy_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create__conj_copy_out_typed_handle();
    return op.call(self, out);
}

// aten::_conj_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _conj_copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create__conj_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach_copy_out, name, "aten::detach_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach_copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach_copy_out, schema_str, "detach_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::detach_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<detach_copy_out::schema> create_detach_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(detach_copy_out::name, detach_copy_out::overload_name)
      .typed<detach_copy_out::schema>();
}

// aten::detach_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & detach_copy_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_detach_copy_out_typed_handle();
    return op.call(self, out);
}

// aten::detach_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & detach_copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_detach_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(row_indices_copy_out, name, "aten::row_indices_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(row_indices_copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(row_indices_copy_out, schema_str, "row_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::row_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<row_indices_copy_out::schema> create_row_indices_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(row_indices_copy_out::name, row_indices_copy_out::overload_name)
      .typed<row_indices_copy_out::schema>();
}

// aten::row_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & row_indices_copy_out::call(const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_row_indices_copy_out_typed_handle();
    return op.call(self, out);
}

// aten::row_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & row_indices_copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    
    static auto op = create_row_indices_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_transformer_encoder_layer_fwd_out, name, "aten::_transformer_encoder_layer_fwd")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_transformer_encoder_layer_fwd_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_transformer_encoder_layer_fwd_out, schema_str, "_transformer_encoder_layer_fwd.out(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_transformer_encoder_layer_fwd.out(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_transformer_encoder_layer_fwd_out::schema> create__transformer_encoder_layer_fwd_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_transformer_encoder_layer_fwd_out::name, _transformer_encoder_layer_fwd_out::overload_name)
      .typed<_transformer_encoder_layer_fwd_out::schema>();
}

// aten::_transformer_encoder_layer_fwd.out(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _transformer_encoder_layer_fwd_out::call(const at::Tensor & src, int64_t embed_dim, int64_t num_heads, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, bool use_gelu, bool norm_first, double eps, const at::Tensor & norm_weight_1, const at::Tensor & norm_bias_1, const at::Tensor & norm_weight_2, const at::Tensor & norm_bias_2, const at::Tensor & ffn_weight_1, const at::Tensor & ffn_bias_1, const at::Tensor & ffn_weight_2, const at::Tensor & ffn_bias_2, const c10::optional<at::Tensor> & mask, c10::optional<int64_t> mask_type, at::Tensor & out) {
    
    static auto op = create__transformer_encoder_layer_fwd_out_typed_handle();
    return op.call(src, embed_dim, num_heads, qkv_weight, qkv_bias, proj_weight, proj_bias, use_gelu, norm_first, eps, norm_weight_1, norm_bias_1, norm_weight_2, norm_bias_2, ffn_weight_1, ffn_bias_1, ffn_weight_2, ffn_bias_2, mask, mask_type, out);
}

// aten::_transformer_encoder_layer_fwd.out(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _transformer_encoder_layer_fwd_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & src, int64_t embed_dim, int64_t num_heads, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, bool use_gelu, bool norm_first, double eps, const at::Tensor & norm_weight_1, const at::Tensor & norm_bias_1, const at::Tensor & norm_weight_2, const at::Tensor & norm_bias_2, const at::Tensor & ffn_weight_1, const at::Tensor & ffn_bias_1, const at::Tensor & ffn_weight_2, const at::Tensor & ffn_bias_2, const c10::optional<at::Tensor> & mask, c10::optional<int64_t> mask_type, at::Tensor & out) {
    
    static auto op = create__transformer_encoder_layer_fwd_out_typed_handle();
    return op.redispatch(dispatchKeySet, src, embed_dim, num_heads, qkv_weight, qkv_bias, proj_weight, proj_bias, use_gelu, norm_first, eps, norm_weight_1, norm_bias_1, norm_weight_2, norm_bias_2, ffn_weight_1, ffn_bias_1, ffn_weight_2, ffn_bias_2, mask, mask_type, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_native_multi_head_attention_out, name, "aten::_native_multi_head_attention")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_native_multi_head_attention_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_native_multi_head_attention_out, schema_str, "_native_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))")

// aten::_native_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<_native_multi_head_attention_out::schema> create__native_multi_head_attention_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_native_multi_head_attention_out::name, _native_multi_head_attention_out::overload_name)
      .typed<_native_multi_head_attention_out::schema>();
}

// aten::_native_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> _native_multi_head_attention_out::call(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, int64_t embed_dim, int64_t num_head, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, const c10::optional<at::Tensor> & mask, bool need_weights, bool average_attn_weights, c10::optional<int64_t> mask_type, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create__native_multi_head_attention_out_typed_handle();
    return op.call(query, key, value, embed_dim, num_head, qkv_weight, qkv_bias, proj_weight, proj_bias, mask, need_weights, average_attn_weights, mask_type, out0, out1);
}

// aten::_native_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> _native_multi_head_attention_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, int64_t embed_dim, int64_t num_head, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, const c10::optional<at::Tensor> & mask, bool need_weights, bool average_attn_weights, c10::optional<int64_t> mask_type, at::Tensor & out0, at::Tensor & out1) {
    
    static auto op = create__native_multi_head_attention_out_typed_handle();
    return op.redispatch(dispatchKeySet, query, key, value, embed_dim, num_head, qkv_weight, qkv_bias, proj_weight, proj_bias, mask, need_weights, average_attn_weights, mask_type, out0, out1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_triton_multi_head_attention_out, name, "aten::_triton_multi_head_attention")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_triton_multi_head_attention_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_triton_multi_head_attention_out, schema_str, "_triton_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_triton_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_triton_multi_head_attention_out::schema> create__triton_multi_head_attention_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_triton_multi_head_attention_out::name, _triton_multi_head_attention_out::overload_name)
      .typed<_triton_multi_head_attention_out::schema>();
}

// aten::_triton_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _triton_multi_head_attention_out::call(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, int64_t embed_dim, int64_t num_head, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, const c10::optional<at::Tensor> & mask, at::Tensor & out) {
    
    static auto op = create__triton_multi_head_attention_out_typed_handle();
    return op.call(query, key, value, embed_dim, num_head, qkv_weight, qkv_bias, proj_weight, proj_bias, mask, out);
}

// aten::_triton_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _triton_multi_head_attention_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, int64_t embed_dim, int64_t num_head, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, const c10::optional<at::Tensor> & mask, at::Tensor & out) {
    
    static auto op = create__triton_multi_head_attention_out_typed_handle();
    return op.redispatch(dispatchKeySet, query, key, value, embed_dim, num_head, qkv_weight, qkv_bias, proj_weight, proj_bias, mask, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw_out, name, "aten::_fused_adamw")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw_out, schema_str, "_fused_adamw.out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -> ()")

// aten::_fused_adamw.out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_fused_adamw_out::schema> create__fused_adamw_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fused_adamw_out::name, _fused_adamw_out::overload_name)
      .typed<_fused_adamw_out::schema>();
}

// aten::_fused_adamw.out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -> ()
void _fused_adamw_out::call(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf, at::TensorList out) {
    
    static auto op = create__fused_adamw_out_typed_handle();
    return op.call(self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf, out);
}

// aten::_fused_adamw.out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -> ()
void _fused_adamw_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf, at::TensorList out) {
    
    static auto op = create__fused_adamw_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw, name, "aten::_fused_adamw")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw, schema_str, "_fused_adamw(Tensor[] self, Tensor[] grads, Tensor[] exp_avgs, Tensor[] exp_avg_sqs, Tensor[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> (Tensor[] self_out, Tensor[] grads_out, Tensor[] exp_avgs_out, Tensor[] exp_avg_sqs_out, Tensor[] max_exp_avg_sqs_out)")

// aten::_fused_adamw(Tensor[] self, Tensor[] grads, Tensor[] exp_avgs, Tensor[] exp_avg_sqs, Tensor[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> (Tensor[] self_out, Tensor[] grads_out, Tensor[] exp_avgs_out, Tensor[] exp_avg_sqs_out, Tensor[] max_exp_avg_sqs_out)
static C10_NOINLINE c10::TypedOperatorHandle<_fused_adamw::schema> create__fused_adamw_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fused_adamw::name, _fused_adamw::overload_name)
      .typed<_fused_adamw::schema>();
}

// aten::_fused_adamw(Tensor[] self, Tensor[] grads, Tensor[] exp_avgs, Tensor[] exp_avg_sqs, Tensor[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> (Tensor[] self_out, Tensor[] grads_out, Tensor[] exp_avgs_out, Tensor[] exp_avg_sqs_out, Tensor[] max_exp_avg_sqs_out)
::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> _fused_adamw::call(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
    
    static auto op = create__fused_adamw_typed_handle();
    return op.call(self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf);
}

// aten::_fused_adamw(Tensor[] self, Tensor[] grads, Tensor[] exp_avgs, Tensor[] exp_avg_sqs, Tensor[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> (Tensor[] self_out, Tensor[] grads_out, Tensor[] exp_avgs_out, Tensor[] exp_avg_sqs_out, Tensor[] max_exp_avg_sqs_out)
::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> _fused_adamw::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
    
    static auto op = create__fused_adamw_typed_handle();
    return op.redispatch(dispatchKeySet, self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw_tensor_lr_out, name, "aten::_fused_adamw")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw_tensor_lr_out, overload_name, "tensor_lr_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw_tensor_lr_out, schema_str, "_fused_adamw.tensor_lr_out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -> ()")

// aten::_fused_adamw.tensor_lr_out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_fused_adamw_tensor_lr_out::schema> create__fused_adamw_tensor_lr_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fused_adamw_tensor_lr_out::name, _fused_adamw_tensor_lr_out::overload_name)
      .typed<_fused_adamw_tensor_lr_out::schema>();
}

// aten::_fused_adamw.tensor_lr_out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -> ()
void _fused_adamw_tensor_lr_out::call(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, const at::Tensor & lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf, at::TensorList out) {
    
    static auto op = create__fused_adamw_tensor_lr_out_typed_handle();
    return op.call(self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf, out);
}

// aten::_fused_adamw.tensor_lr_out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -> ()
void _fused_adamw_tensor_lr_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, const at::Tensor & lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf, at::TensorList out) {
    
    static auto op = create__fused_adamw_tensor_lr_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw_tensor_lr, name, "aten::_fused_adamw")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw_tensor_lr, overload_name, "tensor_lr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_adamw_tensor_lr, schema_str, "_fused_adamw.tensor_lr(Tensor[] self, Tensor[] grads, Tensor[] exp_avgs, Tensor[] exp_avg_sqs, Tensor[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> (Tensor[] self_out, Tensor[] grads_out, Tensor[] exp_avgs_out, Tensor[] exp_avg_sqs_out, Tensor[] max_exp_avg_sqs_out)")

// aten::_fused_adamw.tensor_lr(Tensor[] self, Tensor[] grads, Tensor[] exp_avgs, Tensor[] exp_avg_sqs, Tensor[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> (Tensor[] self_out, Tensor[] grads_out, Tensor[] exp_avgs_out, Tensor[] exp_avg_sqs_out, Tensor[] max_exp_avg_sqs_out)
static C10_NOINLINE c10::TypedOperatorHandle<_fused_adamw_tensor_lr::schema> create__fused_adamw_tensor_lr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fused_adamw_tensor_lr::name, _fused_adamw_tensor_lr::overload_name)
      .typed<_fused_adamw_tensor_lr::schema>();
}

// aten::_fused_adamw.tensor_lr(Tensor[] self, Tensor[] grads, Tensor[] exp_avgs, Tensor[] exp_avg_sqs, Tensor[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> (Tensor[] self_out, Tensor[] grads_out, Tensor[] exp_avgs_out, Tensor[] exp_avg_sqs_out, Tensor[] max_exp_avg_sqs_out)
::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> _fused_adamw_tensor_lr::call(at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, const at::Tensor & lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
    
    static auto op = create__fused_adamw_tensor_lr_typed_handle();
    return op.call(self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf);
}

// aten::_fused_adamw.tensor_lr(Tensor[] self, Tensor[] grads, Tensor[] exp_avgs, Tensor[] exp_avg_sqs, Tensor[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> (Tensor[] self_out, Tensor[] grads_out, Tensor[] exp_avgs_out, Tensor[] exp_avg_sqs_out, Tensor[] max_exp_avg_sqs_out)
::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> _fused_adamw_tensor_lr::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, const at::Tensor & lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
    
    static auto op = create__fused_adamw_tensor_lr_typed_handle();
    return op.redispatch(dispatchKeySet, self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale, found_inf);
}

}} // namespace at::_ops
