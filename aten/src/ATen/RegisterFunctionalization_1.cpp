#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
// @generated by torchgen/gen.py from RegisterFunctionalization.cpp

#include <ATen/core/LegacyTypeDispatch.h>
#include <ATen/EmptyTensor.h>
#include <ATen/FunctionalTensorWrapper.h>
#include <ATen/FunctionalInverses.h>
#include <ATen/MemoryOverlap.h>
#include <torch/library.h>

#ifndef AT_PER_OPERATOR_HEADERS
#include <ATen/Operators.h>
#include <ATen/NativeFunctions.h>
#else
// needed for the meta tensor calls to get stride info in functionalization
#include <ATen/ops/empty_strided_native.h>
// needed for special handling of copy_().
// See Note [functionalizating copy_() and not preserving strides]
#include <ATen/ops/to_ops.h>
#include <ATen/ops/expand_copy_ops.h>

#include <ATen/ops/_new_zeros_with_same_feature_meta_native.h>
#include <ATen/ops/_new_zeros_with_same_feature_meta_ops.h>
#include <ATen/ops/_new_zeros_with_same_feature_meta_native.h>
#include <ATen/ops/_new_zeros_with_same_feature_meta_ops.h>
#include <ATen/ops/_cudnn_init_dropout_state_native.h>
#include <ATen/ops/_cudnn_init_dropout_state_ops.h>
#include <ATen/ops/_cudnn_init_dropout_state_native.h>
#include <ATen/ops/_cudnn_init_dropout_state_ops.h>
#include <ATen/ops/angle_native.h>
#include <ATen/ops/angle_ops.h>
#include <ATen/ops/angle_native.h>
#include <ATen/ops/angle_ops.h>
#include <ATen/ops/sgn_native.h>
#include <ATen/ops/sgn_ops.h>
#include <ATen/ops/sgn_native.h>
#include <ATen/ops/sgn_ops.h>
#include <ATen/ops/sgn_native.h>
#include <ATen/ops/sgn_ops.h>
#include <ATen/ops/_add_relu_native.h>
#include <ATen/ops/_add_relu_ops.h>
#include <ATen/ops/_add_relu_native.h>
#include <ATen/ops/_add_relu_ops.h>
#include <ATen/ops/_add_relu_native.h>
#include <ATen/ops/_add_relu_ops.h>
#include <ATen/ops/_add_relu_native.h>
#include <ATen/ops/_add_relu_ops.h>
#include <ATen/ops/_add_relu_native.h>
#include <ATen/ops/_add_relu_ops.h>
#include <ATen/ops/_add_relu_native.h>
#include <ATen/ops/_add_relu_ops.h>
#include <ATen/ops/_test_functorch_fallback_native.h>
#include <ATen/ops/_test_functorch_fallback_ops.h>
#include <ATen/ops/_test_functorch_fallback_native.h>
#include <ATen/ops/_test_functorch_fallback_ops.h>
#include <ATen/ops/any_native.h>
#include <ATen/ops/any_ops.h>
#include <ATen/ops/any_native.h>
#include <ATen/ops/any_ops.h>
#include <ATen/ops/any_native.h>
#include <ATen/ops/any_ops.h>
#include <ATen/ops/any_native.h>
#include <ATen/ops/any_ops.h>
#include <ATen/ops/any_native.h>
#include <ATen/ops/any_ops.h>
#include <ATen/ops/any_native.h>
#include <ATen/ops/any_ops.h>
#include <ATen/ops/argmin_native.h>
#include <ATen/ops/argmin_ops.h>
#include <ATen/ops/argmin_native.h>
#include <ATen/ops/argmin_ops.h>
#include <ATen/ops/acosh_native.h>
#include <ATen/ops/acosh_ops.h>
#include <ATen/ops/acosh_native.h>
#include <ATen/ops/acosh_ops.h>
#include <ATen/ops/acosh_native.h>
#include <ATen/ops/acosh_ops.h>
#include <ATen/ops/arcsin_native.h>
#include <ATen/ops/arcsin_ops.h>
#include <ATen/ops/arcsin_native.h>
#include <ATen/ops/arcsin_ops.h>
#include <ATen/ops/arcsin_native.h>
#include <ATen/ops/arcsin_ops.h>
#include <ATen/ops/arctan_native.h>
#include <ATen/ops/arctan_ops.h>
#include <ATen/ops/arctan_native.h>
#include <ATen/ops/arctan_ops.h>
#include <ATen/ops/arctan_native.h>
#include <ATen/ops/arctan_ops.h>
#include <ATen/ops/bincount_native.h>
#include <ATen/ops/bincount_ops.h>
#include <ATen/ops/bincount_native.h>
#include <ATen/ops/bincount_ops.h>
#include <ATen/ops/copysign_native.h>
#include <ATen/ops/copysign_ops.h>
#include <ATen/ops/copysign_native.h>
#include <ATen/ops/copysign_ops.h>
#include <ATen/ops/copysign_native.h>
#include <ATen/ops/copysign_ops.h>
#include <ATen/ops/copysign_native.h>
#include <ATen/ops/copysign_ops.h>
#include <ATen/ops/copysign_native.h>
#include <ATen/ops/copysign_ops.h>
#include <ATen/ops/copysign_native.h>
#include <ATen/ops/copysign_ops.h>
#include <ATen/ops/logical_or_native.h>
#include <ATen/ops/logical_or_ops.h>
#include <ATen/ops/logical_or_native.h>
#include <ATen/ops/logical_or_ops.h>
#include <ATen/ops/logical_or_native.h>
#include <ATen/ops/logical_or_ops.h>
#include <ATen/ops/cat_native.h>
#include <ATen/ops/cat_ops.h>
#include <ATen/ops/cat_native.h>
#include <ATen/ops/cat_ops.h>
#include <ATen/ops/cat_native.h>
#include <ATen/ops/cat_ops.h>
#include <ATen/ops/cat_native.h>
#include <ATen/ops/cat_ops.h>
#include <ATen/ops/concat_native.h>
#include <ATen/ops/concat_ops.h>
#include <ATen/ops/concat_native.h>
#include <ATen/ops/concat_ops.h>
#include <ATen/ops/concat_native.h>
#include <ATen/ops/concat_ops.h>
#include <ATen/ops/concat_native.h>
#include <ATen/ops/concat_ops.h>
#include <ATen/ops/ceil_native.h>
#include <ATen/ops/ceil_ops.h>
#include <ATen/ops/ceil_native.h>
#include <ATen/ops/ceil_ops.h>
#include <ATen/ops/ceil_native.h>
#include <ATen/ops/ceil_ops.h>
#include <ATen/ops/polar_native.h>
#include <ATen/ops/polar_ops.h>
#include <ATen/ops/polar_native.h>
#include <ATen/ops/polar_ops.h>
#include <ATen/ops/convolution_native.h>
#include <ATen/ops/convolution_ops.h>
#include <ATen/ops/convolution_native.h>
#include <ATen/ops/convolution_ops.h>
#include <ATen/ops/convolution_overrideable_native.h>
#include <ATen/ops/convolution_overrideable_ops.h>
#include <ATen/ops/convolution_overrideable_native.h>
#include <ATen/ops/convolution_overrideable_ops.h>
#include <ATen/ops/convolution_backward_overrideable_native.h>
#include <ATen/ops/convolution_backward_overrideable_ops.h>
#include <ATen/ops/convolution_backward_overrideable_native.h>
#include <ATen/ops/convolution_backward_overrideable_ops.h>
#include <ATen/ops/cudnn_affine_grid_generator_native.h>
#include <ATen/ops/cudnn_affine_grid_generator_ops.h>
#include <ATen/ops/cudnn_affine_grid_generator_native.h>
#include <ATen/ops/cudnn_affine_grid_generator_ops.h>
#include <ATen/ops/cudnn_batch_norm_backward_native.h>
#include <ATen/ops/cudnn_batch_norm_backward_ops.h>
#include <ATen/ops/cudnn_batch_norm_backward_native.h>
#include <ATen/ops/cudnn_batch_norm_backward_ops.h>
#include <ATen/ops/cudnn_convolution_relu_native.h>
#include <ATen/ops/cudnn_convolution_relu_ops.h>
#include <ATen/ops/cudnn_convolution_relu_native.h>
#include <ATen/ops/cudnn_convolution_relu_ops.h>
#include <ATen/ops/cudnn_convolution_add_relu_native.h>
#include <ATen/ops/cudnn_convolution_add_relu_ops.h>
#include <ATen/ops/cudnn_convolution_add_relu_native.h>
#include <ATen/ops/cudnn_convolution_add_relu_ops.h>
#include <ATen/ops/cummax_native.h>
#include <ATen/ops/cummax_ops.h>
#include <ATen/ops/cummax_native.h>
#include <ATen/ops/cummax_ops.h>
#include <ATen/ops/cummax_native.h>
#include <ATen/ops/cummax_ops.h>
#include <ATen/ops/cummax_native.h>
#include <ATen/ops/cummax_ops.h>
#include <ATen/ops/diff_native.h>
#include <ATen/ops/diff_ops.h>
#include <ATen/ops/diff_native.h>
#include <ATen/ops/diff_ops.h>
#include <ATen/ops/embedding_renorm_native.h>
#include <ATen/ops/embedding_renorm_ops.h>
#include <ATen/ops/embedding_renorm_native.h>
#include <ATen/ops/embedding_renorm_ops.h>
#include <ATen/ops/embedding_renorm_native.h>
#include <ATen/ops/embedding_renorm_ops.h>
#include <ATen/ops/_embedding_bag_forward_only_native.h>
#include <ATen/ops/_embedding_bag_forward_only_ops.h>
#include <ATen/ops/_embedding_bag_forward_only_native.h>
#include <ATen/ops/_embedding_bag_forward_only_ops.h>
#include <ATen/ops/_embedding_bag_dense_backward_native.h>
#include <ATen/ops/_embedding_bag_dense_backward_ops.h>
#include <ATen/ops/_embedding_bag_dense_backward_native.h>
#include <ATen/ops/_embedding_bag_dense_backward_ops.h>
#include <ATen/ops/empty_native.h>
#include <ATen/ops/empty_ops.h>
#include <ATen/ops/empty_native.h>
#include <ATen/ops/empty_ops.h>
#include <ATen/ops/empty_native.h>
#include <ATen/ops/empty_ops.h>
#include <ATen/ops/empty_native.h>
#include <ATen/ops/empty_ops.h>
#include <ATen/ops/empty_like_native.h>
#include <ATen/ops/empty_like_ops.h>
#include <ATen/ops/empty_like_native.h>
#include <ATen/ops/empty_like_ops.h>
#include <ATen/ops/erf_native.h>
#include <ATen/ops/erf_ops.h>
#include <ATen/ops/erf_native.h>
#include <ATen/ops/erf_ops.h>
#include <ATen/ops/erf_native.h>
#include <ATen/ops/erf_ops.h>
#include <ATen/ops/erfc_native.h>
#include <ATen/ops/erfc_ops.h>
#include <ATen/ops/erfc_native.h>
#include <ATen/ops/erfc_ops.h>
#include <ATen/ops/erfc_native.h>
#include <ATen/ops/erfc_ops.h>
#include <ATen/ops/eye_native.h>
#include <ATen/ops/eye_ops.h>
#include <ATen/ops/eye_native.h>
#include <ATen/ops/eye_ops.h>
#include <ATen/ops/eye_native.h>
#include <ATen/ops/eye_ops.h>
#include <ATen/ops/eye_native.h>
#include <ATen/ops/eye_ops.h>
#include <ATen/ops/gcd_native.h>
#include <ATen/ops/gcd_ops.h>
#include <ATen/ops/gcd_native.h>
#include <ATen/ops/gcd_ops.h>
#include <ATen/ops/gcd_native.h>
#include <ATen/ops/gcd_ops.h>
#include <ATen/ops/grid_sampler_2d_backward_native.h>
#include <ATen/ops/grid_sampler_2d_backward_ops.h>
#include <ATen/ops/grid_sampler_2d_backward_native.h>
#include <ATen/ops/grid_sampler_2d_backward_ops.h>
#include <ATen/ops/native_group_norm_backward_native.h>
#include <ATen/ops/native_group_norm_backward_ops.h>
#include <ATen/ops/native_group_norm_backward_native.h>
#include <ATen/ops/native_group_norm_backward_ops.h>
#include <ATen/ops/_fft_r2c_native.h>
#include <ATen/ops/_fft_r2c_ops.h>
#include <ATen/ops/_fft_r2c_native.h>
#include <ATen/ops/_fft_r2c_ops.h>
#include <ATen/ops/index_native.h>
#include <ATen/ops/index_ops.h>
#include <ATen/ops/index_native.h>
#include <ATen/ops/index_ops.h>
#include <ATen/ops/index_put_native.h>
#include <ATen/ops/index_put_ops.h>
#include <ATen/ops/index_put_native.h>
#include <ATen/ops/index_put_ops.h>
#include <ATen/ops/index_put_native.h>
#include <ATen/ops/index_put_ops.h>
#include <ATen/ops/linear_backward_native.h>
#include <ATen/ops/linear_backward_ops.h>
#include <ATen/ops/linear_backward_native.h>
#include <ATen/ops/linear_backward_ops.h>
#include <ATen/ops/mkldnn_linear_backward_weights_native.h>
#include <ATen/ops/mkldnn_linear_backward_weights_ops.h>
#include <ATen/ops/mkldnn_linear_backward_weights_native.h>
#include <ATen/ops/mkldnn_linear_backward_weights_ops.h>
#include <ATen/ops/logaddexp_native.h>
#include <ATen/ops/logaddexp_ops.h>
#include <ATen/ops/logaddexp_native.h>
#include <ATen/ops/logaddexp_ops.h>
#include <ATen/ops/_logcumsumexp_native.h>
#include <ATen/ops/_logcumsumexp_ops.h>
#include <ATen/ops/_logcumsumexp_native.h>
#include <ATen/ops/_logcumsumexp_ops.h>
#include <ATen/ops/logcumsumexp_native.h>
#include <ATen/ops/logcumsumexp_ops.h>
#include <ATen/ops/logcumsumexp_native.h>
#include <ATen/ops/logcumsumexp_ops.h>
#include <ATen/ops/logcumsumexp_native.h>
#include <ATen/ops/logcumsumexp_ops.h>
#include <ATen/ops/logcumsumexp_native.h>
#include <ATen/ops/logcumsumexp_ops.h>
#include <ATen/ops/matmul_backward_native.h>
#include <ATen/ops/matmul_backward_ops.h>
#include <ATen/ops/matmul_backward_native.h>
#include <ATen/ops/matmul_backward_ops.h>
#include <ATen/ops/median_native.h>
#include <ATen/ops/median_ops.h>
#include <ATen/ops/median_native.h>
#include <ATen/ops/median_ops.h>
#include <ATen/ops/median_native.h>
#include <ATen/ops/median_ops.h>
#include <ATen/ops/median_native.h>
#include <ATen/ops/median_ops.h>
#include <ATen/ops/median_native.h>
#include <ATen/ops/median_ops.h>
#include <ATen/ops/median_native.h>
#include <ATen/ops/median_ops.h>
#include <ATen/ops/amin_native.h>
#include <ATen/ops/amin_ops.h>
#include <ATen/ops/amin_native.h>
#include <ATen/ops/amin_ops.h>
#include <ATen/ops/mkldnn_rnn_layer_backward_native.h>
#include <ATen/ops/mkldnn_rnn_layer_backward_ops.h>
#include <ATen/ops/mkldnn_rnn_layer_backward_native.h>
#include <ATen/ops/mkldnn_rnn_layer_backward_ops.h>
#include <ATen/ops/miopen_convolution_native.h>
#include <ATen/ops/miopen_convolution_ops.h>
#include <ATen/ops/miopen_convolution_native.h>
#include <ATen/ops/miopen_convolution_ops.h>
#include <ATen/ops/miopen_depthwise_convolution_native.h>
#include <ATen/ops/miopen_depthwise_convolution_ops.h>
#include <ATen/ops/miopen_depthwise_convolution_native.h>
#include <ATen/ops/miopen_depthwise_convolution_ops.h>
#include <ATen/ops/miopen_rnn_backward_native.h>
#include <ATen/ops/miopen_rnn_backward_ops.h>
#include <ATen/ops/miopen_rnn_backward_native.h>
#include <ATen/ops/miopen_rnn_backward_ops.h>
#include <ATen/ops/native_batch_norm_native.h>
#include <ATen/ops/native_batch_norm_ops.h>
#include <ATen/ops/native_batch_norm_native.h>
#include <ATen/ops/native_batch_norm_ops.h>
#include <ATen/ops/_native_batch_norm_legit_no_training_native.h>
#include <ATen/ops/_native_batch_norm_legit_no_training_ops.h>
#include <ATen/ops/_native_batch_norm_legit_no_training_native.h>
#include <ATen/ops/_native_batch_norm_legit_no_training_ops.h>
#include <ATen/ops/batch_norm_elemt_native.h>
#include <ATen/ops/batch_norm_elemt_ops.h>
#include <ATen/ops/batch_norm_elemt_native.h>
#include <ATen/ops/batch_norm_elemt_ops.h>
#include <ATen/ops/batch_norm_update_stats_native.h>
#include <ATen/ops/batch_norm_update_stats_ops.h>
#include <ATen/ops/batch_norm_update_stats_native.h>
#include <ATen/ops/batch_norm_update_stats_ops.h>
#include <ATen/ops/_nnpack_spatial_convolution_native.h>
#include <ATen/ops/_nnpack_spatial_convolution_ops.h>
#include <ATen/ops/_nnpack_spatial_convolution_native.h>
#include <ATen/ops/_nnpack_spatial_convolution_ops.h>
#include <ATen/ops/ones_like_native.h>
#include <ATen/ops/ones_like_ops.h>
#include <ATen/ops/ones_like_native.h>
#include <ATen/ops/ones_like_ops.h>
#include <ATen/ops/channel_shuffle_native.h>
#include <ATen/ops/channel_shuffle_ops.h>
#include <ATen/ops/channel_shuffle_native.h>
#include <ATen/ops/channel_shuffle_ops.h>
#include <ATen/ops/randint_native.h>
#include <ATen/ops/randint_ops.h>
#include <ATen/ops/randint_native.h>
#include <ATen/ops/randint_ops.h>
#include <ATen/ops/randint_native.h>
#include <ATen/ops/randint_ops.h>
#include <ATen/ops/randint_native.h>
#include <ATen/ops/randint_ops.h>
#include <ATen/ops/randint_native.h>
#include <ATen/ops/randint_ops.h>
#include <ATen/ops/randint_native.h>
#include <ATen/ops/randint_ops.h>
#include <ATen/ops/randint_native.h>
#include <ATen/ops/randint_ops.h>
#include <ATen/ops/randint_native.h>
#include <ATen/ops/randint_ops.h>
#include <ATen/ops/randperm_native.h>
#include <ATen/ops/randperm_ops.h>
#include <ATen/ops/randperm_native.h>
#include <ATen/ops/randperm_ops.h>
#include <ATen/ops/randperm_native.h>
#include <ATen/ops/randperm_ops.h>
#include <ATen/ops/randperm_native.h>
#include <ATen/ops/randperm_ops.h>
#include <ATen/ops/range_native.h>
#include <ATen/ops/range_ops.h>
#include <ATen/ops/range_native.h>
#include <ATen/ops/range_ops.h>
#include <ATen/ops/range_native.h>
#include <ATen/ops/range_ops.h>
#include <ATen/ops/range_native.h>
#include <ATen/ops/range_ops.h>
#include <ATen/ops/reciprocal_native.h>
#include <ATen/ops/reciprocal_ops.h>
#include <ATen/ops/reciprocal_native.h>
#include <ATen/ops/reciprocal_ops.h>
#include <ATen/ops/reciprocal_native.h>
#include <ATen/ops/reciprocal_ops.h>
#include <ATen/ops/gelu_backward_native.h>
#include <ATen/ops/gelu_backward_ops.h>
#include <ATen/ops/gelu_backward_native.h>
#include <ATen/ops/gelu_backward_ops.h>
#include <ATen/ops/hardshrink_native.h>
#include <ATen/ops/hardshrink_ops.h>
#include <ATen/ops/hardshrink_native.h>
#include <ATen/ops/hardshrink_ops.h>
#include <ATen/ops/hardshrink_backward_native.h>
#include <ATen/ops/hardshrink_backward_ops.h>
#include <ATen/ops/hardshrink_backward_native.h>
#include <ATen/ops/hardshrink_backward_ops.h>
#include <ATen/ops/silu_native.h>
#include <ATen/ops/silu_ops.h>
#include <ATen/ops/silu_native.h>
#include <ATen/ops/silu_ops.h>
#include <ATen/ops/silu_native.h>
#include <ATen/ops/silu_ops.h>
#include <ATen/ops/silu_backward_native.h>
#include <ATen/ops/silu_backward_ops.h>
#include <ATen/ops/silu_backward_native.h>
#include <ATen/ops/silu_backward_ops.h>
#include <ATen/ops/sin_native.h>
#include <ATen/ops/sin_ops.h>
#include <ATen/ops/sin_native.h>
#include <ATen/ops/sin_ops.h>
#include <ATen/ops/sin_native.h>
#include <ATen/ops/sin_ops.h>
#include <ATen/ops/_softmax_native.h>
#include <ATen/ops/_softmax_ops.h>
#include <ATen/ops/_softmax_native.h>
#include <ATen/ops/_softmax_ops.h>
#include <ATen/ops/sspaddmm_native.h>
#include <ATen/ops/sspaddmm_ops.h>
#include <ATen/ops/sspaddmm_native.h>
#include <ATen/ops/sspaddmm_ops.h>
#include <ATen/ops/_stack_native.h>
#include <ATen/ops/_stack_ops.h>
#include <ATen/ops/_stack_native.h>
#include <ATen/ops/_stack_ops.h>
#include <ATen/ops/hstack_native.h>
#include <ATen/ops/hstack_ops.h>
#include <ATen/ops/hstack_native.h>
#include <ATen/ops/hstack_ops.h>
#include <ATen/ops/dstack_native.h>
#include <ATen/ops/dstack_ops.h>
#include <ATen/ops/dstack_native.h>
#include <ATen/ops/dstack_ops.h>
#include <ATen/ops/sum_native.h>
#include <ATen/ops/sum_ops.h>
#include <ATen/ops/sum_native.h>
#include <ATen/ops/sum_ops.h>
#include <ATen/ops/sum_native.h>
#include <ATen/ops/sum_ops.h>
#include <ATen/ops/sum_native.h>
#include <ATen/ops/sum_ops.h>
#include <ATen/ops/sum_native.h>
#include <ATen/ops/sum_ops.h>
#include <ATen/ops/sum_native.h>
#include <ATen/ops/sum_ops.h>
#include <ATen/ops/std_native.h>
#include <ATen/ops/std_ops.h>
#include <ATen/ops/std_native.h>
#include <ATen/ops/std_ops.h>
#include <ATen/ops/std_native.h>
#include <ATen/ops/std_ops.h>
#include <ATen/ops/std_native.h>
#include <ATen/ops/std_ops.h>
#include <ATen/ops/std_native.h>
#include <ATen/ops/std_ops.h>
#include <ATen/ops/std_native.h>
#include <ATen/ops/std_ops.h>
#include <ATen/ops/std_native.h>
#include <ATen/ops/std_ops.h>
#include <ATen/ops/std_native.h>
#include <ATen/ops/std_ops.h>
#include <ATen/ops/tan_native.h>
#include <ATen/ops/tan_ops.h>
#include <ATen/ops/tan_native.h>
#include <ATen/ops/tan_ops.h>
#include <ATen/ops/tan_native.h>
#include <ATen/ops/tan_ops.h>
#include <ATen/ops/tensordot_native.h>
#include <ATen/ops/tensordot_ops.h>
#include <ATen/ops/tensordot_native.h>
#include <ATen/ops/tensordot_ops.h>
#include <ATen/ops/threshold_native.h>
#include <ATen/ops/threshold_ops.h>
#include <ATen/ops/threshold_native.h>
#include <ATen/ops/threshold_ops.h>
#include <ATen/ops/threshold_native.h>
#include <ATen/ops/threshold_ops.h>
#include <ATen/ops/_nested_tensor_strides_native.h>
#include <ATen/ops/_nested_tensor_strides_ops.h>
#include <ATen/ops/_nested_tensor_strides_native.h>
#include <ATen/ops/_nested_tensor_strides_ops.h>
#include <ATen/ops/fix_native.h>
#include <ATen/ops/fix_ops.h>
#include <ATen/ops/fix_native.h>
#include <ATen/ops/fix_ops.h>
#include <ATen/ops/fix_native.h>
#include <ATen/ops/fix_ops.h>
#include <ATen/ops/unique_consecutive_native.h>
#include <ATen/ops/unique_consecutive_ops.h>
#include <ATen/ops/unique_consecutive_native.h>
#include <ATen/ops/unique_consecutive_ops.h>
#include <ATen/ops/var_native.h>
#include <ATen/ops/var_ops.h>
#include <ATen/ops/var_native.h>
#include <ATen/ops/var_ops.h>
#include <ATen/ops/var_native.h>
#include <ATen/ops/var_ops.h>
#include <ATen/ops/var_native.h>
#include <ATen/ops/var_ops.h>
#include <ATen/ops/var_native.h>
#include <ATen/ops/var_ops.h>
#include <ATen/ops/var_native.h>
#include <ATen/ops/var_ops.h>
#include <ATen/ops/var_native.h>
#include <ATen/ops/var_ops.h>
#include <ATen/ops/var_native.h>
#include <ATen/ops/var_ops.h>
#include <ATen/ops/_standard_gamma_grad_native.h>
#include <ATen/ops/_standard_gamma_grad_ops.h>
#include <ATen/ops/_standard_gamma_grad_native.h>
#include <ATen/ops/_standard_gamma_grad_ops.h>
#include <ATen/ops/poisson_native.h>
#include <ATen/ops/poisson_ops.h>
#include <ATen/ops/poisson_native.h>
#include <ATen/ops/poisson_ops.h>
#include <ATen/ops/_sparse_csr_sum_native.h>
#include <ATen/ops/_sparse_csr_sum_ops.h>
#include <ATen/ops/_sparse_csr_sum_native.h>
#include <ATen/ops/_sparse_csr_sum_ops.h>
#include <ATen/ops/_sparse_softmax_backward_data_native.h>
#include <ATen/ops/_sparse_softmax_backward_data_ops.h>
#include <ATen/ops/_sparse_softmax_backward_data_native.h>
#include <ATen/ops/_sparse_softmax_backward_data_ops.h>
#include <ATen/ops/_sparse_log_softmax_native.h>
#include <ATen/ops/_sparse_log_softmax_ops.h>
#include <ATen/ops/_sparse_log_softmax_native.h>
#include <ATen/ops/_sparse_log_softmax_ops.h>
#include <ATen/ops/_sparse_log_softmax_backward_data_native.h>
#include <ATen/ops/_sparse_log_softmax_backward_data_ops.h>
#include <ATen/ops/_sparse_log_softmax_backward_data_native.h>
#include <ATen/ops/_sparse_log_softmax_backward_data_ops.h>
#include <ATen/ops/resize_as_sparse_native.h>
#include <ATen/ops/resize_as_sparse_ops.h>
#include <ATen/ops/resize_as_sparse_native.h>
#include <ATen/ops/resize_as_sparse_ops.h>
#include <ATen/ops/resize_as_sparse_native.h>
#include <ATen/ops/resize_as_sparse_ops.h>
#include <ATen/ops/sub_native.h>
#include <ATen/ops/sub_ops.h>
#include <ATen/ops/sub_native.h>
#include <ATen/ops/sub_ops.h>
#include <ATen/ops/sub_native.h>
#include <ATen/ops/sub_ops.h>
#include <ATen/ops/sub_native.h>
#include <ATen/ops/sub_ops.h>
#include <ATen/ops/sub_native.h>
#include <ATen/ops/sub_ops.h>
#include <ATen/ops/sub_native.h>
#include <ATen/ops/sub_ops.h>
#include <ATen/ops/_sparse_mask_projection_native.h>
#include <ATen/ops/_sparse_mask_projection_ops.h>
#include <ATen/ops/_sparse_mask_projection_native.h>
#include <ATen/ops/_sparse_mask_projection_ops.h>
#include <ATen/ops/copy_sparse_to_sparse_native.h>
#include <ATen/ops/copy_sparse_to_sparse_ops.h>
#include <ATen/ops/copy_sparse_to_sparse_native.h>
#include <ATen/ops/copy_sparse_to_sparse_ops.h>
#include <ATen/ops/copy_sparse_to_sparse_native.h>
#include <ATen/ops/copy_sparse_to_sparse_ops.h>
#include <ATen/ops/_to_sparse_native.h>
#include <ATen/ops/_to_sparse_ops.h>
#include <ATen/ops/_to_sparse_native.h>
#include <ATen/ops/_to_sparse_ops.h>
#include <ATen/ops/_to_sparse_native.h>
#include <ATen/ops/_to_sparse_ops.h>
#include <ATen/ops/_to_sparse_native.h>
#include <ATen/ops/_to_sparse_ops.h>
#include <ATen/ops/_to_sparse_bsr_native.h>
#include <ATen/ops/_to_sparse_bsr_ops.h>
#include <ATen/ops/_to_sparse_bsr_native.h>
#include <ATen/ops/_to_sparse_bsr_ops.h>
#include <ATen/ops/mkldnn_reorder_conv2d_weight_native.h>
#include <ATen/ops/mkldnn_reorder_conv2d_weight_ops.h>
#include <ATen/ops/mkldnn_reorder_conv2d_weight_native.h>
#include <ATen/ops/mkldnn_reorder_conv2d_weight_ops.h>
#include <ATen/ops/_lstm_mps_native.h>
#include <ATen/ops/_lstm_mps_ops.h>
#include <ATen/ops/_lstm_mps_native.h>
#include <ATen/ops/_lstm_mps_ops.h>
#include <ATen/ops/_thnn_fused_gru_cell_backward_native.h>
#include <ATen/ops/_thnn_fused_gru_cell_backward_ops.h>
#include <ATen/ops/_thnn_fused_gru_cell_backward_native.h>
#include <ATen/ops/_thnn_fused_gru_cell_backward_ops.h>
#include <ATen/ops/set_native.h>
#include <ATen/ops/set_ops.h>
#include <ATen/ops/set_native.h>
#include <ATen/ops/set_ops.h>
#include <ATen/ops/set_native.h>
#include <ATen/ops/set_ops.h>
#include <ATen/ops/set_native.h>
#include <ATen/ops/set_ops.h>
#include <ATen/ops/set_native.h>
#include <ATen/ops/set_ops.h>
#include <ATen/ops/set_native.h>
#include <ATen/ops/set_ops.h>
#include <ATen/ops/set_native.h>
#include <ATen/ops/set_ops.h>
#include <ATen/ops/set_native.h>
#include <ATen/ops/set_ops.h>
#include <ATen/ops/set_native.h>
#include <ATen/ops/set_ops.h>
#include <ATen/ops/set_native.h>
#include <ATen/ops/set_ops.h>
#include <ATen/ops/set_native.h>
#include <ATen/ops/set_ops.h>
#include <ATen/ops/set_native.h>
#include <ATen/ops/set_ops.h>
#include <ATen/ops/lift_native.h>
#include <ATen/ops/lift_ops.h>
#include <ATen/ops/lift_native.h>
#include <ATen/ops/lift_ops.h>
#include <ATen/ops/masked_scatter_native.h>
#include <ATen/ops/masked_scatter_ops.h>
#include <ATen/ops/masked_scatter_native.h>
#include <ATen/ops/masked_scatter_ops.h>
#include <ATen/ops/masked_scatter_native.h>
#include <ATen/ops/masked_scatter_ops.h>
#include <ATen/ops/index_reduce_native.h>
#include <ATen/ops/index_reduce_ops.h>
#include <ATen/ops/index_reduce_native.h>
#include <ATen/ops/index_reduce_ops.h>
#include <ATen/ops/index_reduce_native.h>
#include <ATen/ops/index_reduce_ops.h>
#include <ATen/ops/eq_native.h>
#include <ATen/ops/eq_ops.h>
#include <ATen/ops/eq_native.h>
#include <ATen/ops/eq_ops.h>
#include <ATen/ops/eq_native.h>
#include <ATen/ops/eq_ops.h>
#include <ATen/ops/eq_native.h>
#include <ATen/ops/eq_ops.h>
#include <ATen/ops/eq_native.h>
#include <ATen/ops/eq_ops.h>
#include <ATen/ops/eq_native.h>
#include <ATen/ops/eq_ops.h>
#include <ATen/ops/bitwise_and_native.h>
#include <ATen/ops/bitwise_and_ops.h>
#include <ATen/ops/bitwise_and_native.h>
#include <ATen/ops/bitwise_and_ops.h>
#include <ATen/ops/bitwise_and_native.h>
#include <ATen/ops/bitwise_and_ops.h>
#include <ATen/ops/bitwise_and_native.h>
#include <ATen/ops/bitwise_and_ops.h>
#include <ATen/ops/bitwise_and_native.h>
#include <ATen/ops/bitwise_and_ops.h>
#include <ATen/ops/bitwise_and_native.h>
#include <ATen/ops/bitwise_and_ops.h>
#include <ATen/ops/bitwise_and_native.h>
#include <ATen/ops/bitwise_and_ops.h>
#include <ATen/ops/bitwise_and_native.h>
#include <ATen/ops/bitwise_and_ops.h>
#include <ATen/ops/bitwise_or_native.h>
#include <ATen/ops/bitwise_or_ops.h>
#include <ATen/ops/bitwise_or_native.h>
#include <ATen/ops/bitwise_or_ops.h>
#include <ATen/ops/bitwise_or_native.h>
#include <ATen/ops/bitwise_or_ops.h>
#include <ATen/ops/bitwise_or_native.h>
#include <ATen/ops/bitwise_or_ops.h>
#include <ATen/ops/bitwise_or_native.h>
#include <ATen/ops/bitwise_or_ops.h>
#include <ATen/ops/bitwise_or_native.h>
#include <ATen/ops/bitwise_or_ops.h>
#include <ATen/ops/bitwise_or_native.h>
#include <ATen/ops/bitwise_or_ops.h>
#include <ATen/ops/bitwise_or_native.h>
#include <ATen/ops/bitwise_or_ops.h>
#include <ATen/ops/bitwise_left_shift_native.h>
#include <ATen/ops/bitwise_left_shift_ops.h>
#include <ATen/ops/bitwise_left_shift_native.h>
#include <ATen/ops/bitwise_left_shift_ops.h>
#include <ATen/ops/bitwise_left_shift_native.h>
#include <ATen/ops/bitwise_left_shift_ops.h>
#include <ATen/ops/bitwise_left_shift_native.h>
#include <ATen/ops/bitwise_left_shift_ops.h>
#include <ATen/ops/bitwise_left_shift_native.h>
#include <ATen/ops/bitwise_left_shift_ops.h>
#include <ATen/ops/bitwise_left_shift_native.h>
#include <ATen/ops/bitwise_left_shift_ops.h>
#include <ATen/ops/bitwise_left_shift_native.h>
#include <ATen/ops/bitwise_left_shift_ops.h>
#include <ATen/ops/bitwise_left_shift_native.h>
#include <ATen/ops/bitwise_left_shift_ops.h>
#include <ATen/ops/rshift_native.h>
#include <ATen/ops/rshift_ops.h>
#include <ATen/ops/rshift_native.h>
#include <ATen/ops/rshift_ops.h>
#include <ATen/ops/rshift_native.h>
#include <ATen/ops/rshift_ops.h>
#include <ATen/ops/rshift_native.h>
#include <ATen/ops/rshift_ops.h>
#include <ATen/ops/rshift_native.h>
#include <ATen/ops/rshift_ops.h>
#include <ATen/ops/rshift_native.h>
#include <ATen/ops/rshift_ops.h>
#include <ATen/ops/tril_native.h>
#include <ATen/ops/tril_ops.h>
#include <ATen/ops/tril_native.h>
#include <ATen/ops/tril_ops.h>
#include <ATen/ops/tril_native.h>
#include <ATen/ops/tril_ops.h>
#include <ATen/ops/triu_native.h>
#include <ATen/ops/triu_ops.h>
#include <ATen/ops/triu_native.h>
#include <ATen/ops/triu_ops.h>
#include <ATen/ops/triu_native.h>
#include <ATen/ops/triu_ops.h>
#include <ATen/ops/digamma_native.h>
#include <ATen/ops/digamma_ops.h>
#include <ATen/ops/digamma_native.h>
#include <ATen/ops/digamma_ops.h>
#include <ATen/ops/digamma_native.h>
#include <ATen/ops/digamma_ops.h>
#include <ATen/ops/lerp_native.h>
#include <ATen/ops/lerp_ops.h>
#include <ATen/ops/lerp_native.h>
#include <ATen/ops/lerp_ops.h>
#include <ATen/ops/lerp_native.h>
#include <ATen/ops/lerp_ops.h>
#include <ATen/ops/lerp_native.h>
#include <ATen/ops/lerp_ops.h>
#include <ATen/ops/lerp_native.h>
#include <ATen/ops/lerp_ops.h>
#include <ATen/ops/lerp_native.h>
#include <ATen/ops/lerp_ops.h>
#include <ATen/ops/uniform_native.h>
#include <ATen/ops/uniform_ops.h>
#include <ATen/ops/uniform_native.h>
#include <ATen/ops/uniform_ops.h>
#include <ATen/ops/uniform_native.h>
#include <ATen/ops/uniform_ops.h>
#include <ATen/ops/tril_indices_native.h>
#include <ATen/ops/tril_indices_ops.h>
#include <ATen/ops/tril_indices_native.h>
#include <ATen/ops/tril_indices_ops.h>
#include <ATen/ops/triu_indices_native.h>
#include <ATen/ops/triu_indices_ops.h>
#include <ATen/ops/triu_indices_native.h>
#include <ATen/ops/triu_indices_ops.h>
#include <ATen/ops/ge_native.h>
#include <ATen/ops/ge_ops.h>
#include <ATen/ops/ge_native.h>
#include <ATen/ops/ge_ops.h>
#include <ATen/ops/ge_native.h>
#include <ATen/ops/ge_ops.h>
#include <ATen/ops/ge_native.h>
#include <ATen/ops/ge_ops.h>
#include <ATen/ops/ge_native.h>
#include <ATen/ops/ge_ops.h>
#include <ATen/ops/ge_native.h>
#include <ATen/ops/ge_ops.h>
#include <ATen/ops/le_native.h>
#include <ATen/ops/le_ops.h>
#include <ATen/ops/le_native.h>
#include <ATen/ops/le_ops.h>
#include <ATen/ops/le_native.h>
#include <ATen/ops/le_ops.h>
#include <ATen/ops/le_native.h>
#include <ATen/ops/le_ops.h>
#include <ATen/ops/le_native.h>
#include <ATen/ops/le_ops.h>
#include <ATen/ops/le_native.h>
#include <ATen/ops/le_ops.h>
#include <ATen/ops/gt_native.h>
#include <ATen/ops/gt_ops.h>
#include <ATen/ops/gt_native.h>
#include <ATen/ops/gt_ops.h>
#include <ATen/ops/gt_native.h>
#include <ATen/ops/gt_ops.h>
#include <ATen/ops/gt_native.h>
#include <ATen/ops/gt_ops.h>
#include <ATen/ops/gt_native.h>
#include <ATen/ops/gt_ops.h>
#include <ATen/ops/gt_native.h>
#include <ATen/ops/gt_ops.h>
#include <ATen/ops/nonzero_native.h>
#include <ATen/ops/nonzero_ops.h>
#include <ATen/ops/nonzero_native.h>
#include <ATen/ops/nonzero_ops.h>
#include <ATen/ops/addcdiv_native.h>
#include <ATen/ops/addcdiv_ops.h>
#include <ATen/ops/addcdiv_native.h>
#include <ATen/ops/addcdiv_ops.h>
#include <ATen/ops/addcdiv_native.h>
#include <ATen/ops/addcdiv_ops.h>
#include <ATen/ops/triangular_solve_native.h>
#include <ATen/ops/triangular_solve_ops.h>
#include <ATen/ops/triangular_solve_native.h>
#include <ATen/ops/triangular_solve_ops.h>
#include <ATen/ops/cholesky_solve_native.h>
#include <ATen/ops/cholesky_solve_ops.h>
#include <ATen/ops/cholesky_solve_native.h>
#include <ATen/ops/cholesky_solve_ops.h>
#include <ATen/ops/cholesky_inverse_native.h>
#include <ATen/ops/cholesky_inverse_ops.h>
#include <ATen/ops/cholesky_inverse_native.h>
#include <ATen/ops/cholesky_inverse_ops.h>
#include <ATen/ops/qr_native.h>
#include <ATen/ops/qr_ops.h>
#include <ATen/ops/qr_native.h>
#include <ATen/ops/qr_ops.h>
#include <ATen/ops/geqrf_native.h>
#include <ATen/ops/geqrf_ops.h>
#include <ATen/ops/geqrf_native.h>
#include <ATen/ops/geqrf_ops.h>
#include <ATen/ops/orgqr_native.h>
#include <ATen/ops/orgqr_ops.h>
#include <ATen/ops/orgqr_native.h>
#include <ATen/ops/orgqr_ops.h>
#include <ATen/ops/lu_solve_native.h>
#include <ATen/ops/lu_solve_ops.h>
#include <ATen/ops/lu_solve_native.h>
#include <ATen/ops/lu_solve_ops.h>
#include <ATen/ops/lgamma_native.h>
#include <ATen/ops/lgamma_ops.h>
#include <ATen/ops/lgamma_native.h>
#include <ATen/ops/lgamma_ops.h>
#include <ATen/ops/lgamma_native.h>
#include <ATen/ops/lgamma_ops.h>
#include <ATen/ops/erfinv_native.h>
#include <ATen/ops/erfinv_ops.h>
#include <ATen/ops/erfinv_native.h>
#include <ATen/ops/erfinv_ops.h>
#include <ATen/ops/erfinv_native.h>
#include <ATen/ops/erfinv_ops.h>
#include <ATen/ops/i0_native.h>
#include <ATen/ops/i0_ops.h>
#include <ATen/ops/i0_native.h>
#include <ATen/ops/i0_ops.h>
#include <ATen/ops/i0_native.h>
#include <ATen/ops/i0_ops.h>
#include <ATen/ops/sign_native.h>
#include <ATen/ops/sign_ops.h>
#include <ATen/ops/sign_native.h>
#include <ATen/ops/sign_ops.h>
#include <ATen/ops/sign_native.h>
#include <ATen/ops/sign_ops.h>
#include <ATen/ops/signbit_native.h>
#include <ATen/ops/signbit_ops.h>
#include <ATen/ops/signbit_native.h>
#include <ATen/ops/signbit_ops.h>
#include <ATen/ops/atan2_native.h>
#include <ATen/ops/atan2_ops.h>
#include <ATen/ops/atan2_native.h>
#include <ATen/ops/atan2_ops.h>
#include <ATen/ops/atan2_native.h>
#include <ATen/ops/atan2_ops.h>
#include <ATen/ops/_histogramdd_bin_edges_native.h>
#include <ATen/ops/_histogramdd_bin_edges_ops.h>
#include <ATen/ops/_histogramdd_bin_edges_native.h>
#include <ATen/ops/_histogramdd_bin_edges_ops.h>
#include <ATen/ops/_histogramdd_from_bin_tensors_native.h>
#include <ATen/ops/_histogramdd_from_bin_tensors_ops.h>
#include <ATen/ops/_histogramdd_from_bin_tensors_native.h>
#include <ATen/ops/_histogramdd_from_bin_tensors_ops.h>
#include <ATen/ops/fmod_native.h>
#include <ATen/ops/fmod_ops.h>
#include <ATen/ops/fmod_native.h>
#include <ATen/ops/fmod_ops.h>
#include <ATen/ops/fmod_native.h>
#include <ATen/ops/fmod_ops.h>
#include <ATen/ops/fmod_native.h>
#include <ATen/ops/fmod_ops.h>
#include <ATen/ops/fmod_native.h>
#include <ATen/ops/fmod_ops.h>
#include <ATen/ops/fmod_native.h>
#include <ATen/ops/fmod_ops.h>
#include <ATen/ops/nextafter_native.h>
#include <ATen/ops/nextafter_ops.h>
#include <ATen/ops/nextafter_native.h>
#include <ATen/ops/nextafter_ops.h>
#include <ATen/ops/nextafter_native.h>
#include <ATen/ops/nextafter_ops.h>
#include <ATen/ops/minimum_native.h>
#include <ATen/ops/minimum_ops.h>
#include <ATen/ops/minimum_native.h>
#include <ATen/ops/minimum_ops.h>
#include <ATen/ops/topk_native.h>
#include <ATen/ops/topk_ops.h>
#include <ATen/ops/topk_native.h>
#include <ATen/ops/topk_ops.h>
#include <ATen/ops/any_native.h>
#include <ATen/ops/any_ops.h>
#include <ATen/ops/any_native.h>
#include <ATen/ops/any_ops.h>
#include <ATen/ops/_foreach_mul_native.h>
#include <ATen/ops/_foreach_mul_ops.h>
#include <ATen/ops/_foreach_mul_native.h>
#include <ATen/ops/_foreach_mul_ops.h>
#include <ATen/ops/_foreach_mul_native.h>
#include <ATen/ops/_foreach_mul_ops.h>
#include <ATen/ops/_foreach_mul_native.h>
#include <ATen/ops/_foreach_mul_ops.h>
#include <ATen/ops/_foreach_mul_native.h>
#include <ATen/ops/_foreach_mul_ops.h>
#include <ATen/ops/_foreach_mul_native.h>
#include <ATen/ops/_foreach_mul_ops.h>
#include <ATen/ops/_foreach_mul_native.h>
#include <ATen/ops/_foreach_mul_ops.h>
#include <ATen/ops/_foreach_mul_native.h>
#include <ATen/ops/_foreach_mul_ops.h>
#include <ATen/ops/_foreach_mul_native.h>
#include <ATen/ops/_foreach_mul_ops.h>
#include <ATen/ops/_foreach_mul_native.h>
#include <ATen/ops/_foreach_mul_ops.h>
#include <ATen/ops/_foreach_mul_native.h>
#include <ATen/ops/_foreach_mul_ops.h>
#include <ATen/ops/_foreach_mul_native.h>
#include <ATen/ops/_foreach_mul_ops.h>
#include <ATen/ops/_foreach_div_native.h>
#include <ATen/ops/_foreach_div_ops.h>
#include <ATen/ops/_foreach_div_native.h>
#include <ATen/ops/_foreach_div_ops.h>
#include <ATen/ops/_foreach_div_native.h>
#include <ATen/ops/_foreach_div_ops.h>
#include <ATen/ops/_foreach_div_native.h>
#include <ATen/ops/_foreach_div_ops.h>
#include <ATen/ops/_foreach_div_native.h>
#include <ATen/ops/_foreach_div_ops.h>
#include <ATen/ops/_foreach_div_native.h>
#include <ATen/ops/_foreach_div_ops.h>
#include <ATen/ops/_foreach_div_native.h>
#include <ATen/ops/_foreach_div_ops.h>
#include <ATen/ops/_foreach_div_native.h>
#include <ATen/ops/_foreach_div_ops.h>
#include <ATen/ops/_foreach_div_native.h>
#include <ATen/ops/_foreach_div_ops.h>
#include <ATen/ops/_foreach_div_native.h>
#include <ATen/ops/_foreach_div_ops.h>
#include <ATen/ops/_foreach_div_native.h>
#include <ATen/ops/_foreach_div_ops.h>
#include <ATen/ops/_foreach_div_native.h>
#include <ATen/ops/_foreach_div_ops.h>
#include <ATen/ops/_foreach_clamp_max_native.h>
#include <ATen/ops/_foreach_clamp_max_ops.h>
#include <ATen/ops/_foreach_clamp_max_native.h>
#include <ATen/ops/_foreach_clamp_max_ops.h>
#include <ATen/ops/_foreach_clamp_max_native.h>
#include <ATen/ops/_foreach_clamp_max_ops.h>
#include <ATen/ops/_foreach_clamp_max_native.h>
#include <ATen/ops/_foreach_clamp_max_ops.h>
#include <ATen/ops/_foreach_clamp_max_native.h>
#include <ATen/ops/_foreach_clamp_max_ops.h>
#include <ATen/ops/_foreach_clamp_max_native.h>
#include <ATen/ops/_foreach_clamp_max_ops.h>
#include <ATen/ops/_foreach_clamp_max_native.h>
#include <ATen/ops/_foreach_clamp_max_ops.h>
#include <ATen/ops/_foreach_clamp_max_native.h>
#include <ATen/ops/_foreach_clamp_max_ops.h>
#include <ATen/ops/_foreach_clamp_max_native.h>
#include <ATen/ops/_foreach_clamp_max_ops.h>
#include <ATen/ops/_foreach_minimum_native.h>
#include <ATen/ops/_foreach_minimum_ops.h>
#include <ATen/ops/_foreach_minimum_native.h>
#include <ATen/ops/_foreach_minimum_ops.h>
#include <ATen/ops/_foreach_minimum_native.h>
#include <ATen/ops/_foreach_minimum_ops.h>
#include <ATen/ops/_foreach_minimum_native.h>
#include <ATen/ops/_foreach_minimum_ops.h>
#include <ATen/ops/_foreach_minimum_native.h>
#include <ATen/ops/_foreach_minimum_ops.h>
#include <ATen/ops/_foreach_minimum_native.h>
#include <ATen/ops/_foreach_minimum_ops.h>
#include <ATen/ops/_foreach_minimum_native.h>
#include <ATen/ops/_foreach_minimum_ops.h>
#include <ATen/ops/_foreach_minimum_native.h>
#include <ATen/ops/_foreach_minimum_ops.h>
#include <ATen/ops/_foreach_minimum_native.h>
#include <ATen/ops/_foreach_minimum_ops.h>
#include <ATen/ops/_foreach_expm1_native.h>
#include <ATen/ops/_foreach_expm1_ops.h>
#include <ATen/ops/_foreach_expm1_native.h>
#include <ATen/ops/_foreach_expm1_ops.h>
#include <ATen/ops/_foreach_expm1_native.h>
#include <ATen/ops/_foreach_expm1_ops.h>
#include <ATen/ops/_foreach_frac_native.h>
#include <ATen/ops/_foreach_frac_ops.h>
#include <ATen/ops/_foreach_frac_native.h>
#include <ATen/ops/_foreach_frac_ops.h>
#include <ATen/ops/_foreach_frac_native.h>
#include <ATen/ops/_foreach_frac_ops.h>
#include <ATen/ops/_foreach_reciprocal_native.h>
#include <ATen/ops/_foreach_reciprocal_ops.h>
#include <ATen/ops/_foreach_reciprocal_native.h>
#include <ATen/ops/_foreach_reciprocal_ops.h>
#include <ATen/ops/_foreach_reciprocal_native.h>
#include <ATen/ops/_foreach_reciprocal_ops.h>
#include <ATen/ops/_foreach_sin_native.h>
#include <ATen/ops/_foreach_sin_ops.h>
#include <ATen/ops/_foreach_sin_native.h>
#include <ATen/ops/_foreach_sin_ops.h>
#include <ATen/ops/_foreach_sin_native.h>
#include <ATen/ops/_foreach_sin_ops.h>
#include <ATen/ops/_foreach_tanh_native.h>
#include <ATen/ops/_foreach_tanh_ops.h>
#include <ATen/ops/_foreach_tanh_native.h>
#include <ATen/ops/_foreach_tanh_ops.h>
#include <ATen/ops/_foreach_tanh_native.h>
#include <ATen/ops/_foreach_tanh_ops.h>
#include <ATen/ops/_foreach_trunc_native.h>
#include <ATen/ops/_foreach_trunc_ops.h>
#include <ATen/ops/_foreach_trunc_native.h>
#include <ATen/ops/_foreach_trunc_ops.h>
#include <ATen/ops/_foreach_trunc_native.h>
#include <ATen/ops/_foreach_trunc_ops.h>
#include <ATen/ops/_foreach_copy_native.h>
#include <ATen/ops/_foreach_copy_ops.h>
#include <ATen/ops/_foreach_copy_native.h>
#include <ATen/ops/_foreach_copy_ops.h>
#include <ATen/ops/_foreach_copy_native.h>
#include <ATen/ops/_foreach_copy_ops.h>
#include <ATen/ops/_convert_indices_from_csr_to_coo_native.h>
#include <ATen/ops/_convert_indices_from_csr_to_coo_ops.h>
#include <ATen/ops/_convert_indices_from_csr_to_coo_native.h>
#include <ATen/ops/_convert_indices_from_csr_to_coo_ops.h>
#include <ATen/ops/multilabel_margin_loss_backward_native.h>
#include <ATen/ops/multilabel_margin_loss_backward_ops.h>
#include <ATen/ops/multilabel_margin_loss_backward_native.h>
#include <ATen/ops/multilabel_margin_loss_backward_ops.h>
#include <ATen/ops/nll_loss_native.h>
#include <ATen/ops/nll_loss_ops.h>
#include <ATen/ops/nll_loss_native.h>
#include <ATen/ops/nll_loss_ops.h>
#include <ATen/ops/nll_loss_backward_native.h>
#include <ATen/ops/nll_loss_backward_ops.h>
#include <ATen/ops/nll_loss_backward_native.h>
#include <ATen/ops/nll_loss_backward_ops.h>
#include <ATen/ops/nll_loss2d_native.h>
#include <ATen/ops/nll_loss2d_ops.h>
#include <ATen/ops/nll_loss2d_native.h>
#include <ATen/ops/nll_loss2d_ops.h>
#include <ATen/ops/nll_loss2d_backward_native.h>
#include <ATen/ops/nll_loss2d_backward_ops.h>
#include <ATen/ops/nll_loss2d_backward_native.h>
#include <ATen/ops/nll_loss2d_backward_ops.h>
#include <ATen/ops/smooth_l1_loss_native.h>
#include <ATen/ops/smooth_l1_loss_ops.h>
#include <ATen/ops/smooth_l1_loss_native.h>
#include <ATen/ops/smooth_l1_loss_ops.h>
#include <ATen/ops/huber_loss_native.h>
#include <ATen/ops/huber_loss_ops.h>
#include <ATen/ops/huber_loss_native.h>
#include <ATen/ops/huber_loss_ops.h>
#include <ATen/ops/soft_margin_loss_backward_native.h>
#include <ATen/ops/soft_margin_loss_backward_ops.h>
#include <ATen/ops/soft_margin_loss_backward_native.h>
#include <ATen/ops/soft_margin_loss_backward_ops.h>
#include <ATen/ops/elu_native.h>
#include <ATen/ops/elu_ops.h>
#include <ATen/ops/elu_native.h>
#include <ATen/ops/elu_ops.h>
#include <ATen/ops/elu_native.h>
#include <ATen/ops/elu_ops.h>
#include <ATen/ops/glu_native.h>
#include <ATen/ops/glu_ops.h>
#include <ATen/ops/glu_native.h>
#include <ATen/ops/glu_ops.h>
#include <ATen/ops/hardsigmoid_native.h>
#include <ATen/ops/hardsigmoid_ops.h>
#include <ATen/ops/hardsigmoid_native.h>
#include <ATen/ops/hardsigmoid_ops.h>
#include <ATen/ops/hardsigmoid_native.h>
#include <ATen/ops/hardsigmoid_ops.h>
#include <ATen/ops/leaky_relu_backward_native.h>
#include <ATen/ops/leaky_relu_backward_ops.h>
#include <ATen/ops/leaky_relu_backward_native.h>
#include <ATen/ops/leaky_relu_backward_ops.h>
#include <ATen/ops/softshrink_native.h>
#include <ATen/ops/softshrink_ops.h>
#include <ATen/ops/softshrink_native.h>
#include <ATen/ops/softshrink_ops.h>
#include <ATen/ops/_adaptive_avg_pool2d_native.h>
#include <ATen/ops/_adaptive_avg_pool2d_ops.h>
#include <ATen/ops/_adaptive_avg_pool2d_native.h>
#include <ATen/ops/_adaptive_avg_pool2d_ops.h>
#include <ATen/ops/_adaptive_avg_pool3d_native.h>
#include <ATen/ops/_adaptive_avg_pool3d_ops.h>
#include <ATen/ops/_adaptive_avg_pool3d_native.h>
#include <ATen/ops/_adaptive_avg_pool3d_ops.h>
#include <ATen/ops/avg_pool3d_native.h>
#include <ATen/ops/avg_pool3d_ops.h>
#include <ATen/ops/avg_pool3d_native.h>
#include <ATen/ops/avg_pool3d_ops.h>
#include <ATen/ops/fractional_max_pool3d_native.h>
#include <ATen/ops/fractional_max_pool3d_ops.h>
#include <ATen/ops/fractional_max_pool3d_native.h>
#include <ATen/ops/fractional_max_pool3d_ops.h>
#include <ATen/ops/max_pool2d_with_indices_backward_native.h>
#include <ATen/ops/max_pool2d_with_indices_backward_ops.h>
#include <ATen/ops/max_pool2d_with_indices_backward_native.h>
#include <ATen/ops/max_pool2d_with_indices_backward_ops.h>
#include <ATen/ops/max_unpool3d_native.h>
#include <ATen/ops/max_unpool3d_ops.h>
#include <ATen/ops/max_unpool3d_native.h>
#include <ATen/ops/max_unpool3d_ops.h>
#include <ATen/ops/reflection_pad1d_native.h>
#include <ATen/ops/reflection_pad1d_ops.h>
#include <ATen/ops/reflection_pad1d_native.h>
#include <ATen/ops/reflection_pad1d_ops.h>
#include <ATen/ops/reflection_pad1d_backward_native.h>
#include <ATen/ops/reflection_pad1d_backward_ops.h>
#include <ATen/ops/reflection_pad1d_backward_native.h>
#include <ATen/ops/reflection_pad1d_backward_ops.h>
#include <ATen/ops/reflection_pad2d_native.h>
#include <ATen/ops/reflection_pad2d_ops.h>
#include <ATen/ops/reflection_pad2d_native.h>
#include <ATen/ops/reflection_pad2d_ops.h>
#include <ATen/ops/reflection_pad3d_native.h>
#include <ATen/ops/reflection_pad3d_ops.h>
#include <ATen/ops/reflection_pad3d_native.h>
#include <ATen/ops/reflection_pad3d_ops.h>
#include <ATen/ops/replication_pad1d_native.h>
#include <ATen/ops/replication_pad1d_ops.h>
#include <ATen/ops/replication_pad1d_native.h>
#include <ATen/ops/replication_pad1d_ops.h>
#include <ATen/ops/replication_pad1d_backward_native.h>
#include <ATen/ops/replication_pad1d_backward_ops.h>
#include <ATen/ops/replication_pad1d_backward_native.h>
#include <ATen/ops/replication_pad1d_backward_ops.h>
#include <ATen/ops/replication_pad2d_backward_native.h>
#include <ATen/ops/replication_pad2d_backward_ops.h>
#include <ATen/ops/replication_pad2d_backward_native.h>
#include <ATen/ops/replication_pad2d_backward_ops.h>
#include <ATen/ops/replication_pad3d_native.h>
#include <ATen/ops/replication_pad3d_ops.h>
#include <ATen/ops/replication_pad3d_native.h>
#include <ATen/ops/replication_pad3d_ops.h>
#include <ATen/ops/replication_pad3d_backward_native.h>
#include <ATen/ops/replication_pad3d_backward_ops.h>
#include <ATen/ops/replication_pad3d_backward_native.h>
#include <ATen/ops/replication_pad3d_backward_ops.h>
#include <ATen/ops/upsample_nearest2d_native.h>
#include <ATen/ops/upsample_nearest2d_ops.h>
#include <ATen/ops/upsample_nearest2d_native.h>
#include <ATen/ops/upsample_nearest2d_ops.h>
#include <ATen/ops/_upsample_nearest_exact2d_native.h>
#include <ATen/ops/_upsample_nearest_exact2d_ops.h>
#include <ATen/ops/_upsample_nearest_exact2d_native.h>
#include <ATen/ops/_upsample_nearest_exact2d_ops.h>
#include <ATen/ops/_upsample_nearest_exact2d_backward_native.h>
#include <ATen/ops/_upsample_nearest_exact2d_backward_ops.h>
#include <ATen/ops/_upsample_nearest_exact2d_backward_native.h>
#include <ATen/ops/_upsample_nearest_exact2d_backward_ops.h>
#include <ATen/ops/_upsample_nearest_exact3d_backward_native.h>
#include <ATen/ops/_upsample_nearest_exact3d_backward_ops.h>
#include <ATen/ops/_upsample_nearest_exact3d_backward_native.h>
#include <ATen/ops/_upsample_nearest_exact3d_backward_ops.h>
#include <ATen/ops/sigmoid_backward_native.h>
#include <ATen/ops/sigmoid_backward_ops.h>
#include <ATen/ops/sigmoid_backward_native.h>
#include <ATen/ops/sigmoid_backward_ops.h>
#include <ATen/ops/slow_conv_transpose2d_native.h>
#include <ATen/ops/slow_conv_transpose2d_ops.h>
#include <ATen/ops/slow_conv_transpose2d_native.h>
#include <ATen/ops/slow_conv_transpose2d_ops.h>
#include <ATen/ops/_slow_conv2d_forward_native.h>
#include <ATen/ops/_slow_conv2d_forward_ops.h>
#include <ATen/ops/_slow_conv2d_forward_native.h>
#include <ATen/ops/_slow_conv2d_forward_ops.h>
#include <ATen/ops/conv_depthwise3d_native.h>
#include <ATen/ops/conv_depthwise3d_ops.h>
#include <ATen/ops/conv_depthwise3d_native.h>
#include <ATen/ops/conv_depthwise3d_ops.h>
#include <ATen/ops/slow_conv_dilated2d_native.h>
#include <ATen/ops/slow_conv_dilated2d_ops.h>
#include <ATen/ops/slow_conv_dilated2d_native.h>
#include <ATen/ops/slow_conv_dilated2d_ops.h>
#include <ATen/ops/special_ndtri_native.h>
#include <ATen/ops/special_ndtri_ops.h>
#include <ATen/ops/special_ndtri_native.h>
#include <ATen/ops/special_ndtri_ops.h>
#include <ATen/ops/special_erfc_native.h>
#include <ATen/ops/special_erfc_ops.h>
#include <ATen/ops/special_erfc_native.h>
#include <ATen/ops/special_erfc_ops.h>
#include <ATen/ops/special_logit_native.h>
#include <ATen/ops/special_logit_ops.h>
#include <ATen/ops/special_logit_native.h>
#include <ATen/ops/special_logit_ops.h>
#include <ATen/ops/special_polygamma_native.h>
#include <ATen/ops/special_polygamma_ops.h>
#include <ATen/ops/special_polygamma_native.h>
#include <ATen/ops/special_polygamma_ops.h>
#include <ATen/ops/special_sinc_native.h>
#include <ATen/ops/special_sinc_ops.h>
#include <ATen/ops/special_sinc_native.h>
#include <ATen/ops/special_sinc_ops.h>
#include <ATen/ops/fft_ifft_native.h>
#include <ATen/ops/fft_ifft_ops.h>
#include <ATen/ops/fft_ifft_native.h>
#include <ATen/ops/fft_ifft_ops.h>
#include <ATen/ops/fft_ihfft_native.h>
#include <ATen/ops/fft_ihfft_ops.h>
#include <ATen/ops/fft_ihfft_native.h>
#include <ATen/ops/fft_ihfft_ops.h>
#include <ATen/ops/fft_ifft2_native.h>
#include <ATen/ops/fft_ifft2_ops.h>
#include <ATen/ops/fft_ifft2_native.h>
#include <ATen/ops/fft_ifft2_ops.h>
#include <ATen/ops/fft_ihfftn_native.h>
#include <ATen/ops/fft_ihfftn_ops.h>
#include <ATen/ops/fft_ihfftn_native.h>
#include <ATen/ops/fft_ihfftn_ops.h>
#include <ATen/ops/linalg_cholesky_ex_native.h>
#include <ATen/ops/linalg_cholesky_ex_ops.h>
#include <ATen/ops/linalg_cholesky_ex_native.h>
#include <ATen/ops/linalg_cholesky_ex_ops.h>
#include <ATen/ops/linalg_cross_native.h>
#include <ATen/ops/linalg_cross_ops.h>
#include <ATen/ops/linalg_cross_native.h>
#include <ATen/ops/linalg_cross_ops.h>
#include <ATen/ops/linalg_lu_solve_native.h>
#include <ATen/ops/linalg_lu_solve_ops.h>
#include <ATen/ops/linalg_lu_solve_native.h>
#include <ATen/ops/linalg_lu_solve_ops.h>
#include <ATen/ops/linalg_matmul_native.h>
#include <ATen/ops/linalg_matmul_ops.h>
#include <ATen/ops/linalg_matmul_native.h>
#include <ATen/ops/linalg_matmul_ops.h>
#include <ATen/ops/linalg_vecdot_native.h>
#include <ATen/ops/linalg_vecdot_ops.h>
#include <ATen/ops/linalg_vecdot_native.h>
#include <ATen/ops/linalg_vecdot_ops.h>
#include <ATen/ops/linalg_eigvalsh_native.h>
#include <ATen/ops/linalg_eigvalsh_ops.h>
#include <ATen/ops/linalg_eigvalsh_native.h>
#include <ATen/ops/linalg_eigvalsh_ops.h>
#include <ATen/ops/linalg_inv_ex_native.h>
#include <ATen/ops/linalg_inv_ex_ops.h>
#include <ATen/ops/linalg_inv_ex_native.h>
#include <ATen/ops/linalg_inv_ex_ops.h>
#include <ATen/ops/inverse_native.h>
#include <ATen/ops/inverse_ops.h>
#include <ATen/ops/inverse_native.h>
#include <ATen/ops/inverse_ops.h>
#include <ATen/ops/outer_native.h>
#include <ATen/ops/outer_ops.h>
#include <ATen/ops/outer_native.h>
#include <ATen/ops/outer_ops.h>
#include <ATen/ops/linalg_svd_native.h>
#include <ATen/ops/linalg_svd_ops.h>
#include <ATen/ops/linalg_svd_native.h>
#include <ATen/ops/linalg_svd_ops.h>
#include <ATen/ops/linalg_cond_native.h>
#include <ATen/ops/linalg_cond_ops.h>
#include <ATen/ops/linalg_cond_native.h>
#include <ATen/ops/linalg_cond_ops.h>
#include <ATen/ops/linalg_cond_native.h>
#include <ATen/ops/linalg_cond_ops.h>
#include <ATen/ops/linalg_cond_native.h>
#include <ATen/ops/linalg_cond_ops.h>
#include <ATen/ops/linalg_solve_ex_native.h>
#include <ATen/ops/linalg_solve_ex_ops.h>
#include <ATen/ops/linalg_solve_ex_native.h>
#include <ATen/ops/linalg_solve_ex_ops.h>
#include <ATen/ops/linalg_tensorsolve_native.h>
#include <ATen/ops/linalg_tensorsolve_ops.h>
#include <ATen/ops/linalg_tensorsolve_native.h>
#include <ATen/ops/linalg_tensorsolve_ops.h>
#include <ATen/ops/linalg_matrix_rank_native.h>
#include <ATen/ops/linalg_matrix_rank_ops.h>
#include <ATen/ops/linalg_matrix_rank_native.h>
#include <ATen/ops/linalg_matrix_rank_ops.h>
#include <ATen/ops/linalg_matrix_rank_native.h>
#include <ATen/ops/linalg_matrix_rank_ops.h>
#include <ATen/ops/linalg_matrix_rank_native.h>
#include <ATen/ops/linalg_matrix_rank_ops.h>
#include <ATen/ops/linalg_matrix_rank_native.h>
#include <ATen/ops/linalg_matrix_rank_ops.h>
#include <ATen/ops/linalg_matrix_rank_native.h>
#include <ATen/ops/linalg_matrix_rank_ops.h>
#include <ATen/ops/linalg_matrix_rank_native.h>
#include <ATen/ops/linalg_matrix_rank_ops.h>
#include <ATen/ops/linalg_matrix_rank_native.h>
#include <ATen/ops/linalg_matrix_rank_ops.h>
#include <ATen/ops/_test_optional_floatlist_native.h>
#include <ATen/ops/_test_optional_floatlist_ops.h>
#include <ATen/ops/_test_optional_floatlist_native.h>
#include <ATen/ops/_test_optional_floatlist_ops.h>
#include <ATen/ops/_test_warn_in_autograd_native.h>
#include <ATen/ops/_test_warn_in_autograd_ops.h>
#include <ATen/ops/_test_warn_in_autograd_native.h>
#include <ATen/ops/_test_warn_in_autograd_ops.h>
#include <ATen/ops/_segment_reduce_backward_native.h>
#include <ATen/ops/_segment_reduce_backward_ops.h>
#include <ATen/ops/_segment_reduce_backward_native.h>
#include <ATen/ops/_segment_reduce_backward_ops.h>
#include <ATen/ops/_sparse_broadcast_to_copy_native.h>
#include <ATen/ops/_sparse_broadcast_to_copy_ops.h>
#include <ATen/ops/_sparse_broadcast_to_copy_native.h>
#include <ATen/ops/_sparse_broadcast_to_copy_ops.h>
#include <ATen/ops/unsqueeze_copy_native.h>
#include <ATen/ops/unsqueeze_copy_ops.h>
#include <ATen/ops/unsqueeze_copy_native.h>
#include <ATen/ops/unsqueeze_copy_ops.h>
#include <ATen/ops/values_copy_native.h>
#include <ATen/ops/values_copy_ops.h>
#include <ATen/ops/values_copy_native.h>
#include <ATen/ops/values_copy_ops.h>
#include <ATen/ops/to_padded_tensor_native.h>
#include <ATen/ops/to_padded_tensor_ops.h>
#include <ATen/ops/to_padded_tensor_native.h>
#include <ATen/ops/to_padded_tensor_ops.h>
#include <ATen/ops/_triton_scaled_dot_attention_native.h>
#include <ATen/ops/_triton_scaled_dot_attention_ops.h>
#include <ATen/ops/_triton_scaled_dot_attention_native.h>
#include <ATen/ops/_triton_scaled_dot_attention_ops.h>
#include <ATen/ops/special_bessel_y0_native.h>
#include <ATen/ops/special_bessel_y0_ops.h>
#include <ATen/ops/special_bessel_y0_native.h>
#include <ATen/ops/special_bessel_y0_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_t_native.h>
#include <ATen/ops/special_chebyshev_polynomial_t_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_t_native.h>
#include <ATen/ops/special_chebyshev_polynomial_t_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_t_native.h>
#include <ATen/ops/special_chebyshev_polynomial_t_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_t_native.h>
#include <ATen/ops/special_chebyshev_polynomial_t_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_t_native.h>
#include <ATen/ops/special_chebyshev_polynomial_t_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_t_native.h>
#include <ATen/ops/special_chebyshev_polynomial_t_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_u_native.h>
#include <ATen/ops/special_chebyshev_polynomial_u_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_u_native.h>
#include <ATen/ops/special_chebyshev_polynomial_u_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_u_native.h>
#include <ATen/ops/special_chebyshev_polynomial_u_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_u_native.h>
#include <ATen/ops/special_chebyshev_polynomial_u_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_u_native.h>
#include <ATen/ops/special_chebyshev_polynomial_u_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_u_native.h>
#include <ATen/ops/special_chebyshev_polynomial_u_ops.h>
#include <ATen/ops/special_hermite_polynomial_h_native.h>
#include <ATen/ops/special_hermite_polynomial_h_ops.h>
#include <ATen/ops/special_hermite_polynomial_h_native.h>
#include <ATen/ops/special_hermite_polynomial_h_ops.h>
#include <ATen/ops/special_hermite_polynomial_h_native.h>
#include <ATen/ops/special_hermite_polynomial_h_ops.h>
#include <ATen/ops/special_hermite_polynomial_h_native.h>
#include <ATen/ops/special_hermite_polynomial_h_ops.h>
#include <ATen/ops/special_hermite_polynomial_h_native.h>
#include <ATen/ops/special_hermite_polynomial_h_ops.h>
#include <ATen/ops/special_hermite_polynomial_h_native.h>
#include <ATen/ops/special_hermite_polynomial_h_ops.h>
#include <ATen/ops/special_modified_bessel_k1_native.h>
#include <ATen/ops/special_modified_bessel_k1_ops.h>
#include <ATen/ops/special_modified_bessel_k1_native.h>
#include <ATen/ops/special_modified_bessel_k1_ops.h>
#include <ATen/ops/special_scaled_modified_bessel_k0_native.h>
#include <ATen/ops/special_scaled_modified_bessel_k0_ops.h>
#include <ATen/ops/special_scaled_modified_bessel_k0_native.h>
#include <ATen/ops/special_scaled_modified_bessel_k0_ops.h>
#include <ATen/ops/special_scaled_modified_bessel_k1_native.h>
#include <ATen/ops/special_scaled_modified_bessel_k1_ops.h>
#include <ATen/ops/special_scaled_modified_bessel_k1_native.h>
#include <ATen/ops/special_scaled_modified_bessel_k1_ops.h>
#include <ATen/ops/special_spherical_bessel_j0_native.h>
#include <ATen/ops/special_spherical_bessel_j0_ops.h>
#include <ATen/ops/special_spherical_bessel_j0_native.h>
#include <ATen/ops/special_spherical_bessel_j0_ops.h>
#include <ATen/ops/_fused_sgd_native.h>
#include <ATen/ops/_fused_sgd_ops.h>
#include <ATen/ops/_fused_sgd_native.h>
#include <ATen/ops/_fused_sgd_ops.h>
#include <ATen/ops/_fused_sgd_native.h>
#include <ATen/ops/_fused_sgd_ops.h>
#include <ATen/ops/_fused_sgd_native.h>
#include <ATen/ops/_fused_sgd_ops.h>
#include <ATen/ops/_fused_sgd_native.h>
#include <ATen/ops/_fused_sgd_ops.h>
#include <ATen/ops/_fused_sgd_native.h>
#include <ATen/ops/_fused_sgd_ops.h>
#include <ATen/ops/refine_names_native.h>
#include <ATen/ops/refine_names_ops.h>
#include <ATen/ops/real_native.h>
#include <ATen/ops/real_ops.h>
#include <ATen/ops/_neg_view_native.h>
#include <ATen/ops/_neg_view_ops.h>
#include <ATen/ops/_neg_view_copy_native.h>
#include <ATen/ops/_neg_view_copy_ops.h>
#include <ATen/ops/diagonal_native.h>
#include <ATen/ops/diagonal_ops.h>
#include <ATen/ops/diagonal_copy_native.h>
#include <ATen/ops/diagonal_copy_ops.h>
#include <ATen/ops/diagonal_native.h>
#include <ATen/ops/diagonal_ops.h>
#include <ATen/ops/narrow_native.h>
#include <ATen/ops/narrow_ops.h>
#include <ATen/ops/narrow_copy_native.h>
#include <ATen/ops/narrow_copy_ops.h>
#include <ATen/ops/narrow_native.h>
#include <ATen/ops/narrow_ops.h>
#include <ATen/ops/numpy_T_native.h>
#include <ATen/ops/numpy_T_ops.h>
#include <ATen/ops/select_native.h>
#include <ATen/ops/select_ops.h>
#include <ATen/ops/select_native.h>
#include <ATen/ops/select_ops.h>
#include <ATen/ops/select_copy_native.h>
#include <ATen/ops/select_copy_ops.h>
#include <ATen/ops/slice_inverse_native.h>
#include <ATen/ops/slice_inverse_ops.h>
#include <ATen/ops/slice_scatter_native.h>
#include <ATen/ops/slice_scatter_ops.h>
#include <ATen/ops/split_with_sizes_native.h>
#include <ATen/ops/split_with_sizes_ops.h>
#include <ATen/ops/split_with_sizes_copy_native.h>
#include <ATen/ops/split_with_sizes_copy_ops.h>
#include <ATen/ops/vsplit_native.h>
#include <ATen/ops/vsplit_ops.h>
#include <ATen/ops/vsplit_native.h>
#include <ATen/ops/vsplit_ops.h>
#include <ATen/ops/squeeze_native.h>
#include <ATen/ops/squeeze_ops.h>
#include <ATen/ops/squeeze_copy_native.h>
#include <ATen/ops/squeeze_copy_ops.h>
#include <ATen/ops/squeeze_native.h>
#include <ATen/ops/squeeze_ops.h>
#include <ATen/ops/squeeze_copy_native.h>
#include <ATen/ops/squeeze_copy_ops.h>
#include <ATen/ops/squeeze_native.h>
#include <ATen/ops/squeeze_ops.h>
#include <ATen/ops/squeeze_native.h>
#include <ATen/ops/squeeze_ops.h>
#include <ATen/ops/squeeze_copy_native.h>
#include <ATen/ops/squeeze_copy_ops.h>
#include <ATen/ops/view_as_native.h>
#include <ATen/ops/view_as_ops.h>
#include <ATen/ops/unbind_native.h>
#include <ATen/ops/unbind_ops.h>
#include <ATen/ops/unbind_copy_native.h>
#include <ATen/ops/unbind_copy_ops.h>
#include <ATen/ops/unbind_native.h>
#include <ATen/ops/unbind_ops.h>
#include <ATen/ops/alias_native.h>
#include <ATen/ops/alias_ops.h>
#include <ATen/ops/alias_copy_native.h>
#include <ATen/ops/alias_copy_ops.h>
#include <ATen/ops/_cast_Char_native.h>
#include <ATen/ops/_cast_Char_ops.h>
#include <ATen/ops/_cast_Half_native.h>
#include <ATen/ops/_cast_Half_ops.h>
#include <ATen/ops/_backward_native.h>
#include <ATen/ops/_backward_ops.h>
#include <ATen/ops/set_data_native.h>
#include <ATen/ops/set_data_ops.h>
#include <ATen/ops/data_native.h>
#include <ATen/ops/data_ops.h>
#include <ATen/ops/requires_grad_native.h>
#include <ATen/ops/requires_grad_ops.h>
#include <ATen/ops/align_as_native.h>
#include <ATen/ops/align_as_ops.h>
#include <ATen/ops/_assert_scalar_native.h>
#include <ATen/ops/_assert_scalar_ops.h>
#include <ATen/ops/_functional_assert_async_native.h>
#include <ATen/ops/_functional_assert_async_ops.h>
#include <ATen/ops/sym_constrain_range_native.h>
#include <ATen/ops/sym_constrain_range_ops.h>
#include <ATen/ops/sym_constrain_range_for_size_native.h>
#include <ATen/ops/sym_constrain_range_for_size_ops.h>
#include <ATen/ops/_functional_sym_constrain_range_for_size_native.h>
#include <ATen/ops/_functional_sym_constrain_range_for_size_ops.h>
#include <ATen/ops/_make_dep_token_native.h>
#include <ATen/ops/_make_dep_token_ops.h>
#include <ATen/ops/_sobol_engine_ff_native.h>
#include <ATen/ops/_sobol_engine_ff_ops.h>
#include <ATen/ops/feature_alpha_dropout_native.h>
#include <ATen/ops/feature_alpha_dropout_ops.h>
#include <ATen/ops/feature_alpha_dropout_native.h>
#include <ATen/ops/feature_alpha_dropout_ops.h>
#include <ATen/ops/_is_all_true_native.h>
#include <ATen/ops/_is_all_true_ops.h>
#include <ATen/ops/allclose_native.h>
#include <ATen/ops/allclose_ops.h>
#include <ATen/ops/atleast_1d_native.h>
#include <ATen/ops/atleast_1d_ops.h>
#include <ATen/ops/atleast_1d_native.h>
#include <ATen/ops/atleast_1d_ops.h>
#include <ATen/ops/_batch_norm_impl_index_native.h>
#include <ATen/ops/_batch_norm_impl_index_ops.h>
#include <ATen/ops/bilinear_native.h>
#include <ATen/ops/bilinear_ops.h>
#include <ATen/ops/unsafe_chunk_native.h>
#include <ATen/ops/unsafe_chunk_ops.h>
#include <ATen/ops/conv_transpose1d_native.h>
#include <ATen/ops/conv_transpose1d_ops.h>
#include <ATen/ops/conv_transpose2d_native.h>
#include <ATen/ops/conv_transpose2d_ops.h>
#include <ATen/ops/conv_transpose3d_native.h>
#include <ATen/ops/conv_transpose3d_ops.h>
#include <ATen/ops/cosine_embedding_loss_native.h>
#include <ATen/ops/cosine_embedding_loss_ops.h>
#include <ATen/ops/cumulative_trapezoid_native.h>
#include <ATen/ops/cumulative_trapezoid_ops.h>
#include <ATen/ops/cumulative_trapezoid_native.h>
#include <ATen/ops/cumulative_trapezoid_ops.h>
#include <ATen/ops/ctc_loss_native.h>
#include <ATen/ops/ctc_loss_ops.h>
#include <ATen/ops/ctc_loss_native.h>
#include <ATen/ops/ctc_loss_ops.h>
#include <ATen/ops/gradient_native.h>
#include <ATen/ops/gradient_ops.h>
#include <ATen/ops/gradient_native.h>
#include <ATen/ops/gradient_ops.h>
#include <ATen/ops/gradient_native.h>
#include <ATen/ops/gradient_ops.h>
#include <ATen/ops/gradient_native.h>
#include <ATen/ops/gradient_ops.h>
#include <ATen/ops/gradient_native.h>
#include <ATen/ops/gradient_ops.h>
#include <ATen/ops/gradient_native.h>
#include <ATen/ops/gradient_ops.h>
#include <ATen/ops/gradient_native.h>
#include <ATen/ops/gradient_ops.h>
#include <ATen/ops/einsum_native.h>
#include <ATen/ops/einsum_ops.h>
#include <ATen/ops/grid_sampler_native.h>
#include <ATen/ops/grid_sampler_ops.h>
#include <ATen/ops/_grid_sampler_2d_cpu_fallback_backward_native.h>
#include <ATen/ops/_grid_sampler_2d_cpu_fallback_backward_ops.h>
#include <ATen/ops/hinge_embedding_loss_native.h>
#include <ATen/ops/hinge_embedding_loss_ops.h>
#include <ATen/ops/_cufft_set_plan_cache_max_size_native.h>
#include <ATen/ops/_cufft_set_plan_cache_max_size_ops.h>
#include <ATen/ops/_cufft_clear_plan_cache_native.h>
#include <ATen/ops/_cufft_clear_plan_cache_ops.h>
#include <ATen/ops/_unsafe_index_put_native.h>
#include <ATen/ops/_unsafe_index_put_ops.h>
#include <ATen/ops/isclose_native.h>
#include <ATen/ops/isclose_ops.h>
#include <ATen/ops/is_floating_point_native.h>
#include <ATen/ops/is_floating_point_ops.h>
#include <ATen/ops/is_conj_native.h>
#include <ATen/ops/is_conj_ops.h>
#include <ATen/ops/isreal_native.h>
#include <ATen/ops/isreal_ops.h>
#include <ATen/ops/is_nonzero_native.h>
#include <ATen/ops/is_nonzero_ops.h>
#include <ATen/ops/layer_norm_native.h>
#include <ATen/ops/layer_norm_ops.h>
#include <ATen/ops/_cslt_sparse_mm_native.h>
#include <ATen/ops/_cslt_sparse_mm_ops.h>
#include <ATen/ops/_cslt_sparse_mm_search_native.h>
#include <ATen/ops/_cslt_sparse_mm_search_ops.h>
#include <ATen/ops/_sparse_semi_structured_linear_native.h>
#include <ATen/ops/_sparse_semi_structured_linear_ops.h>
#include <ATen/ops/_mixed_dtypes_linear_native.h>
#include <ATen/ops/_mixed_dtypes_linear_ops.h>
#include <ATen/ops/fbgemm_pack_gemm_matrix_fp16_native.h>
#include <ATen/ops/fbgemm_pack_gemm_matrix_fp16_ops.h>
#include <ATen/ops/fbgemm_pack_quantized_matrix_native.h>
#include <ATen/ops/fbgemm_pack_quantized_matrix_ops.h>
#include <ATen/ops/fbgemm_pack_quantized_matrix_native.h>
#include <ATen/ops/fbgemm_pack_quantized_matrix_ops.h>
#include <ATen/ops/value_selecting_reduction_backward_native.h>
#include <ATen/ops/value_selecting_reduction_backward_ops.h>
#include <ATen/ops/max_pool1d_native.h>
#include <ATen/ops/max_pool1d_ops.h>
#include <ATen/ops/_convert_weight_to_int4pack_native.h>
#include <ATen/ops/_convert_weight_to_int4pack_ops.h>
#include <ATen/ops/pairwise_distance_native.h>
#include <ATen/ops/pairwise_distance_ops.h>
#include <ATen/ops/pdist_native.h>
#include <ATen/ops/pdist_ops.h>
#include <ATen/ops/_nested_select_backward_native.h>
#include <ATen/ops/_nested_select_backward_ops.h>
#include <ATen/ops/size_native.h>
#include <ATen/ops/size_ops.h>
#include <ATen/ops/size_native.h>
#include <ATen/ops/size_ops.h>
#include <ATen/ops/sym_storage_offset_native.h>
#include <ATen/ops/sym_storage_offset_ops.h>
#include <ATen/ops/smm_native.h>
#include <ATen/ops/smm_ops.h>
#include <ATen/ops/stride_native.h>
#include <ATen/ops/stride_ops.h>
#include <ATen/ops/stride_native.h>
#include <ATen/ops/stride_ops.h>
#include <ATen/ops/sum_to_size_native.h>
#include <ATen/ops/sum_to_size_ops.h>
#include <ATen/ops/std_native.h>
#include <ATen/ops/std_ops.h>
#include <ATen/ops/one_hot_native.h>
#include <ATen/ops/one_hot_ops.h>
#include <ATen/ops/fliplr_native.h>
#include <ATen/ops/fliplr_ops.h>
#include <ATen/ops/triplet_margin_loss_native.h>
#include <ATen/ops/triplet_margin_loss_ops.h>
#include <ATen/ops/type_as_native.h>
#include <ATen/ops/type_as_ops.h>
#include <ATen/ops/var_native.h>
#include <ATen/ops/var_ops.h>
#include <ATen/ops/_sparse_log_softmax_native.h>
#include <ATen/ops/_sparse_log_softmax_ops.h>
#include <ATen/ops/_sparse_log_softmax_native.h>
#include <ATen/ops/_sparse_log_softmax_ops.h>
#include <ATen/ops/sparse_csc_tensor_native.h>
#include <ATen/ops/sparse_csc_tensor_ops.h>
#include <ATen/ops/sparse_csc_tensor_native.h>
#include <ATen/ops/sparse_csc_tensor_ops.h>
#include <ATen/ops/_sparse_bsr_tensor_unsafe_native.h>
#include <ATen/ops/_sparse_bsr_tensor_unsafe_ops.h>
#include <ATen/ops/_validate_sparse_compressed_tensor_args_native.h>
#include <ATen/ops/_validate_sparse_compressed_tensor_args_ops.h>
#include <ATen/ops/_validate_sparse_csr_tensor_args_native.h>
#include <ATen/ops/_validate_sparse_csr_tensor_args_ops.h>
#include <ATen/ops/_validate_sparse_csc_tensor_args_native.h>
#include <ATen/ops/_validate_sparse_csc_tensor_args_ops.h>
#include <ATen/ops/_to_cpu_native.h>
#include <ATen/ops/_to_cpu_ops.h>
#include <ATen/ops/_dimI_native.h>
#include <ATen/ops/_dimI_ops.h>
#include <ATen/ops/q_per_channel_axis_native.h>
#include <ATen/ops/q_per_channel_axis_ops.h>
#include <ATen/ops/fake_quantize_per_tensor_affine_cachemask_backward_native.h>
#include <ATen/ops/fake_quantize_per_tensor_affine_cachemask_backward_ops.h>
#include <ATen/ops/fake_quantize_per_channel_affine_cachemask_backward_native.h>
#include <ATen/ops/fake_quantize_per_channel_affine_cachemask_backward_ops.h>
#include <ATen/ops/rnn_tanh_cell_native.h>
#include <ATen/ops/rnn_tanh_cell_ops.h>
#include <ATen/ops/set_native.h>
#include <ATen/ops/set_ops.h>
#include <ATen/ops/masked_scatter_backward_native.h>
#include <ATen/ops/masked_scatter_backward_ops.h>
#include <ATen/ops/xor_native.h>
#include <ATen/ops/xor_ops.h>
#include <ATen/ops/xor_native.h>
#include <ATen/ops/xor_ops.h>
#include <ATen/ops/xor_native.h>
#include <ATen/ops/xor_ops.h>
#include <ATen/ops/xor_native.h>
#include <ATen/ops/xor_ops.h>
#include <ATen/ops/trace_backward_native.h>
#include <ATen/ops/trace_backward_ops.h>
#include <ATen/ops/masked_select_backward_native.h>
#include <ATen/ops/masked_select_backward_ops.h>
#include <ATen/ops/argwhere_native.h>
#include <ATen/ops/argwhere_ops.h>
#include <ATen/ops/cross_entropy_loss_native.h>
#include <ATen/ops/cross_entropy_loss_ops.h>
#include <ATen/ops/equal_native.h>
#include <ATen/ops/equal_ops.h>
#include <ATen/ops/adaptive_avg_pool3d_backward_native.h>
#include <ATen/ops/adaptive_avg_pool3d_backward_ops.h>
#include <ATen/ops/_pad_enum_native.h>
#include <ATen/ops/_pad_enum_ops.h>
#include <ATen/ops/pad_native.h>
#include <ATen/ops/pad_ops.h>
#include <ATen/ops/upsample_nearest2d_native.h>
#include <ATen/ops/upsample_nearest2d_ops.h>
#include <ATen/ops/_upsample_nearest_exact2d_native.h>
#include <ATen/ops/_upsample_nearest_exact2d_ops.h>
#include <ATen/ops/_add_batch_dim_native.h>
#include <ATen/ops/_add_batch_dim_ops.h>
#include <ATen/ops/_remove_batch_dim_native.h>
#include <ATen/ops/_remove_batch_dim_ops.h>
#include <ATen/ops/fft_ifftshift_native.h>
#include <ATen/ops/fft_ifftshift_ops.h>
#include <ATen/ops/_test_serialization_subcmul_native.h>
#include <ATen/ops/_test_serialization_subcmul_ops.h>
#include <ATen/ops/unflatten_dense_tensors_native.h>
#include <ATen/ops/unflatten_dense_tensors_ops.h>
#include <ATen/ops/_scaled_dot_product_flash_attention_native.h>
#include <ATen/ops/_scaled_dot_product_flash_attention_ops.h>
#include <ATen/ops/_scaled_dot_product_efficient_attention_native.h>
#include <ATen/ops/_scaled_dot_product_efficient_attention_ops.h>
#include <ATen/ops/_flash_attention_backward_native.h>
#include <ATen/ops/_flash_attention_backward_ops.h>
#include <ATen/ops/_efficient_attention_forward_native.h>
#include <ATen/ops/_efficient_attention_forward_ops.h>
#include <ATen/ops/_propagate_xla_data_native.h>
#include <ATen/ops/_propagate_xla_data_ops.h>
#endif

namespace at {
namespace functionalization {

// This keyset is used by functionalization when it calls into meta kernels
// to accurately propagate stride metadata.
// Exclude any modes: the purpose of calling into meta kernels is only as an implementation
// detail to perform shape inference, and we don't want any modal keys to run.
// Specifically, we want to prevent functionalization and Python modes from running.
constexpr auto exclude_keys_for_meta_dispatch =
    c10::functorch_transforms_ks |
    c10::DispatchKeySet({
        c10::DispatchKey::FuncTorchDynamicLayerBackMode,
        c10::DispatchKey::FuncTorchDynamicLayerFrontMode,
        c10::DispatchKey::Python,
        c10::DispatchKey::PreDispatch,

    });

// Helper around at::has_internal_overlap.
// The ATen util is used in hot-path eager mode: it's always fast,
// but might return TOO_HARD sometimes.
// During functionalization, we're ok taking a bit longer
// to detect memory overlap.
inline bool has_internal_overlap_helper(const at::Tensor t) {
  auto has_overlap = at::has_internal_overlap(t);
  if (has_overlap == at::MemOverlap::Yes) return true;
  if (has_overlap == at::MemOverlap::No) return false;
  return false;
}


inline Tensor to_meta(const Tensor& t) {
    if (!t.defined()) return t;
    return at::native::empty_strided_meta_symint(t.sym_sizes(), t.sym_strides(),
/*dtype=*/c10::make_optional(t.scalar_type()), /*layout=*/c10::make_optional(t.layout()),
/*device=*/c10::make_optional(c10::Device(kMeta)), /*pin_memory=*/c10::nullopt);
}

inline c10::optional<Tensor> to_meta(const c10::optional<Tensor>& t) {
  if (t.has_value()) {
    return c10::make_optional<Tensor>(to_meta(*t));
  }
  return c10::nullopt;
}

inline std::vector<Tensor> to_meta(at::ITensorListRef t_list) {
  std::vector<Tensor> outputs;
  outputs.reserve(t_list.size());
  for (const auto& tensor : t_list) {
    outputs.push_back(to_meta(tensor));
  }
  return outputs;
}

inline c10::List<Tensor> to_meta(const c10::List<Tensor>& t_list) {
  c10::List<Tensor> outputs;
  outputs.reserve(t_list.size());
  for (const auto i : c10::irange(t_list.size())) {
    outputs.push_back(to_meta(t_list[i]));
  }
  return outputs;
}

inline c10::List<c10::optional<Tensor>> to_meta(const c10::List<c10::optional<Tensor>>& t_list) {
  c10::List<c10::optional<Tensor>> outputs;
  outputs.reserve(t_list.size());
  for (const auto i : c10::irange(t_list.size())) {
    outputs.push_back(to_meta(t_list[i]));
  }
  return outputs;
}



    at::Tensor & _new_zeros_with_same_feature_meta_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, int64_t self_num_batch_dims, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_new_zeros_with_same_feature_meta_out::call(self_meta, other_meta, self_num_batch_dims, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_new_zeros_with_same_feature_meta_out::call(self_, other_, self_num_batch_dims, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_new_zeros_with_same_feature_meta::call(self_, other_, self_num_batch_dims);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _cudnn_init_dropout_state_out_out(c10::DispatchKeySet dispatchKeySet, double dropout, bool train, int64_t dropout_seed, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_cudnn_init_dropout_state_out::call(dropout, train, dropout_seed, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_cudnn_init_dropout_state_out::call(dropout, train, dropout_seed, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_cudnn_init_dropout_state::call(dropout, train, dropout_seed, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & angle_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::angle_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::angle_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::angle::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sgn_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sgn_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sgn_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sgn::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sgn_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sgn_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sgn_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sgn::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & _add_relu_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_add_relu_out::call(self_meta, other_meta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_add_relu_out::call(self_, other_, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_add_relu_Tensor::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _add_relu__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_add_relu__Tensor::call(self_meta, other_meta, alpha);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_add_relu__Tensor::call(self_, other_, alpha);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_add_relu_Tensor::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & _add_relu_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_add_relu_Scalar_out::call(self_meta, other, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_add_relu_Scalar_out::call(self_, other, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_add_relu_Scalar::call(self_, other, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _add_relu__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_add_relu__Scalar::call(self_meta, other, alpha);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_add_relu__Scalar::call(self_, other, alpha);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_add_relu_Scalar::call(self_, other, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & _test_functorch_fallback_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_test_functorch_fallback_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_test_functorch_fallback_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_test_functorch_fallback::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & any_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::any_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::any_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::any_dim::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & any_out_dims_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::any_dims_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::any_dims_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::any_dims::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & any_out_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::any_dimname_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::any_dimname_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::any_dimname::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & argmin_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::argmin_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::argmin_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::argmin::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & acosh_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::acosh_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::acosh_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::acosh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & acosh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::acosh_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::acosh_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::acosh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & arcsin_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arcsin_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arcsin_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arcsin::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & arcsin_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arcsin_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arcsin_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arcsin::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & arctan_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arctan_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arctan_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arctan::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & arctan_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arctan_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arctan_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arctan::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bincount_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & weights, int64_t minlength, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weights_meta = to_meta(weights);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bincount_out::call(self_meta, weights_meta, minlength, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::optional<at::Tensor> weights_;
      if (at::functionalization::impl::isFunctionalTensor(weights)) {
        at::functionalization::impl::sync(weights);
        weights_ = at::functionalization::impl::from_functional_tensor(weights);
      } else {
        weights_ = weights;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weights))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bincount_out::call(self_, weights_, minlength, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bincount::call(self_, weights_, minlength);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & copysign_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::copysign_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::copysign_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::copysign_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & copysign__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::copysign__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::copysign__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::copysign_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & copysign_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::copysign_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::copysign_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::copysign_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & copysign__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::copysign__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::copysign__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::copysign_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & logical_or_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logical_or_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logical_or_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logical_or::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & logical_or_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logical_or_::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logical_or_::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logical_or::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & cat_out_out(c10::DispatchKeySet dispatchKeySet, const at::ITensorListRef & tensors, int64_t dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cat_out::call(tensors_meta, dim, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = {tensors.begin(), tensors.end()};
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cat_out::call(tensors_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cat::call(tensors_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cat_out_names_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cat_names_out::call(tensors_meta, dim, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cat_names_out::call(tensors_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cat_names::call(tensors_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & concat_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::concat_out::call(tensors_meta, dim, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::concat_out::call(tensors_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::concat::call(tensors_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & concat_out_names_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::concat_names_out::call(tensors_meta, dim, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::concat_names_out::call(tensors_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::concat_names::call(tensors_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & ceil_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ceil_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ceil_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ceil::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & ceil_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ceil_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ceil_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ceil::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & polar_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & abs, const at::Tensor & angle, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto abs_meta = to_meta(abs);
        auto angle_meta = to_meta(angle);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::polar_out::call(abs_meta, angle_meta, out_meta);
      }
      
      at::Tensor abs_;
      if (at::functionalization::impl::isFunctionalTensor(abs)) {
        at::functionalization::impl::sync(abs);
        abs_ = at::functionalization::impl::from_functional_tensor(abs);
      } else {
        abs_ = abs;
      }
      
      at::Tensor angle_;
      if (at::functionalization::impl::isFunctionalTensor(angle)) {
        at::functionalization::impl::sync(angle);
        angle_ = at::functionalization::impl::from_functional_tensor(angle);
      } else {
        angle_ = angle;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || abs.device().type() == c10::DeviceType::XLA || angle.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(abs) || at::functionalization::impl::isFunctionalTensor(angle))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::polar_out::call(abs_, angle_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::polar::call(abs_, angle_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & convolution_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::convolution_out::call(input_meta, weight_meta, bias_meta, stride, padding, dilation, transposed, output_padding, groups, out_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::convolution_out::call(input_, weight_, bias_, stride, padding, dilation, transposed, output_padding, groups, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::convolution::call(input_, weight_, bias_, stride, padding, dilation, transposed, output_padding, groups);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & convolution_overrideable_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::convolution_overrideable_out::call(input_meta, weight_meta, bias_meta, stride, padding, dilation, transposed, output_padding, groups, out_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::convolution_overrideable_out::call(input_, weight_, bias_, stride, padding, dilation, transposed, output_padding, groups, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::convolution_overrideable::call(input_, weight_, bias_, stride, padding, dilation, transposed, output_padding, groups);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> convolution_backward_overrideable_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::convolution_backward_overrideable_out::call(grad_output_meta, input_meta, weight_meta, stride, padding, dilation, transposed, output_padding, groups, output_mask, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || input.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::convolution_backward_overrideable_out::call(grad_output_, input_, weight_, stride, padding, dilation, transposed, output_padding, groups, output_mask, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::convolution_backward_overrideable::call(grad_output_, input_, weight_, stride, padding, dilation, transposed, output_padding, groups, output_mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & cudnn_affine_grid_generator_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto theta_meta = to_meta(theta);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cudnn_affine_grid_generator_out::call(theta_meta, N, C, H, W, out_meta);
      }
      
      at::Tensor theta_;
      if (at::functionalization::impl::isFunctionalTensor(theta)) {
        at::functionalization::impl::sync(theta);
        theta_ = at::functionalization::impl::from_functional_tensor(theta);
      } else {
        theta_ = theta;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || theta.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(theta))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cudnn_affine_grid_generator_out::call(theta_, N, C, H, W, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cudnn_affine_grid_generator::call(theta_, N, C, H, W);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> cudnn_batch_norm_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var, double epsilon, const at::Tensor & reserveSpace, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto grad_output_meta = to_meta(grad_output);
        auto weight_meta = to_meta(weight);
        auto running_mean_meta = to_meta(running_mean);
        auto running_var_meta = to_meta(running_var);
        auto save_mean_meta = to_meta(save_mean);
        auto save_var_meta = to_meta(save_var);
        auto reserveSpace_meta = to_meta(reserveSpace);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cudnn_batch_norm_backward_out::call(input_meta, grad_output_meta, weight_meta, running_mean_meta, running_var_meta, save_mean_meta, save_var_meta, epsilon, reserveSpace_meta, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> running_mean_;
      if (at::functionalization::impl::isFunctionalTensor(running_mean)) {
        at::functionalization::impl::sync(running_mean);
        running_mean_ = at::functionalization::impl::from_functional_tensor(running_mean);
      } else {
        running_mean_ = running_mean;
      }
      
      c10::optional<at::Tensor> running_var_;
      if (at::functionalization::impl::isFunctionalTensor(running_var)) {
        at::functionalization::impl::sync(running_var);
        running_var_ = at::functionalization::impl::from_functional_tensor(running_var);
      } else {
        running_var_ = running_var;
      }
      
      c10::optional<at::Tensor> save_mean_;
      if (at::functionalization::impl::isFunctionalTensor(save_mean)) {
        at::functionalization::impl::sync(save_mean);
        save_mean_ = at::functionalization::impl::from_functional_tensor(save_mean);
      } else {
        save_mean_ = save_mean;
      }
      
      c10::optional<at::Tensor> save_var_;
      if (at::functionalization::impl::isFunctionalTensor(save_var)) {
        at::functionalization::impl::sync(save_var);
        save_var_ = at::functionalization::impl::from_functional_tensor(save_var);
      } else {
        save_var_ = save_var;
      }
      
      at::Tensor reserveSpace_;
      if (at::functionalization::impl::isFunctionalTensor(reserveSpace)) {
        at::functionalization::impl::sync(reserveSpace);
        reserveSpace_ = at::functionalization::impl::from_functional_tensor(reserveSpace);
      } else {
        reserveSpace_ = reserveSpace;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || grad_output.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA || reserveSpace.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(running_mean) || at::functionalization::impl::isFunctionalTensor(running_var) || at::functionalization::impl::isFunctionalTensor(save_mean) || at::functionalization::impl::isFunctionalTensor(save_var) || at::functionalization::impl::isFunctionalTensor(reserveSpace))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::cudnn_batch_norm_backward_out::call(input_, grad_output_, weight_, running_mean_, running_var_, save_mean_, save_var_, epsilon, reserveSpace_, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cudnn_batch_norm_backward::call(input_, grad_output_, weight_, running_mean_, running_var_, save_mean_, save_var_, epsilon, reserveSpace_);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & cudnn_convolution_relu_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cudnn_convolution_relu_out::call(self_meta, weight_meta, bias_meta, stride, padding, dilation, groups, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cudnn_convolution_relu_out::call(self_, weight_, bias_, stride, padding, dilation, groups, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cudnn_convolution_relu::call(self_, weight_, bias_, stride, padding, dilation, groups);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cudnn_convolution_add_relu_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const at::Tensor & z, const c10::optional<at::Scalar> & alpha, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto z_meta = to_meta(z);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cudnn_convolution_add_relu_out::call(self_meta, weight_meta, z_meta, alpha, bias_meta, stride, padding, dilation, groups, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor z_;
      if (at::functionalization::impl::isFunctionalTensor(z)) {
        at::functionalization::impl::sync(z);
        z_ = at::functionalization::impl::from_functional_tensor(z);
      } else {
        z_ = z;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA || z.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(z) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cudnn_convolution_add_relu_out::call(self_, weight_, z_, alpha, bias_, stride, padding, dilation, groups, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cudnn_convolution_add_relu::call(self_, weight_, z_, alpha, bias_, stride, padding, dilation, groups);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> cummax_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cummax_out::call(self_meta, dim, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::cummax_out::call(self_, dim, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cummax::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> cummax_out_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cummax_dimname_out::call(self_meta, dim, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::cummax_dimname_out::call(self_, dim, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cummax_dimname::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    at::Tensor & diff_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto prepend_meta = to_meta(prepend);
        auto append_meta = to_meta(append);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::diff_out::call(self_meta, n, dim, prepend_meta, append_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::optional<at::Tensor> prepend_;
      if (at::functionalization::impl::isFunctionalTensor(prepend)) {
        at::functionalization::impl::sync(prepend);
        prepend_ = at::functionalization::impl::from_functional_tensor(prepend);
      } else {
        prepend_ = prepend;
      }
      
      c10::optional<at::Tensor> append_;
      if (at::functionalization::impl::isFunctionalTensor(append)) {
        at::functionalization::impl::sync(append);
        append_ = at::functionalization::impl::from_functional_tensor(append);
      } else {
        append_ = append;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(prepend) || at::functionalization::impl::isFunctionalTensor(append))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::diff_out::call(self_, n, dim, prepend_, append_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::diff::call(self_, n, dim, prepend_, append_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & embedding_renorm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, double max_norm, double norm_type, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::embedding_renorm_out::call(self_meta, indices_meta, max_norm, norm_type, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::embedding_renorm_out::call(self_, indices_, max_norm, norm_type, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::embedding_renorm::call(self_, indices_, max_norm, norm_type);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & embedding_renorm_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & indices, double max_norm, double norm_type) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::embedding_renorm_::call(self_meta, indices_meta, max_norm, norm_type);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::embedding_renorm_::call(self_, indices_, max_norm, norm_type);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::embedding_renorm::call(self_, indices_, max_norm, norm_type);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> _embedding_bag_forward_only_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto weight_meta = to_meta(weight);
        auto indices_meta = to_meta(indices);
        auto offsets_meta = to_meta(offsets);
        auto per_sample_weights_meta = to_meta(per_sample_weights);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        auto out3_meta = to_meta(out3);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_embedding_bag_forward_only_out::call(weight_meta, indices_meta, offsets_meta, scale_grad_by_freq, mode, sparse, per_sample_weights_meta, include_last_offset, padding_idx, out0_meta, out1_meta, out2_meta, out3_meta);
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor offsets_;
      if (at::functionalization::impl::isFunctionalTensor(offsets)) {
        at::functionalization::impl::sync(offsets);
        offsets_ = at::functionalization::impl::from_functional_tensor(offsets);
      } else {
        offsets_ = offsets;
      }
      
      c10::optional<at::Tensor> per_sample_weights_;
      if (at::functionalization::impl::isFunctionalTensor(per_sample_weights)) {
        at::functionalization::impl::sync(per_sample_weights);
        per_sample_weights_ = at::functionalization::impl::from_functional_tensor(per_sample_weights);
      } else {
        per_sample_weights_ = per_sample_weights;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      
      at::Tensor out3_;
      if (at::functionalization::impl::isFunctionalTensor(out3)) {
        at::functionalization::impl::sync(out3);
        out3_ = at::functionalization::impl::from_functional_tensor(out3);
      } else {
        out3_ = out3;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2) && at::functionalization::impl::isFunctionalTensor(out3))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || weight.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA || offsets.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(offsets) || at::functionalization::impl::isFunctionalTensor(per_sample_weights))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_embedding_bag_forward_only_out::call(weight_, indices_, offsets_, scale_grad_by_freq, mode, sparse, per_sample_weights_, include_last_offset, padding_idx, out0_, out1_, out2_, out3_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_embedding_bag_forward_only::call(weight_, indices_, offsets_, scale_grad_by_freq, mode, sparse, per_sample_weights_, include_last_offset, padding_idx);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
  at::functionalization::impl::propagate_xla_data(out3, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(out3, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(out3);
  at::functionalization::impl::sync(out3);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3);
      }
    }

    at::Tensor & _embedding_bag_dense_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, c10::SymInt num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_meta = to_meta(grad);
        auto indices_meta = to_meta(indices);
        auto offset2bag_meta = to_meta(offset2bag);
        auto bag_size_meta = to_meta(bag_size);
        auto maximum_indices_meta = to_meta(maximum_indices);
        auto per_sample_weights_meta = to_meta(per_sample_weights);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_embedding_bag_dense_backward_out::call(grad_meta, indices_meta, offset2bag_meta, bag_size_meta, maximum_indices_meta, num_weights, scale_grad_by_freq, mode, per_sample_weights_meta, padding_idx, out_meta);
      }
      
      at::Tensor grad_;
      if (at::functionalization::impl::isFunctionalTensor(grad)) {
        at::functionalization::impl::sync(grad);
        grad_ = at::functionalization::impl::from_functional_tensor(grad);
      } else {
        grad_ = grad;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor offset2bag_;
      if (at::functionalization::impl::isFunctionalTensor(offset2bag)) {
        at::functionalization::impl::sync(offset2bag);
        offset2bag_ = at::functionalization::impl::from_functional_tensor(offset2bag);
      } else {
        offset2bag_ = offset2bag;
      }
      
      at::Tensor bag_size_;
      if (at::functionalization::impl::isFunctionalTensor(bag_size)) {
        at::functionalization::impl::sync(bag_size);
        bag_size_ = at::functionalization::impl::from_functional_tensor(bag_size);
      } else {
        bag_size_ = bag_size;
      }
      
      at::Tensor maximum_indices_;
      if (at::functionalization::impl::isFunctionalTensor(maximum_indices)) {
        at::functionalization::impl::sync(maximum_indices);
        maximum_indices_ = at::functionalization::impl::from_functional_tensor(maximum_indices);
      } else {
        maximum_indices_ = maximum_indices;
      }
      
      c10::optional<at::Tensor> per_sample_weights_;
      if (at::functionalization::impl::isFunctionalTensor(per_sample_weights)) {
        at::functionalization::impl::sync(per_sample_weights);
        per_sample_weights_ = at::functionalization::impl::from_functional_tensor(per_sample_weights);
      } else {
        per_sample_weights_ = per_sample_weights;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA || offset2bag.device().type() == c10::DeviceType::XLA || bag_size.device().type() == c10::DeviceType::XLA || maximum_indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad) || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(offset2bag) || at::functionalization::impl::isFunctionalTensor(bag_size) || at::functionalization::impl::isFunctionalTensor(maximum_indices) || at::functionalization::impl::isFunctionalTensor(per_sample_weights))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_embedding_bag_dense_backward_out::call(grad_, indices_, offset2bag_, bag_size_, maximum_indices_, num_weights, scale_grad_by_freq, mode, per_sample_weights_, padding_idx, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_embedding_bag_dense_backward::call(grad_, indices_, offset2bag_, bag_size_, maximum_indices_, num_weights, scale_grad_by_freq, mode, per_sample_weights_, padding_idx);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & empty_out_names_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::empty_names_out::call(size, names, memory_format, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::empty_names_out::call(size, names, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::empty_names::call(size, names, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & empty_out_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::empty_out::call(size, memory_format, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::empty_out::call(size, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::empty_memory_format::call(size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & empty_like_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::empty_like_out::call(self_meta, memory_format, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::empty_like_out::call(self_, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::empty_like::call(self_, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & erf_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::erf_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::erf_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::erf::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & erf_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::erf_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::erf_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::erf::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & erfc_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::erfc_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::erfc_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::erfc::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & erfc_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::erfc_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::erfc_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::erfc::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & eye_out_out(c10::DispatchKeySet dispatchKeySet, c10::SymInt n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::eye_out::call(n, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::eye_out::call(n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::eye::call(n, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & eye_out_m_out(c10::DispatchKeySet dispatchKeySet, c10::SymInt n, c10::SymInt m, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::eye_m_out::call(n, m, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::eye_m_out::call(n, m, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::eye_m::call(n, m, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & gcd_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::gcd_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::gcd_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::gcd::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & gcd_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::gcd_::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::gcd_::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::gcd::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> grid_sampler_2d_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, ::std::array<bool,2> output_mask, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto input_meta = to_meta(input);
        auto grid_meta = to_meta(grid);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::grid_sampler_2d_backward_out::call(grad_output_meta, input_meta, grid_meta, interpolation_mode, padding_mode, align_corners, output_mask, out0_meta, out1_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor grid_;
      if (at::functionalization::impl::isFunctionalTensor(grid)) {
        at::functionalization::impl::sync(grid);
        grid_ = at::functionalization::impl::from_functional_tensor(grid);
      } else {
        grid_ = grid;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || input.device().type() == c10::DeviceType::XLA || grid.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(grid))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::grid_sampler_2d_backward_out::call(grad_output_, input_, grid_, interpolation_mode, padding_mode, align_corners, output_mask, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::grid_sampler_2d_backward::call(grad_output_, input_, grid_, interpolation_mode, padding_mode, align_corners, output_mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_group_norm_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_out_meta = to_meta(grad_out);
        auto input_meta = to_meta(input);
        auto mean_meta = to_meta(mean);
        auto rstd_meta = to_meta(rstd);
        auto weight_meta = to_meta(weight);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::native_group_norm_backward_out::call(grad_out_meta, input_meta, mean_meta, rstd_meta, weight_meta, N, C, HxW, group, output_mask, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor grad_out_;
      if (at::functionalization::impl::isFunctionalTensor(grad_out)) {
        at::functionalization::impl::sync(grad_out);
        grad_out_ = at::functionalization::impl::from_functional_tensor(grad_out);
      } else {
        grad_out_ = grad_out;
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor mean_;
      if (at::functionalization::impl::isFunctionalTensor(mean)) {
        at::functionalization::impl::sync(mean);
        mean_ = at::functionalization::impl::from_functional_tensor(mean);
      } else {
        mean_ = mean;
      }
      
      at::Tensor rstd_;
      if (at::functionalization::impl::isFunctionalTensor(rstd)) {
        at::functionalization::impl::sync(rstd);
        rstd_ = at::functionalization::impl::from_functional_tensor(rstd);
      } else {
        rstd_ = rstd;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_out.device().type() == c10::DeviceType::XLA || input.device().type() == c10::DeviceType::XLA || mean.device().type() == c10::DeviceType::XLA || rstd.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_out) || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(mean) || at::functionalization::impl::isFunctionalTensor(rstd) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::native_group_norm_backward_out::call(grad_out_, input_, mean_, rstd_, weight_, N, C, HxW, group, output_mask, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::native_group_norm_backward::call(grad_out_, input_, mean_, rstd_, weight_, N, C, HxW, group, output_mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & _fft_r2c_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fft_r2c_out::call(self_meta, dim, normalization, onesided, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_fft_r2c_out::call(self_, dim, normalization, onesided, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fft_r2c::call(self_, dim, normalization, onesided);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & index_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_Tensor_out::call(self_meta, indices_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::List<c10::optional<at::Tensor>> indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_Tensor_out::call(self_, indices_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_Tensor::call(self_, indices_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & index_put_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto values_meta = to_meta(values);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_put_out::call(self_meta, indices_meta, values_meta, accumulate, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::List<c10::optional<at::Tensor>> indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || values.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(values))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_put_out::call(self_, indices_, values_, accumulate, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_put::call(self_, indices_, values_, accumulate);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & index_put_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto values_meta = to_meta(values);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_put_::call(self_meta, indices_meta, values_meta, accumulate);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::List<c10::optional<at::Tensor>> indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || values.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(values))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_put_::call(self_, indices_, values_, accumulate);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_put::call(self_, indices_, values_, accumulate);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linear_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grad_output_meta = to_meta(grad_output);
        auto weight_meta = to_meta(weight);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linear_backward_out::call(self_meta, grad_output_meta, weight_meta, output_mask, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || grad_output.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::linear_backward_out::call(self_, grad_output_, weight_, output_mask, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linear_backward::call(self_, grad_output_, weight_, output_mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> mkldnn_linear_backward_weights_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, bool bias_defined, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_linear_backward_weights_out::call(grad_output_meta, input_meta, weight_meta, bias_defined, out0_meta, out1_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || input.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::mkldnn_linear_backward_weights_out::call(grad_output_, input_, weight_, bias_defined, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_linear_backward_weights::call(grad_output_, input_, weight_, bias_defined);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & logaddexp_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logaddexp_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logaddexp_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logaddexp::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _logcumsumexp_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_logcumsumexp_out::call(self_meta, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_logcumsumexp_out::call(self_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_logcumsumexp::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & logcumsumexp_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logcumsumexp_out::call(self_meta, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logcumsumexp_out::call(self_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logcumsumexp::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & logcumsumexp_out_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logcumsumexp_dimname_out::call(self_meta, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logcumsumexp_dimname_out::call(self_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logcumsumexp_dimname::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> matmul_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, const at::Tensor & other, ::std::array<bool,2> mask, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_meta = to_meta(grad);
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::matmul_backward_out::call(grad_meta, self_meta, other_meta, mask, out0_meta, out1_meta);
      }
      
      at::Tensor grad_;
      if (at::functionalization::impl::isFunctionalTensor(grad)) {
        at::functionalization::impl::sync(grad);
        grad_ = at::functionalization::impl::from_functional_tensor(grad);
      } else {
        grad_ = grad;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::matmul_backward_out::call(grad_, self_, other_, mask, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::matmul_backward::call(grad_, self_, other_, mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & median_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::median_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::median_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::median::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> median_out_dim_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::median_dim_values::call(self_meta, dim, keepdim, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::median_dim_values::call(self_, dim, keepdim, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::median_dim::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> median_out_names_dim_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::median_names_dim_values::call(self_meta, dim, keepdim, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::median_names_dim_values::call(self_, dim, keepdim, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::median_names_dim::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    at::Tensor & amin_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::amin_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::amin_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::amin::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> mkldnn_rnn_layer_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight1, const at::Tensor & weight2, const at::Tensor & weight3, const at::Tensor & weight4, const at::Tensor & hx_, const at::Tensor & cx_tmp, const at::Tensor & output, const at::Tensor & hy_, const at::Tensor & cy_, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, bool reverse, int64_t mode, int64_t hidden_size, int64_t num_layers, bool has_biases, bool train, bool bidirectional, at::IntArrayRef batch_sizes, bool batch_first, const at::Tensor & workspace, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3, at::Tensor & out4, at::Tensor & out5, at::Tensor & out6) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight1_meta = to_meta(weight1);
        auto weight2_meta = to_meta(weight2);
        auto weight3_meta = to_meta(weight3);
        auto weight4_meta = to_meta(weight4);
        auto hx__meta = to_meta(hx_);
        auto cx_tmp_meta = to_meta(cx_tmp);
        auto output_meta = to_meta(output);
        auto hy__meta = to_meta(hy_);
        auto cy__meta = to_meta(cy_);
        auto grad_output_meta = to_meta(grad_output);
        auto grad_hy_meta = to_meta(grad_hy);
        auto grad_cy_meta = to_meta(grad_cy);
        auto workspace_meta = to_meta(workspace);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        auto out3_meta = to_meta(out3);
        auto out4_meta = to_meta(out4);
        auto out5_meta = to_meta(out5);
        auto out6_meta = to_meta(out6);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_rnn_layer_backward_out::call(input_meta, weight1_meta, weight2_meta, weight3_meta, weight4_meta, hx__meta, cx_tmp_meta, output_meta, hy__meta, cy__meta, grad_output_meta, grad_hy_meta, grad_cy_meta, reverse, mode, hidden_size, num_layers, has_biases, train, bidirectional, batch_sizes, batch_first, workspace_meta, out0_meta, out1_meta, out2_meta, out3_meta, out4_meta, out5_meta, out6_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor weight1_;
      if (at::functionalization::impl::isFunctionalTensor(weight1)) {
        at::functionalization::impl::sync(weight1);
        weight1_ = at::functionalization::impl::from_functional_tensor(weight1);
      } else {
        weight1_ = weight1;
      }
      
      at::Tensor weight2_;
      if (at::functionalization::impl::isFunctionalTensor(weight2)) {
        at::functionalization::impl::sync(weight2);
        weight2_ = at::functionalization::impl::from_functional_tensor(weight2);
      } else {
        weight2_ = weight2;
      }
      
      at::Tensor weight3_;
      if (at::functionalization::impl::isFunctionalTensor(weight3)) {
        at::functionalization::impl::sync(weight3);
        weight3_ = at::functionalization::impl::from_functional_tensor(weight3);
      } else {
        weight3_ = weight3;
      }
      
      at::Tensor weight4_;
      if (at::functionalization::impl::isFunctionalTensor(weight4)) {
        at::functionalization::impl::sync(weight4);
        weight4_ = at::functionalization::impl::from_functional_tensor(weight4);
      } else {
        weight4_ = weight4;
      }
      
      at::Tensor hx__;
      if (at::functionalization::impl::isFunctionalTensor(hx_)) {
        at::functionalization::impl::sync(hx_);
        hx__ = at::functionalization::impl::from_functional_tensor(hx_);
      } else {
        hx__ = hx_;
      }
      
      at::Tensor cx_tmp_;
      if (at::functionalization::impl::isFunctionalTensor(cx_tmp)) {
        at::functionalization::impl::sync(cx_tmp);
        cx_tmp_ = at::functionalization::impl::from_functional_tensor(cx_tmp);
      } else {
        cx_tmp_ = cx_tmp;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor hy__;
      if (at::functionalization::impl::isFunctionalTensor(hy_)) {
        at::functionalization::impl::sync(hy_);
        hy__ = at::functionalization::impl::from_functional_tensor(hy_);
      } else {
        hy__ = hy_;
      }
      
      at::Tensor cy__;
      if (at::functionalization::impl::isFunctionalTensor(cy_)) {
        at::functionalization::impl::sync(cy_);
        cy__ = at::functionalization::impl::from_functional_tensor(cy_);
      } else {
        cy__ = cy_;
      }
      
      c10::optional<at::Tensor> grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      c10::optional<at::Tensor> grad_hy_;
      if (at::functionalization::impl::isFunctionalTensor(grad_hy)) {
        at::functionalization::impl::sync(grad_hy);
        grad_hy_ = at::functionalization::impl::from_functional_tensor(grad_hy);
      } else {
        grad_hy_ = grad_hy;
      }
      
      c10::optional<at::Tensor> grad_cy_;
      if (at::functionalization::impl::isFunctionalTensor(grad_cy)) {
        at::functionalization::impl::sync(grad_cy);
        grad_cy_ = at::functionalization::impl::from_functional_tensor(grad_cy);
      } else {
        grad_cy_ = grad_cy;
      }
      
      at::Tensor workspace_;
      if (at::functionalization::impl::isFunctionalTensor(workspace)) {
        at::functionalization::impl::sync(workspace);
        workspace_ = at::functionalization::impl::from_functional_tensor(workspace);
      } else {
        workspace_ = workspace;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      
      at::Tensor out3_;
      if (at::functionalization::impl::isFunctionalTensor(out3)) {
        at::functionalization::impl::sync(out3);
        out3_ = at::functionalization::impl::from_functional_tensor(out3);
      } else {
        out3_ = out3;
      }
      
      at::Tensor out4_;
      if (at::functionalization::impl::isFunctionalTensor(out4)) {
        at::functionalization::impl::sync(out4);
        out4_ = at::functionalization::impl::from_functional_tensor(out4);
      } else {
        out4_ = out4;
      }
      
      at::Tensor out5_;
      if (at::functionalization::impl::isFunctionalTensor(out5)) {
        at::functionalization::impl::sync(out5);
        out5_ = at::functionalization::impl::from_functional_tensor(out5);
      } else {
        out5_ = out5;
      }
      
      at::Tensor out6_;
      if (at::functionalization::impl::isFunctionalTensor(out6)) {
        at::functionalization::impl::sync(out6);
        out6_ = at::functionalization::impl::from_functional_tensor(out6);
      } else {
        out6_ = out6;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2) && at::functionalization::impl::isFunctionalTensor(out3) && at::functionalization::impl::isFunctionalTensor(out4) && at::functionalization::impl::isFunctionalTensor(out5) && at::functionalization::impl::isFunctionalTensor(out6))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || weight1.device().type() == c10::DeviceType::XLA || weight2.device().type() == c10::DeviceType::XLA || weight3.device().type() == c10::DeviceType::XLA || weight4.device().type() == c10::DeviceType::XLA || hx_.device().type() == c10::DeviceType::XLA || cx_tmp.device().type() == c10::DeviceType::XLA || output.device().type() == c10::DeviceType::XLA || hy_.device().type() == c10::DeviceType::XLA || cy_.device().type() == c10::DeviceType::XLA || workspace.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight1) || at::functionalization::impl::isFunctionalTensor(weight2) || at::functionalization::impl::isFunctionalTensor(weight3) || at::functionalization::impl::isFunctionalTensor(weight4) || at::functionalization::impl::isFunctionalTensor(hx_) || at::functionalization::impl::isFunctionalTensor(cx_tmp) || at::functionalization::impl::isFunctionalTensor(output) || at::functionalization::impl::isFunctionalTensor(hy_) || at::functionalization::impl::isFunctionalTensor(cy_) || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(grad_hy) || at::functionalization::impl::isFunctionalTensor(grad_cy) || at::functionalization::impl::isFunctionalTensor(workspace))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::mkldnn_rnn_layer_backward_out::call(input_, weight1_, weight2_, weight3_, weight4_, hx__, cx_tmp_, output_, hy__, cy__, grad_output_, grad_hy_, grad_cy_, reverse, mode, hidden_size, num_layers, has_biases, train, bidirectional, batch_sizes, batch_first, workspace_, out0_, out1_, out2_, out3_, out4_, out5_, out6_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3, out4, out5, out6);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_rnn_layer_backward::call(input_, weight1_, weight2_, weight3_, weight4_, hx__, cx_tmp_, output_, hy__, cy__, grad_output_, grad_hy_, grad_cy_, reverse, mode, hidden_size, num_layers, has_biases, train, bidirectional, batch_sizes, batch_first, workspace_);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
  at::functionalization::impl::propagate_xla_data(out3, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(out3, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(out3);
  at::functionalization::impl::sync(out3);
  at::functionalization::impl::propagate_xla_data(out4, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(out4, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(out4);
  at::functionalization::impl::sync(out4);
  at::functionalization::impl::propagate_xla_data(out5, std::get<5>(tmp_output));
  at::functionalization::impl::replace_(out5, std::get<5>(tmp_output));
  at::functionalization::impl::commit_update(out5);
  at::functionalization::impl::sync(out5);
  at::functionalization::impl::propagate_xla_data(out6, std::get<6>(tmp_output));
  at::functionalization::impl::replace_(out6, std::get<6>(tmp_output));
  at::functionalization::impl::commit_update(out6);
  at::functionalization::impl::sync(out6);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3, out4, out5, out6);
      }
    }

    at::Tensor & miopen_convolution_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::miopen_convolution_out::call(self_meta, weight_meta, bias_meta, padding, stride, dilation, groups, benchmark, deterministic, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::miopen_convolution_out::call(self_, weight_, bias_, padding, stride, dilation, groups, benchmark, deterministic, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::miopen_convolution::call(self_, weight_, bias_, padding, stride, dilation, groups, benchmark, deterministic);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & miopen_depthwise_convolution_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::miopen_depthwise_convolution_out::call(self_meta, weight_meta, bias_meta, padding, stride, dilation, groups, benchmark, deterministic, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::miopen_depthwise_convolution_out::call(self_, weight_, bias_, padding, stride, dilation, groups, benchmark, deterministic, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::miopen_depthwise_convolution::call(self_, weight_, bias_, padding, stride, dilation, groups, benchmark, deterministic);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    void miopen_rnn_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, ::std::array<bool,4> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::TensorList out3) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto weight_buf_meta = to_meta(weight_buf);
        auto hx_meta = to_meta(hx);
        auto cx_meta = to_meta(cx);
        auto output_meta = to_meta(output);
        auto grad_output_meta = to_meta(grad_output);
        auto grad_hy_meta = to_meta(grad_hy);
        auto grad_cy_meta = to_meta(grad_cy);
        auto dropout_state_meta = to_meta(dropout_state);
        auto reserve_meta = to_meta(reserve);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        auto out3_meta = to_meta(out3);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::miopen_rnn_backward_out::call(input_meta, weight_meta, weight_stride0, weight_buf_meta, hx_meta, cx_meta, output_meta, grad_output_meta, grad_hy_meta, grad_cy_meta, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state_meta, reserve_meta, output_mask, out0_meta, out1_meta, out2_meta, out3_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      ::std::vector<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight.vec();
      }
      
      at::Tensor weight_buf_;
      if (at::functionalization::impl::isFunctionalTensor(weight_buf)) {
        at::functionalization::impl::sync(weight_buf);
        weight_buf_ = at::functionalization::impl::from_functional_tensor(weight_buf);
      } else {
        weight_buf_ = weight_buf;
      }
      
      at::Tensor hx_;
      if (at::functionalization::impl::isFunctionalTensor(hx)) {
        at::functionalization::impl::sync(hx);
        hx_ = at::functionalization::impl::from_functional_tensor(hx);
      } else {
        hx_ = hx;
      }
      
      c10::optional<at::Tensor> cx_;
      if (at::functionalization::impl::isFunctionalTensor(cx)) {
        at::functionalization::impl::sync(cx);
        cx_ = at::functionalization::impl::from_functional_tensor(cx);
      } else {
        cx_ = cx;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      c10::optional<at::Tensor> grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      c10::optional<at::Tensor> grad_hy_;
      if (at::functionalization::impl::isFunctionalTensor(grad_hy)) {
        at::functionalization::impl::sync(grad_hy);
        grad_hy_ = at::functionalization::impl::from_functional_tensor(grad_hy);
      } else {
        grad_hy_ = grad_hy;
      }
      
      c10::optional<at::Tensor> grad_cy_;
      if (at::functionalization::impl::isFunctionalTensor(grad_cy)) {
        at::functionalization::impl::sync(grad_cy);
        grad_cy_ = at::functionalization::impl::from_functional_tensor(grad_cy);
      } else {
        grad_cy_ = grad_cy;
      }
      
      c10::optional<at::Tensor> dropout_state_;
      if (at::functionalization::impl::isFunctionalTensor(dropout_state)) {
        at::functionalization::impl::sync(dropout_state);
        dropout_state_ = at::functionalization::impl::from_functional_tensor(dropout_state);
      } else {
        dropout_state_ = dropout_state;
      }
      
      at::Tensor reserve_;
      if (at::functionalization::impl::isFunctionalTensor(reserve)) {
        at::functionalization::impl::sync(reserve);
        reserve_ = at::functionalization::impl::from_functional_tensor(reserve);
      } else {
        reserve_ = reserve;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      
      ::std::vector<at::Tensor> out3_;
      if (at::functionalization::impl::isFunctionalTensor(out3)) {
        at::functionalization::impl::sync(out3);
        out3_ = at::functionalization::impl::from_functional_tensor(out3);
      } else {
        out3_ = out3.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2) && at::functionalization::impl::isFunctionalTensor(out3))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || weight_buf.device().type() == c10::DeviceType::XLA || hx.device().type() == c10::DeviceType::XLA || output.device().type() == c10::DeviceType::XLA || reserve.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(weight_buf) || at::functionalization::impl::isFunctionalTensor(hx) || at::functionalization::impl::isFunctionalTensor(cx) || at::functionalization::impl::isFunctionalTensor(output) || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(grad_hy) || at::functionalization::impl::isFunctionalTensor(grad_cy) || at::functionalization::impl::isFunctionalTensor(dropout_state) || at::functionalization::impl::isFunctionalTensor(reserve))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::miopen_rnn_backward_out::call(input_, weight_, weight_stride0, weight_buf_, hx_, cx_, output_, grad_output_, grad_hy_, grad_cy_, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state_, reserve_, output_mask, out0_, out1_, out2_, out3_);
         ;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::miopen_rnn_backward::call(input_, weight_, weight_stride0, weight_buf_, hx_, cx_, output_, grad_output_, grad_hy_, grad_cy_, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state_, reserve_, output_mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
  at::functionalization::impl::propagate_xla_data(out3, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(out3, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(out3);
  at::functionalization::impl::sync(out3);
    
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_batch_norm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, at::Tensor & out, at::Tensor & save_mean, at::Tensor & save_invstd) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto running_mean_meta = to_meta(running_mean);
        auto running_var_meta = to_meta(running_var);
        auto out_meta = to_meta(out);
        auto save_mean_meta = to_meta(save_mean);
        auto save_invstd_meta = to_meta(save_invstd);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::native_batch_norm_out::call(input_meta, weight_meta, bias_meta, running_mean_meta, running_var_meta, training, momentum, eps, out_meta, save_mean_meta, save_invstd_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      c10::optional<at::Tensor> running_mean_;
      if (at::functionalization::impl::isFunctionalTensor(running_mean)) {
        at::functionalization::impl::sync(running_mean);
        running_mean_ = at::functionalization::impl::from_functional_tensor(running_mean);
      } else {
        running_mean_ = running_mean;
      }
      
      c10::optional<at::Tensor> running_var_;
      if (at::functionalization::impl::isFunctionalTensor(running_var)) {
        at::functionalization::impl::sync(running_var);
        running_var_ = at::functionalization::impl::from_functional_tensor(running_var);
      } else {
        running_var_ = running_var;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      
      at::Tensor save_mean_;
      if (at::functionalization::impl::isFunctionalTensor(save_mean)) {
        at::functionalization::impl::sync(save_mean);
        save_mean_ = at::functionalization::impl::from_functional_tensor(save_mean);
      } else {
        save_mean_ = save_mean;
      }
      
      at::Tensor save_invstd_;
      if (at::functionalization::impl::isFunctionalTensor(save_invstd)) {
        at::functionalization::impl::sync(save_invstd);
        save_invstd_ = at::functionalization::impl::from_functional_tensor(save_invstd);
      } else {
        save_invstd_ = save_invstd;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out) && at::functionalization::impl::isFunctionalTensor(save_mean) && at::functionalization::impl::isFunctionalTensor(save_invstd))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias) || at::functionalization::impl::isFunctionalTensor(running_mean) || at::functionalization::impl::isFunctionalTensor(running_var))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::native_batch_norm_out::call(input_, weight_, bias_, running_mean_, running_var_, training, momentum, eps, out_, save_mean_, save_invstd_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out, save_mean, save_invstd);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::native_batch_norm::call(input_, weight_, bias_, running_mean_, running_var_, training, momentum, eps);
        }
          at::functionalization::impl::propagate_xla_data(out, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
  at::functionalization::impl::propagate_xla_data(save_mean, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(save_mean, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(save_mean);
  at::functionalization::impl::sync(save_mean);
  at::functionalization::impl::propagate_xla_data(save_invstd, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(save_invstd, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(save_invstd);
  at::functionalization::impl::sync(save_invstd);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out, save_mean, save_invstd);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _native_batch_norm_legit_no_training_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & running_mean, const at::Tensor & running_var, double momentum, double eps, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto running_mean_meta = to_meta(running_mean);
        auto running_var_meta = to_meta(running_var);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_native_batch_norm_legit_no_training_out::call(input_meta, weight_meta, bias_meta, running_mean_meta, running_var_meta, momentum, eps, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor running_mean_;
      if (at::functionalization::impl::isFunctionalTensor(running_mean)) {
        at::functionalization::impl::sync(running_mean);
        running_mean_ = at::functionalization::impl::from_functional_tensor(running_mean);
      } else {
        running_mean_ = running_mean;
      }
      
      at::Tensor running_var_;
      if (at::functionalization::impl::isFunctionalTensor(running_var)) {
        at::functionalization::impl::sync(running_var);
        running_var_ = at::functionalization::impl::from_functional_tensor(running_var);
      } else {
        running_var_ = running_var;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || running_mean.device().type() == c10::DeviceType::XLA || running_var.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias) || at::functionalization::impl::isFunctionalTensor(running_mean) || at::functionalization::impl::isFunctionalTensor(running_var))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_native_batch_norm_legit_no_training_out::call(input_, weight_, bias_, running_mean_, running_var_, momentum, eps, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_native_batch_norm_legit_no_training::call(input_, weight_, bias_, running_mean_, running_var_, momentum, eps);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & batch_norm_elemt_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & invstd, double eps, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto mean_meta = to_meta(mean);
        auto invstd_meta = to_meta(invstd);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::batch_norm_elemt_out::call(input_meta, weight_meta, bias_meta, mean_meta, invstd_meta, eps, out_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor mean_;
      if (at::functionalization::impl::isFunctionalTensor(mean)) {
        at::functionalization::impl::sync(mean);
        mean_ = at::functionalization::impl::from_functional_tensor(mean);
      } else {
        mean_ = mean;
      }
      
      at::Tensor invstd_;
      if (at::functionalization::impl::isFunctionalTensor(invstd)) {
        at::functionalization::impl::sync(invstd);
        invstd_ = at::functionalization::impl::from_functional_tensor(invstd);
      } else {
        invstd_ = invstd;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || mean.device().type() == c10::DeviceType::XLA || invstd.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias) || at::functionalization::impl::isFunctionalTensor(mean) || at::functionalization::impl::isFunctionalTensor(invstd))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::batch_norm_elemt_out::call(input_, weight_, bias_, mean_, invstd_, eps, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::batch_norm_elemt::call(input_, weight_, bias_, mean_, invstd_, eps);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> batch_norm_update_stats_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto running_mean_meta = to_meta(running_mean);
        auto running_var_meta = to_meta(running_var);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::batch_norm_update_stats_out::call(input_meta, running_mean_meta, running_var_meta, momentum, out0_meta, out1_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      c10::optional<at::Tensor> running_mean_;
      if (at::functionalization::impl::isFunctionalTensor(running_mean)) {
        at::functionalization::impl::sync(running_mean);
        running_mean_ = at::functionalization::impl::from_functional_tensor(running_mean);
      } else {
        running_mean_ = running_mean;
      }
      
      c10::optional<at::Tensor> running_var_;
      if (at::functionalization::impl::isFunctionalTensor(running_var)) {
        at::functionalization::impl::sync(running_var);
        running_var_ = at::functionalization::impl::from_functional_tensor(running_var);
      } else {
        running_var_ = running_var;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(running_mean) || at::functionalization::impl::isFunctionalTensor(running_var))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::batch_norm_update_stats_out::call(input_, running_mean_, running_var_, momentum, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::batch_norm_update_stats::call(input_, running_mean_, running_var_, momentum);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & _nnpack_spatial_convolution_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_nnpack_spatial_convolution_out::call(input_meta, weight_meta, bias_meta, padding, stride, out_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_nnpack_spatial_convolution_out::call(input_, weight_, bias_, padding, stride, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_nnpack_spatial_convolution::call(input_, weight_, bias_, padding, stride);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & ones_like_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ones_like_out::call(self_meta, memory_format, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ones_like_out::call(self_, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ones_like::call(self_, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & channel_shuffle_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt groups, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::channel_shuffle_out::call(self_meta, groups, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::channel_shuffle_out::call(self_, groups, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::channel_shuffle::call(self_, groups);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & randint_out_out(c10::DispatchKeySet dispatchKeySet, c10::SymInt high, c10::SymIntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::randint_out::call(high, size, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::randint_out::call(high, size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::randint::call(high, size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & randint_out_generator_out(c10::DispatchKeySet dispatchKeySet, c10::SymInt high, c10::SymIntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::randint_generator_out::call(high, size, generator, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::randint_generator_out::call(high, size, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::randint_generator::call(high, size, generator, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & randint_out_low_out(c10::DispatchKeySet dispatchKeySet, c10::SymInt low, c10::SymInt high, c10::SymIntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::randint_low_out::call(low, high, size, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::randint_low_out::call(low, high, size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::randint_low::call(low, high, size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & randint_out_low_generator_out(c10::DispatchKeySet dispatchKeySet, c10::SymInt low, c10::SymInt high, c10::SymIntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::randint_low_generator_out::call(low, high, size, generator, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::randint_low_generator_out::call(low, high, size, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::randint_low_generator::call(low, high, size, generator, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & randperm_out_out(c10::DispatchKeySet dispatchKeySet, c10::SymInt n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::randperm_out::call(n, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::randperm_out::call(n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::randperm::call(n, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & randperm_out_generator_out(c10::DispatchKeySet dispatchKeySet, c10::SymInt n, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::randperm_generator_out::call(n, generator, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::randperm_generator_out::call(n, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::randperm_generator::call(n, generator, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & range_out_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::range_out::call(start, end, step, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::range_out::call(start, end, step, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::range_step::call(start, end, step, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & range_out_out_(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::range_out_::call(start, end, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::range_out_::call(start, end, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::range::call(start, end, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & reciprocal_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::reciprocal_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::reciprocal_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::reciprocal::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & reciprocal_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::reciprocal_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::reciprocal_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::reciprocal::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & gelu_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::string_view approximate, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::gelu_backward_grad_input::call(grad_output_meta, self_meta, approximate, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::gelu_backward_grad_input::call(grad_output_, self_, approximate, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::gelu_backward::call(grad_output_, self_, approximate);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & hardshrink_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hardshrink_out::call(self_meta, lambd, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hardshrink_out::call(self_, lambd, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hardshrink::call(self_, lambd);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hardshrink_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_out_meta = to_meta(grad_out);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hardshrink_backward_grad_input::call(grad_out_meta, self_meta, lambd, grad_input_meta);
      }
      
      at::Tensor grad_out_;
      if (at::functionalization::impl::isFunctionalTensor(grad_out)) {
        at::functionalization::impl::sync(grad_out);
        grad_out_ = at::functionalization::impl::from_functional_tensor(grad_out);
      } else {
        grad_out_ = grad_out;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_out.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_out) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hardshrink_backward_grad_input::call(grad_out_, self_, lambd, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hardshrink_backward::call(grad_out_, self_, lambd);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & silu_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::silu_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::silu_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::silu::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & silu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::silu_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::silu_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::silu::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & silu_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::silu_backward_grad_input::call(grad_output_meta, self_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::silu_backward_grad_input::call(grad_output_, self_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::silu_backward::call(grad_output_, self_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & sin_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sin_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sin_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sin::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sin_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sin_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sin_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sin::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & _softmax_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_softmax_out::call(self_meta, dim, half_to_float, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_softmax_out::call(self_, dim, half_to_float, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_softmax::call(self_, dim, half_to_float);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sspaddmm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mat1_meta = to_meta(mat1);
        auto mat2_meta = to_meta(mat2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sspaddmm_out::call(self_meta, mat1_meta, mat2_meta, beta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mat1_;
      if (at::functionalization::impl::isFunctionalTensor(mat1)) {
        at::functionalization::impl::sync(mat1);
        mat1_ = at::functionalization::impl::from_functional_tensor(mat1);
      } else {
        mat1_ = mat1;
      }
      
      at::Tensor mat2_;
      if (at::functionalization::impl::isFunctionalTensor(mat2)) {
        at::functionalization::impl::sync(mat2);
        mat2_ = at::functionalization::impl::from_functional_tensor(mat2);
      } else {
        mat2_ = mat2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mat1.device().type() == c10::DeviceType::XLA || mat2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mat1) || at::functionalization::impl::isFunctionalTensor(mat2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sspaddmm_out::call(self_, mat1_, mat2_, beta, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sspaddmm::call(self_, mat1_, mat2_, beta, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _stack_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_stack_out::call(tensors_meta, dim, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_stack_out::call(tensors_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_stack::call(tensors_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hstack_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hstack_out::call(tensors_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hstack_out::call(tensors_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hstack::call(tensors_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & dstack_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::dstack_out::call(tensors_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::dstack_out::call(tensors_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::dstack::call(tensors_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sum_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sum_out::call(self_meta, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sum_out::call(self_, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sum::call(self_, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sum_out_IntList_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sum_IntList_out::call(self_meta, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sum_IntList_out::call(self_, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sum_dim_IntList::call(self_, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sum_out_DimnameList_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sum_DimnameList_out::call(self_meta, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sum_DimnameList_out::call(self_, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sum_dim_DimnameList::call(self_, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & std_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::std_out::call(self_meta, dim, unbiased, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::std_out::call(self_, dim, unbiased, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::std_dim::call(self_, dim, unbiased, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & std_out_correction_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::std_correction_out::call(self_meta, dim, correction, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::std_correction_out::call(self_, dim, correction, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::std_correction::call(self_, dim, correction, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & std_out_names_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::std_names_out::call(self_meta, dim, unbiased, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::std_names_out::call(self_, dim, unbiased, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::std_names_dim::call(self_, dim, unbiased, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & std_out_correction_names_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::std_correction_names_out::call(self_meta, dim, correction, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::std_correction_names_out::call(self_, dim, correction, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::std_correction_names::call(self_, dim, correction, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & tan_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::tan_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::tan_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::tan::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & tan_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::tan_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::tan_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::tan::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & tensordot_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::tensordot_out::call(self_meta, other_meta, dims_self, dims_other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::tensordot_out::call(self_, other_, dims_self, dims_other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::tensordot::call(self_, other_, dims_self, dims_other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & threshold_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::threshold_out::call(self_meta, threshold, value, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::threshold_out::call(self_, threshold, value, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::threshold::call(self_, threshold, value);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & threshold_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::threshold_::call(self_meta, threshold, value);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::threshold_::call(self_, threshold, value);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::threshold::call(self_, threshold, value);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & _nested_tensor_strides_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_nested_tensor_strides_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_nested_tensor_strides_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_nested_tensor_strides::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fix_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fix_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fix_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fix::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fix_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fix_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fix_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fix::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> unique_consecutive_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool return_inverse, bool return_counts, c10::optional<int64_t> dim, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::unique_consecutive_out::call(self_meta, return_inverse, return_counts, dim, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::unique_consecutive_out::call(self_, return_inverse, return_counts, dim, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::unique_consecutive::call(self_, return_inverse, return_counts, dim);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & var_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::var_out::call(self_meta, dim, unbiased, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::var_out::call(self_, dim, unbiased, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::var_dim::call(self_, dim, unbiased, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & var_out_correction_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::var_correction_out::call(self_meta, dim, correction, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::var_correction_out::call(self_, dim, correction, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::var_correction::call(self_, dim, correction, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & var_out_names_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::var_names_out::call(self_meta, dim, unbiased, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::var_names_out::call(self_, dim, unbiased, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::var_names_dim::call(self_, dim, unbiased, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & var_out_correction_names_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::var_correction_names_out::call(self_meta, dim, correction, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::var_correction_names_out::call(self_, dim, correction, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::var_correction_names::call(self_, dim, correction, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _standard_gamma_grad_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & output, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto output_meta = to_meta(output);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_standard_gamma_grad_out::call(self_meta, output_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_standard_gamma_grad_out::call(self_, output_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_standard_gamma_grad::call(self_, output_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & poisson_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::poisson_out::call(self_meta, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::poisson_out::call(self_, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::poisson::call(self_, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _sparse_csr_sum_out_dim_dtype_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sparse_csr_sum_dim_dtype_out::call(self_meta, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sparse_csr_sum_dim_dtype_out::call(self_, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sparse_csr_sum_dim_dtype::call(self_, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _sparse_softmax_backward_data_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto output_meta = to_meta(output);
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sparse_softmax_backward_data_out::call(grad_output_meta, output_meta, dim, self_meta, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sparse_softmax_backward_data_out::call(grad_output_, output_, dim, self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sparse_softmax_backward_data::call(grad_output_, output_, dim, self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _sparse_log_softmax_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sparse_log_softmax_out::call(self_meta, dim, half_to_float, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sparse_log_softmax_out::call(self_, dim, half_to_float, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sparse_log_softmax::call(self_, dim, half_to_float);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _sparse_log_softmax_backward_data_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto output_meta = to_meta(output);
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sparse_log_softmax_backward_data_out::call(grad_output_meta, output_meta, dim, self_meta, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sparse_log_softmax_backward_data_out::call(grad_output_, output_, dim, self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sparse_log_softmax_backward_data::call(grad_output_, output_, dim, self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & resize_as_sparse_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & the_template, const at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto the_template_meta = to_meta(the_template);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::resize_as_sparse_out::call(self_meta, the_template_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor the_template_;
      if (at::functionalization::impl::isFunctionalTensor(the_template)) {
        at::functionalization::impl::sync(the_template);
        the_template_ = at::functionalization::impl::from_functional_tensor(the_template);
      } else {
        the_template_ = the_template;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || the_template.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(the_template))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::resize_as_sparse_out::call(self_, the_template_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::resize_as_sparse::call(self_, the_template_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & resize_as_sparse_(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & the_template) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto the_template_meta = to_meta(the_template);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::resize_as_sparse_::call(self_meta, the_template_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor the_template_;
      if (at::functionalization::impl::isFunctionalTensor(the_template)) {
        at::functionalization::impl::sync(the_template);
        the_template_ = at::functionalization::impl::from_functional_tensor(the_template);
      } else {
        the_template_ = the_template;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || the_template.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(the_template))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::resize_as_sparse_::call(self_, the_template_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::resize_as_sparse::call(self_, the_template_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & sub_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sub_out::call(self_meta, other_meta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sub_out::call(self_, other_, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sub_Tensor::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sub__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sub__Tensor::call(self_meta, other_meta, alpha);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sub__Tensor::call(self_, other_, alpha);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sub_Tensor::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & sub_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sub_Scalar_out::call(self_meta, other, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sub_Scalar_out::call(self_, other, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sub_Scalar::call(self_, other, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sub__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sub__Scalar::call(self_meta, other, alpha);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sub__Scalar::call(self_, other, alpha);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sub_Scalar::call(self_, other, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & _sparse_mask_projection_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, bool accumulate_matches, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mask_meta = to_meta(mask);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sparse_mask_projection_out::call(self_meta, mask_meta, accumulate_matches, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mask.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mask))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sparse_mask_projection_out::call(self_, mask_, accumulate_matches, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sparse_mask_projection::call(self_, mask_, accumulate_matches);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & copy_sparse_to_sparse_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & src, bool non_blocking, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto src_meta = to_meta(src);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::copy_sparse_to_sparse_out::call(self_meta, src_meta, non_blocking, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::copy_sparse_to_sparse_out::call(self_, src_, non_blocking, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::copy_sparse_to_sparse::call(self_, src_, non_blocking);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & copy_sparse_to_sparse_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & src, bool non_blocking) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto src_meta = to_meta(src);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::copy_sparse_to_sparse_::call(self_meta, src_meta, non_blocking);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::copy_sparse_to_sparse_::call(self_, src_, non_blocking);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::copy_sparse_to_sparse::call(self_, src_, non_blocking);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & _to_sparse_out_sparse_dim_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sparse_dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_to_sparse_sparse_dim_out::call(self_meta, sparse_dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_to_sparse_sparse_dim_out::call(self_, sparse_dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_to_sparse_sparse_dim::call(self_, sparse_dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _to_sparse_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Layout> layout, at::OptionalIntArrayRef blocksize, c10::optional<int64_t> dense_dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_to_sparse_out::call(self_meta, layout, blocksize, dense_dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_to_sparse_out::call(self_, layout, blocksize, dense_dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_to_sparse::call(self_, layout, blocksize, dense_dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _to_sparse_bsr_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_to_sparse_bsr_out::call(self_meta, blocksize, dense_dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_to_sparse_bsr_out::call(self_, blocksize, dense_dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_to_sparse_bsr::call(self_, blocksize, dense_dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mkldnn_reorder_conv2d_weight_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::OptionalSymIntArrayRef input_size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_reorder_conv2d_weight_out::call(self_meta, padding, stride, dilation, groups, input_size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mkldnn_reorder_conv2d_weight_out::call(self_, padding, stride, dilation, groups, input_size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_reorder_conv2d_weight::call(self_, padding, stride, dilation, groups, input_size);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> _lstm_mps_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3, at::Tensor & out4, at::Tensor & out5) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto hx_meta = to_meta(hx);
        auto params_meta = to_meta(params);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        auto out3_meta = to_meta(out3);
        auto out4_meta = to_meta(out4);
        auto out5_meta = to_meta(out5);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_lstm_mps_out::call(input_meta, hx_meta, params_meta, has_biases, num_layers, dropout, train, bidirectional, batch_first, out0_meta, out1_meta, out2_meta, out3_meta, out4_meta, out5_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      ::std::vector<at::Tensor> hx_;
      if (at::functionalization::impl::isFunctionalTensor(hx)) {
        at::functionalization::impl::sync(hx);
        hx_ = at::functionalization::impl::from_functional_tensor(hx);
      } else {
        hx_ = hx.vec();
      }
      
      ::std::vector<at::Tensor> params_;
      if (at::functionalization::impl::isFunctionalTensor(params)) {
        at::functionalization::impl::sync(params);
        params_ = at::functionalization::impl::from_functional_tensor(params);
      } else {
        params_ = params.vec();
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      
      at::Tensor out3_;
      if (at::functionalization::impl::isFunctionalTensor(out3)) {
        at::functionalization::impl::sync(out3);
        out3_ = at::functionalization::impl::from_functional_tensor(out3);
      } else {
        out3_ = out3;
      }
      
      at::Tensor out4_;
      if (at::functionalization::impl::isFunctionalTensor(out4)) {
        at::functionalization::impl::sync(out4);
        out4_ = at::functionalization::impl::from_functional_tensor(out4);
      } else {
        out4_ = out4;
      }
      
      at::Tensor out5_;
      if (at::functionalization::impl::isFunctionalTensor(out5)) {
        at::functionalization::impl::sync(out5);
        out5_ = at::functionalization::impl::from_functional_tensor(out5);
      } else {
        out5_ = out5;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2) && at::functionalization::impl::isFunctionalTensor(out3) && at::functionalization::impl::isFunctionalTensor(out4) && at::functionalization::impl::isFunctionalTensor(out5))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(hx) || at::functionalization::impl::isFunctionalTensor(params))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_lstm_mps_out::call(input_, hx_, params_, has_biases, num_layers, dropout, train, bidirectional, batch_first, out0_, out1_, out2_, out3_, out4_, out5_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3, out4, out5);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_lstm_mps::call(input_, hx_, params_, has_biases, num_layers, dropout, train, bidirectional, batch_first);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
  at::functionalization::impl::propagate_xla_data(out3, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(out3, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(out3);
  at::functionalization::impl::sync(out3);
  at::functionalization::impl::propagate_xla_data(out4, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(out4, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(out4);
  at::functionalization::impl::sync(out4);
  at::functionalization::impl::propagate_xla_data(out5, std::get<5>(tmp_output));
  at::functionalization::impl::replace_(out5, std::get<5>(tmp_output));
  at::functionalization::impl::commit_update(out5);
  at::functionalization::impl::sync(out5);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3, out4, out5);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> _thnn_fused_gru_cell_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_hy, const at::Tensor & workspace, bool has_bias, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3, at::Tensor & out4) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_hy_meta = to_meta(grad_hy);
        auto workspace_meta = to_meta(workspace);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        auto out3_meta = to_meta(out3);
        auto out4_meta = to_meta(out4);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_thnn_fused_gru_cell_backward_out::call(grad_hy_meta, workspace_meta, has_bias, out0_meta, out1_meta, out2_meta, out3_meta, out4_meta);
      }
      
      at::Tensor grad_hy_;
      if (at::functionalization::impl::isFunctionalTensor(grad_hy)) {
        at::functionalization::impl::sync(grad_hy);
        grad_hy_ = at::functionalization::impl::from_functional_tensor(grad_hy);
      } else {
        grad_hy_ = grad_hy;
      }
      
      at::Tensor workspace_;
      if (at::functionalization::impl::isFunctionalTensor(workspace)) {
        at::functionalization::impl::sync(workspace);
        workspace_ = at::functionalization::impl::from_functional_tensor(workspace);
      } else {
        workspace_ = workspace;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      
      at::Tensor out3_;
      if (at::functionalization::impl::isFunctionalTensor(out3)) {
        at::functionalization::impl::sync(out3);
        out3_ = at::functionalization::impl::from_functional_tensor(out3);
      } else {
        out3_ = out3;
      }
      
      at::Tensor out4_;
      if (at::functionalization::impl::isFunctionalTensor(out4)) {
        at::functionalization::impl::sync(out4);
        out4_ = at::functionalization::impl::from_functional_tensor(out4);
      } else {
        out4_ = out4;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2) && at::functionalization::impl::isFunctionalTensor(out3) && at::functionalization::impl::isFunctionalTensor(out4))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_hy.device().type() == c10::DeviceType::XLA || workspace.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_hy) || at::functionalization::impl::isFunctionalTensor(workspace))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_thnn_fused_gru_cell_backward_out::call(grad_hy_, workspace_, has_bias, out0_, out1_, out2_, out3_, out4_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3, out4);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_thnn_fused_gru_cell_backward::call(grad_hy_, workspace_, has_bias);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
  at::functionalization::impl::propagate_xla_data(out3, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(out3, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(out3);
  at::functionalization::impl::sync(out3);
  at::functionalization::impl::propagate_xla_data(out4, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(out4, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(out4);
  at::functionalization::impl::sync(out4);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3, out4);
      }
    }

    at::Tensor & set_out_source_Storage_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Storage source, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::set_source_Storage_out::call(self_meta, source, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::set_source_Storage_out::call(self_, source, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::set_source_Storage::call(self_, source);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & set__source_Storage(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Storage source) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::set__source_Storage::call(self_meta, source);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::set__source_Storage::call(self_, source);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::set_source_Storage::call(self_, source);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & set_out_source_Storage_storage_offset_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Storage source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::set_source_Storage_storage_offset_out::call(self_meta, source, storage_offset, size, stride, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::set_source_Storage_storage_offset_out::call(self_, source, storage_offset, size, stride, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::set_source_Storage_storage_offset::call(self_, source, storage_offset, size, stride);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & set__source_Storage_storage_offset(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Storage source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::set__source_Storage_storage_offset::call(self_meta, source, storage_offset, size, stride);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::set__source_Storage_storage_offset::call(self_, source, storage_offset, size, stride);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::set_source_Storage_storage_offset::call(self_, source, storage_offset, size, stride);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & set_out_source_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & source, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto source_meta = to_meta(source);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::set_source_Tensor_out::call(self_meta, source_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor source_;
      if (at::functionalization::impl::isFunctionalTensor(source)) {
        at::functionalization::impl::sync(source);
        source_ = at::functionalization::impl::from_functional_tensor(source);
      } else {
        source_ = source;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || source.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(source))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::set_source_Tensor_out::call(self_, source_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::set_source_Tensor::call(self_, source_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & set__source_Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & source) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto source_meta = to_meta(source);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::set__source_Tensor::call(self_meta, source_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor source_;
      if (at::functionalization::impl::isFunctionalTensor(source)) {
        at::functionalization::impl::sync(source);
        source_ = at::functionalization::impl::from_functional_tensor(source);
      } else {
        source_ = source;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || source.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(source))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::set__source_Tensor::call(self_, source_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::set_source_Tensor::call(self_, source_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & set_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::set_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::set_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::set::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & set_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::set_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::set_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::set::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & lift_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lift_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lift_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lift::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & masked_scatter_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, const at::Tensor & source, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mask_meta = to_meta(mask);
        auto source_meta = to_meta(source);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::masked_scatter_out::call(self_meta, mask_meta, source_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor source_;
      if (at::functionalization::impl::isFunctionalTensor(source)) {
        at::functionalization::impl::sync(source);
        source_ = at::functionalization::impl::from_functional_tensor(source);
      } else {
        source_ = source;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mask.device().type() == c10::DeviceType::XLA || source.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mask) || at::functionalization::impl::isFunctionalTensor(source))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::masked_scatter_out::call(self_, mask_, source_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::masked_scatter::call(self_, mask_, source_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & masked_scatter_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mask, const at::Tensor & source) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mask_meta = to_meta(mask);
        auto source_meta = to_meta(source);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::masked_scatter_::call(self_meta, mask_meta, source_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor source_;
      if (at::functionalization::impl::isFunctionalTensor(source)) {
        at::functionalization::impl::sync(source);
        source_ = at::functionalization::impl::from_functional_tensor(source);
      } else {
        source_ = source;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || mask.device().type() == c10::DeviceType::XLA || source.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(mask) || at::functionalization::impl::isFunctionalTensor(source))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::masked_scatter_::call(self_, mask_, source_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::masked_scatter::call(self_, mask_, source_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & index_reduce_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, c10::string_view reduce, bool include_self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto source_meta = to_meta(source);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_reduce_out::call(self_meta, dim, index_meta, source_meta, reduce, include_self, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor source_;
      if (at::functionalization::impl::isFunctionalTensor(source)) {
        at::functionalization::impl::sync(source);
        source_ = at::functionalization::impl::from_functional_tensor(source);
      } else {
        source_ = source;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA || source.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(source))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_reduce_out::call(self_, dim, index_, source_, reduce, include_self, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_reduce::call(self_, dim, index_, source_, reduce, include_self);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & index_reduce_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, c10::string_view reduce, bool include_self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto source_meta = to_meta(source);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_reduce_::call(self_meta, dim, index_meta, source_meta, reduce, include_self);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor source_;
      if (at::functionalization::impl::isFunctionalTensor(source)) {
        at::functionalization::impl::sync(source);
        source_ = at::functionalization::impl::from_functional_tensor(source);
      } else {
        source_ = source;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || index.device().type() == c10::DeviceType::XLA || source.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(source))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_reduce_::call(self_, dim, index_, source_, reduce, include_self);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_reduce::call(self_, dim, index_, source_, reduce, include_self);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & eq_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::eq_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::eq_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::eq_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & eq__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::eq__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::eq__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::eq_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & eq_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::eq_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::eq_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::eq_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & eq__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::eq__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::eq__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::eq_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bitwise_and_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_and_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_and_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_and_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_and__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_and__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_and__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_and_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bitwise_and_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_and_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_and_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_and_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_and__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_and__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_and__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_and_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bitwise_and_out_Scalar_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_and_Scalar_Tensor_out::call(self, other_meta, out_meta);
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_and_Scalar_Tensor_out::call(self, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_and_Scalar_Tensor::call(self, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_or_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_or_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_or_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_or_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_or__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_or__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_or__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_or_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bitwise_or_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_or_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_or_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_or_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_or__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_or__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_or__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_or_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bitwise_or_out_Scalar_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_or_Scalar_Tensor_out::call(self, other_meta, out_meta);
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_or_Scalar_Tensor_out::call(self, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_or_Scalar_Tensor::call(self, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_left_shift_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_left_shift_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_left_shift_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_left_shift_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_left_shift__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_left_shift__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_left_shift__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_left_shift_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bitwise_left_shift_out_Tensor_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_left_shift_Tensor_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_left_shift_Tensor_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_left_shift_Tensor_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_left_shift__Tensor_Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_left_shift__Tensor_Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_left_shift__Tensor_Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_left_shift_Tensor_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bitwise_left_shift_out_Scalar_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_left_shift_Scalar_Tensor_out::call(self, other_meta, out_meta);
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_left_shift_Scalar_Tensor_out::call(self, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_left_shift_Scalar_Tensor::call(self, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & __rshift___out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::__rshift___Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::__rshift___Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::__rshift___Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & __irshift___Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::__irshift___Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::__irshift___Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::__rshift___Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & __rshift___out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::__rshift___Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::__rshift___Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::__rshift___Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & __irshift___Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::__irshift___Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::__irshift___Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::__rshift___Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & tril_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::tril_out::call(self_meta, diagonal, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::tril_out::call(self_, diagonal, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::tril::call(self_, diagonal);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & tril_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t diagonal) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::tril_::call(self_meta, diagonal);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::tril_::call(self_, diagonal);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::tril::call(self_, diagonal);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & triu_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::triu_out::call(self_meta, diagonal, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::triu_out::call(self_, diagonal, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::triu::call(self_, diagonal);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & triu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t diagonal) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::triu_::call(self_meta, diagonal);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::triu_::call(self_, diagonal);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::triu::call(self_, diagonal);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & digamma_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::digamma_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::digamma_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::digamma::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & digamma_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::digamma_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::digamma_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::digamma::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & lerp_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto end_meta = to_meta(end);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lerp_Scalar_out::call(self_meta, end_meta, weight, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor end_;
      if (at::functionalization::impl::isFunctionalTensor(end)) {
        at::functionalization::impl::sync(end);
        end_ = at::functionalization::impl::from_functional_tensor(end);
      } else {
        end_ = end;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || end.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(end))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lerp_Scalar_out::call(self_, end_, weight, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lerp_Scalar::call(self_, end_, weight);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & lerp__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto end_meta = to_meta(end);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lerp__Scalar::call(self_meta, end_meta, weight);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor end_;
      if (at::functionalization::impl::isFunctionalTensor(end)) {
        at::functionalization::impl::sync(end);
        end_ = at::functionalization::impl::from_functional_tensor(end);
      } else {
        end_ = end;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || end.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(end))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lerp__Scalar::call(self_, end_, weight);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lerp_Scalar::call(self_, end_, weight);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & lerp_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto end_meta = to_meta(end);
        auto weight_meta = to_meta(weight);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lerp_Tensor_out::call(self_meta, end_meta, weight_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor end_;
      if (at::functionalization::impl::isFunctionalTensor(end)) {
        at::functionalization::impl::sync(end);
        end_ = at::functionalization::impl::from_functional_tensor(end);
      } else {
        end_ = end;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || end.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(end) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lerp_Tensor_out::call(self_, end_, weight_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lerp_Tensor::call(self_, end_, weight_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & lerp__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto end_meta = to_meta(end);
        auto weight_meta = to_meta(weight);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lerp__Tensor::call(self_meta, end_meta, weight_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor end_;
      if (at::functionalization::impl::isFunctionalTensor(end)) {
        at::functionalization::impl::sync(end);
        end_ = at::functionalization::impl::from_functional_tensor(end);
      } else {
        end_ = end;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || end.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(end) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lerp__Tensor::call(self_, end_, weight_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lerp_Tensor::call(self_, end_, weight_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & uniform_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double from, double to, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::uniform_out::call(self_meta, from, to, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::uniform_out::call(self_, from, to, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::uniform::call(self_, from, to, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & uniform_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double from, double to, c10::optional<at::Generator> generator) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::uniform_::call(self_meta, from, to, generator);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::uniform_::call(self_, from, to, generator);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::uniform::call(self_, from, to, generator);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & tril_indices_out_out(c10::DispatchKeySet dispatchKeySet, int64_t row, int64_t col, int64_t offset, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::tril_indices_out::call(row, col, offset, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::tril_indices_out::call(row, col, offset, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::tril_indices::call(row, col, offset, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & triu_indices_out_out(c10::DispatchKeySet dispatchKeySet, int64_t row, int64_t col, int64_t offset, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::triu_indices_out::call(row, col, offset, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::triu_indices_out::call(row, col, offset, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::triu_indices::call(row, col, offset, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & ge_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ge_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ge_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ge_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & ge__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ge__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ge__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ge_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & ge_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ge_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ge_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ge_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & ge__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ge__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ge__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ge_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & le_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::le_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::le_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::le_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & le__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::le__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::le__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::le_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & le_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::le_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::le_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::le_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & le__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::le__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::le__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::le_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & gt_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::gt_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::gt_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::gt_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & gt__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::gt__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::gt__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::gt_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & gt_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::gt_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::gt_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::gt_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & gt__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::gt__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::gt__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::gt_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & nonzero_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nonzero_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nonzero_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nonzero::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & addcdiv_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::addcdiv_out::call(self_meta, tensor1_meta, tensor2_meta, value, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1;
      }
      
      at::Tensor tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || tensor1.device().type() == c10::DeviceType::XLA || tensor2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::addcdiv_out::call(self_, tensor1_, tensor2_, value, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::addcdiv::call(self_, tensor1_, tensor2_, value);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & addcdiv_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::addcdiv_::call(self_meta, tensor1_meta, tensor2_meta, value);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1;
      }
      
      at::Tensor tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || tensor1.device().type() == c10::DeviceType::XLA || tensor2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::addcdiv_::call(self_, tensor1_, tensor2_, value);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::addcdiv::call(self_, tensor1_, tensor2_, value);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> triangular_solve_out_X(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular, at::Tensor & X, at::Tensor & M) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto A_meta = to_meta(A);
        auto X_meta = to_meta(X);
        auto M_meta = to_meta(M);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::triangular_solve_X::call(self_meta, A_meta, upper, transpose, unitriangular, X_meta, M_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor X_;
      if (at::functionalization::impl::isFunctionalTensor(X)) {
        at::functionalization::impl::sync(X);
        X_ = at::functionalization::impl::from_functional_tensor(X);
      } else {
        X_ = X;
      }
      
      at::Tensor M_;
      if (at::functionalization::impl::isFunctionalTensor(M)) {
        at::functionalization::impl::sync(M);
        M_ = at::functionalization::impl::from_functional_tensor(M);
      } else {
        M_ = M;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(X) && at::functionalization::impl::isFunctionalTensor(M))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::triangular_solve_X::call(self_, A_, upper, transpose, unitriangular, X_, M_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(X, M);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::triangular_solve::call(self_, A_, upper, transpose, unitriangular);
        }
          at::functionalization::impl::propagate_xla_data(X, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(X, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(X);
  at::functionalization::impl::sync(X);
  at::functionalization::impl::propagate_xla_data(M, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(M, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(M);
  at::functionalization::impl::sync(M);
    return ::std::tuple<at::Tensor &,at::Tensor &>(X, M);
      }
    }

    at::Tensor & cholesky_solve_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, bool upper, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto input2_meta = to_meta(input2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cholesky_solve_out::call(self_meta, input2_meta, upper, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor input2_;
      if (at::functionalization::impl::isFunctionalTensor(input2)) {
        at::functionalization::impl::sync(input2);
        input2_ = at::functionalization::impl::from_functional_tensor(input2);
      } else {
        input2_ = input2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || input2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(input2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cholesky_solve_out::call(self_, input2_, upper, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cholesky_solve::call(self_, input2_, upper);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cholesky_inverse_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cholesky_inverse_out::call(self_meta, upper, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cholesky_inverse_out::call(self_, upper, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cholesky_inverse::call(self_, upper);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> qr_out_Q(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool some, at::Tensor & Q, at::Tensor & R) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto Q_meta = to_meta(Q);
        auto R_meta = to_meta(R);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::qr_Q::call(self_meta, some, Q_meta, R_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor Q_;
      if (at::functionalization::impl::isFunctionalTensor(Q)) {
        at::functionalization::impl::sync(Q);
        Q_ = at::functionalization::impl::from_functional_tensor(Q);
      } else {
        Q_ = Q;
      }
      
      at::Tensor R_;
      if (at::functionalization::impl::isFunctionalTensor(R)) {
        at::functionalization::impl::sync(R);
        R_ = at::functionalization::impl::from_functional_tensor(R);
      } else {
        R_ = R;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(Q) && at::functionalization::impl::isFunctionalTensor(R))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::qr_Q::call(self_, some, Q_, R_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(Q, R);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::qr::call(self_, some);
        }
          at::functionalization::impl::propagate_xla_data(Q, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(Q, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(Q);
  at::functionalization::impl::sync(Q);
  at::functionalization::impl::propagate_xla_data(R, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(R, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(R);
  at::functionalization::impl::sync(R);
    return ::std::tuple<at::Tensor &,at::Tensor &>(Q, R);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> geqrf_out_a(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & a, at::Tensor & tau) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto a_meta = to_meta(a);
        auto tau_meta = to_meta(tau);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::geqrf_a::call(self_meta, a_meta, tau_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor a_;
      if (at::functionalization::impl::isFunctionalTensor(a)) {
        at::functionalization::impl::sync(a);
        a_ = at::functionalization::impl::from_functional_tensor(a);
      } else {
        a_ = a;
      }
      
      at::Tensor tau_;
      if (at::functionalization::impl::isFunctionalTensor(tau)) {
        at::functionalization::impl::sync(tau);
        tau_ = at::functionalization::impl::from_functional_tensor(tau);
      } else {
        tau_ = tau;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(a) && at::functionalization::impl::isFunctionalTensor(tau))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::geqrf_a::call(self_, a_, tau_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(a, tau);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::geqrf::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(a, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(a, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(a);
  at::functionalization::impl::sync(a);
  at::functionalization::impl::propagate_xla_data(tau, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(tau, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(tau);
  at::functionalization::impl::sync(tau);
    return ::std::tuple<at::Tensor &,at::Tensor &>(a, tau);
      }
    }

    at::Tensor & orgqr_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto input2_meta = to_meta(input2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::orgqr_out::call(self_meta, input2_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor input2_;
      if (at::functionalization::impl::isFunctionalTensor(input2)) {
        at::functionalization::impl::sync(input2);
        input2_ = at::functionalization::impl::from_functional_tensor(input2);
      } else {
        input2_ = input2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || input2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(input2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::orgqr_out::call(self_, input2_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::orgqr::call(self_, input2_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & lu_solve_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto LU_data_meta = to_meta(LU_data);
        auto LU_pivots_meta = to_meta(LU_pivots);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lu_solve_out::call(self_meta, LU_data_meta, LU_pivots_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor LU_data_;
      if (at::functionalization::impl::isFunctionalTensor(LU_data)) {
        at::functionalization::impl::sync(LU_data);
        LU_data_ = at::functionalization::impl::from_functional_tensor(LU_data);
      } else {
        LU_data_ = LU_data;
      }
      
      at::Tensor LU_pivots_;
      if (at::functionalization::impl::isFunctionalTensor(LU_pivots)) {
        at::functionalization::impl::sync(LU_pivots);
        LU_pivots_ = at::functionalization::impl::from_functional_tensor(LU_pivots);
      } else {
        LU_pivots_ = LU_pivots;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || LU_data.device().type() == c10::DeviceType::XLA || LU_pivots.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(LU_data) || at::functionalization::impl::isFunctionalTensor(LU_pivots))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lu_solve_out::call(self_, LU_data_, LU_pivots_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lu_solve::call(self_, LU_data_, LU_pivots_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & lgamma_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lgamma_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lgamma_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lgamma::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & lgamma_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lgamma_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lgamma_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lgamma::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & erfinv_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::erfinv_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::erfinv_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::erfinv::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & erfinv_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::erfinv_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::erfinv_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::erfinv::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & i0_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::i0_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::i0_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::i0::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & i0_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::i0_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::i0_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::i0::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & sign_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sign_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sign_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sign::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sign_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sign_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sign_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sign::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & signbit_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::signbit_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::signbit_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::signbit::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & atan2_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::atan2_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::atan2_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::atan2::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & atan2_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::atan2_::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::atan2_::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::atan2::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    void _histogramdd_bin_edges_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_histogramdd_bin_edges_out::call(self_meta, bins, range, weight_meta, density, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_histogramdd_bin_edges_out::call(self_, bins, range, weight_, density, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_histogramdd_bin_edges::call(self_, bins, range, weight_, density);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    at::Tensor & _histogramdd_from_bin_tensors_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::TensorList bins, const c10::optional<at::Tensor> & weight, bool density, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto bins_meta = to_meta(bins);
        auto weight_meta = to_meta(weight);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_histogramdd_from_bin_tensors_out::call(self_meta, bins_meta, weight_meta, density, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      ::std::vector<at::Tensor> bins_;
      if (at::functionalization::impl::isFunctionalTensor(bins)) {
        at::functionalization::impl::sync(bins);
        bins_ = at::functionalization::impl::from_functional_tensor(bins);
      } else {
        bins_ = bins.vec();
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(bins) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_histogramdd_from_bin_tensors_out::call(self_, bins_, weight_, density, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_histogramdd_from_bin_tensors::call(self_, bins_, weight_, density);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fmod_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fmod_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fmod_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fmod_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fmod__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fmod__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fmod__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fmod_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & fmod_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fmod_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fmod_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fmod_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fmod__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fmod__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fmod__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fmod_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & nextafter_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nextafter_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nextafter_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nextafter::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & nextafter_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nextafter_::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nextafter_::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nextafter::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & minimum_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::minimum_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::minimum_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::minimum::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> topk_out_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt k, int64_t dim, bool largest, bool sorted, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::topk_values::call(self_meta, k, dim, largest, sorted, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::topk_values::call(self_, k, dim, largest, sorted, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::topk::call(self_, k, dim, largest, sorted);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    at::Tensor & any_out_all_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::any_all_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::any_all_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::any::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    void _foreach_mul_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_mul_Scalar_out::call(self_meta, scalar, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_mul_Scalar_out::call(self_, scalar, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_mul_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_mul__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_mul__Scalar::call(self_meta, scalar);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_mul__Scalar::call(self_, scalar);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_mul_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_mul_out_List_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_mul_List_out::call(self_meta, other_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_mul_List_out::call(self_, other_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_mul_List::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_mul__List(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_mul__List::call(self_meta, other_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_mul__List::call(self_, other_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_mul_List::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_mul_out_ScalarList_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_mul_ScalarList_out::call(self_meta, scalars, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_mul_ScalarList_out::call(self_, scalars, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_mul_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_mul__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_mul__ScalarList::call(self_meta, scalars);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_mul__ScalarList::call(self_, scalars);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_mul_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_mul_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Tensor & other, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_mul_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_mul_Tensor_out::call(self_, other_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_mul_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_mul__Tensor(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_mul__Tensor::call(self_meta, other_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_mul__Tensor::call(self_, other_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_mul_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_div_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_div_Scalar_out::call(self_meta, scalar, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_div_Scalar_out::call(self_, scalar, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_div_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_div__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_div__Scalar::call(self_meta, scalar);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_div__Scalar::call(self_, scalar);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_div_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_div_out_List_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_div_List_out::call(self_meta, other_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_div_List_out::call(self_, other_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_div_List::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_div__List(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_div__List::call(self_meta, other_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_div__List::call(self_, other_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_div_List::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_div_out_ScalarList_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_div_ScalarList_out::call(self_meta, scalars, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_div_ScalarList_out::call(self_, scalars, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_div_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_div__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_div__ScalarList::call(self_meta, scalars);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_div__ScalarList::call(self_, scalars);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_div_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_div_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Tensor & other, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_div_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_div_Tensor_out::call(self_, other_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_div_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_div__Tensor(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_div__Tensor::call(self_meta, other_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_div__Tensor::call(self_, other_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_div_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_clamp_max_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_clamp_max_Scalar_out::call(self_meta, scalar, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_clamp_max_Scalar_out::call(self_, scalar, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_clamp_max_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_clamp_max__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_clamp_max__Scalar::call(self_meta, scalar);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_clamp_max__Scalar::call(self_, scalar);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_clamp_max_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_clamp_max_out_List_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_clamp_max_List_out::call(self_meta, other_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_clamp_max_List_out::call(self_, other_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_clamp_max_List::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_clamp_max__List(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_clamp_max__List::call(self_meta, other_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_clamp_max__List::call(self_, other_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_clamp_max_List::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_clamp_max_out_ScalarList_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_clamp_max_ScalarList_out::call(self_meta, scalars, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_clamp_max_ScalarList_out::call(self_, scalars, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_clamp_max_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_clamp_max__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_clamp_max__ScalarList::call(self_meta, scalars);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_clamp_max__ScalarList::call(self_, scalars);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_clamp_max_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_minimum_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_minimum_Scalar_out::call(self_meta, scalar, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_minimum_Scalar_out::call(self_, scalar, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_minimum_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_minimum__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_minimum__Scalar::call(self_meta, scalar);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_minimum__Scalar::call(self_, scalar);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_minimum_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_minimum_out_List_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_minimum_List_out::call(self_meta, other_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_minimum_List_out::call(self_, other_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_minimum_List::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_minimum__List(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_minimum__List::call(self_meta, other_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_minimum__List::call(self_, other_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_minimum_List::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_minimum_out_ScalarList_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_minimum_ScalarList_out::call(self_meta, scalars, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_minimum_ScalarList_out::call(self_, scalars, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_minimum_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_minimum__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_minimum__ScalarList::call(self_meta, scalars);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_minimum__ScalarList::call(self_, scalars);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_minimum_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_expm1_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_expm1_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_expm1_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_expm1::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_expm1_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_expm1_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_expm1_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_expm1::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_frac_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_frac_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_frac_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_frac::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_frac_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_frac_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_frac_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_frac::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_reciprocal_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_reciprocal_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_reciprocal_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_reciprocal::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_reciprocal_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_reciprocal_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_reciprocal_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_reciprocal::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_sin_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sin_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sin_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sin::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_sin_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sin_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sin_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sin::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_tanh_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_tanh_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_tanh_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_tanh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_tanh_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_tanh_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_tanh_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_tanh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_trunc_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_trunc_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_trunc_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_trunc::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_trunc_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_trunc_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_trunc_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_trunc::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_copy_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList src, bool non_blocking, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto src_meta = to_meta(src);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_copy_out::call(self_meta, src_meta, non_blocking, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_copy_out::call(self_, src_, non_blocking, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_copy::call(self_, src_, non_blocking);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_copy_(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList src, bool non_blocking) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto src_meta = to_meta(src);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_copy_::call(self_meta, src_meta, non_blocking);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_copy_::call(self_, src_, non_blocking);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_copy::call(self_, src_, non_blocking);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    at::Tensor & _convert_indices_from_csr_to_coo_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & crow_indices, const at::Tensor & col_indices, bool out_int32, bool transpose, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto crow_indices_meta = to_meta(crow_indices);
        auto col_indices_meta = to_meta(col_indices);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_convert_indices_from_csr_to_coo_out::call(crow_indices_meta, col_indices_meta, out_int32, transpose, out_meta);
      }
      
      at::Tensor crow_indices_;
      if (at::functionalization::impl::isFunctionalTensor(crow_indices)) {
        at::functionalization::impl::sync(crow_indices);
        crow_indices_ = at::functionalization::impl::from_functional_tensor(crow_indices);
      } else {
        crow_indices_ = crow_indices;
      }
      
      at::Tensor col_indices_;
      if (at::functionalization::impl::isFunctionalTensor(col_indices)) {
        at::functionalization::impl::sync(col_indices);
        col_indices_ = at::functionalization::impl::from_functional_tensor(col_indices);
      } else {
        col_indices_ = col_indices;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || crow_indices.device().type() == c10::DeviceType::XLA || col_indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(crow_indices) || at::functionalization::impl::isFunctionalTensor(col_indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_convert_indices_from_csr_to_coo_out::call(crow_indices_, col_indices_, out_int32, transpose, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_convert_indices_from_csr_to_coo::call(crow_indices_, col_indices_, out_int32, transpose);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & multilabel_margin_loss_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, const at::Tensor & is_target, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto is_target_meta = to_meta(is_target);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::multilabel_margin_loss_backward_grad_input::call(grad_output_meta, self_meta, target_meta, reduction, is_target_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      at::Tensor is_target_;
      if (at::functionalization::impl::isFunctionalTensor(is_target)) {
        at::functionalization::impl::sync(is_target);
        is_target_ = at::functionalization::impl::from_functional_tensor(is_target);
      } else {
        is_target_ = is_target;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA || is_target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target) || at::functionalization::impl::isFunctionalTensor(is_target))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::multilabel_margin_loss_backward_grad_input::call(grad_output_, self_, target_, reduction, is_target_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::multilabel_margin_loss_backward::call(grad_output_, self_, target_, reduction, is_target_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & nll_loss_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto weight_meta = to_meta(weight);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nll_loss_out::call(self_meta, target_meta, weight_meta, reduction, ignore_index, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nll_loss_out::call(self_, target_, weight_, reduction, ignore_index, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nll_loss::call(self_, target_, weight_, reduction, ignore_index);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & nll_loss_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto weight_meta = to_meta(weight);
        auto total_weight_meta = to_meta(total_weight);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nll_loss_backward_grad_input::call(grad_output_meta, self_meta, target_meta, weight_meta, reduction, ignore_index, total_weight_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor total_weight_;
      if (at::functionalization::impl::isFunctionalTensor(total_weight)) {
        at::functionalization::impl::sync(total_weight);
        total_weight_ = at::functionalization::impl::from_functional_tensor(total_weight);
      } else {
        total_weight_ = total_weight;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA || total_weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(total_weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nll_loss_backward_grad_input::call(grad_output_, self_, target_, weight_, reduction, ignore_index, total_weight_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nll_loss_backward::call(grad_output_, self_, target_, weight_, reduction, ignore_index, total_weight_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & nll_loss2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto weight_meta = to_meta(weight);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nll_loss2d_out::call(self_meta, target_meta, weight_meta, reduction, ignore_index, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nll_loss2d_out::call(self_, target_, weight_, reduction, ignore_index, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nll_loss2d::call(self_, target_, weight_, reduction, ignore_index);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & nll_loss2d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto weight_meta = to_meta(weight);
        auto total_weight_meta = to_meta(total_weight);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nll_loss2d_backward_grad_input::call(grad_output_meta, self_meta, target_meta, weight_meta, reduction, ignore_index, total_weight_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor total_weight_;
      if (at::functionalization::impl::isFunctionalTensor(total_weight)) {
        at::functionalization::impl::sync(total_weight);
        total_weight_ = at::functionalization::impl::from_functional_tensor(total_weight);
      } else {
        total_weight_ = total_weight;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA || total_weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(total_weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nll_loss2d_backward_grad_input::call(grad_output_, self_, target_, weight_, reduction, ignore_index, total_weight_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nll_loss2d_backward::call(grad_output_, self_, target_, weight_, reduction, ignore_index, total_weight_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & smooth_l1_loss_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::smooth_l1_loss_out::call(self_meta, target_meta, reduction, beta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::smooth_l1_loss_out::call(self_, target_, reduction, beta, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::smooth_l1_loss::call(self_, target_, reduction, beta);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & huber_loss_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::huber_loss_out::call(self_meta, target_meta, reduction, delta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::huber_loss_out::call(self_, target_, reduction, delta, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::huber_loss::call(self_, target_, reduction, delta);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & soft_margin_loss_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::soft_margin_loss_backward_grad_input::call(grad_output_meta, self_meta, target_meta, reduction, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::soft_margin_loss_backward_grad_input::call(grad_output_, self_, target_, reduction, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::soft_margin_loss_backward::call(grad_output_, self_, target_, reduction);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & elu_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::elu_out::call(self_meta, alpha, scale, input_scale, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::elu_out::call(self_, alpha, scale, input_scale, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::elu::call(self_, alpha, scale, input_scale);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & elu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::elu_::call(self_meta, alpha, scale, input_scale);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::elu_::call(self_, alpha, scale, input_scale);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::elu::call(self_, alpha, scale, input_scale);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & glu_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::glu_out::call(self_meta, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::glu_out::call(self_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::glu::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hardsigmoid_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hardsigmoid_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hardsigmoid_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hardsigmoid::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hardsigmoid_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hardsigmoid_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hardsigmoid_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hardsigmoid::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & leaky_relu_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::leaky_relu_backward_grad_input::call(grad_output_meta, self_meta, negative_slope, self_is_result, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::leaky_relu_backward_grad_input::call(grad_output_, self_, negative_slope, self_is_result, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::leaky_relu_backward::call(grad_output_, self_, negative_slope, self_is_result);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & softshrink_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::softshrink_out::call(self_meta, lambd, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::softshrink_out::call(self_, lambd, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::softshrink::call(self_, lambd);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _adaptive_avg_pool2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_adaptive_avg_pool2d_out::call(self_meta, output_size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_adaptive_avg_pool2d_out::call(self_, output_size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_adaptive_avg_pool2d::call(self_, output_size);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _adaptive_avg_pool3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_adaptive_avg_pool3d_out::call(self_meta, output_size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_adaptive_avg_pool3d_out::call(self_, output_size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_adaptive_avg_pool3d::call(self_, output_size);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & avg_pool3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::avg_pool3d_out::call(self_meta, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::avg_pool3d_out::call(self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::avg_pool3d::call(self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> fractional_max_pool3d_out_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto random_samples_meta = to_meta(random_samples);
        auto output_meta = to_meta(output);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fractional_max_pool3d_output::call(self_meta, kernel_size, output_size, random_samples_meta, output_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor random_samples_;
      if (at::functionalization::impl::isFunctionalTensor(random_samples)) {
        at::functionalization::impl::sync(random_samples);
        random_samples_ = at::functionalization::impl::from_functional_tensor(random_samples);
      } else {
        random_samples_ = random_samples;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(output) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || random_samples.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(random_samples))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::fractional_max_pool3d_output::call(self_, kernel_size, output_size, random_samples_, output_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(output, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fractional_max_pool3d::call(self_, kernel_size, output_size, random_samples_);
        }
          at::functionalization::impl::propagate_xla_data(output, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(output, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(output);
  at::functionalization::impl::sync(output);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(output, indices);
      }
    }

    at::Tensor & max_pool2d_with_indices_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::max_pool2d_with_indices_backward_grad_input::call(grad_output_meta, self_meta, kernel_size, stride, padding, dilation, ceil_mode, indices_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::max_pool2d_with_indices_backward_grad_input::call(grad_output_, self_, kernel_size, stride, padding, dilation, ceil_mode, indices_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::max_pool2d_with_indices_backward::call(grad_output_, self_, kernel_size, stride, padding, dilation, ceil_mode, indices_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & max_unpool3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::max_unpool3d_out::call(self_meta, indices_meta, output_size, stride, padding, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::max_unpool3d_out::call(self_, indices_, output_size, stride, padding, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::max_unpool3d::call(self_, indices_, output_size, stride, padding);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & reflection_pad1d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::reflection_pad1d_out::call(self_meta, padding, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::reflection_pad1d_out::call(self_, padding, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::reflection_pad1d::call(self_, padding);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & reflection_pad1d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::reflection_pad1d_backward_grad_input::call(grad_output_meta, self_meta, padding, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::reflection_pad1d_backward_grad_input::call(grad_output_, self_, padding, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::reflection_pad1d_backward::call(grad_output_, self_, padding);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & reflection_pad2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::reflection_pad2d_out::call(self_meta, padding, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::reflection_pad2d_out::call(self_, padding, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::reflection_pad2d::call(self_, padding);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & reflection_pad3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::reflection_pad3d_out::call(self_meta, padding, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::reflection_pad3d_out::call(self_, padding, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::reflection_pad3d::call(self_, padding);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & replication_pad1d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::replication_pad1d_out::call(self_meta, padding, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::replication_pad1d_out::call(self_, padding, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::replication_pad1d::call(self_, padding);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & replication_pad1d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::replication_pad1d_backward_grad_input::call(grad_output_meta, self_meta, padding, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::replication_pad1d_backward_grad_input::call(grad_output_, self_, padding, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::replication_pad1d_backward::call(grad_output_, self_, padding);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & replication_pad2d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::replication_pad2d_backward_grad_input::call(grad_output_meta, self_meta, padding, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::replication_pad2d_backward_grad_input::call(grad_output_, self_, padding, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::replication_pad2d_backward::call(grad_output_, self_, padding);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & replication_pad3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::replication_pad3d_out::call(self_meta, padding, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::replication_pad3d_out::call(self_, padding, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::replication_pad3d::call(self_, padding);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & replication_pad3d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::replication_pad3d_backward_grad_input::call(grad_output_meta, self_meta, padding, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::replication_pad3d_backward_grad_input::call(grad_output_, self_, padding, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::replication_pad3d_backward::call(grad_output_, self_, padding);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & upsample_nearest2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::upsample_nearest2d_out::call(self_meta, output_size, scales_h, scales_w, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::upsample_nearest2d_out::call(self_, output_size, scales_h, scales_w, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::upsample_nearest2d::call(self_, output_size, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _upsample_nearest_exact2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_upsample_nearest_exact2d_out::call(self_meta, output_size, scales_h, scales_w, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_upsample_nearest_exact2d_out::call(self_, output_size, scales_h, scales_w, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_upsample_nearest_exact2d::call(self_, output_size, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _upsample_nearest_exact2d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_upsample_nearest_exact2d_backward_grad_input::call(grad_output_meta, output_size, input_size, scales_h, scales_w, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_upsample_nearest_exact2d_backward_grad_input::call(grad_output_, output_size, input_size, scales_h, scales_w, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_upsample_nearest_exact2d_backward::call(grad_output_, output_size, input_size, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & _upsample_nearest_exact3d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_upsample_nearest_exact3d_backward_grad_input::call(grad_output_meta, output_size, input_size, scales_d, scales_h, scales_w, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_upsample_nearest_exact3d_backward_grad_input::call(grad_output_, output_size, input_size, scales_d, scales_h, scales_w, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_upsample_nearest_exact3d_backward::call(grad_output_, output_size, input_size, scales_d, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & sigmoid_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto output_meta = to_meta(output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sigmoid_backward_grad_input::call(grad_output_meta, output_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sigmoid_backward_grad_input::call(grad_output_, output_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sigmoid_backward::call(grad_output_, output_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & slow_conv_transpose2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef dilation, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::slow_conv_transpose2d_out::call(self_meta, weight_meta, kernel_size, bias_meta, stride, padding, output_padding, dilation, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::slow_conv_transpose2d_out::call(self_, weight_, kernel_size, bias_, stride, padding, output_padding, dilation, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::slow_conv_transpose2d::call(self_, weight_, kernel_size, bias_, stride, padding, output_padding, dilation);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _slow_conv2d_forward_out_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor & output) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto output_meta = to_meta(output);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_slow_conv2d_forward_output::call(self_meta, weight_meta, kernel_size, bias_meta, stride, padding, output_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(output))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_slow_conv2d_forward_output::call(self_, weight_, kernel_size, bias_, stride, padding, output_);
         return output;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_slow_conv2d_forward::call(self_, weight_, kernel_size, bias_, stride, padding);
        }
          at::functionalization::impl::propagate_xla_data(output, tmp_output);
  at::functionalization::impl::replace_(output, tmp_output);
  at::functionalization::impl::commit_update(output);
  at::functionalization::impl::sync(output);
    return output;
      }
    }

    at::Tensor & conv_depthwise3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::conv_depthwise3d_out::call(self_meta, weight_meta, kernel_size, bias_meta, stride, padding, dilation, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::conv_depthwise3d_out::call(self_, weight_, kernel_size, bias_, stride, padding, dilation, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::conv_depthwise3d::call(self_, weight_, kernel_size, bias_, stride, padding, dilation);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & slow_conv_dilated2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::slow_conv_dilated2d_out::call(self_meta, weight_meta, kernel_size, bias_meta, stride, padding, dilation, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::slow_conv_dilated2d_out::call(self_, weight_, kernel_size, bias_, stride, padding, dilation, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::slow_conv_dilated2d::call(self_, weight_, kernel_size, bias_, stride, padding, dilation);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_ndtri_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_ndtri_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_ndtri_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_ndtri::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_erfc_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_erfc_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_erfc_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_erfc::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_logit_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_logit_out::call(self_meta, eps, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_logit_out::call(self_, eps, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_logit::call(self_, eps);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_polygamma_out_out(c10::DispatchKeySet dispatchKeySet, int64_t n, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_polygamma_out::call(n, self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_polygamma_out::call(n, self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_polygamma::call(n, self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_sinc_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_sinc_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_sinc_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_sinc::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_ifft_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_ifft_out::call(self_meta, n, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_ifft_out::call(self_, n, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_ifft::call(self_, n, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_ihfft_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_ihfft_out::call(self_meta, n, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_ihfft_out::call(self_, n, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_ihfft::call(self_, n, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_ifft2_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_ifft2_out::call(self_meta, s, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_ifft2_out::call(self_, s, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_ifft2::call(self_, s, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & fft_ihfftn_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_ihfftn_out::call(self_meta, s, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_ihfftn_out::call(self_, s, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_ihfftn::call(self_, s, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> linalg_cholesky_ex_out_L(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper, bool check_errors, at::Tensor & L, at::Tensor & info) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto L_meta = to_meta(L);
        auto info_meta = to_meta(info);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_cholesky_ex_L::call(self_meta, upper, check_errors, L_meta, info_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor L_;
      if (at::functionalization::impl::isFunctionalTensor(L)) {
        at::functionalization::impl::sync(L);
        L_ = at::functionalization::impl::from_functional_tensor(L);
      } else {
        L_ = L;
      }
      
      at::Tensor info_;
      if (at::functionalization::impl::isFunctionalTensor(info)) {
        at::functionalization::impl::sync(info);
        info_ = at::functionalization::impl::from_functional_tensor(info);
      } else {
        info_ = info;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(L) && at::functionalization::impl::isFunctionalTensor(info))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::linalg_cholesky_ex_L::call(self_, upper, check_errors, L_, info_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(L, info);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_cholesky_ex::call(self_, upper, check_errors);
        }
          at::functionalization::impl::propagate_xla_data(L, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(L, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(L);
  at::functionalization::impl::sync(L);
  at::functionalization::impl::propagate_xla_data(info, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(info, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(info);
  at::functionalization::impl::sync(info);
    return ::std::tuple<at::Tensor &,at::Tensor &>(L, info);
      }
    }

    at::Tensor & linalg_cross_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, int64_t dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_cross_out::call(self_meta, other_meta, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_cross_out::call(self_, other_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_cross::call(self_, other_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_lu_solve_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & LU, const at::Tensor & pivots, const at::Tensor & B, bool left, bool adjoint, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto LU_meta = to_meta(LU);
        auto pivots_meta = to_meta(pivots);
        auto B_meta = to_meta(B);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_lu_solve_out::call(LU_meta, pivots_meta, B_meta, left, adjoint, out_meta);
      }
      
      at::Tensor LU_;
      if (at::functionalization::impl::isFunctionalTensor(LU)) {
        at::functionalization::impl::sync(LU);
        LU_ = at::functionalization::impl::from_functional_tensor(LU);
      } else {
        LU_ = LU;
      }
      
      at::Tensor pivots_;
      if (at::functionalization::impl::isFunctionalTensor(pivots)) {
        at::functionalization::impl::sync(pivots);
        pivots_ = at::functionalization::impl::from_functional_tensor(pivots);
      } else {
        pivots_ = pivots;
      }
      
      at::Tensor B_;
      if (at::functionalization::impl::isFunctionalTensor(B)) {
        at::functionalization::impl::sync(B);
        B_ = at::functionalization::impl::from_functional_tensor(B);
      } else {
        B_ = B;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || LU.device().type() == c10::DeviceType::XLA || pivots.device().type() == c10::DeviceType::XLA || B.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(LU) || at::functionalization::impl::isFunctionalTensor(pivots) || at::functionalization::impl::isFunctionalTensor(B))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_lu_solve_out::call(LU_, pivots_, B_, left, adjoint, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_lu_solve::call(LU_, pivots_, B_, left, adjoint);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_matmul_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_matmul_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_matmul_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_matmul::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_vecdot_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & y, int64_t dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto y_meta = to_meta(y);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_vecdot_out::call(x_meta, y_meta, dim, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor y_;
      if (at::functionalization::impl::isFunctionalTensor(y)) {
        at::functionalization::impl::sync(y);
        y_ = at::functionalization::impl::from_functional_tensor(y);
      } else {
        y_ = y;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA || y.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(y))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_vecdot_out::call(x_, y_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_vecdot::call(x_, y_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_eigvalsh_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view UPLO, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_eigvalsh_out::call(self_meta, UPLO, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_eigvalsh_out::call(self_, UPLO, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_eigvalsh::call(self_, UPLO);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> linalg_inv_ex_out_inverse(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, bool check_errors, at::Tensor & inverse, at::Tensor & info) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto inverse_meta = to_meta(inverse);
        auto info_meta = to_meta(info);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_inv_ex_inverse::call(A_meta, check_errors, inverse_meta, info_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor inverse_;
      if (at::functionalization::impl::isFunctionalTensor(inverse)) {
        at::functionalization::impl::sync(inverse);
        inverse_ = at::functionalization::impl::from_functional_tensor(inverse);
      } else {
        inverse_ = inverse;
      }
      
      at::Tensor info_;
      if (at::functionalization::impl::isFunctionalTensor(info)) {
        at::functionalization::impl::sync(info);
        info_ = at::functionalization::impl::from_functional_tensor(info);
      } else {
        info_ = info;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(inverse) && at::functionalization::impl::isFunctionalTensor(info))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::linalg_inv_ex_inverse::call(A_, check_errors, inverse_, info_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(inverse, info);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_inv_ex::call(A_, check_errors);
        }
          at::functionalization::impl::propagate_xla_data(inverse, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(inverse, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(inverse);
  at::functionalization::impl::sync(inverse);
  at::functionalization::impl::propagate_xla_data(info, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(info, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(info);
  at::functionalization::impl::sync(info);
    return ::std::tuple<at::Tensor &,at::Tensor &>(inverse, info);
      }
    }

    at::Tensor & inverse_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::inverse_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::inverse_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::inverse::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & outer_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto vec2_meta = to_meta(vec2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::outer_out::call(self_meta, vec2_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor vec2_;
      if (at::functionalization::impl::isFunctionalTensor(vec2)) {
        at::functionalization::impl::sync(vec2);
        vec2_ = at::functionalization::impl::from_functional_tensor(vec2);
      } else {
        vec2_ = vec2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || vec2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(vec2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::outer_out::call(self_, vec2_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::outer::call(self_, vec2_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_svd_out_U(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, bool full_matrices, c10::optional<c10::string_view> driver, at::Tensor & U, at::Tensor & S, at::Tensor & Vh) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto U_meta = to_meta(U);
        auto S_meta = to_meta(S);
        auto Vh_meta = to_meta(Vh);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_svd_U::call(A_meta, full_matrices, driver, U_meta, S_meta, Vh_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor U_;
      if (at::functionalization::impl::isFunctionalTensor(U)) {
        at::functionalization::impl::sync(U);
        U_ = at::functionalization::impl::from_functional_tensor(U);
      } else {
        U_ = U;
      }
      
      at::Tensor S_;
      if (at::functionalization::impl::isFunctionalTensor(S)) {
        at::functionalization::impl::sync(S);
        S_ = at::functionalization::impl::from_functional_tensor(S);
      } else {
        S_ = S;
      }
      
      at::Tensor Vh_;
      if (at::functionalization::impl::isFunctionalTensor(Vh)) {
        at::functionalization::impl::sync(Vh);
        Vh_ = at::functionalization::impl::from_functional_tensor(Vh);
      } else {
        Vh_ = Vh;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(U) && at::functionalization::impl::isFunctionalTensor(S) && at::functionalization::impl::isFunctionalTensor(Vh))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::linalg_svd_U::call(A_, full_matrices, driver, U_, S_, Vh_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(U, S, Vh);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_svd::call(A_, full_matrices, driver);
        }
          at::functionalization::impl::propagate_xla_data(U, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(U, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(U);
  at::functionalization::impl::sync(U);
  at::functionalization::impl::propagate_xla_data(S, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(S, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(S);
  at::functionalization::impl::sync(S);
  at::functionalization::impl::propagate_xla_data(Vh, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(Vh, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(Vh);
  at::functionalization::impl::sync(Vh);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(U, S, Vh);
      }
    }

    at::Tensor & linalg_cond_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_cond_out::call(self_meta, p, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_cond_out::call(self_, p, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_cond::call(self_, p);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_cond_out_p_str_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view p, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_cond_p_str_out::call(self_meta, p, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_cond_p_str_out::call(self_, p, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_cond_p_str::call(self_, p);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> linalg_solve_ex_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, const at::Tensor & B, bool left, bool check_errors, at::Tensor & result, at::Tensor & info) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto B_meta = to_meta(B);
        auto result_meta = to_meta(result);
        auto info_meta = to_meta(info);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_solve_ex_out::call(A_meta, B_meta, left, check_errors, result_meta, info_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor B_;
      if (at::functionalization::impl::isFunctionalTensor(B)) {
        at::functionalization::impl::sync(B);
        B_ = at::functionalization::impl::from_functional_tensor(B);
      } else {
        B_ = B;
      }
      
      at::Tensor result_;
      if (at::functionalization::impl::isFunctionalTensor(result)) {
        at::functionalization::impl::sync(result);
        result_ = at::functionalization::impl::from_functional_tensor(result);
      } else {
        result_ = result;
      }
      
      at::Tensor info_;
      if (at::functionalization::impl::isFunctionalTensor(info)) {
        at::functionalization::impl::sync(info);
        info_ = at::functionalization::impl::from_functional_tensor(info);
      } else {
        info_ = info;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(result) && at::functionalization::impl::isFunctionalTensor(info))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA || B.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A) || at::functionalization::impl::isFunctionalTensor(B))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::linalg_solve_ex_out::call(A_, B_, left, check_errors, result_, info_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(result, info);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_solve_ex::call(A_, B_, left, check_errors);
        }
          at::functionalization::impl::propagate_xla_data(result, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(result, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(result);
  at::functionalization::impl::sync(result);
  at::functionalization::impl::propagate_xla_data(info, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(info, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(info);
  at::functionalization::impl::sync(info);
    return ::std::tuple<at::Tensor &,at::Tensor &>(result, info);
      }
    }

    at::Tensor & linalg_tensorsolve_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::OptionalIntArrayRef dims, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_tensorsolve_out::call(self_meta, other_meta, dims, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_tensorsolve_out::call(self_, other_, dims, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_tensorsolve::call(self_, other_, dims);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_matrix_rank_out_atol_rtol_tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto atol_meta = to_meta(atol);
        auto rtol_meta = to_meta(rtol);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_matrix_rank_atol_rtol_tensor_out::call(input_meta, atol_meta, rtol_meta, hermitian, out_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      c10::optional<at::Tensor> atol_;
      if (at::functionalization::impl::isFunctionalTensor(atol)) {
        at::functionalization::impl::sync(atol);
        atol_ = at::functionalization::impl::from_functional_tensor(atol);
      } else {
        atol_ = atol;
      }
      
      c10::optional<at::Tensor> rtol_;
      if (at::functionalization::impl::isFunctionalTensor(rtol)) {
        at::functionalization::impl::sync(rtol);
        rtol_ = at::functionalization::impl::from_functional_tensor(rtol);
      } else {
        rtol_ = rtol;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(atol) || at::functionalization::impl::isFunctionalTensor(rtol))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_matrix_rank_atol_rtol_tensor_out::call(input_, atol_, rtol_, hermitian, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_matrix_rank_atol_rtol_tensor::call(input_, atol_, rtol_, hermitian);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_matrix_rank_out_atol_rtol_float_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_matrix_rank_atol_rtol_float_out::call(self_meta, atol, rtol, hermitian, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_matrix_rank_atol_rtol_float_out::call(self_, atol, rtol, hermitian, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_matrix_rank_atol_rtol_float::call(self_, atol, rtol, hermitian);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_matrix_rank_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double tol, bool hermitian, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_matrix_rank_out::call(self_meta, tol, hermitian, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_matrix_rank_out::call(self_, tol, hermitian, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_matrix_rank::call(self_, tol, hermitian);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_matrix_rank_out_out_tol_tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & tol, bool hermitian, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto tol_meta = to_meta(tol);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_matrix_rank_out_tol_tensor::call(input_meta, tol_meta, hermitian, out_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor tol_;
      if (at::functionalization::impl::isFunctionalTensor(tol)) {
        at::functionalization::impl::sync(tol);
        tol_ = at::functionalization::impl::from_functional_tensor(tol);
      } else {
        tol_ = tol;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || tol.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(tol))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_matrix_rank_out_tol_tensor::call(input_, tol_, hermitian, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_matrix_rank_tol_tensor::call(input_, tol_, hermitian);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _test_optional_floatlist_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & values, c10::optional<at::ArrayRef<double>> addends, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto values_meta = to_meta(values);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_test_optional_floatlist_out::call(values_meta, addends, out_meta);
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || values.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(values))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_test_optional_floatlist_out::call(values_, addends, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_test_optional_floatlist::call(values_, addends);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _test_warn_in_autograd_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_test_warn_in_autograd_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_test_warn_in_autograd_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_test_warn_in_autograd::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _segment_reduce_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & output, const at::Tensor & data, c10::string_view reduce, const c10::optional<at::Tensor> & lengths, const c10::optional<at::Tensor> & offsets, int64_t axis, const c10::optional<at::Scalar> & initial, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_meta = to_meta(grad);
        auto output_meta = to_meta(output);
        auto data_meta = to_meta(data);
        auto lengths_meta = to_meta(lengths);
        auto offsets_meta = to_meta(offsets);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_segment_reduce_backward_out::call(grad_meta, output_meta, data_meta, reduce, lengths_meta, offsets_meta, axis, initial, out_meta);
      }
      
      at::Tensor grad_;
      if (at::functionalization::impl::isFunctionalTensor(grad)) {
        at::functionalization::impl::sync(grad);
        grad_ = at::functionalization::impl::from_functional_tensor(grad);
      } else {
        grad_ = grad;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor data_;
      if (at::functionalization::impl::isFunctionalTensor(data)) {
        at::functionalization::impl::sync(data);
        data_ = at::functionalization::impl::from_functional_tensor(data);
      } else {
        data_ = data;
      }
      
      c10::optional<at::Tensor> lengths_;
      if (at::functionalization::impl::isFunctionalTensor(lengths)) {
        at::functionalization::impl::sync(lengths);
        lengths_ = at::functionalization::impl::from_functional_tensor(lengths);
      } else {
        lengths_ = lengths;
      }
      
      c10::optional<at::Tensor> offsets_;
      if (at::functionalization::impl::isFunctionalTensor(offsets)) {
        at::functionalization::impl::sync(offsets);
        offsets_ = at::functionalization::impl::from_functional_tensor(offsets);
      } else {
        offsets_ = offsets;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad.device().type() == c10::DeviceType::XLA || output.device().type() == c10::DeviceType::XLA || data.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad) || at::functionalization::impl::isFunctionalTensor(output) || at::functionalization::impl::isFunctionalTensor(data) || at::functionalization::impl::isFunctionalTensor(lengths) || at::functionalization::impl::isFunctionalTensor(offsets))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_segment_reduce_backward_out::call(grad_, output_, data_, reduce, lengths_, offsets_, axis, initial, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_segment_reduce_backward::call(grad_, output_, data_, reduce, lengths_, offsets_, axis, initial);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _sparse_broadcast_to_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sparse_broadcast_to_copy_out::call(self_meta, size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sparse_broadcast_to_copy_out::call(self_, size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sparse_broadcast_to_copy::call(self_, size);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & unsqueeze_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::unsqueeze_copy_out::call(self_meta, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::unsqueeze_copy_out::call(self_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::unsqueeze_copy::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & values_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::values_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::values_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::values_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & to_padded_tensor_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double padding, at::OptionalSymIntArrayRef output_size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::to_padded_tensor_out::call(self_meta, padding, output_size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::to_padded_tensor_out::call(self_, padding, output_size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::to_padded_tensor::call(self_, padding, output_size);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _triton_scaled_dot_attention_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & q, const at::Tensor & k, const at::Tensor & v, double dropout_p, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto q_meta = to_meta(q);
        auto k_meta = to_meta(k);
        auto v_meta = to_meta(v);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_triton_scaled_dot_attention_out::call(q_meta, k_meta, v_meta, dropout_p, out_meta);
      }
      
      at::Tensor q_;
      if (at::functionalization::impl::isFunctionalTensor(q)) {
        at::functionalization::impl::sync(q);
        q_ = at::functionalization::impl::from_functional_tensor(q);
      } else {
        q_ = q;
      }
      
      at::Tensor k_;
      if (at::functionalization::impl::isFunctionalTensor(k)) {
        at::functionalization::impl::sync(k);
        k_ = at::functionalization::impl::from_functional_tensor(k);
      } else {
        k_ = k;
      }
      
      at::Tensor v_;
      if (at::functionalization::impl::isFunctionalTensor(v)) {
        at::functionalization::impl::sync(v);
        v_ = at::functionalization::impl::from_functional_tensor(v);
      } else {
        v_ = v;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || q.device().type() == c10::DeviceType::XLA || k.device().type() == c10::DeviceType::XLA || v.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(q) || at::functionalization::impl::isFunctionalTensor(k) || at::functionalization::impl::isFunctionalTensor(v))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_triton_scaled_dot_attention_out::call(q_, k_, v_, dropout_p, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_triton_scaled_dot_attention::call(q_, k_, v_, dropout_p);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_bessel_y0_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_bessel_y0_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_bessel_y0_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_bessel_y0::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_chebyshev_polynomial_t_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_chebyshev_polynomial_t_out::call(x_meta, n_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_chebyshev_polynomial_t_out::call(x_, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_chebyshev_polynomial_t::call(x_, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_chebyshev_polynomial_t_out_x_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_chebyshev_polynomial_t_x_scalar_out::call(x, n_meta, out_meta);
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_chebyshev_polynomial_t_x_scalar_out::call(x, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_chebyshev_polynomial_t_x_scalar::call(x, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_chebyshev_polynomial_t_out_n_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_chebyshev_polynomial_t_n_scalar_out::call(x_meta, n, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_chebyshev_polynomial_t_n_scalar_out::call(x_, n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_chebyshev_polynomial_t_n_scalar::call(x_, n);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_chebyshev_polynomial_u_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_chebyshev_polynomial_u_out::call(x_meta, n_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_chebyshev_polynomial_u_out::call(x_, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_chebyshev_polynomial_u::call(x_, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_chebyshev_polynomial_u_out_x_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_chebyshev_polynomial_u_x_scalar_out::call(x, n_meta, out_meta);
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_chebyshev_polynomial_u_x_scalar_out::call(x, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_chebyshev_polynomial_u_x_scalar::call(x, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_chebyshev_polynomial_u_out_n_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_chebyshev_polynomial_u_n_scalar_out::call(x_meta, n, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_chebyshev_polynomial_u_n_scalar_out::call(x_, n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_chebyshev_polynomial_u_n_scalar::call(x_, n);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_hermite_polynomial_h_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_hermite_polynomial_h_out::call(x_meta, n_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_hermite_polynomial_h_out::call(x_, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_hermite_polynomial_h::call(x_, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_hermite_polynomial_h_out_x_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_hermite_polynomial_h_x_scalar_out::call(x, n_meta, out_meta);
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_hermite_polynomial_h_x_scalar_out::call(x, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_hermite_polynomial_h_x_scalar::call(x, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_hermite_polynomial_h_out_n_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_hermite_polynomial_h_n_scalar_out::call(x_meta, n, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_hermite_polynomial_h_n_scalar_out::call(x_, n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_hermite_polynomial_h_n_scalar::call(x_, n);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_modified_bessel_k1_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_modified_bessel_k1_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_modified_bessel_k1_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_modified_bessel_k1::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_scaled_modified_bessel_k0_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_scaled_modified_bessel_k0_out::call(x_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_scaled_modified_bessel_k0_out::call(x_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_scaled_modified_bessel_k0::call(x_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_scaled_modified_bessel_k1_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_scaled_modified_bessel_k1_out::call(x_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_scaled_modified_bessel_k1_out::call(x_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_scaled_modified_bessel_k1::call(x_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_spherical_bessel_j0_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_spherical_bessel_j0_out::call(x_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_spherical_bessel_j0_out::call(x_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_spherical_bessel_j0::call(x_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    void _fused_sgd_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList momentum_buffer_list, double weight_decay, double momentum, double lr, double dampening, bool nesterov, bool maximize, bool is_first_step, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grads_meta = to_meta(grads);
        auto momentum_buffer_list_meta = to_meta(momentum_buffer_list);
        auto grad_scale_meta = to_meta(grad_scale);
        auto found_inf_meta = to_meta(found_inf);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_sgd_out::call(self_meta, grads_meta, momentum_buffer_list_meta, weight_decay, momentum, lr, dampening, nesterov, maximize, is_first_step, grad_scale_meta, found_inf_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> grads_;
      if (at::functionalization::impl::isFunctionalTensor(grads)) {
        at::functionalization::impl::sync(grads);
        grads_ = at::functionalization::impl::from_functional_tensor(grads);
      } else {
        grads_ = grads.vec();
      }
      
      ::std::vector<at::Tensor> momentum_buffer_list_;
      if (at::functionalization::impl::isFunctionalTensor(momentum_buffer_list)) {
        at::functionalization::impl::sync(momentum_buffer_list);
        momentum_buffer_list_ = at::functionalization::impl::from_functional_tensor(momentum_buffer_list);
      } else {
        momentum_buffer_list_ = momentum_buffer_list.vec();
      }
      
      c10::optional<at::Tensor> grad_scale_;
      if (at::functionalization::impl::isFunctionalTensor(grad_scale)) {
        at::functionalization::impl::sync(grad_scale);
        grad_scale_ = at::functionalization::impl::from_functional_tensor(grad_scale);
      } else {
        grad_scale_ = grad_scale;
      }
      
      c10::optional<at::Tensor> found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grads) && at::functionalization::impl::isFunctionalTensor(momentum_buffer_list) && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(grad_scale) || at::functionalization::impl::isFunctionalTensor(found_inf))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_fused_sgd_out::call(self_, grads_, momentum_buffer_list_, weight_decay, momentum, lr, dampening, nesterov, maximize, is_first_step, grad_scale_, found_inf_, out_);
         ;
        }
      } else {
        ::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_sgd::call(self_, grads_, momentum_buffer_list_, weight_decay, momentum, lr, dampening, nesterov, maximize, is_first_step, grad_scale_, found_inf_);
        }
          at::functionalization::impl::propagate_xla_data(grads, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(grads, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(grads);
  at::functionalization::impl::sync(grads);
  at::functionalization::impl::propagate_xla_data(momentum_buffer_list, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(momentum_buffer_list, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(momentum_buffer_list);
  at::functionalization::impl::sync(momentum_buffer_list);
  at::functionalization::impl::propagate_xla_data(out, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _fused_sgd_(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList momentum_buffer_list, double weight_decay, double momentum, double lr, double dampening, bool nesterov, bool maximize, bool is_first_step, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grads_meta = to_meta(grads);
        auto momentum_buffer_list_meta = to_meta(momentum_buffer_list);
        auto grad_scale_meta = to_meta(grad_scale);
        auto found_inf_meta = to_meta(found_inf);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_sgd_::call(self_meta, grads_meta, momentum_buffer_list_meta, weight_decay, momentum, lr, dampening, nesterov, maximize, is_first_step, grad_scale_meta, found_inf_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> grads_;
      if (at::functionalization::impl::isFunctionalTensor(grads)) {
        at::functionalization::impl::sync(grads);
        grads_ = at::functionalization::impl::from_functional_tensor(grads);
      } else {
        grads_ = grads.vec();
      }
      
      ::std::vector<at::Tensor> momentum_buffer_list_;
      if (at::functionalization::impl::isFunctionalTensor(momentum_buffer_list)) {
        at::functionalization::impl::sync(momentum_buffer_list);
        momentum_buffer_list_ = at::functionalization::impl::from_functional_tensor(momentum_buffer_list);
      } else {
        momentum_buffer_list_ = momentum_buffer_list.vec();
      }
      
      c10::optional<at::Tensor> grad_scale_;
      if (at::functionalization::impl::isFunctionalTensor(grad_scale)) {
        at::functionalization::impl::sync(grad_scale);
        grad_scale_ = at::functionalization::impl::from_functional_tensor(grad_scale);
      } else {
        grad_scale_ = grad_scale;
      }
      
      c10::optional<at::Tensor> found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self) && at::functionalization::impl::isFunctionalTensor(grads) && at::functionalization::impl::isFunctionalTensor(momentum_buffer_list))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(grad_scale) || at::functionalization::impl::isFunctionalTensor(found_inf))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_fused_sgd_::call(self_, grads_, momentum_buffer_list_, weight_decay, momentum, lr, dampening, nesterov, maximize, is_first_step, grad_scale_, found_inf_);
         ;
        }
      } else {
        ::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_sgd::call(self_, grads_, momentum_buffer_list_, weight_decay, momentum, lr, dampening, nesterov, maximize, is_first_step, grad_scale_, found_inf_);
        }
          at::functionalization::impl::propagate_xla_data(self, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(self, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
  at::functionalization::impl::propagate_xla_data(grads, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(grads, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(grads);
  at::functionalization::impl::sync(grads);
  at::functionalization::impl::propagate_xla_data(momentum_buffer_list, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(momentum_buffer_list, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(momentum_buffer_list);
  at::functionalization::impl::sync(momentum_buffer_list);
    
      }
    }

    void _fused_sgd_out_tensor_lr_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList momentum_buffer_list, double weight_decay, double momentum, const at::Tensor & lr, double dampening, bool nesterov, bool maximize, bool is_first_step, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grads_meta = to_meta(grads);
        auto momentum_buffer_list_meta = to_meta(momentum_buffer_list);
        auto lr_meta = to_meta(lr);
        auto grad_scale_meta = to_meta(grad_scale);
        auto found_inf_meta = to_meta(found_inf);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_sgd_tensor_lr_out::call(self_meta, grads_meta, momentum_buffer_list_meta, weight_decay, momentum, lr_meta, dampening, nesterov, maximize, is_first_step, grad_scale_meta, found_inf_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> grads_;
      if (at::functionalization::impl::isFunctionalTensor(grads)) {
        at::functionalization::impl::sync(grads);
        grads_ = at::functionalization::impl::from_functional_tensor(grads);
      } else {
        grads_ = grads.vec();
      }
      
      ::std::vector<at::Tensor> momentum_buffer_list_;
      if (at::functionalization::impl::isFunctionalTensor(momentum_buffer_list)) {
        at::functionalization::impl::sync(momentum_buffer_list);
        momentum_buffer_list_ = at::functionalization::impl::from_functional_tensor(momentum_buffer_list);
      } else {
        momentum_buffer_list_ = momentum_buffer_list.vec();
      }
      
      at::Tensor lr_;
      if (at::functionalization::impl::isFunctionalTensor(lr)) {
        at::functionalization::impl::sync(lr);
        lr_ = at::functionalization::impl::from_functional_tensor(lr);
      } else {
        lr_ = lr;
      }
      
      c10::optional<at::Tensor> grad_scale_;
      if (at::functionalization::impl::isFunctionalTensor(grad_scale)) {
        at::functionalization::impl::sync(grad_scale);
        grad_scale_ = at::functionalization::impl::from_functional_tensor(grad_scale);
      } else {
        grad_scale_ = grad_scale;
      }
      
      c10::optional<at::Tensor> found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grads) && at::functionalization::impl::isFunctionalTensor(momentum_buffer_list) && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || lr.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(lr) || at::functionalization::impl::isFunctionalTensor(grad_scale) || at::functionalization::impl::isFunctionalTensor(found_inf))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_fused_sgd_tensor_lr_out::call(self_, grads_, momentum_buffer_list_, weight_decay, momentum, lr_, dampening, nesterov, maximize, is_first_step, grad_scale_, found_inf_, out_);
         ;
        }
      } else {
        ::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_sgd_tensor_lr::call(self_, grads_, momentum_buffer_list_, weight_decay, momentum, lr_, dampening, nesterov, maximize, is_first_step, grad_scale_, found_inf_);
        }
          at::functionalization::impl::propagate_xla_data(grads, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(grads, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(grads);
  at::functionalization::impl::sync(grads);
  at::functionalization::impl::propagate_xla_data(momentum_buffer_list, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(momentum_buffer_list, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(momentum_buffer_list);
  at::functionalization::impl::sync(momentum_buffer_list);
  at::functionalization::impl::propagate_xla_data(out, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _fused_sgd__tensor_lr(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList momentum_buffer_list, double weight_decay, double momentum, const at::Tensor & lr, double dampening, bool nesterov, bool maximize, bool is_first_step, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grads_meta = to_meta(grads);
        auto momentum_buffer_list_meta = to_meta(momentum_buffer_list);
        auto lr_meta = to_meta(lr);
        auto grad_scale_meta = to_meta(grad_scale);
        auto found_inf_meta = to_meta(found_inf);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_sgd__tensor_lr::call(self_meta, grads_meta, momentum_buffer_list_meta, weight_decay, momentum, lr_meta, dampening, nesterov, maximize, is_first_step, grad_scale_meta, found_inf_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> grads_;
      if (at::functionalization::impl::isFunctionalTensor(grads)) {
        at::functionalization::impl::sync(grads);
        grads_ = at::functionalization::impl::from_functional_tensor(grads);
      } else {
        grads_ = grads.vec();
      }
      
      ::std::vector<at::Tensor> momentum_buffer_list_;
      if (at::functionalization::impl::isFunctionalTensor(momentum_buffer_list)) {
        at::functionalization::impl::sync(momentum_buffer_list);
        momentum_buffer_list_ = at::functionalization::impl::from_functional_tensor(momentum_buffer_list);
      } else {
        momentum_buffer_list_ = momentum_buffer_list.vec();
      }
      
      at::Tensor lr_;
      if (at::functionalization::impl::isFunctionalTensor(lr)) {
        at::functionalization::impl::sync(lr);
        lr_ = at::functionalization::impl::from_functional_tensor(lr);
      } else {
        lr_ = lr;
      }
      
      c10::optional<at::Tensor> grad_scale_;
      if (at::functionalization::impl::isFunctionalTensor(grad_scale)) {
        at::functionalization::impl::sync(grad_scale);
        grad_scale_ = at::functionalization::impl::from_functional_tensor(grad_scale);
      } else {
        grad_scale_ = grad_scale;
      }
      
      c10::optional<at::Tensor> found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self) && at::functionalization::impl::isFunctionalTensor(grads) && at::functionalization::impl::isFunctionalTensor(momentum_buffer_list))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || lr.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(lr) || at::functionalization::impl::isFunctionalTensor(grad_scale) || at::functionalization::impl::isFunctionalTensor(found_inf))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_fused_sgd__tensor_lr::call(self_, grads_, momentum_buffer_list_, weight_decay, momentum, lr_, dampening, nesterov, maximize, is_first_step, grad_scale_, found_inf_);
         ;
        }
      } else {
        ::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_sgd_tensor_lr::call(self_, grads_, momentum_buffer_list_, weight_decay, momentum, lr_, dampening, nesterov, maximize, is_first_step, grad_scale_, found_inf_);
        }
          at::functionalization::impl::propagate_xla_data(self, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(self, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
  at::functionalization::impl::propagate_xla_data(grads, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(grads, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(grads);
  at::functionalization::impl::sync(grads);
  at::functionalization::impl::propagate_xla_data(momentum_buffer_list, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(momentum_buffer_list, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(momentum_buffer_list);
  at::functionalization::impl::sync(momentum_buffer_list);
    
      }
    }

    at::Tensor _neg_view(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::_neg_view::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::_neg_view::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::_neg_view::call(self_);
        } else {
          tmp_output = at::_ops::_neg_view_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::_neg_view::call(base);
          } else {
            return at::_ops::_neg_view_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::_neg_view_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor diagonal(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::diagonal::call(self_, offset, dim1, dim2);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::diagonal::call(self_meta, offset, dim1, dim2);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::diagonal::call(self_, offset, dim1, dim2);
        } else {
          tmp_output = at::_ops::diagonal_copy::call(self_, offset, dim1, dim2);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, offset = offset, dim1 = dim1, dim2 = dim2](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::diagonal::call(base, offset, dim1, dim2);
          } else {
            return at::_ops::diagonal_copy::call(base, offset, dim1, dim2);
          }
        },
        [inverse_return_mode = inverse_return_mode, offset = offset, dim1 = dim1, dim2 = dim2](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::diagonal_inverse(base, mutated_view, inverse_return_mode, offset, dim1, dim2);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor select_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::SymInt index) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::select_int::call(self_, dim, index);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::select_int::call(self_meta, dim, index);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::select_int::call(self_, dim, index);
        } else {
          tmp_output = at::_ops::select_copy_int::call(self_, dim, index);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, dim = dim, index = index](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::select_int::call(base, dim, index);
          } else {
            return at::_ops::select_copy_int::call(base, dim, index);
          }
        },
        [inverse_return_mode = inverse_return_mode, dim = dim, index = index](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::select_int_inverse(base, mutated_view, inverse_return_mode, dim, index);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor slice_inverse(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & src, int64_t dim, c10::optional<c10::SymInt> start, c10::optional<c10::SymInt> end, c10::SymInt step) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::slice_inverse::call(self_, src_, dim, start, end, step);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        auto src_meta = to_meta(src);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::slice_inverse::call(self_meta, src_meta, dim, start, end, step);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::slice_inverse::call(self_, src_, dim, start, end, step);
        } else {
          tmp_output = at::_ops::slice_scatter::call(self_, src_, dim, start, end, step);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, src = src, dim = dim, start = start, end = end, step = step](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::slice_inverse::call(base, src, dim, start, end, step);
          } else {
            return at::_ops::slice_scatter::call(base, src, dim, start, end, step);
          }
        },
        [inverse_return_mode = inverse_return_mode, src = src, dim = dim, start = start, end = end, step = step](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::slice_inverse_inverse(base, mutated_view, inverse_return_mode, src, dim, start, end, step);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    ::std::vector<at::Tensor> split_with_sizes(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::split_with_sizes::call(self_, split_sizes, dim);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      ::std::vector<at::Tensor> reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::split_with_sizes::call(self_meta, split_sizes, dim);
      }
      ::std::vector<at::Tensor> tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::split_with_sizes::call(self_, split_sizes, dim);
        } else {
          tmp_output = at::_ops::split_with_sizes_copy::call(self_, split_sizes, dim);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, split_sizes = split_sizes.vec(), dim = dim](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::split_with_sizes::call(base, split_sizes, dim)[mutated_view_idx];
          } else {
            return at::_ops::split_with_sizes_copy::call(base, split_sizes, dim)[mutated_view_idx];
          }
        },
        [inverse_return_mode = inverse_return_mode, split_sizes = split_sizes.vec(), dim = dim](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::split_with_sizes_inverse(base, mutated_view, inverse_return_mode, mutated_view_idx, split_sizes, dim);
        },
        /*is_multi_output=*/true
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor squeeze(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::squeeze::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::squeeze::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::squeeze::call(self_);
        } else {
          tmp_output = at::_ops::squeeze_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::squeeze::call(base);
          } else {
            return at::_ops::squeeze_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::squeeze_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor & squeeze_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::squeeze_::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::squeeze::call(base);
          } else {
            return at::_ops::squeeze_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::squeeze_inverse(base, mutated_view, inverse_return_mode);
        }
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::squeeze_::call(self_meta);
      }
      // This function adds the above view meta to the current tensor and replays them off the base,
      // mutating the size/stride info of the current FunctionalTensorWrapper.
      // Because of this, we need to make sure to run the reference shape function above,
      // BEFORE doing this (otherwise we'll end up runnin the reference function using the wrong sizes/strides)
      at::functionalization::impl::mutate_view_meta(self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      // XLA/LTC don't implement the logic to propagate strides correctly, so we need to rely
      // on a reference implementation here (instead of relying on the output from the forward lambda
      // having the correct stride info)
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(self, reference_tensor_output);
      }
      return self;
    }

    at::Tensor squeeze_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::squeeze_dim::call(self_, dim);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::squeeze_dim::call(self_meta, dim);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::squeeze_dim::call(self_, dim);
        } else {
          tmp_output = at::_ops::squeeze_copy_dim::call(self_, dim);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, dim = dim](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::squeeze_dim::call(base, dim);
          } else {
            return at::_ops::squeeze_copy_dim::call(base, dim);
          }
        },
        [inverse_return_mode = inverse_return_mode, dim = dim](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::squeeze_dim_inverse(base, mutated_view, inverse_return_mode, dim);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor & squeeze__dim(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim) {
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::squeeze__dim::call(self_, dim);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, dim = dim](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::squeeze_dim::call(base, dim);
          } else {
            return at::_ops::squeeze_copy_dim::call(base, dim);
          }
        },
        [inverse_return_mode = inverse_return_mode, dim = dim](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::squeeze_dim_inverse(base, mutated_view, inverse_return_mode, dim);
        }
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::squeeze__dim::call(self_meta, dim);
      }
      // This function adds the above view meta to the current tensor and replays them off the base,
      // mutating the size/stride info of the current FunctionalTensorWrapper.
      // Because of this, we need to make sure to run the reference shape function above,
      // BEFORE doing this (otherwise we'll end up runnin the reference function using the wrong sizes/strides)
      at::functionalization::impl::mutate_view_meta(self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      // XLA/LTC don't implement the logic to propagate strides correctly, so we need to rely
      // on a reference implementation here (instead of relying on the output from the forward lambda
      // having the correct stride info)
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(self, reference_tensor_output);
      }
      return self;
    }

    at::Tensor squeeze_dims(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::squeeze_dims::call(self_, dim);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::squeeze_dims::call(self_meta, dim);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::squeeze_dims::call(self_, dim);
        } else {
          tmp_output = at::_ops::squeeze_copy_dims::call(self_, dim);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, dim = dim.vec()](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::squeeze_dims::call(base, dim);
          } else {
            return at::_ops::squeeze_copy_dims::call(base, dim);
          }
        },
        [inverse_return_mode = inverse_return_mode, dim = dim.vec()](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::squeeze_dims_inverse(base, mutated_view, inverse_return_mode, dim);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor & squeeze__dims(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::IntArrayRef dim) {
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::squeeze__dims::call(self_, dim);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, dim = dim.vec()](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::squeeze_dims::call(base, dim);
          } else {
            return at::_ops::squeeze_copy_dims::call(base, dim);
          }
        },
        [inverse_return_mode = inverse_return_mode, dim = dim.vec()](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::squeeze_dims_inverse(base, mutated_view, inverse_return_mode, dim);
        }
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::squeeze__dims::call(self_meta, dim);
      }
      // This function adds the above view meta to the current tensor and replays them off the base,
      // mutating the size/stride info of the current FunctionalTensorWrapper.
      // Because of this, we need to make sure to run the reference shape function above,
      // BEFORE doing this (otherwise we'll end up runnin the reference function using the wrong sizes/strides)
      at::functionalization::impl::mutate_view_meta(self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      // XLA/LTC don't implement the logic to propagate strides correctly, so we need to rely
      // on a reference implementation here (instead of relying on the output from the forward lambda
      // having the correct stride info)
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(self, reference_tensor_output);
      }
      return self;
    }

    ::std::vector<at::Tensor> unbind_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::unbind_int::call(self_, dim);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      ::std::vector<at::Tensor> reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::unbind_int::call(self_meta, dim);
      }
      ::std::vector<at::Tensor> tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::unbind_int::call(self_, dim);
        } else {
          tmp_output = at::_ops::unbind_copy_int::call(self_, dim);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, dim = dim](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::unbind_int::call(base, dim)[mutated_view_idx];
          } else {
            return at::_ops::unbind_copy_int::call(base, dim)[mutated_view_idx];
          }
        },
        [inverse_return_mode = inverse_return_mode, dim = dim](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::unbind_int_inverse(base, mutated_view, inverse_return_mode, mutated_view_idx, dim);
        },
        /*is_multi_output=*/true
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor alias(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::alias::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::alias::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::alias::call(self_);
        } else {
          tmp_output = at::_ops::alias_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::alias::call(base);
          } else {
            return at::_ops::alias_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::alias_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

}  // namespace functionalization

namespace {

TORCH_LIBRARY_IMPL(aten, Functionalize, m) {
  m.impl("_new_zeros_with_same_feature_meta.out", TORCH_FN(functionalization::_new_zeros_with_same_feature_meta_out_out));
  m.impl("_cudnn_init_dropout_state.out", TORCH_FN(functionalization::_cudnn_init_dropout_state_out_out));
  m.impl("angle.out", TORCH_FN(functionalization::angle_out_out));
  m.impl("sgn.out", TORCH_FN(functionalization::sgn_out_out));
  m.impl("sgn_", TORCH_FN(functionalization::sgn_));
  m.impl("_add_relu.out", TORCH_FN(functionalization::_add_relu_out_out));
  m.impl("_add_relu_.Tensor", TORCH_FN(functionalization::_add_relu__Tensor));
  m.impl("_add_relu.Scalar_out", TORCH_FN(functionalization::_add_relu_out_Scalar_out));
  m.impl("_add_relu_.Scalar", TORCH_FN(functionalization::_add_relu__Scalar));
  m.impl("_test_functorch_fallback.out", TORCH_FN(functionalization::_test_functorch_fallback_out_out));
  m.impl("any.out", TORCH_FN(functionalization::any_out_out));
  m.impl("any.dims_out", TORCH_FN(functionalization::any_out_dims_out));
  m.impl("argmin.out", TORCH_FN(functionalization::argmin_out_out));
  m.impl("acosh.out", TORCH_FN(functionalization::acosh_out_out));
  m.impl("acosh_", TORCH_FN(functionalization::acosh_));
  m.impl("bincount.out", TORCH_FN(functionalization::bincount_out_out));
  m.impl("copysign.out", TORCH_FN(functionalization::copysign_out_out));
  m.impl("copysign_.Tensor", TORCH_FN(functionalization::copysign__Tensor));
  m.impl("copysign.Scalar_out", TORCH_FN(functionalization::copysign_out_Scalar_out));
  m.impl("copysign_.Scalar", TORCH_FN(functionalization::copysign__Scalar));
  m.impl("logical_or.out", TORCH_FN(functionalization::logical_or_out_out));
  m.impl("logical_or_", TORCH_FN(functionalization::logical_or_));
  m.impl("cat.out", TORCH_FN(functionalization::cat_out_out));
  m.impl("ceil.out", TORCH_FN(functionalization::ceil_out_out));
  m.impl("ceil_", TORCH_FN(functionalization::ceil_));
  m.impl("polar.out", TORCH_FN(functionalization::polar_out_out));
  m.impl("convolution.out", TORCH_FN(functionalization::convolution_out_out));
  m.impl("convolution_overrideable.out", TORCH_FN(functionalization::convolution_overrideable_out_out));
  m.impl("convolution_backward_overrideable.out", TORCH_FN(functionalization::convolution_backward_overrideable_out_out));
  m.impl("cudnn_affine_grid_generator.out", TORCH_FN(functionalization::cudnn_affine_grid_generator_out_out));
  m.impl("cudnn_batch_norm_backward.out", TORCH_FN(functionalization::cudnn_batch_norm_backward_out_out));
  m.impl("cudnn_convolution_relu.out", TORCH_FN(functionalization::cudnn_convolution_relu_out_out));
  m.impl("cudnn_convolution_add_relu.out", TORCH_FN(functionalization::cudnn_convolution_add_relu_out_out));
  m.impl("cummax.out", TORCH_FN(functionalization::cummax_out_out));
  m.impl("embedding_renorm.out", TORCH_FN(functionalization::embedding_renorm_out_out));
  m.impl("embedding_renorm_", TORCH_FN(functionalization::embedding_renorm_));
  m.impl("_embedding_bag_forward_only.out", TORCH_FN(functionalization::_embedding_bag_forward_only_out_out));
  m.impl("_embedding_bag_dense_backward.out", TORCH_FN(functionalization::_embedding_bag_dense_backward_out_out));
  m.impl("empty.names_out", TORCH_FN(functionalization::empty_out_names_out));
  m.impl("empty_like.out", TORCH_FN(functionalization::empty_like_out_out));
  m.impl("erf.out", TORCH_FN(functionalization::erf_out_out));
  m.impl("erf_", TORCH_FN(functionalization::erf_));
  m.impl("erfc.out", TORCH_FN(functionalization::erfc_out_out));
  m.impl("erfc_", TORCH_FN(functionalization::erfc_));
  m.impl("eye.out", TORCH_FN(functionalization::eye_out_out));
  m.impl("eye.m_out", TORCH_FN(functionalization::eye_out_m_out));
  m.impl("gcd.out", TORCH_FN(functionalization::gcd_out_out));
  m.impl("gcd_", TORCH_FN(functionalization::gcd_));
  m.impl("grid_sampler_2d_backward.out", TORCH_FN(functionalization::grid_sampler_2d_backward_out_out));
  m.impl("native_group_norm_backward.out", TORCH_FN(functionalization::native_group_norm_backward_out_out));
  m.impl("_fft_r2c.out", TORCH_FN(functionalization::_fft_r2c_out_out));
  m.impl("index.Tensor_out", TORCH_FN(functionalization::index_out_Tensor_out));
  m.impl("index_put.out", TORCH_FN(functionalization::index_put_out_out));
  m.impl("index_put_", TORCH_FN(functionalization::index_put_));
  m.impl("linear_backward.out", TORCH_FN(functionalization::linear_backward_out_out));
  m.impl("mkldnn_linear_backward_weights.out", TORCH_FN(functionalization::mkldnn_linear_backward_weights_out_out));
  m.impl("logaddexp.out", TORCH_FN(functionalization::logaddexp_out_out));
  m.impl("_logcumsumexp.out", TORCH_FN(functionalization::_logcumsumexp_out_out));
  m.impl("logcumsumexp.out", TORCH_FN(functionalization::logcumsumexp_out_out));
  m.impl("matmul_backward.out", TORCH_FN(functionalization::matmul_backward_out_out));
  m.impl("median.out", TORCH_FN(functionalization::median_out_out));
  m.impl("median.dim_values", TORCH_FN(functionalization::median_out_dim_values));
  m.impl("amin.out", TORCH_FN(functionalization::amin_out_out));
  m.impl("mkldnn_rnn_layer_backward.out", TORCH_FN(functionalization::mkldnn_rnn_layer_backward_out_out));
  m.impl("miopen_convolution.out", TORCH_FN(functionalization::miopen_convolution_out_out));
  m.impl("miopen_depthwise_convolution.out", TORCH_FN(functionalization::miopen_depthwise_convolution_out_out));
  m.impl("miopen_rnn_backward.out", TORCH_FN(functionalization::miopen_rnn_backward_out_out));
  m.impl("native_batch_norm.out", TORCH_FN(functionalization::native_batch_norm_out_out));
  m.impl("_native_batch_norm_legit_no_training.out", TORCH_FN(functionalization::_native_batch_norm_legit_no_training_out_out));
  m.impl("batch_norm_elemt.out", TORCH_FN(functionalization::batch_norm_elemt_out_out));
  m.impl("batch_norm_update_stats.out", TORCH_FN(functionalization::batch_norm_update_stats_out_out));
  m.impl("_nnpack_spatial_convolution.out", TORCH_FN(functionalization::_nnpack_spatial_convolution_out_out));
  m.impl("ones_like.out", TORCH_FN(functionalization::ones_like_out_out));
  m.impl("channel_shuffle.out", TORCH_FN(functionalization::channel_shuffle_out_out));
  m.impl("randint.out", TORCH_FN(functionalization::randint_out_out));
  m.impl("randint.generator_out", TORCH_FN(functionalization::randint_out_generator_out));
  m.impl("randint.low_out", TORCH_FN(functionalization::randint_out_low_out));
  m.impl("randint.low_generator_out", TORCH_FN(functionalization::randint_out_low_generator_out));
  m.impl("randperm.out", TORCH_FN(functionalization::randperm_out_out));
  m.impl("randperm.generator_out", TORCH_FN(functionalization::randperm_out_generator_out));
  m.impl("range.out", TORCH_FN(functionalization::range_out_out));
  m.impl("range.out_", TORCH_FN(functionalization::range_out_out_));
  m.impl("reciprocal.out", TORCH_FN(functionalization::reciprocal_out_out));
  m.impl("reciprocal_", TORCH_FN(functionalization::reciprocal_));
  m.impl("gelu_backward.grad_input", TORCH_FN(functionalization::gelu_backward_out_grad_input));
  m.impl("hardshrink.out", TORCH_FN(functionalization::hardshrink_out_out));
  m.impl("hardshrink_backward.grad_input", TORCH_FN(functionalization::hardshrink_backward_out_grad_input));
  m.impl("silu.out", TORCH_FN(functionalization::silu_out_out));
  m.impl("silu_", TORCH_FN(functionalization::silu_));
  m.impl("silu_backward.grad_input", TORCH_FN(functionalization::silu_backward_out_grad_input));
  m.impl("sin.out", TORCH_FN(functionalization::sin_out_out));
  m.impl("sin_", TORCH_FN(functionalization::sin_));
  m.impl("_softmax.out", TORCH_FN(functionalization::_softmax_out_out));
  m.impl("sspaddmm.out", TORCH_FN(functionalization::sspaddmm_out_out));
  m.impl("_stack.out", TORCH_FN(functionalization::_stack_out_out));
  m.impl("sum.out", TORCH_FN(functionalization::sum_out_out));
  m.impl("sum.IntList_out", TORCH_FN(functionalization::sum_out_IntList_out));
  m.impl("std.correction_out", TORCH_FN(functionalization::std_out_correction_out));
  m.impl("tan.out", TORCH_FN(functionalization::tan_out_out));
  m.impl("tan_", TORCH_FN(functionalization::tan_));
  m.impl("threshold.out", TORCH_FN(functionalization::threshold_out_out));
  m.impl("threshold_", TORCH_FN(functionalization::threshold_));
  m.impl("_nested_tensor_strides.out", TORCH_FN(functionalization::_nested_tensor_strides_out_out));
  m.impl("unique_consecutive.out", TORCH_FN(functionalization::unique_consecutive_out_out));
  m.impl("var.correction_out", TORCH_FN(functionalization::var_out_correction_out));
  m.impl("_standard_gamma_grad.out", TORCH_FN(functionalization::_standard_gamma_grad_out_out));
  m.impl("poisson.out", TORCH_FN(functionalization::poisson_out_out));
  m.impl("_sparse_csr_sum.dim_dtype_out", TORCH_FN(functionalization::_sparse_csr_sum_out_dim_dtype_out));
  m.impl("_sparse_softmax_backward_data.out", TORCH_FN(functionalization::_sparse_softmax_backward_data_out_out));
  m.impl("_sparse_log_softmax.out", TORCH_FN(functionalization::_sparse_log_softmax_out_out));
  m.impl("_sparse_log_softmax_backward_data.out", TORCH_FN(functionalization::_sparse_log_softmax_backward_data_out_out));
  m.impl("resize_as_sparse.out", TORCH_FN(functionalization::resize_as_sparse_out_out));
  m.impl("resize_as_sparse_", TORCH_FN(functionalization::resize_as_sparse_));
  m.impl("sub.out", TORCH_FN(functionalization::sub_out_out));
  m.impl("sub_.Tensor", TORCH_FN(functionalization::sub__Tensor));
  m.impl("sub.Scalar_out", TORCH_FN(functionalization::sub_out_Scalar_out));
  m.impl("sub_.Scalar", TORCH_FN(functionalization::sub__Scalar));
  m.impl("_sparse_mask_projection.out", TORCH_FN(functionalization::_sparse_mask_projection_out_out));
  m.impl("copy_sparse_to_sparse.out", TORCH_FN(functionalization::copy_sparse_to_sparse_out_out));
  m.impl("copy_sparse_to_sparse_", TORCH_FN(functionalization::copy_sparse_to_sparse_));
  m.impl("_to_sparse.sparse_dim_out", TORCH_FN(functionalization::_to_sparse_out_sparse_dim_out));
  m.impl("_to_sparse.out", TORCH_FN(functionalization::_to_sparse_out_out));
  m.impl("_to_sparse_bsr.out", TORCH_FN(functionalization::_to_sparse_bsr_out_out));
  m.impl("mkldnn_reorder_conv2d_weight.out", TORCH_FN(functionalization::mkldnn_reorder_conv2d_weight_out_out));
  m.impl("_lstm_mps.out", TORCH_FN(functionalization::_lstm_mps_out_out));
  m.impl("_thnn_fused_gru_cell_backward.out", TORCH_FN(functionalization::_thnn_fused_gru_cell_backward_out_out));
  m.impl("set.source_Storage_out", TORCH_FN(functionalization::set_out_source_Storage_out));
  m.impl("set_.source_Storage", TORCH_FN(functionalization::set__source_Storage));
  m.impl("set.source_Storage_storage_offset_out", TORCH_FN(functionalization::set_out_source_Storage_storage_offset_out));
  m.impl("set_.source_Storage_storage_offset", TORCH_FN(functionalization::set__source_Storage_storage_offset));
  m.impl("set.out", TORCH_FN(functionalization::set_out_out));
  m.impl("set_", TORCH_FN(functionalization::set_));
  m.impl("masked_scatter.out", TORCH_FN(functionalization::masked_scatter_out_out));
  m.impl("masked_scatter_", TORCH_FN(functionalization::masked_scatter_));
  m.impl("index_reduce.out", TORCH_FN(functionalization::index_reduce_out_out));
  m.impl("index_reduce_", TORCH_FN(functionalization::index_reduce_));
  m.impl("eq.Scalar_out", TORCH_FN(functionalization::eq_out_Scalar_out));
  m.impl("eq_.Scalar", TORCH_FN(functionalization::eq__Scalar));
  m.impl("eq.Tensor_out", TORCH_FN(functionalization::eq_out_Tensor_out));
  m.impl("eq_.Tensor", TORCH_FN(functionalization::eq__Tensor));
  m.impl("bitwise_and.Tensor_out", TORCH_FN(functionalization::bitwise_and_out_Tensor_out));
  m.impl("bitwise_and_.Tensor", TORCH_FN(functionalization::bitwise_and__Tensor));
  m.impl("bitwise_and.Scalar_out", TORCH_FN(functionalization::bitwise_and_out_Scalar_out));
  m.impl("bitwise_and_.Scalar", TORCH_FN(functionalization::bitwise_and__Scalar));
  m.impl("bitwise_and.Scalar_Tensor_out", TORCH_FN(functionalization::bitwise_and_out_Scalar_Tensor_out));
  m.impl("bitwise_or.Tensor_out", TORCH_FN(functionalization::bitwise_or_out_Tensor_out));
  m.impl("bitwise_or_.Tensor", TORCH_FN(functionalization::bitwise_or__Tensor));
  m.impl("bitwise_or.Scalar_out", TORCH_FN(functionalization::bitwise_or_out_Scalar_out));
  m.impl("bitwise_or_.Scalar", TORCH_FN(functionalization::bitwise_or__Scalar));
  m.impl("bitwise_or.Scalar_Tensor_out", TORCH_FN(functionalization::bitwise_or_out_Scalar_Tensor_out));
  m.impl("bitwise_left_shift.Tensor_out", TORCH_FN(functionalization::bitwise_left_shift_out_Tensor_out));
  m.impl("bitwise_left_shift_.Tensor", TORCH_FN(functionalization::bitwise_left_shift__Tensor));
  m.impl("bitwise_left_shift.Tensor_Scalar_out", TORCH_FN(functionalization::bitwise_left_shift_out_Tensor_Scalar_out));
  m.impl("bitwise_left_shift_.Tensor_Scalar", TORCH_FN(functionalization::bitwise_left_shift__Tensor_Scalar));
  m.impl("bitwise_left_shift.Scalar_Tensor_out", TORCH_FN(functionalization::bitwise_left_shift_out_Scalar_Tensor_out));
  m.impl("__rshift__.Scalar_out", TORCH_FN(functionalization::__rshift___out_Scalar_out));
  m.impl("__irshift__.Scalar", TORCH_FN(functionalization::__irshift___Scalar));
  m.impl("__rshift__.Tensor_out", TORCH_FN(functionalization::__rshift___out_Tensor_out));
  m.impl("__irshift__.Tensor", TORCH_FN(functionalization::__irshift___Tensor));
  m.impl("tril.out", TORCH_FN(functionalization::tril_out_out));
  m.impl("tril_", TORCH_FN(functionalization::tril_));
  m.impl("triu.out", TORCH_FN(functionalization::triu_out_out));
  m.impl("triu_", TORCH_FN(functionalization::triu_));
  m.impl("digamma.out", TORCH_FN(functionalization::digamma_out_out));
  m.impl("digamma_", TORCH_FN(functionalization::digamma_));
  m.impl("lerp.Scalar_out", TORCH_FN(functionalization::lerp_out_Scalar_out));
  m.impl("lerp_.Scalar", TORCH_FN(functionalization::lerp__Scalar));
  m.impl("lerp.Tensor_out", TORCH_FN(functionalization::lerp_out_Tensor_out));
  m.impl("lerp_.Tensor", TORCH_FN(functionalization::lerp__Tensor));
  m.impl("uniform.out", TORCH_FN(functionalization::uniform_out_out));
  m.impl("uniform_", TORCH_FN(functionalization::uniform_));
  m.impl("tril_indices.out", TORCH_FN(functionalization::tril_indices_out_out));
  m.impl("triu_indices.out", TORCH_FN(functionalization::triu_indices_out_out));
  m.impl("ge.Scalar_out", TORCH_FN(functionalization::ge_out_Scalar_out));
  m.impl("ge_.Scalar", TORCH_FN(functionalization::ge__Scalar));
  m.impl("ge.Tensor_out", TORCH_FN(functionalization::ge_out_Tensor_out));
  m.impl("ge_.Tensor", TORCH_FN(functionalization::ge__Tensor));
  m.impl("le.Scalar_out", TORCH_FN(functionalization::le_out_Scalar_out));
  m.impl("le_.Scalar", TORCH_FN(functionalization::le__Scalar));
  m.impl("le.Tensor_out", TORCH_FN(functionalization::le_out_Tensor_out));
  m.impl("le_.Tensor", TORCH_FN(functionalization::le__Tensor));
  m.impl("gt.Scalar_out", TORCH_FN(functionalization::gt_out_Scalar_out));
  m.impl("gt_.Scalar", TORCH_FN(functionalization::gt__Scalar));
  m.impl("gt.Tensor_out", TORCH_FN(functionalization::gt_out_Tensor_out));
  m.impl("gt_.Tensor", TORCH_FN(functionalization::gt__Tensor));
  m.impl("nonzero.out", TORCH_FN(functionalization::nonzero_out_out));
  m.impl("addcdiv.out", TORCH_FN(functionalization::addcdiv_out_out));
  m.impl("addcdiv_", TORCH_FN(functionalization::addcdiv_));
  m.impl("triangular_solve.X", TORCH_FN(functionalization::triangular_solve_out_X));
  m.impl("cholesky_solve.out", TORCH_FN(functionalization::cholesky_solve_out_out));
  m.impl("cholesky_inverse.out", TORCH_FN(functionalization::cholesky_inverse_out_out));
  m.impl("geqrf.a", TORCH_FN(functionalization::geqrf_out_a));
  m.impl("lgamma.out", TORCH_FN(functionalization::lgamma_out_out));
  m.impl("lgamma_", TORCH_FN(functionalization::lgamma_));
  m.impl("erfinv.out", TORCH_FN(functionalization::erfinv_out_out));
  m.impl("erfinv_", TORCH_FN(functionalization::erfinv_));
  m.impl("i0.out", TORCH_FN(functionalization::i0_out_out));
  m.impl("i0_", TORCH_FN(functionalization::i0_));
  m.impl("sign.out", TORCH_FN(functionalization::sign_out_out));
  m.impl("sign_", TORCH_FN(functionalization::sign_));
  m.impl("signbit.out", TORCH_FN(functionalization::signbit_out_out));
  m.impl("atan2.out", TORCH_FN(functionalization::atan2_out_out));
  m.impl("atan2_", TORCH_FN(functionalization::atan2_));
  m.impl("_histogramdd_bin_edges.out", TORCH_FN(functionalization::_histogramdd_bin_edges_out_out));
  m.impl("_histogramdd_from_bin_tensors.out", TORCH_FN(functionalization::_histogramdd_from_bin_tensors_out_out));
  m.impl("fmod.Scalar_out", TORCH_FN(functionalization::fmod_out_Scalar_out));
  m.impl("fmod_.Scalar", TORCH_FN(functionalization::fmod__Scalar));
  m.impl("fmod.Tensor_out", TORCH_FN(functionalization::fmod_out_Tensor_out));
  m.impl("fmod_.Tensor", TORCH_FN(functionalization::fmod__Tensor));
  m.impl("nextafter.out", TORCH_FN(functionalization::nextafter_out_out));
  m.impl("nextafter_", TORCH_FN(functionalization::nextafter_));
  m.impl("minimum.out", TORCH_FN(functionalization::minimum_out_out));
  m.impl("topk.values", TORCH_FN(functionalization::topk_out_values));
  m.impl("any.all_out", TORCH_FN(functionalization::any_out_all_out));
  m.impl("_foreach_mul.Scalar_out", TORCH_FN(functionalization::_foreach_mul_out_Scalar_out));
  m.impl("_foreach_mul_.Scalar", TORCH_FN(functionalization::_foreach_mul__Scalar));
  m.impl("_foreach_mul.List_out", TORCH_FN(functionalization::_foreach_mul_out_List_out));
  m.impl("_foreach_mul_.List", TORCH_FN(functionalization::_foreach_mul__List));
  m.impl("_foreach_mul.ScalarList_out", TORCH_FN(functionalization::_foreach_mul_out_ScalarList_out));
  m.impl("_foreach_mul_.ScalarList", TORCH_FN(functionalization::_foreach_mul__ScalarList));
  m.impl("_foreach_mul.Tensor_out", TORCH_FN(functionalization::_foreach_mul_out_Tensor_out));
  m.impl("_foreach_mul_.Tensor", TORCH_FN(functionalization::_foreach_mul__Tensor));
  m.impl("_foreach_div.Scalar_out", TORCH_FN(functionalization::_foreach_div_out_Scalar_out));
  m.impl("_foreach_div_.Scalar", TORCH_FN(functionalization::_foreach_div__Scalar));
  m.impl("_foreach_div.List_out", TORCH_FN(functionalization::_foreach_div_out_List_out));
  m.impl("_foreach_div_.List", TORCH_FN(functionalization::_foreach_div__List));
  m.impl("_foreach_div.ScalarList_out", TORCH_FN(functionalization::_foreach_div_out_ScalarList_out));
  m.impl("_foreach_div_.ScalarList", TORCH_FN(functionalization::_foreach_div__ScalarList));
  m.impl("_foreach_div.Tensor_out", TORCH_FN(functionalization::_foreach_div_out_Tensor_out));
  m.impl("_foreach_div_.Tensor", TORCH_FN(functionalization::_foreach_div__Tensor));
  m.impl("_foreach_clamp_max.Scalar_out", TORCH_FN(functionalization::_foreach_clamp_max_out_Scalar_out));
  m.impl("_foreach_clamp_max_.Scalar", TORCH_FN(functionalization::_foreach_clamp_max__Scalar));
  m.impl("_foreach_clamp_max.List_out", TORCH_FN(functionalization::_foreach_clamp_max_out_List_out));
  m.impl("_foreach_clamp_max_.List", TORCH_FN(functionalization::_foreach_clamp_max__List));
  m.impl("_foreach_clamp_max.ScalarList_out", TORCH_FN(functionalization::_foreach_clamp_max_out_ScalarList_out));
  m.impl("_foreach_clamp_max_.ScalarList", TORCH_FN(functionalization::_foreach_clamp_max__ScalarList));
  m.impl("_foreach_minimum.Scalar_out", TORCH_FN(functionalization::_foreach_minimum_out_Scalar_out));
  m.impl("_foreach_minimum_.Scalar", TORCH_FN(functionalization::_foreach_minimum__Scalar));
  m.impl("_foreach_minimum.List_out", TORCH_FN(functionalization::_foreach_minimum_out_List_out));
  m.impl("_foreach_minimum_.List", TORCH_FN(functionalization::_foreach_minimum__List));
  m.impl("_foreach_minimum.ScalarList_out", TORCH_FN(functionalization::_foreach_minimum_out_ScalarList_out));
  m.impl("_foreach_minimum_.ScalarList", TORCH_FN(functionalization::_foreach_minimum__ScalarList));
  m.impl("_foreach_expm1.out", TORCH_FN(functionalization::_foreach_expm1_out_out));
  m.impl("_foreach_expm1_", TORCH_FN(functionalization::_foreach_expm1_));
  m.impl("_foreach_frac.out", TORCH_FN(functionalization::_foreach_frac_out_out));
  m.impl("_foreach_frac_", TORCH_FN(functionalization::_foreach_frac_));
  m.impl("_foreach_reciprocal.out", TORCH_FN(functionalization::_foreach_reciprocal_out_out));
  m.impl("_foreach_reciprocal_", TORCH_FN(functionalization::_foreach_reciprocal_));
  m.impl("_foreach_sin.out", TORCH_FN(functionalization::_foreach_sin_out_out));
  m.impl("_foreach_sin_", TORCH_FN(functionalization::_foreach_sin_));
  m.impl("_foreach_tanh.out", TORCH_FN(functionalization::_foreach_tanh_out_out));
  m.impl("_foreach_tanh_", TORCH_FN(functionalization::_foreach_tanh_));
  m.impl("_foreach_trunc.out", TORCH_FN(functionalization::_foreach_trunc_out_out));
  m.impl("_foreach_trunc_", TORCH_FN(functionalization::_foreach_trunc_));
  m.impl("_foreach_copy.out", TORCH_FN(functionalization::_foreach_copy_out_out));
  m.impl("_foreach_copy_", TORCH_FN(functionalization::_foreach_copy_));
  m.impl("_convert_indices_from_csr_to_coo.out", TORCH_FN(functionalization::_convert_indices_from_csr_to_coo_out_out));
  m.impl("multilabel_margin_loss_backward.grad_input", TORCH_FN(functionalization::multilabel_margin_loss_backward_out_grad_input));
  m.impl("nll_loss_backward.grad_input", TORCH_FN(functionalization::nll_loss_backward_out_grad_input));
  m.impl("nll_loss2d_backward.grad_input", TORCH_FN(functionalization::nll_loss2d_backward_out_grad_input));
  m.impl("smooth_l1_loss.out", TORCH_FN(functionalization::smooth_l1_loss_out_out));
  m.impl("huber_loss.out", TORCH_FN(functionalization::huber_loss_out_out));
  m.impl("soft_margin_loss_backward.grad_input", TORCH_FN(functionalization::soft_margin_loss_backward_out_grad_input));
  m.impl("elu.out", TORCH_FN(functionalization::elu_out_out));
  m.impl("elu_", TORCH_FN(functionalization::elu_));
  m.impl("glu.out", TORCH_FN(functionalization::glu_out_out));
  m.impl("hardsigmoid.out", TORCH_FN(functionalization::hardsigmoid_out_out));
  m.impl("hardsigmoid_", TORCH_FN(functionalization::hardsigmoid_));
  m.impl("leaky_relu_backward.grad_input", TORCH_FN(functionalization::leaky_relu_backward_out_grad_input));
  m.impl("softshrink.out", TORCH_FN(functionalization::softshrink_out_out));
  m.impl("_adaptive_avg_pool2d.out", TORCH_FN(functionalization::_adaptive_avg_pool2d_out_out));
  m.impl("_adaptive_avg_pool3d.out", TORCH_FN(functionalization::_adaptive_avg_pool3d_out_out));
  m.impl("avg_pool3d.out", TORCH_FN(functionalization::avg_pool3d_out_out));
  m.impl("fractional_max_pool3d.output", TORCH_FN(functionalization::fractional_max_pool3d_out_output));
  m.impl("max_pool2d_with_indices_backward.grad_input", TORCH_FN(functionalization::max_pool2d_with_indices_backward_out_grad_input));
  m.impl("max_unpool3d.out", TORCH_FN(functionalization::max_unpool3d_out_out));
  m.impl("reflection_pad1d.out", TORCH_FN(functionalization::reflection_pad1d_out_out));
  m.impl("reflection_pad1d_backward.grad_input", TORCH_FN(functionalization::reflection_pad1d_backward_out_grad_input));
  m.impl("reflection_pad2d.out", TORCH_FN(functionalization::reflection_pad2d_out_out));
  m.impl("reflection_pad3d.out", TORCH_FN(functionalization::reflection_pad3d_out_out));
  m.impl("replication_pad1d.out", TORCH_FN(functionalization::replication_pad1d_out_out));
  m.impl("replication_pad1d_backward.grad_input", TORCH_FN(functionalization::replication_pad1d_backward_out_grad_input));
  m.impl("replication_pad2d_backward.grad_input", TORCH_FN(functionalization::replication_pad2d_backward_out_grad_input));
  m.impl("replication_pad3d.out", TORCH_FN(functionalization::replication_pad3d_out_out));
  m.impl("replication_pad3d_backward.grad_input", TORCH_FN(functionalization::replication_pad3d_backward_out_grad_input));
  m.impl("upsample_nearest2d.out", TORCH_FN(functionalization::upsample_nearest2d_out_out));
  m.impl("_upsample_nearest_exact2d.out", TORCH_FN(functionalization::_upsample_nearest_exact2d_out_out));
  m.impl("_upsample_nearest_exact2d_backward.grad_input", TORCH_FN(functionalization::_upsample_nearest_exact2d_backward_out_grad_input));
  m.impl("_upsample_nearest_exact3d_backward.grad_input", TORCH_FN(functionalization::_upsample_nearest_exact3d_backward_out_grad_input));
  m.impl("sigmoid_backward.grad_input", TORCH_FN(functionalization::sigmoid_backward_out_grad_input));
  m.impl("slow_conv_transpose2d.out", TORCH_FN(functionalization::slow_conv_transpose2d_out_out));
  m.impl("_slow_conv2d_forward.output", TORCH_FN(functionalization::_slow_conv2d_forward_out_output));
  m.impl("conv_depthwise3d.out", TORCH_FN(functionalization::conv_depthwise3d_out_out));
  m.impl("slow_conv_dilated2d.out", TORCH_FN(functionalization::slow_conv_dilated2d_out_out));
  m.impl("special_ndtri.out", TORCH_FN(functionalization::special_ndtri_out_out));
  m.impl("linalg_cholesky_ex.L", TORCH_FN(functionalization::linalg_cholesky_ex_out_L));
  m.impl("linalg_cross.out", TORCH_FN(functionalization::linalg_cross_out_out));
  m.impl("linalg_lu_solve.out", TORCH_FN(functionalization::linalg_lu_solve_out_out));
  m.impl("linalg_inv_ex.inverse", TORCH_FN(functionalization::linalg_inv_ex_out_inverse));
  m.impl("_test_optional_floatlist.out", TORCH_FN(functionalization::_test_optional_floatlist_out_out));
  m.impl("_test_warn_in_autograd.out", TORCH_FN(functionalization::_test_warn_in_autograd_out_out));
  m.impl("_segment_reduce_backward.out", TORCH_FN(functionalization::_segment_reduce_backward_out_out));
  m.impl("_sparse_broadcast_to_copy.out", TORCH_FN(functionalization::_sparse_broadcast_to_copy_out_out));
  m.impl("unsqueeze_copy.out", TORCH_FN(functionalization::unsqueeze_copy_out_out));
  m.impl("values_copy.out", TORCH_FN(functionalization::values_copy_out_out));
  m.impl("to_padded_tensor.out", TORCH_FN(functionalization::to_padded_tensor_out_out));
  m.impl("_triton_scaled_dot_attention.out", TORCH_FN(functionalization::_triton_scaled_dot_attention_out_out));
  m.impl("special_bessel_y0.out", TORCH_FN(functionalization::special_bessel_y0_out_out));
  m.impl("special_chebyshev_polynomial_t.out", TORCH_FN(functionalization::special_chebyshev_polynomial_t_out_out));
  m.impl("special_chebyshev_polynomial_t.x_scalar_out", TORCH_FN(functionalization::special_chebyshev_polynomial_t_out_x_scalar_out));
  m.impl("special_chebyshev_polynomial_t.n_scalar_out", TORCH_FN(functionalization::special_chebyshev_polynomial_t_out_n_scalar_out));
  m.impl("special_chebyshev_polynomial_u.out", TORCH_FN(functionalization::special_chebyshev_polynomial_u_out_out));
  m.impl("special_chebyshev_polynomial_u.x_scalar_out", TORCH_FN(functionalization::special_chebyshev_polynomial_u_out_x_scalar_out));
  m.impl("special_chebyshev_polynomial_u.n_scalar_out", TORCH_FN(functionalization::special_chebyshev_polynomial_u_out_n_scalar_out));
  m.impl("special_hermite_polynomial_h.out", TORCH_FN(functionalization::special_hermite_polynomial_h_out_out));
  m.impl("special_hermite_polynomial_h.x_scalar_out", TORCH_FN(functionalization::special_hermite_polynomial_h_out_x_scalar_out));
  m.impl("special_hermite_polynomial_h.n_scalar_out", TORCH_FN(functionalization::special_hermite_polynomial_h_out_n_scalar_out));
  m.impl("special_modified_bessel_k1.out", TORCH_FN(functionalization::special_modified_bessel_k1_out_out));
  m.impl("special_scaled_modified_bessel_k0.out", TORCH_FN(functionalization::special_scaled_modified_bessel_k0_out_out));
  m.impl("special_scaled_modified_bessel_k1.out", TORCH_FN(functionalization::special_scaled_modified_bessel_k1_out_out));
  m.impl("special_spherical_bessel_j0.out", TORCH_FN(functionalization::special_spherical_bessel_j0_out_out));
  m.impl("_fused_sgd.out", TORCH_FN(functionalization::_fused_sgd_out_out));
  m.impl("_fused_sgd_", TORCH_FN(functionalization::_fused_sgd_));
  m.impl("_fused_sgd.tensor_lr_out", TORCH_FN(functionalization::_fused_sgd_out_tensor_lr_out));
  m.impl("_fused_sgd_.tensor_lr", TORCH_FN(functionalization::_fused_sgd__tensor_lr));
  m.impl("_neg_view", TORCH_FN(functionalization::_neg_view));
  m.impl("diagonal", TORCH_FN(functionalization::diagonal));
  m.impl("select.int", TORCH_FN(functionalization::select_int));
  m.impl("slice_inverse", TORCH_FN(functionalization::slice_inverse));
  m.impl("split_with_sizes", TORCH_FN(functionalization::split_with_sizes));
  m.impl("squeeze", TORCH_FN(functionalization::squeeze));
  m.impl("squeeze_", TORCH_FN(functionalization::squeeze_));
  m.impl("squeeze.dim", TORCH_FN(functionalization::squeeze_dim));
  m.impl("squeeze_.dim", TORCH_FN(functionalization::squeeze__dim));
  m.impl("squeeze.dims", TORCH_FN(functionalization::squeeze_dims));
  m.impl("squeeze_.dims", TORCH_FN(functionalization::squeeze__dims));
  m.impl("unbind.int", TORCH_FN(functionalization::unbind_int));
  m.impl("alias", TORCH_FN(functionalization::alias));;
}

}  // namespace

} // namespace at
