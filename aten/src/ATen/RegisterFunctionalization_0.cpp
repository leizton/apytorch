#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
// @generated by torchgen/gen.py from RegisterFunctionalization.cpp

#include <ATen/core/LegacyTypeDispatch.h>
#include <ATen/EmptyTensor.h>
#include <ATen/FunctionalTensorWrapper.h>
#include <ATen/FunctionalInverses.h>
#include <ATen/MemoryOverlap.h>
#include <torch/library.h>

#ifndef AT_PER_OPERATOR_HEADERS
#include <ATen/Operators.h>
#include <ATen/NativeFunctions.h>
#else
// needed for the meta tensor calls to get stride info in functionalization
#include <ATen/ops/empty_strided_native.h>
// needed for special handling of copy_().
// See Note [functionalizating copy_() and not preserving strides]
#include <ATen/ops/to_ops.h>
#include <ATen/ops/expand_copy_ops.h>

#include <ATen/ops/_cudnn_ctc_loss_native.h>
#include <ATen/ops/_cudnn_ctc_loss_ops.h>
#include <ATen/ops/_cudnn_ctc_loss_native.h>
#include <ATen/ops/_cudnn_ctc_loss_ops.h>
#include <ATen/ops/_cudnn_rnn_native.h>
#include <ATen/ops/_cudnn_rnn_ops.h>
#include <ATen/ops/_cudnn_rnn_native.h>
#include <ATen/ops/_cudnn_rnn_ops.h>
#include <ATen/ops/_cudnn_rnn_backward_native.h>
#include <ATen/ops/_cudnn_rnn_backward_ops.h>
#include <ATen/ops/_cudnn_rnn_backward_native.h>
#include <ATen/ops/_cudnn_rnn_backward_ops.h>
#include <ATen/ops/_fused_dropout_native.h>
#include <ATen/ops/_fused_dropout_ops.h>
#include <ATen/ops/_fused_dropout_native.h>
#include <ATen/ops/_fused_dropout_ops.h>
#include <ATen/ops/conj_physical_native.h>
#include <ATen/ops/conj_physical_ops.h>
#include <ATen/ops/conj_physical_native.h>
#include <ATen/ops/conj_physical_ops.h>
#include <ATen/ops/conj_physical_native.h>
#include <ATen/ops/conj_physical_ops.h>
#include <ATen/ops/add_native.h>
#include <ATen/ops/add_ops.h>
#include <ATen/ops/add_native.h>
#include <ATen/ops/add_ops.h>
#include <ATen/ops/add_native.h>
#include <ATen/ops/add_ops.h>
#include <ATen/ops/add_native.h>
#include <ATen/ops/add_ops.h>
#include <ATen/ops/add_native.h>
#include <ATen/ops/add_ops.h>
#include <ATen/ops/add_native.h>
#include <ATen/ops/add_ops.h>
#include <ATen/ops/addmv_native.h>
#include <ATen/ops/addmv_ops.h>
#include <ATen/ops/addmv_native.h>
#include <ATen/ops/addmv_ops.h>
#include <ATen/ops/addmv_native.h>
#include <ATen/ops/addmv_ops.h>
#include <ATen/ops/addr_native.h>
#include <ATen/ops/addr_ops.h>
#include <ATen/ops/addr_native.h>
#include <ATen/ops/addr_ops.h>
#include <ATen/ops/addr_native.h>
#include <ATen/ops/addr_ops.h>
#include <ATen/ops/all_native.h>
#include <ATen/ops/all_ops.h>
#include <ATen/ops/all_native.h>
#include <ATen/ops/all_ops.h>
#include <ATen/ops/all_native.h>
#include <ATen/ops/all_ops.h>
#include <ATen/ops/all_native.h>
#include <ATen/ops/all_ops.h>
#include <ATen/ops/all_native.h>
#include <ATen/ops/all_ops.h>
#include <ATen/ops/all_native.h>
#include <ATen/ops/all_ops.h>
#include <ATen/ops/argmax_native.h>
#include <ATen/ops/argmax_ops.h>
#include <ATen/ops/argmax_native.h>
#include <ATen/ops/argmax_ops.h>
#include <ATen/ops/atan_native.h>
#include <ATen/ops/atan_ops.h>
#include <ATen/ops/atan_native.h>
#include <ATen/ops/atan_ops.h>
#include <ATen/ops/atan_native.h>
#include <ATen/ops/atan_ops.h>
#include <ATen/ops/bartlett_window_native.h>
#include <ATen/ops/bartlett_window_ops.h>
#include <ATen/ops/bartlett_window_native.h>
#include <ATen/ops/bartlett_window_ops.h>
#include <ATen/ops/bartlett_window_native.h>
#include <ATen/ops/bartlett_window_ops.h>
#include <ATen/ops/bartlett_window_native.h>
#include <ATen/ops/bartlett_window_ops.h>
#include <ATen/ops/binary_cross_entropy_native.h>
#include <ATen/ops/binary_cross_entropy_ops.h>
#include <ATen/ops/binary_cross_entropy_native.h>
#include <ATen/ops/binary_cross_entropy_ops.h>
#include <ATen/ops/bitwise_not_native.h>
#include <ATen/ops/bitwise_not_ops.h>
#include <ATen/ops/bitwise_not_native.h>
#include <ATen/ops/bitwise_not_ops.h>
#include <ATen/ops/bitwise_not_native.h>
#include <ATen/ops/bitwise_not_ops.h>
#include <ATen/ops/logical_xor_native.h>
#include <ATen/ops/logical_xor_ops.h>
#include <ATen/ops/logical_xor_native.h>
#include <ATen/ops/logical_xor_ops.h>
#include <ATen/ops/logical_xor_native.h>
#include <ATen/ops/logical_xor_ops.h>
#include <ATen/ops/blackman_window_native.h>
#include <ATen/ops/blackman_window_ops.h>
#include <ATen/ops/blackman_window_native.h>
#include <ATen/ops/blackman_window_ops.h>
#include <ATen/ops/blackman_window_native.h>
#include <ATen/ops/blackman_window_ops.h>
#include <ATen/ops/blackman_window_native.h>
#include <ATen/ops/blackman_window_ops.h>
#include <ATen/ops/clamp_native.h>
#include <ATen/ops/clamp_ops.h>
#include <ATen/ops/clamp_native.h>
#include <ATen/ops/clamp_ops.h>
#include <ATen/ops/clamp_native.h>
#include <ATen/ops/clamp_ops.h>
#include <ATen/ops/clamp_native.h>
#include <ATen/ops/clamp_ops.h>
#include <ATen/ops/clamp_native.h>
#include <ATen/ops/clamp_ops.h>
#include <ATen/ops/clamp_native.h>
#include <ATen/ops/clamp_ops.h>
#include <ATen/ops/_convolution_native.h>
#include <ATen/ops/_convolution_ops.h>
#include <ATen/ops/_convolution_native.h>
#include <ATen/ops/_convolution_ops.h>
#include <ATen/ops/copy_native.h>
#include <ATen/ops/copy_ops.h>
#include <ATen/ops/copy_native.h>
#include <ATen/ops/copy_ops.h>
#include <ATen/ops/copy_native.h>
#include <ATen/ops/copy_ops.h>
#include <ATen/ops/cudnn_convolution_native.h>
#include <ATen/ops/cudnn_convolution_ops.h>
#include <ATen/ops/cudnn_convolution_native.h>
#include <ATen/ops/cudnn_convolution_ops.h>
#include <ATen/ops/divide_native.h>
#include <ATen/ops/divide_ops.h>
#include <ATen/ops/divide_native.h>
#include <ATen/ops/divide_ops.h>
#include <ATen/ops/divide_native.h>
#include <ATen/ops/divide_ops.h>
#include <ATen/ops/divide_native.h>
#include <ATen/ops/divide_ops.h>
#include <ATen/ops/divide_native.h>
#include <ATen/ops/divide_ops.h>
#include <ATen/ops/divide_native.h>
#include <ATen/ops/divide_ops.h>
#include <ATen/ops/true_divide_native.h>
#include <ATen/ops/true_divide_ops.h>
#include <ATen/ops/true_divide_native.h>
#include <ATen/ops/true_divide_ops.h>
#include <ATen/ops/true_divide_native.h>
#include <ATen/ops/true_divide_ops.h>
#include <ATen/ops/dot_native.h>
#include <ATen/ops/dot_ops.h>
#include <ATen/ops/dot_native.h>
#include <ATen/ops/dot_ops.h>
#include <ATen/ops/vdot_native.h>
#include <ATen/ops/vdot_ops.h>
#include <ATen/ops/vdot_native.h>
#include <ATen/ops/vdot_ops.h>
#include <ATen/ops/row_stack_native.h>
#include <ATen/ops/row_stack_ops.h>
#include <ATen/ops/row_stack_native.h>
#include <ATen/ops/row_stack_ops.h>
#include <ATen/ops/new_empty_native.h>
#include <ATen/ops/new_empty_ops.h>
#include <ATen/ops/new_empty_native.h>
#include <ATen/ops/new_empty_ops.h>
#include <ATen/ops/new_empty_strided_native.h>
#include <ATen/ops/new_empty_strided_ops.h>
#include <ATen/ops/new_empty_strided_native.h>
#include <ATen/ops/new_empty_strided_ops.h>
#include <ATen/ops/_empty_affine_quantized_native.h>
#include <ATen/ops/_empty_affine_quantized_ops.h>
#include <ATen/ops/_empty_affine_quantized_native.h>
#include <ATen/ops/_empty_affine_quantized_ops.h>
#include <ATen/ops/_resize_output_native.h>
#include <ATen/ops/_resize_output_ops.h>
#include <ATen/ops/_resize_output_native.h>
#include <ATen/ops/_resize_output_ops.h>
#include <ATen/ops/_resize_output_native.h>
#include <ATen/ops/_resize_output_ops.h>
#include <ATen/ops/frac_native.h>
#include <ATen/ops/frac_ops.h>
#include <ATen/ops/frac_native.h>
#include <ATen/ops/frac_ops.h>
#include <ATen/ops/frac_native.h>
#include <ATen/ops/frac_ops.h>
#include <ATen/ops/full_like_native.h>
#include <ATen/ops/full_like_ops.h>
#include <ATen/ops/full_like_native.h>
#include <ATen/ops/full_like_ops.h>
#include <ATen/ops/lcm_native.h>
#include <ATen/ops/lcm_ops.h>
#include <ATen/ops/lcm_native.h>
#include <ATen/ops/lcm_ops.h>
#include <ATen/ops/lcm_native.h>
#include <ATen/ops/lcm_ops.h>
#include <ATen/ops/hann_window_native.h>
#include <ATen/ops/hann_window_ops.h>
#include <ATen/ops/hann_window_native.h>
#include <ATen/ops/hann_window_ops.h>
#include <ATen/ops/hann_window_native.h>
#include <ATen/ops/hann_window_ops.h>
#include <ATen/ops/hann_window_native.h>
#include <ATen/ops/hann_window_ops.h>
#include <ATen/ops/kaiser_window_native.h>
#include <ATen/ops/kaiser_window_ops.h>
#include <ATen/ops/kaiser_window_native.h>
#include <ATen/ops/kaiser_window_ops.h>
#include <ATen/ops/kaiser_window_native.h>
#include <ATen/ops/kaiser_window_ops.h>
#include <ATen/ops/kaiser_window_native.h>
#include <ATen/ops/kaiser_window_ops.h>
#include <ATen/ops/kaiser_window_native.h>
#include <ATen/ops/kaiser_window_ops.h>
#include <ATen/ops/kaiser_window_native.h>
#include <ATen/ops/kaiser_window_ops.h>
#include <ATen/ops/_index_put_impl_native.h>
#include <ATen/ops/_index_put_impl_ops.h>
#include <ATen/ops/_index_put_impl_native.h>
#include <ATen/ops/_index_put_impl_ops.h>
#include <ATen/ops/_index_put_impl_native.h>
#include <ATen/ops/_index_put_impl_ops.h>
#include <ATen/ops/kthvalue_native.h>
#include <ATen/ops/kthvalue_ops.h>
#include <ATen/ops/kthvalue_native.h>
#include <ATen/ops/kthvalue_ops.h>
#include <ATen/ops/kthvalue_native.h>
#include <ATen/ops/kthvalue_ops.h>
#include <ATen/ops/kthvalue_native.h>
#include <ATen/ops/kthvalue_ops.h>
#include <ATen/ops/native_layer_norm_native.h>
#include <ATen/ops/native_layer_norm_ops.h>
#include <ATen/ops/native_layer_norm_native.h>
#include <ATen/ops/native_layer_norm_ops.h>
#include <ATen/ops/native_layer_norm_backward_native.h>
#include <ATen/ops/native_layer_norm_backward_ops.h>
#include <ATen/ops/native_layer_norm_backward_native.h>
#include <ATen/ops/native_layer_norm_backward_ops.h>
#include <ATen/ops/mkldnn_linear_backward_input_native.h>
#include <ATen/ops/mkldnn_linear_backward_input_ops.h>
#include <ATen/ops/mkldnn_linear_backward_input_native.h>
#include <ATen/ops/mkldnn_linear_backward_input_ops.h>
#include <ATen/ops/mkldnn_linear_backward_native.h>
#include <ATen/ops/mkldnn_linear_backward_ops.h>
#include <ATen/ops/mkldnn_linear_backward_native.h>
#include <ATen/ops/mkldnn_linear_backward_ops.h>
#include <ATen/ops/log10_native.h>
#include <ATen/ops/log10_ops.h>
#include <ATen/ops/log10_native.h>
#include <ATen/ops/log10_ops.h>
#include <ATen/ops/log10_native.h>
#include <ATen/ops/log10_ops.h>
#include <ATen/ops/log1p_native.h>
#include <ATen/ops/log1p_ops.h>
#include <ATen/ops/log1p_native.h>
#include <ATen/ops/log1p_ops.h>
#include <ATen/ops/log1p_native.h>
#include <ATen/ops/log1p_ops.h>
#include <ATen/ops/logsumexp_native.h>
#include <ATen/ops/logsumexp_ops.h>
#include <ATen/ops/logsumexp_native.h>
#include <ATen/ops/logsumexp_ops.h>
#include <ATen/ops/logsumexp_native.h>
#include <ATen/ops/logsumexp_ops.h>
#include <ATen/ops/logsumexp_native.h>
#include <ATen/ops/logsumexp_ops.h>
#include <ATen/ops/matmul_native.h>
#include <ATen/ops/matmul_ops.h>
#include <ATen/ops/matmul_native.h>
#include <ATen/ops/matmul_ops.h>
#include <ATen/ops/max_pool2d_backward_native.h>
#include <ATen/ops/max_pool2d_backward_ops.h>
#include <ATen/ops/max_pool2d_backward_native.h>
#include <ATen/ops/max_pool2d_backward_ops.h>
#include <ATen/ops/mkldnn_max_pool2d_native.h>
#include <ATen/ops/mkldnn_max_pool2d_ops.h>
#include <ATen/ops/mkldnn_max_pool2d_native.h>
#include <ATen/ops/mkldnn_max_pool2d_ops.h>
#include <ATen/ops/quantized_max_pool1d_native.h>
#include <ATen/ops/quantized_max_pool1d_ops.h>
#include <ATen/ops/quantized_max_pool1d_native.h>
#include <ATen/ops/quantized_max_pool1d_ops.h>
#include <ATen/ops/nanmean_native.h>
#include <ATen/ops/nanmean_ops.h>
#include <ATen/ops/nanmean_native.h>
#include <ATen/ops/nanmean_ops.h>
#include <ATen/ops/_mps_convolution_native.h>
#include <ATen/ops/_mps_convolution_ops.h>
#include <ATen/ops/_mps_convolution_native.h>
#include <ATen/ops/_mps_convolution_ops.h>
#include <ATen/ops/mkldnn_convolution_native.h>
#include <ATen/ops/mkldnn_convolution_ops.h>
#include <ATen/ops/mkldnn_convolution_native.h>
#include <ATen/ops/mkldnn_convolution_ops.h>
#include <ATen/ops/mkldnn_rnn_layer_native.h>
#include <ATen/ops/mkldnn_rnn_layer_ops.h>
#include <ATen/ops/mkldnn_rnn_layer_native.h>
#include <ATen/ops/mkldnn_rnn_layer_ops.h>
#include <ATen/ops/miopen_batch_norm_native.h>
#include <ATen/ops/miopen_batch_norm_ops.h>
#include <ATen/ops/miopen_batch_norm_native.h>
#include <ATen/ops/miopen_batch_norm_ops.h>
#include <ATen/ops/miopen_batch_norm_backward_native.h>
#include <ATen/ops/miopen_batch_norm_backward_ops.h>
#include <ATen/ops/miopen_batch_norm_backward_native.h>
#include <ATen/ops/miopen_batch_norm_backward_ops.h>
#include <ATen/ops/miopen_convolution_transpose_native.h>
#include <ATen/ops/miopen_convolution_transpose_ops.h>
#include <ATen/ops/miopen_convolution_transpose_native.h>
#include <ATen/ops/miopen_convolution_transpose_ops.h>
#include <ATen/ops/mm_native.h>
#include <ATen/ops/mm_ops.h>
#include <ATen/ops/mm_native.h>
#include <ATen/ops/mm_ops.h>
#include <ATen/ops/_sparse_sparse_matmul_native.h>
#include <ATen/ops/_sparse_sparse_matmul_ops.h>
#include <ATen/ops/_sparse_sparse_matmul_native.h>
#include <ATen/ops/_sparse_sparse_matmul_ops.h>
#include <ATen/ops/mul_native.h>
#include <ATen/ops/mul_ops.h>
#include <ATen/ops/mul_native.h>
#include <ATen/ops/mul_ops.h>
#include <ATen/ops/mul_native.h>
#include <ATen/ops/mul_ops.h>
#include <ATen/ops/mul_native.h>
#include <ATen/ops/mul_ops.h>
#include <ATen/ops/mul_native.h>
#include <ATen/ops/mul_ops.h>
#include <ATen/ops/mul_native.h>
#include <ATen/ops/mul_ops.h>
#include <ATen/ops/mvlgamma_native.h>
#include <ATen/ops/mvlgamma_ops.h>
#include <ATen/ops/mvlgamma_native.h>
#include <ATen/ops/mvlgamma_ops.h>
#include <ATen/ops/mvlgamma_native.h>
#include <ATen/ops/mvlgamma_ops.h>
#include <ATen/ops/batch_norm_backward_reduce_native.h>
#include <ATen/ops/batch_norm_backward_reduce_ops.h>
#include <ATen/ops/batch_norm_backward_reduce_native.h>
#include <ATen/ops/batch_norm_backward_reduce_ops.h>
#include <ATen/ops/deg2rad_native.h>
#include <ATen/ops/deg2rad_ops.h>
#include <ATen/ops/deg2rad_native.h>
#include <ATen/ops/deg2rad_ops.h>
#include <ATen/ops/deg2rad_native.h>
#include <ATen/ops/deg2rad_ops.h>
#include <ATen/ops/randint_like_native.h>
#include <ATen/ops/randint_like_ops.h>
#include <ATen/ops/randint_like_native.h>
#include <ATen/ops/randint_like_ops.h>
#include <ATen/ops/randint_like_native.h>
#include <ATen/ops/randint_like_ops.h>
#include <ATen/ops/randint_like_native.h>
#include <ATen/ops/randint_like_ops.h>
#include <ATen/ops/repeat_native.h>
#include <ATen/ops/repeat_ops.h>
#include <ATen/ops/repeat_native.h>
#include <ATen/ops/repeat_ops.h>
#include <ATen/ops/_mkldnn_reshape_native.h>
#include <ATen/ops/_mkldnn_reshape_ops.h>
#include <ATen/ops/_mkldnn_reshape_native.h>
#include <ATen/ops/_mkldnn_reshape_ops.h>
#include <ATen/ops/round_native.h>
#include <ATen/ops/round_ops.h>
#include <ATen/ops/round_native.h>
#include <ATen/ops/round_ops.h>
#include <ATen/ops/round_native.h>
#include <ATen/ops/round_ops.h>
#include <ATen/ops/round_native.h>
#include <ATen/ops/round_ops.h>
#include <ATen/ops/round_native.h>
#include <ATen/ops/round_ops.h>
#include <ATen/ops/round_native.h>
#include <ATen/ops/round_ops.h>
#include <ATen/ops/rsqrt_native.h>
#include <ATen/ops/rsqrt_ops.h>
#include <ATen/ops/rsqrt_native.h>
#include <ATen/ops/rsqrt_ops.h>
#include <ATen/ops/rsqrt_native.h>
#include <ATen/ops/rsqrt_ops.h>
#include <ATen/ops/celu_native.h>
#include <ATen/ops/celu_ops.h>
#include <ATen/ops/celu_native.h>
#include <ATen/ops/celu_ops.h>
#include <ATen/ops/celu_native.h>
#include <ATen/ops/celu_ops.h>
#include <ATen/ops/sigmoid_native.h>
#include <ATen/ops/sigmoid_ops.h>
#include <ATen/ops/sigmoid_native.h>
#include <ATen/ops/sigmoid_ops.h>
#include <ATen/ops/sigmoid_native.h>
#include <ATen/ops/sigmoid_ops.h>
#include <ATen/ops/sinc_native.h>
#include <ATen/ops/sinc_ops.h>
#include <ATen/ops/sinc_native.h>
#include <ATen/ops/sinc_ops.h>
#include <ATen/ops/sinc_native.h>
#include <ATen/ops/sinc_ops.h>
#include <ATen/ops/sinh_native.h>
#include <ATen/ops/sinh_ops.h>
#include <ATen/ops/sinh_native.h>
#include <ATen/ops/sinh_ops.h>
#include <ATen/ops/sinh_native.h>
#include <ATen/ops/sinh_ops.h>
#include <ATen/ops/slice_backward_native.h>
#include <ATen/ops/slice_backward_ops.h>
#include <ATen/ops/slice_backward_native.h>
#include <ATen/ops/slice_backward_ops.h>
#include <ATen/ops/as_strided_scatter_native.h>
#include <ATen/ops/as_strided_scatter_ops.h>
#include <ATen/ops/as_strided_scatter_native.h>
#include <ATen/ops/as_strided_scatter_ops.h>
#include <ATen/ops/std_mean_native.h>
#include <ATen/ops/std_mean_ops.h>
#include <ATen/ops/std_mean_native.h>
#include <ATen/ops/std_mean_ops.h>
#include <ATen/ops/_mkldnn_transpose_native.h>
#include <ATen/ops/_mkldnn_transpose_ops.h>
#include <ATen/ops/_mkldnn_transpose_native.h>
#include <ATen/ops/_mkldnn_transpose_ops.h>
#include <ATen/ops/_mkldnn_transpose_native.h>
#include <ATen/ops/_mkldnn_transpose_ops.h>
#include <ATen/ops/flip_native.h>
#include <ATen/ops/flip_ops.h>
#include <ATen/ops/flip_native.h>
#include <ATen/ops/flip_ops.h>
#include <ATen/ops/_nested_tensor_from_mask_native.h>
#include <ATen/ops/_nested_tensor_from_mask_ops.h>
#include <ATen/ops/_nested_tensor_from_mask_native.h>
#include <ATen/ops/_nested_tensor_from_mask_ops.h>
#include <ATen/ops/_nested_from_padded_and_nested_example_native.h>
#include <ATen/ops/_nested_from_padded_and_nested_example_ops.h>
#include <ATen/ops/_nested_from_padded_and_nested_example_native.h>
#include <ATen/ops/_nested_from_padded_and_nested_example_ops.h>
#include <ATen/ops/unique_dim_native.h>
#include <ATen/ops/unique_dim_ops.h>
#include <ATen/ops/unique_dim_native.h>
#include <ATen/ops/unique_dim_ops.h>
#include <ATen/ops/_unsafe_view_native.h>
#include <ATen/ops/_unsafe_view_ops.h>
#include <ATen/ops/_unsafe_view_native.h>
#include <ATen/ops/_unsafe_view_ops.h>
#include <ATen/ops/var_mean_native.h>
#include <ATen/ops/var_mean_ops.h>
#include <ATen/ops/var_mean_native.h>
#include <ATen/ops/var_mean_ops.h>
#include <ATen/ops/zeros_native.h>
#include <ATen/ops/zeros_ops.h>
#include <ATen/ops/zeros_native.h>
#include <ATen/ops/zeros_ops.h>
#include <ATen/ops/zeros_native.h>
#include <ATen/ops/zeros_ops.h>
#include <ATen/ops/zeros_native.h>
#include <ATen/ops/zeros_ops.h>
#include <ATen/ops/zeros_like_native.h>
#include <ATen/ops/zeros_like_ops.h>
#include <ATen/ops/zeros_like_native.h>
#include <ATen/ops/zeros_like_ops.h>
#include <ATen/ops/_sparse_csr_prod_native.h>
#include <ATen/ops/_sparse_csr_prod_ops.h>
#include <ATen/ops/_sparse_csr_prod_native.h>
#include <ATen/ops/_sparse_csr_prod_ops.h>
#include <ATen/ops/_spdiags_native.h>
#include <ATen/ops/_spdiags_ops.h>
#include <ATen/ops/_spdiags_native.h>
#include <ATen/ops/_spdiags_ops.h>
#include <ATen/ops/rsub_native.h>
#include <ATen/ops/rsub_ops.h>
#include <ATen/ops/rsub_native.h>
#include <ATen/ops/rsub_ops.h>
#include <ATen/ops/rsub_native.h>
#include <ATen/ops/rsub_ops.h>
#include <ATen/ops/rsub_native.h>
#include <ATen/ops/rsub_ops.h>
#include <ATen/ops/_sparse_addmm_native.h>
#include <ATen/ops/_sparse_addmm_ops.h>
#include <ATen/ops/_sparse_addmm_native.h>
#include <ATen/ops/_sparse_addmm_ops.h>
#include <ATen/ops/sparse_coo_tensor_native.h>
#include <ATen/ops/sparse_coo_tensor_ops.h>
#include <ATen/ops/sparse_coo_tensor_native.h>
#include <ATen/ops/sparse_coo_tensor_ops.h>
#include <ATen/ops/sparse_resize_native.h>
#include <ATen/ops/sparse_resize_ops.h>
#include <ATen/ops/sparse_resize_native.h>
#include <ATen/ops/sparse_resize_ops.h>
#include <ATen/ops/sparse_resize_native.h>
#include <ATen/ops/sparse_resize_ops.h>
#include <ATen/ops/sparse_mask_native.h>
#include <ATen/ops/sparse_mask_ops.h>
#include <ATen/ops/sparse_mask_native.h>
#include <ATen/ops/sparse_mask_ops.h>
#include <ATen/ops/_coalesce_native.h>
#include <ATen/ops/_coalesce_ops.h>
#include <ATen/ops/_coalesce_native.h>
#include <ATen/ops/_coalesce_ops.h>
#include <ATen/ops/_to_sparse_csr_native.h>
#include <ATen/ops/_to_sparse_csr_ops.h>
#include <ATen/ops/_to_sparse_csr_native.h>
#include <ATen/ops/_to_sparse_csr_ops.h>
#include <ATen/ops/_to_sparse_csc_native.h>
#include <ATen/ops/_to_sparse_csc_ops.h>
#include <ATen/ops/_to_sparse_csc_native.h>
#include <ATen/ops/_to_sparse_csc_ops.h>
#include <ATen/ops/dequantize_native.h>
#include <ATen/ops/dequantize_ops.h>
#include <ATen/ops/dequantize_native.h>
#include <ATen/ops/dequantize_ops.h>
#include <ATen/ops/dequantize_native.h>
#include <ATen/ops/dequantize_ops.h>
#include <ATen/ops/dequantize_native.h>
#include <ATen/ops/dequantize_ops.h>
#include <ATen/ops/q_per_channel_zero_points_native.h>
#include <ATen/ops/q_per_channel_zero_points_ops.h>
#include <ATen/ops/q_per_channel_zero_points_native.h>
#include <ATen/ops/q_per_channel_zero_points_ops.h>
#include <ATen/ops/_fake_quantize_learnable_per_channel_affine_native.h>
#include <ATen/ops/_fake_quantize_learnable_per_channel_affine_ops.h>
#include <ATen/ops/_fake_quantize_learnable_per_channel_affine_native.h>
#include <ATen/ops/_fake_quantize_learnable_per_channel_affine_ops.h>
#include <ATen/ops/_fused_moving_avg_obs_fq_helper_native.h>
#include <ATen/ops/_fused_moving_avg_obs_fq_helper_ops.h>
#include <ATen/ops/_fused_moving_avg_obs_fq_helper_native.h>
#include <ATen/ops/_fused_moving_avg_obs_fq_helper_ops.h>
#include <ATen/ops/_fused_moving_avg_obs_fq_helper_native.h>
#include <ATen/ops/_fused_moving_avg_obs_fq_helper_ops.h>
#include <ATen/ops/_to_copy_native.h>
#include <ATen/ops/_to_copy_ops.h>
#include <ATen/ops/_to_copy_native.h>
#include <ATen/ops/_to_copy_ops.h>
#include <ATen/ops/_thnn_fused_gru_cell_native.h>
#include <ATen/ops/_thnn_fused_gru_cell_ops.h>
#include <ATen/ops/_thnn_fused_gru_cell_native.h>
#include <ATen/ops/_thnn_fused_gru_cell_ops.h>
#include <ATen/ops/_pack_padded_sequence_native.h>
#include <ATen/ops/_pack_padded_sequence_ops.h>
#include <ATen/ops/_pack_padded_sequence_native.h>
#include <ATen/ops/_pack_padded_sequence_ops.h>
#include <ATen/ops/scatter_reduce_native.h>
#include <ATen/ops/scatter_reduce_ops.h>
#include <ATen/ops/scatter_reduce_native.h>
#include <ATen/ops/scatter_reduce_ops.h>
#include <ATen/ops/scatter_reduce_native.h>
#include <ATen/ops/scatter_reduce_ops.h>
#include <ATen/ops/bitwise_xor_native.h>
#include <ATen/ops/bitwise_xor_ops.h>
#include <ATen/ops/bitwise_xor_native.h>
#include <ATen/ops/bitwise_xor_ops.h>
#include <ATen/ops/bitwise_xor_native.h>
#include <ATen/ops/bitwise_xor_ops.h>
#include <ATen/ops/bitwise_xor_native.h>
#include <ATen/ops/bitwise_xor_ops.h>
#include <ATen/ops/bitwise_xor_native.h>
#include <ATen/ops/bitwise_xor_ops.h>
#include <ATen/ops/bitwise_xor_native.h>
#include <ATen/ops/bitwise_xor_ops.h>
#include <ATen/ops/bitwise_xor_native.h>
#include <ATen/ops/bitwise_xor_ops.h>
#include <ATen/ops/bitwise_xor_native.h>
#include <ATen/ops/bitwise_xor_ops.h>
#include <ATen/ops/addbmm_native.h>
#include <ATen/ops/addbmm_ops.h>
#include <ATen/ops/addbmm_native.h>
#include <ATen/ops/addbmm_ops.h>
#include <ATen/ops/addbmm_native.h>
#include <ATen/ops/addbmm_ops.h>
#include <ATen/ops/random_native.h>
#include <ATen/ops/random_ops.h>
#include <ATen/ops/random_native.h>
#include <ATen/ops/random_ops.h>
#include <ATen/ops/random_native.h>
#include <ATen/ops/random_ops.h>
#include <ATen/ops/random_native.h>
#include <ATen/ops/random_ops.h>
#include <ATen/ops/random_native.h>
#include <ATen/ops/random_ops.h>
#include <ATen/ops/random_native.h>
#include <ATen/ops/random_ops.h>
#include <ATen/ops/random_native.h>
#include <ATen/ops/random_ops.h>
#include <ATen/ops/random_native.h>
#include <ATen/ops/random_ops.h>
#include <ATen/ops/random_native.h>
#include <ATen/ops/random_ops.h>
#include <ATen/ops/exponential_native.h>
#include <ATen/ops/exponential_ops.h>
#include <ATen/ops/exponential_native.h>
#include <ATen/ops/exponential_ops.h>
#include <ATen/ops/exponential_native.h>
#include <ATen/ops/exponential_ops.h>
#include <ATen/ops/geometric_native.h>
#include <ATen/ops/geometric_ops.h>
#include <ATen/ops/geometric_native.h>
#include <ATen/ops/geometric_ops.h>
#include <ATen/ops/geometric_native.h>
#include <ATen/ops/geometric_ops.h>
#include <ATen/ops/cross_native.h>
#include <ATen/ops/cross_ops.h>
#include <ATen/ops/cross_native.h>
#include <ATen/ops/cross_ops.h>
#include <ATen/ops/trace_native.h>
#include <ATen/ops/trace_ops.h>
#include <ATen/ops/trace_native.h>
#include <ATen/ops/trace_ops.h>
#include <ATen/ops/take_along_dim_native.h>
#include <ATen/ops/take_along_dim_ops.h>
#include <ATen/ops/take_along_dim_native.h>
#include <ATen/ops/take_along_dim_ops.h>
#include <ATen/ops/index_select_native.h>
#include <ATen/ops/index_select_ops.h>
#include <ATen/ops/index_select_native.h>
#include <ATen/ops/index_select_ops.h>
#include <ATen/ops/index_select_native.h>
#include <ATen/ops/index_select_ops.h>
#include <ATen/ops/index_select_native.h>
#include <ATen/ops/index_select_ops.h>
#include <ATen/ops/masked_select_native.h>
#include <ATen/ops/masked_select_ops.h>
#include <ATen/ops/masked_select_native.h>
#include <ATen/ops/masked_select_ops.h>
#include <ATen/ops/linalg_solve_triangular_native.h>
#include <ATen/ops/linalg_solve_triangular_ops.h>
#include <ATen/ops/linalg_solve_triangular_native.h>
#include <ATen/ops/linalg_solve_triangular_ops.h>
#include <ATen/ops/svd_native.h>
#include <ATen/ops/svd_ops.h>
#include <ATen/ops/svd_native.h>
#include <ATen/ops/svd_ops.h>
#include <ATen/ops/multinomial_native.h>
#include <ATen/ops/multinomial_ops.h>
#include <ATen/ops/multinomial_native.h>
#include <ATen/ops/multinomial_ops.h>
#include <ATen/ops/histogram_native.h>
#include <ATen/ops/histogram_ops.h>
#include <ATen/ops/histogram_native.h>
#include <ATen/ops/histogram_ops.h>
#include <ATen/ops/histogram_native.h>
#include <ATen/ops/histogram_ops.h>
#include <ATen/ops/histogram_native.h>
#include <ATen/ops/histogram_ops.h>
#include <ATen/ops/igammac_native.h>
#include <ATen/ops/igammac_ops.h>
#include <ATen/ops/igammac_native.h>
#include <ATen/ops/igammac_ops.h>
#include <ATen/ops/igammac_native.h>
#include <ATen/ops/igammac_ops.h>
#include <ATen/ops/remainder_native.h>
#include <ATen/ops/remainder_ops.h>
#include <ATen/ops/remainder_native.h>
#include <ATen/ops/remainder_ops.h>
#include <ATen/ops/remainder_native.h>
#include <ATen/ops/remainder_ops.h>
#include <ATen/ops/remainder_native.h>
#include <ATen/ops/remainder_ops.h>
#include <ATen/ops/remainder_native.h>
#include <ATen/ops/remainder_ops.h>
#include <ATen/ops/remainder_native.h>
#include <ATen/ops/remainder_ops.h>
#include <ATen/ops/remainder_native.h>
#include <ATen/ops/remainder_ops.h>
#include <ATen/ops/remainder_native.h>
#include <ATen/ops/remainder_ops.h>
#include <ATen/ops/quantile_native.h>
#include <ATen/ops/quantile_ops.h>
#include <ATen/ops/quantile_native.h>
#include <ATen/ops/quantile_ops.h>
#include <ATen/ops/quantile_native.h>
#include <ATen/ops/quantile_ops.h>
#include <ATen/ops/quantile_native.h>
#include <ATen/ops/quantile_ops.h>
#include <ATen/ops/nanquantile_native.h>
#include <ATen/ops/nanquantile_ops.h>
#include <ATen/ops/nanquantile_native.h>
#include <ATen/ops/nanquantile_ops.h>
#include <ATen/ops/nanquantile_native.h>
#include <ATen/ops/nanquantile_ops.h>
#include <ATen/ops/nanquantile_native.h>
#include <ATen/ops/nanquantile_ops.h>
#include <ATen/ops/sort_native.h>
#include <ATen/ops/sort_ops.h>
#include <ATen/ops/sort_native.h>
#include <ATen/ops/sort_ops.h>
#include <ATen/ops/sort_native.h>
#include <ATen/ops/sort_ops.h>
#include <ATen/ops/sort_native.h>
#include <ATen/ops/sort_ops.h>
#include <ATen/ops/sort_native.h>
#include <ATen/ops/sort_ops.h>
#include <ATen/ops/sort_native.h>
#include <ATen/ops/sort_ops.h>
#include <ATen/ops/sort_native.h>
#include <ATen/ops/sort_ops.h>
#include <ATen/ops/sort_native.h>
#include <ATen/ops/sort_ops.h>
#include <ATen/ops/argsort_native.h>
#include <ATen/ops/argsort_ops.h>
#include <ATen/ops/argsort_native.h>
#include <ATen/ops/argsort_ops.h>
#include <ATen/ops/all_native.h>
#include <ATen/ops/all_ops.h>
#include <ATen/ops/all_native.h>
#include <ATen/ops/all_ops.h>
#include <ATen/ops/renorm_native.h>
#include <ATen/ops/renorm_ops.h>
#include <ATen/ops/renorm_native.h>
#include <ATen/ops/renorm_ops.h>
#include <ATen/ops/renorm_native.h>
#include <ATen/ops/renorm_ops.h>
#include <ATen/ops/unfold_backward_native.h>
#include <ATen/ops/unfold_backward_ops.h>
#include <ATen/ops/unfold_backward_native.h>
#include <ATen/ops/unfold_backward_ops.h>
#include <ATen/ops/pow_native.h>
#include <ATen/ops/pow_ops.h>
#include <ATen/ops/pow_native.h>
#include <ATen/ops/pow_ops.h>
#include <ATen/ops/pow_native.h>
#include <ATen/ops/pow_ops.h>
#include <ATen/ops/pow_native.h>
#include <ATen/ops/pow_ops.h>
#include <ATen/ops/pow_native.h>
#include <ATen/ops/pow_ops.h>
#include <ATen/ops/pow_native.h>
#include <ATen/ops/pow_ops.h>
#include <ATen/ops/pow_native.h>
#include <ATen/ops/pow_ops.h>
#include <ATen/ops/pow_native.h>
#include <ATen/ops/pow_ops.h>
#include <ATen/ops/float_power_native.h>
#include <ATen/ops/float_power_ops.h>
#include <ATen/ops/float_power_native.h>
#include <ATen/ops/float_power_ops.h>
#include <ATen/ops/float_power_native.h>
#include <ATen/ops/float_power_ops.h>
#include <ATen/ops/float_power_native.h>
#include <ATen/ops/float_power_ops.h>
#include <ATen/ops/float_power_native.h>
#include <ATen/ops/float_power_ops.h>
#include <ATen/ops/float_power_native.h>
#include <ATen/ops/float_power_ops.h>
#include <ATen/ops/float_power_native.h>
#include <ATen/ops/float_power_ops.h>
#include <ATen/ops/float_power_native.h>
#include <ATen/ops/float_power_ops.h>
#include <ATen/ops/normal_native.h>
#include <ATen/ops/normal_ops.h>
#include <ATen/ops/normal_native.h>
#include <ATen/ops/normal_ops.h>
#include <ATen/ops/normal_native.h>
#include <ATen/ops/normal_ops.h>
#include <ATen/ops/normal_native.h>
#include <ATen/ops/normal_ops.h>
#include <ATen/ops/normal_native.h>
#include <ATen/ops/normal_ops.h>
#include <ATen/ops/normal_native.h>
#include <ATen/ops/normal_ops.h>
#include <ATen/ops/normal_native.h>
#include <ATen/ops/normal_ops.h>
#include <ATen/ops/normal_native.h>
#include <ATen/ops/normal_ops.h>
#include <ATen/ops/normal_native.h>
#include <ATen/ops/normal_ops.h>
#include <ATen/ops/normal_native.h>
#include <ATen/ops/normal_ops.h>
#include <ATen/ops/normal_native.h>
#include <ATen/ops/normal_ops.h>
#include <ATen/ops/_amp_update_scale_native.h>
#include <ATen/ops/_amp_update_scale_ops.h>
#include <ATen/ops/_amp_update_scale_native.h>
#include <ATen/ops/_amp_update_scale_ops.h>
#include <ATen/ops/_amp_update_scale_native.h>
#include <ATen/ops/_amp_update_scale_ops.h>
#include <ATen/ops/_foreach_atan_native.h>
#include <ATen/ops/_foreach_atan_ops.h>
#include <ATen/ops/_foreach_atan_native.h>
#include <ATen/ops/_foreach_atan_ops.h>
#include <ATen/ops/_foreach_atan_native.h>
#include <ATen/ops/_foreach_atan_ops.h>
#include <ATen/ops/_foreach_erf_native.h>
#include <ATen/ops/_foreach_erf_ops.h>
#include <ATen/ops/_foreach_erf_native.h>
#include <ATen/ops/_foreach_erf_ops.h>
#include <ATen/ops/_foreach_erf_native.h>
#include <ATen/ops/_foreach_erf_ops.h>
#include <ATen/ops/_foreach_erfc_native.h>
#include <ATen/ops/_foreach_erfc_ops.h>
#include <ATen/ops/_foreach_erfc_native.h>
#include <ATen/ops/_foreach_erfc_ops.h>
#include <ATen/ops/_foreach_erfc_native.h>
#include <ATen/ops/_foreach_erfc_ops.h>
#include <ATen/ops/_foreach_lerp_native.h>
#include <ATen/ops/_foreach_lerp_ops.h>
#include <ATen/ops/_foreach_lerp_native.h>
#include <ATen/ops/_foreach_lerp_ops.h>
#include <ATen/ops/_foreach_lerp_native.h>
#include <ATen/ops/_foreach_lerp_ops.h>
#include <ATen/ops/_foreach_lerp_native.h>
#include <ATen/ops/_foreach_lerp_ops.h>
#include <ATen/ops/_foreach_lerp_native.h>
#include <ATen/ops/_foreach_lerp_ops.h>
#include <ATen/ops/_foreach_lerp_native.h>
#include <ATen/ops/_foreach_lerp_ops.h>
#include <ATen/ops/_foreach_lgamma_native.h>
#include <ATen/ops/_foreach_lgamma_ops.h>
#include <ATen/ops/_foreach_lgamma_native.h>
#include <ATen/ops/_foreach_lgamma_ops.h>
#include <ATen/ops/_foreach_lgamma_native.h>
#include <ATen/ops/_foreach_lgamma_ops.h>
#include <ATen/ops/_foreach_log_native.h>
#include <ATen/ops/_foreach_log_ops.h>
#include <ATen/ops/_foreach_log_native.h>
#include <ATen/ops/_foreach_log_ops.h>
#include <ATen/ops/_foreach_log_native.h>
#include <ATen/ops/_foreach_log_ops.h>
#include <ATen/ops/_foreach_sinh_native.h>
#include <ATen/ops/_foreach_sinh_ops.h>
#include <ATen/ops/_foreach_sinh_native.h>
#include <ATen/ops/_foreach_sinh_ops.h>
#include <ATen/ops/_foreach_sinh_native.h>
#include <ATen/ops/_foreach_sinh_ops.h>
#include <ATen/ops/_convert_indices_from_coo_to_csr_native.h>
#include <ATen/ops/_convert_indices_from_coo_to_csr_ops.h>
#include <ATen/ops/_convert_indices_from_coo_to_csr_native.h>
#include <ATen/ops/_convert_indices_from_coo_to_csr_ops.h>
#include <ATen/ops/multi_margin_loss_backward_native.h>
#include <ATen/ops/multi_margin_loss_backward_ops.h>
#include <ATen/ops/multi_margin_loss_backward_native.h>
#include <ATen/ops/multi_margin_loss_backward_ops.h>
#include <ATen/ops/nll_loss_forward_native.h>
#include <ATen/ops/nll_loss_forward_ops.h>
#include <ATen/ops/nll_loss_forward_native.h>
#include <ATen/ops/nll_loss_forward_ops.h>
#include <ATen/ops/hardswish_native.h>
#include <ATen/ops/hardswish_ops.h>
#include <ATen/ops/hardswish_native.h>
#include <ATen/ops/hardswish_ops.h>
#include <ATen/ops/hardswish_native.h>
#include <ATen/ops/hardswish_ops.h>
#include <ATen/ops/hardswish_backward_native.h>
#include <ATen/ops/hardswish_backward_ops.h>
#include <ATen/ops/hardswish_backward_native.h>
#include <ATen/ops/hardswish_backward_ops.h>
#include <ATen/ops/softshrink_backward_native.h>
#include <ATen/ops/softshrink_backward_ops.h>
#include <ATen/ops/softshrink_backward_native.h>
#include <ATen/ops/softshrink_backward_ops.h>
#include <ATen/ops/mkldnn_adaptive_avg_pool2d_backward_native.h>
#include <ATen/ops/mkldnn_adaptive_avg_pool2d_backward_ops.h>
#include <ATen/ops/mkldnn_adaptive_avg_pool2d_backward_native.h>
#include <ATen/ops/mkldnn_adaptive_avg_pool2d_backward_ops.h>
#include <ATen/ops/_adaptive_avg_pool2d_backward_native.h>
#include <ATen/ops/_adaptive_avg_pool2d_backward_ops.h>
#include <ATen/ops/_adaptive_avg_pool2d_backward_native.h>
#include <ATen/ops/_adaptive_avg_pool2d_backward_ops.h>
#include <ATen/ops/adaptive_avg_pool3d_native.h>
#include <ATen/ops/adaptive_avg_pool3d_ops.h>
#include <ATen/ops/adaptive_avg_pool3d_native.h>
#include <ATen/ops/adaptive_avg_pool3d_ops.h>
#include <ATen/ops/adaptive_max_pool3d_backward_native.h>
#include <ATen/ops/adaptive_max_pool3d_backward_ops.h>
#include <ATen/ops/adaptive_max_pool3d_backward_native.h>
#include <ATen/ops/adaptive_max_pool3d_backward_ops.h>
#include <ATen/ops/avg_pool2d_native.h>
#include <ATen/ops/avg_pool2d_ops.h>
#include <ATen/ops/avg_pool2d_native.h>
#include <ATen/ops/avg_pool2d_ops.h>
#include <ATen/ops/avg_pool3d_backward_native.h>
#include <ATen/ops/avg_pool3d_backward_ops.h>
#include <ATen/ops/avg_pool3d_backward_native.h>
#include <ATen/ops/avg_pool3d_backward_ops.h>
#include <ATen/ops/fractional_max_pool2d_native.h>
#include <ATen/ops/fractional_max_pool2d_ops.h>
#include <ATen/ops/fractional_max_pool2d_native.h>
#include <ATen/ops/fractional_max_pool2d_ops.h>
#include <ATen/ops/reflection_pad3d_backward_native.h>
#include <ATen/ops/reflection_pad3d_backward_ops.h>
#include <ATen/ops/reflection_pad3d_backward_native.h>
#include <ATen/ops/reflection_pad3d_backward_ops.h>
#include <ATen/ops/replication_pad2d_native.h>
#include <ATen/ops/replication_pad2d_ops.h>
#include <ATen/ops/replication_pad2d_native.h>
#include <ATen/ops/replication_pad2d_ops.h>
#include <ATen/ops/_upsample_bilinear2d_aa_backward_native.h>
#include <ATen/ops/_upsample_bilinear2d_aa_backward_ops.h>
#include <ATen/ops/_upsample_bilinear2d_aa_backward_native.h>
#include <ATen/ops/_upsample_bilinear2d_aa_backward_ops.h>
#include <ATen/ops/_upsample_bicubic2d_aa_native.h>
#include <ATen/ops/_upsample_bicubic2d_aa_ops.h>
#include <ATen/ops/_upsample_bicubic2d_aa_native.h>
#include <ATen/ops/_upsample_bicubic2d_aa_ops.h>
#include <ATen/ops/upsample_trilinear3d_native.h>
#include <ATen/ops/upsample_trilinear3d_ops.h>
#include <ATen/ops/upsample_trilinear3d_native.h>
#include <ATen/ops/upsample_trilinear3d_ops.h>
#include <ATen/ops/_upsample_nearest_exact1d_backward_native.h>
#include <ATen/ops/_upsample_nearest_exact1d_backward_ops.h>
#include <ATen/ops/_upsample_nearest_exact1d_backward_native.h>
#include <ATen/ops/_upsample_nearest_exact1d_backward_ops.h>
#include <ATen/ops/upsample_nearest2d_backward_native.h>
#include <ATen/ops/upsample_nearest2d_backward_ops.h>
#include <ATen/ops/upsample_nearest2d_backward_native.h>
#include <ATen/ops/upsample_nearest2d_backward_ops.h>
#include <ATen/ops/tanh_backward_native.h>
#include <ATen/ops/tanh_backward_ops.h>
#include <ATen/ops/tanh_backward_native.h>
#include <ATen/ops/tanh_backward_ops.h>
#include <ATen/ops/_conv_depthwise2d_native.h>
#include <ATen/ops/_conv_depthwise2d_ops.h>
#include <ATen/ops/_conv_depthwise2d_native.h>
#include <ATen/ops/_conv_depthwise2d_ops.h>
#include <ATen/ops/col2im_native.h>
#include <ATen/ops/col2im_ops.h>
#include <ATen/ops/col2im_native.h>
#include <ATen/ops/col2im_ops.h>
#include <ATen/ops/column_stack_native.h>
#include <ATen/ops/column_stack_ops.h>
#include <ATen/ops/column_stack_native.h>
#include <ATen/ops/column_stack_ops.h>
#include <ATen/ops/im2col_native.h>
#include <ATen/ops/im2col_ops.h>
#include <ATen/ops/im2col_native.h>
#include <ATen/ops/im2col_ops.h>
#include <ATen/ops/isinf_native.h>
#include <ATen/ops/isinf_ops.h>
#include <ATen/ops/isinf_native.h>
#include <ATen/ops/isinf_ops.h>
#include <ATen/ops/isneginf_native.h>
#include <ATen/ops/isneginf_ops.h>
#include <ATen/ops/isneginf_native.h>
#include <ATen/ops/isneginf_ops.h>
#include <ATen/ops/special_expm1_native.h>
#include <ATen/ops/special_expm1_ops.h>
#include <ATen/ops/special_expm1_native.h>
#include <ATen/ops/special_expm1_ops.h>
#include <ATen/ops/special_erf_native.h>
#include <ATen/ops/special_erf_ops.h>
#include <ATen/ops/special_erf_native.h>
#include <ATen/ops/special_erf_ops.h>
#include <ATen/ops/special_logsumexp_native.h>
#include <ATen/ops/special_logsumexp_ops.h>
#include <ATen/ops/special_logsumexp_native.h>
#include <ATen/ops/special_logsumexp_ops.h>
#include <ATen/ops/special_log1p_native.h>
#include <ATen/ops/special_log1p_ops.h>
#include <ATen/ops/special_log1p_native.h>
#include <ATen/ops/special_log1p_ops.h>
#include <ATen/ops/special_gammaincc_native.h>
#include <ATen/ops/special_gammaincc_ops.h>
#include <ATen/ops/special_gammaincc_native.h>
#include <ATen/ops/special_gammaincc_ops.h>
#include <ATen/ops/special_multigammaln_native.h>
#include <ATen/ops/special_multigammaln_ops.h>
#include <ATen/ops/special_multigammaln_native.h>
#include <ATen/ops/special_multigammaln_ops.h>
#include <ATen/ops/fft_rfft2_native.h>
#include <ATen/ops/fft_rfft2_ops.h>
#include <ATen/ops/fft_rfft2_native.h>
#include <ATen/ops/fft_rfft2_ops.h>
#include <ATen/ops/fft_irfft2_native.h>
#include <ATen/ops/fft_irfft2_ops.h>
#include <ATen/ops/fft_irfft2_native.h>
#include <ATen/ops/fft_irfft2_ops.h>
#include <ATen/ops/fft_ihfft2_native.h>
#include <ATen/ops/fft_ihfft2_ops.h>
#include <ATen/ops/fft_ihfft2_native.h>
#include <ATen/ops/fft_ihfft2_ops.h>
#include <ATen/ops/fft_fftn_native.h>
#include <ATen/ops/fft_fftn_ops.h>
#include <ATen/ops/fft_fftn_native.h>
#include <ATen/ops/fft_fftn_ops.h>
#include <ATen/ops/fft_irfftn_native.h>
#include <ATen/ops/fft_irfftn_ops.h>
#include <ATen/ops/fft_irfftn_native.h>
#include <ATen/ops/fft_irfftn_ops.h>
#include <ATen/ops/linalg_lu_factor_native.h>
#include <ATen/ops/linalg_lu_factor_ops.h>
#include <ATen/ops/linalg_lu_factor_native.h>
#include <ATen/ops/linalg_lu_factor_ops.h>
#include <ATen/ops/linalg_lu_factor_ex_native.h>
#include <ATen/ops/linalg_lu_factor_ex_ops.h>
#include <ATen/ops/linalg_lu_factor_ex_native.h>
#include <ATen/ops/linalg_lu_factor_ex_ops.h>
#include <ATen/ops/linalg_ldl_factor_native.h>
#include <ATen/ops/linalg_ldl_factor_ops.h>
#include <ATen/ops/linalg_ldl_factor_native.h>
#include <ATen/ops/linalg_ldl_factor_ops.h>
#include <ATen/ops/linalg_ldl_solve_native.h>
#include <ATen/ops/linalg_ldl_solve_ops.h>
#include <ATen/ops/linalg_ldl_solve_native.h>
#include <ATen/ops/linalg_ldl_solve_ops.h>
#include <ATen/ops/_linalg_slogdet_native.h>
#include <ATen/ops/_linalg_slogdet_ops.h>
#include <ATen/ops/_linalg_slogdet_native.h>
#include <ATen/ops/_linalg_slogdet_ops.h>
#include <ATen/ops/linalg_eigvals_native.h>
#include <ATen/ops/linalg_eigvals_ops.h>
#include <ATen/ops/linalg_eigvals_native.h>
#include <ATen/ops/linalg_eigvals_ops.h>
#include <ATen/ops/linalg_eigh_native.h>
#include <ATen/ops/linalg_eigh_ops.h>
#include <ATen/ops/linalg_eigh_native.h>
#include <ATen/ops/linalg_eigh_ops.h>
#include <ATen/ops/linalg_householder_product_native.h>
#include <ATen/ops/linalg_householder_product_ops.h>
#include <ATen/ops/linalg_householder_product_native.h>
#include <ATen/ops/linalg_householder_product_ops.h>
#include <ATen/ops/ger_native.h>
#include <ATen/ops/ger_ops.h>
#include <ATen/ops/ger_native.h>
#include <ATen/ops/ger_ops.h>
#include <ATen/ops/linalg_norm_native.h>
#include <ATen/ops/linalg_norm_ops.h>
#include <ATen/ops/linalg_norm_native.h>
#include <ATen/ops/linalg_norm_ops.h>
#include <ATen/ops/linalg_norm_native.h>
#include <ATen/ops/linalg_norm_ops.h>
#include <ATen/ops/linalg_norm_native.h>
#include <ATen/ops/linalg_norm_ops.h>
#include <ATen/ops/linalg_vector_norm_native.h>
#include <ATen/ops/linalg_vector_norm_ops.h>
#include <ATen/ops/linalg_vector_norm_native.h>
#include <ATen/ops/linalg_vector_norm_ops.h>
#include <ATen/ops/_linalg_solve_ex_native.h>
#include <ATen/ops/_linalg_solve_ex_ops.h>
#include <ATen/ops/_linalg_solve_ex_native.h>
#include <ATen/ops/_linalg_solve_ex_ops.h>
#include <ATen/ops/linalg_solve_native.h>
#include <ATen/ops/linalg_solve_ops.h>
#include <ATen/ops/linalg_solve_native.h>
#include <ATen/ops/linalg_solve_ops.h>
#include <ATen/ops/linalg_multi_dot_native.h>
#include <ATen/ops/linalg_multi_dot_ops.h>
#include <ATen/ops/linalg_multi_dot_native.h>
#include <ATen/ops/linalg_multi_dot_ops.h>
#include <ATen/ops/_test_optional_filled_intlist_native.h>
#include <ATen/ops/_test_optional_filled_intlist_ops.h>
#include <ATen/ops/_test_optional_filled_intlist_native.h>
#include <ATen/ops/_test_optional_filled_intlist_ops.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_native.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_ops.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_native.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_ops.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_view_copy_native.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_view_copy_ops.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_view_copy_native.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_view_copy_ops.h>
#include <ATen/ops/segment_reduce_native.h>
#include <ATen/ops/segment_reduce_ops.h>
#include <ATen/ops/segment_reduce_native.h>
#include <ATen/ops/segment_reduce_ops.h>
#include <ATen/ops/_nested_tensor_from_tensor_list_native.h>
#include <ATen/ops/_nested_tensor_from_tensor_list_ops.h>
#include <ATen/ops/_nested_tensor_from_tensor_list_native.h>
#include <ATen/ops/_nested_tensor_from_tensor_list_ops.h>
#include <ATen/ops/diagonal_copy_native.h>
#include <ATen/ops/diagonal_copy_ops.h>
#include <ATen/ops/diagonal_copy_native.h>
#include <ATen/ops/diagonal_copy_ops.h>
#include <ATen/ops/detach_copy_native.h>
#include <ATen/ops/detach_copy_ops.h>
#include <ATen/ops/detach_copy_native.h>
#include <ATen/ops/detach_copy_ops.h>
#include <ATen/ops/slice_copy_native.h>
#include <ATen/ops/slice_copy_ops.h>
#include <ATen/ops/slice_copy_native.h>
#include <ATen/ops/slice_copy_ops.h>
#include <ATen/ops/transpose_copy_native.h>
#include <ATen/ops/transpose_copy_ops.h>
#include <ATen/ops/transpose_copy_native.h>
#include <ATen/ops/transpose_copy_ops.h>
#include <ATen/ops/indices_copy_native.h>
#include <ATen/ops/indices_copy_ops.h>
#include <ATen/ops/indices_copy_native.h>
#include <ATen/ops/indices_copy_ops.h>
#include <ATen/ops/row_indices_copy_native.h>
#include <ATen/ops/row_indices_copy_ops.h>
#include <ATen/ops/row_indices_copy_native.h>
#include <ATen/ops/row_indices_copy_ops.h>
#include <ATen/ops/_triton_multi_head_attention_native.h>
#include <ATen/ops/_triton_multi_head_attention_ops.h>
#include <ATen/ops/_triton_multi_head_attention_native.h>
#include <ATen/ops/_triton_multi_head_attention_ops.h>
#include <ATen/ops/special_bessel_j1_native.h>
#include <ATen/ops/special_bessel_j1_ops.h>
#include <ATen/ops/special_bessel_j1_native.h>
#include <ATen/ops/special_bessel_j1_ops.h>
#include <ATen/ops/special_bessel_y1_native.h>
#include <ATen/ops/special_bessel_y1_ops.h>
#include <ATen/ops/special_bessel_y1_native.h>
#include <ATen/ops/special_bessel_y1_ops.h>
#include <ATen/ops/special_legendre_polynomial_p_native.h>
#include <ATen/ops/special_legendre_polynomial_p_ops.h>
#include <ATen/ops/special_legendre_polynomial_p_native.h>
#include <ATen/ops/special_legendre_polynomial_p_ops.h>
#include <ATen/ops/special_legendre_polynomial_p_native.h>
#include <ATen/ops/special_legendre_polynomial_p_ops.h>
#include <ATen/ops/special_legendre_polynomial_p_native.h>
#include <ATen/ops/special_legendre_polynomial_p_ops.h>
#include <ATen/ops/special_legendre_polynomial_p_native.h>
#include <ATen/ops/special_legendre_polynomial_p_ops.h>
#include <ATen/ops/special_legendre_polynomial_p_native.h>
#include <ATen/ops/special_legendre_polynomial_p_ops.h>
#include <ATen/ops/special_modified_bessel_i0_native.h>
#include <ATen/ops/special_modified_bessel_i0_ops.h>
#include <ATen/ops/special_modified_bessel_i0_native.h>
#include <ATen/ops/special_modified_bessel_i0_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_t_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_u_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_u_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_u_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_u_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_u_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_u_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_u_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_u_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_u_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_u_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_u_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_u_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_w_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_w_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_w_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_w_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_w_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_w_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_w_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_w_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_w_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_w_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_w_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_w_ops.h>
#include <ATen/ops/_fused_adam_native.h>
#include <ATen/ops/_fused_adam_ops.h>
#include <ATen/ops/_fused_adam_native.h>
#include <ATen/ops/_fused_adam_ops.h>
#include <ATen/ops/_fused_adam_native.h>
#include <ATen/ops/_fused_adam_ops.h>
#include <ATen/ops/_fused_adam_native.h>
#include <ATen/ops/_fused_adam_ops.h>
#include <ATen/ops/_fused_adam_native.h>
#include <ATen/ops/_fused_adam_ops.h>
#include <ATen/ops/_fused_adam_native.h>
#include <ATen/ops/_fused_adam_ops.h>
#include <ATen/ops/rename_native.h>
#include <ATen/ops/rename_ops.h>
#include <ATen/ops/imag_native.h>
#include <ATen/ops/imag_ops.h>
#include <ATen/ops/_conj_native.h>
#include <ATen/ops/_conj_ops.h>
#include <ATen/ops/_conj_copy_native.h>
#include <ATen/ops/_conj_copy_ops.h>
#include <ATen/ops/conj_native.h>
#include <ATen/ops/conj_ops.h>
#include <ATen/ops/resolve_conj_native.h>
#include <ATen/ops/resolve_conj_ops.h>
#include <ATen/ops/as_strided_native.h>
#include <ATen/ops/as_strided_ops.h>
#include <ATen/ops/as_strided_copy_native.h>
#include <ATen/ops/as_strided_copy_ops.h>
#include <ATen/ops/_sparse_broadcast_to_native.h>
#include <ATen/ops/_sparse_broadcast_to_ops.h>
#include <ATen/ops/_sparse_broadcast_to_copy_native.h>
#include <ATen/ops/_sparse_broadcast_to_copy_ops.h>
#include <ATen/ops/chunk_native.h>
#include <ATen/ops/chunk_ops.h>
#include <ATen/ops/tensor_split_native.h>
#include <ATen/ops/tensor_split_ops.h>
#include <ATen/ops/tensor_split_native.h>
#include <ATen/ops/tensor_split_ops.h>
#include <ATen/ops/tensor_split_native.h>
#include <ATen/ops/tensor_split_ops.h>
#include <ATen/ops/expand_as_native.h>
#include <ATen/ops/expand_as_ops.h>
#include <ATen/ops/unflatten_native.h>
#include <ATen/ops/unflatten_ops.h>
#include <ATen/ops/unflatten_native.h>
#include <ATen/ops/unflatten_ops.h>
#include <ATen/ops/permute_native.h>
#include <ATen/ops/permute_ops.h>
#include <ATen/ops/permute_copy_native.h>
#include <ATen/ops/permute_copy_ops.h>
#include <ATen/ops/movedim_native.h>
#include <ATen/ops/movedim_ops.h>
#include <ATen/ops/movedim_native.h>
#include <ATen/ops/movedim_ops.h>
#include <ATen/ops/mH_native.h>
#include <ATen/ops/mH_ops.h>
#include <ATen/ops/pin_memory_native.h>
#include <ATen/ops/pin_memory_ops.h>
#include <ATen/ops/_reshape_alias_native.h>
#include <ATen/ops/_reshape_alias_ops.h>
#include <ATen/ops/_reshape_alias_copy_native.h>
#include <ATen/ops/_reshape_alias_copy_ops.h>
#include <ATen/ops/detach_native.h>
#include <ATen/ops/detach_ops.h>
#include <ATen/ops/detach_copy_native.h>
#include <ATen/ops/detach_copy_ops.h>
#include <ATen/ops/split_native.h>
#include <ATen/ops/split_ops.h>
#include <ATen/ops/split_copy_native.h>
#include <ATen/ops/split_copy_ops.h>
#include <ATen/ops/split_native.h>
#include <ATen/ops/split_ops.h>
#include <ATen/ops/dsplit_native.h>
#include <ATen/ops/dsplit_ops.h>
#include <ATen/ops/dsplit_native.h>
#include <ATen/ops/dsplit_ops.h>
#include <ATen/ops/positive_native.h>
#include <ATen/ops/positive_ops.h>
#include <ATen/ops/values_native.h>
#include <ATen/ops/values_ops.h>
#include <ATen/ops/values_copy_native.h>
#include <ATen/ops/values_copy_ops.h>
#include <ATen/ops/row_indices_native.h>
#include <ATen/ops/row_indices_ops.h>
#include <ATen/ops/row_indices_copy_native.h>
#include <ATen/ops/row_indices_copy_ops.h>
#include <ATen/ops/lift_fresh_native.h>
#include <ATen/ops/lift_fresh_ops.h>
#include <ATen/ops/lift_fresh_copy_native.h>
#include <ATen/ops/lift_fresh_copy_ops.h>
#include <ATen/ops/swapdims_native.h>
#include <ATen/ops/swapdims_ops.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_view_native.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_view_ops.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_view_copy_native.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_view_copy_ops.h>
#include <ATen/ops/_cast_Double_native.h>
#include <ATen/ops/_cast_Double_ops.h>
#include <ATen/ops/_cast_Int_native.h>
#include <ATen/ops/_cast_Int_ops.h>
#include <ATen/ops/_cast_Short_native.h>
#include <ATen/ops/_cast_Short_ops.h>
#include <ATen/ops/output_nr_native.h>
#include <ATen/ops/output_nr_ops.h>
#include <ATen/ops/_functional_assert_scalar_native.h>
#include <ATen/ops/_functional_assert_scalar_ops.h>
#include <ATen/ops/_functional_sym_constrain_range_native.h>
#include <ATen/ops/_functional_sym_constrain_range_ops.h>
#include <ATen/ops/_cudnn_ctc_loss_native.h>
#include <ATen/ops/_cudnn_ctc_loss_ops.h>
#include <ATen/ops/_use_cudnn_rnn_flatten_weight_native.h>
#include <ATen/ops/_use_cudnn_rnn_flatten_weight_ops.h>
#include <ATen/ops/_sobol_engine_initialize_state_native.h>
#include <ATen/ops/_sobol_engine_initialize_state_ops.h>
#include <ATen/ops/_shape_as_tensor_native.h>
#include <ATen/ops/_shape_as_tensor_ops.h>
#include <ATen/ops/_dim_arange_native.h>
#include <ATen/ops/_dim_arange_ops.h>
#include <ATen/ops/_lazy_clone_native.h>
#include <ATen/ops/_lazy_clone_ops.h>
#include <ATen/ops/cudnn_is_acceptable_native.h>
#include <ATen/ops/cudnn_is_acceptable_ops.h>
#include <ATen/ops/_convolution_native.h>
#include <ATen/ops/_convolution_ops.h>
#include <ATen/ops/conv2d_native.h>
#include <ATen/ops/conv2d_ops.h>
#include <ATen/ops/conv2d_native.h>
#include <ATen/ops/conv2d_ops.h>
#include <ATen/ops/conv_tbc_backward_native.h>
#include <ATen/ops/conv_tbc_backward_ops.h>
#include <ATen/ops/cov_native.h>
#include <ATen/ops/cov_ops.h>
#include <ATen/ops/_cummin_helper_native.h>
#include <ATen/ops/_cummin_helper_ops.h>
#include <ATen/ops/divide_native.h>
#include <ATen/ops/divide_ops.h>
#include <ATen/ops/divide_native.h>
#include <ATen/ops/divide_ops.h>
#include <ATen/ops/divide_native.h>
#include <ATen/ops/divide_ops.h>
#include <ATen/ops/divide_native.h>
#include <ATen/ops/divide_ops.h>
#include <ATen/ops/true_divide_native.h>
#include <ATen/ops/true_divide_ops.h>
#include <ATen/ops/true_divide_native.h>
#include <ATen/ops/true_divide_ops.h>
#include <ATen/ops/embedding_backward_native.h>
#include <ATen/ops/embedding_backward_ops.h>
#include <ATen/ops/embedding_bag_native.h>
#include <ATen/ops/embedding_bag_ops.h>
#include <ATen/ops/embedding_bag_native.h>
#include <ATen/ops/embedding_bag_ops.h>
#include <ATen/ops/_embedding_bag_backward_native.h>
#include <ATen/ops/_embedding_bag_backward_ops.h>
#include <ATen/ops/_cufft_get_plan_cache_size_native.h>
#include <ATen/ops/_cufft_get_plan_cache_size_ops.h>
#include <ATen/ops/_cufft_get_plan_cache_max_size_native.h>
#include <ATen/ops/_cufft_get_plan_cache_max_size_ops.h>
#include <ATen/ops/_is_zerotensor_native.h>
#include <ATen/ops/_is_zerotensor_ops.h>
#include <ATen/ops/is_inference_native.h>
#include <ATen/ops/is_inference_ops.h>
#include <ATen/ops/kl_div_native.h>
#include <ATen/ops/kl_div_ops.h>
#include <ATen/ops/margin_ranking_loss_native.h>
#include <ATen/ops/margin_ranking_loss_ops.h>
#include <ATen/ops/matrix_exp_native.h>
#include <ATen/ops/matrix_exp_ops.h>
#include <ATen/ops/miopen_convolution_relu_native.h>
#include <ATen/ops/miopen_convolution_relu_ops.h>
#include <ATen/ops/_weight_int4pack_mm_native.h>
#include <ATen/ops/_weight_int4pack_mm_ops.h>
#include <ATen/ops/_sparse_mm_native.h>
#include <ATen/ops/_sparse_mm_ops.h>
#include <ATen/ops/_sparse_mm_native.h>
#include <ATen/ops/_sparse_mm_ops.h>
#include <ATen/ops/_nnpack_available_native.h>
#include <ATen/ops/_nnpack_available_ops.h>
#include <ATen/ops/_reshape_copy_native.h>
#include <ATen/ops/_reshape_copy_ops.h>
#include <ATen/ops/relu6_native.h>
#include <ATen/ops/relu6_ops.h>
#include <ATen/ops/relu6_native.h>
#include <ATen/ops/relu6_ops.h>
#include <ATen/ops/_prelu_kernel_backward_native.h>
#include <ATen/ops/_prelu_kernel_backward_ops.h>
#include <ATen/ops/selu_native.h>
#include <ATen/ops/selu_ops.h>
#include <ATen/ops/selu_native.h>
#include <ATen/ops/selu_ops.h>
#include <ATen/ops/sym_size_native.h>
#include <ATen/ops/sym_size_ops.h>
#include <ATen/ops/istft_native.h>
#include <ATen/ops/istft_ops.h>
#include <ATen/ops/sym_stride_native.h>
#include <ATen/ops/sym_stride_ops.h>
#include <ATen/ops/_nested_sum_backward_native.h>
#include <ATen/ops/_nested_sum_backward_ops.h>
#include <ATen/ops/std_mean_native.h>
#include <ATen/ops/std_mean_ops.h>
#include <ATen/ops/std_mean_native.h>
#include <ATen/ops/std_mean_ops.h>
#include <ATen/ops/std_mean_native.h>
#include <ATen/ops/std_mean_ops.h>
#include <ATen/ops/std_mean_native.h>
#include <ATen/ops/std_mean_ops.h>
#include <ATen/ops/flipud_native.h>
#include <ATen/ops/flipud_ops.h>
#include <ATen/ops/_nested_tensor_from_mask_left_aligned_native.h>
#include <ATen/ops/_nested_tensor_from_mask_left_aligned_ops.h>
#include <ATen/ops/var_mean_native.h>
#include <ATen/ops/var_mean_ops.h>
#include <ATen/ops/var_mean_native.h>
#include <ATen/ops/var_mean_ops.h>
#include <ATen/ops/var_mean_native.h>
#include <ATen/ops/var_mean_ops.h>
#include <ATen/ops/var_mean_native.h>
#include <ATen/ops/var_mean_ops.h>
#include <ATen/ops/norm_except_dim_native.h>
#include <ATen/ops/norm_except_dim_ops.h>
#include <ATen/ops/_sparse_compressed_tensor_unsafe_native.h>
#include <ATen/ops/_sparse_compressed_tensor_unsafe_ops.h>
#include <ATen/ops/sparse_coo_tensor_native.h>
#include <ATen/ops/sparse_coo_tensor_ops.h>
#include <ATen/ops/sparse_coo_tensor_native.h>
#include <ATen/ops/sparse_coo_tensor_ops.h>
#include <ATen/ops/_validate_sparse_bsc_tensor_args_native.h>
#include <ATen/ops/_validate_sparse_bsc_tensor_args_ops.h>
#include <ATen/ops/dense_dim_native.h>
#include <ATen/ops/dense_dim_ops.h>
#include <ATen/ops/_dimV_native.h>
#include <ATen/ops/_dimV_ops.h>
#include <ATen/ops/to_mkldnn_backward_native.h>
#include <ATen/ops/to_mkldnn_backward_ops.h>
#include <ATen/ops/q_zero_point_native.h>
#include <ATen/ops/q_zero_point_ops.h>
#include <ATen/ops/qscheme_native.h>
#include <ATen/ops/qscheme_ops.h>
#include <ATen/ops/fake_quantize_per_tensor_affine_native.h>
#include <ATen/ops/fake_quantize_per_tensor_affine_ops.h>
#include <ATen/ops/fake_quantize_per_tensor_affine_native.h>
#include <ATen/ops/fake_quantize_per_tensor_affine_ops.h>
#include <ATen/ops/fused_moving_avg_obs_fake_quant_native.h>
#include <ATen/ops/fused_moving_avg_obs_fake_quant_ops.h>
#include <ATen/ops/_choose_qparams_per_tensor_native.h>
#include <ATen/ops/_choose_qparams_per_tensor_ops.h>
#include <ATen/ops/meshgrid_native.h>
#include <ATen/ops/meshgrid_ops.h>
#include <ATen/ops/meshgrid_native.h>
#include <ATen/ops/meshgrid_ops.h>
#include <ATen/ops/cartesian_prod_native.h>
#include <ATen/ops/cartesian_prod_ops.h>
#include <ATen/ops/can_cast_native.h>
#include <ATen/ops/can_cast_ops.h>
#include <ATen/ops/promote_types_native.h>
#include <ATen/ops/promote_types_ops.h>
#include <ATen/ops/_local_scalar_dense_native.h>
#include <ATen/ops/_local_scalar_dense_ops.h>
#include <ATen/ops/rnn_tanh_native.h>
#include <ATen/ops/rnn_tanh_ops.h>
#include <ATen/ops/rnn_tanh_native.h>
#include <ATen/ops/rnn_tanh_ops.h>
#include <ATen/ops/gru_cell_native.h>
#include <ATen/ops/gru_cell_ops.h>
#include <ATen/ops/rnn_relu_cell_native.h>
#include <ATen/ops/rnn_relu_cell_ops.h>
#include <ATen/ops/_pad_packed_sequence_native.h>
#include <ATen/ops/_pad_packed_sequence_ops.h>
#include <ATen/ops/is_set_to_native.h>
#include <ATen/ops/is_set_to_ops.h>
#include <ATen/ops/gather_backward_native.h>
#include <ATen/ops/gather_backward_ops.h>
#include <ATen/ops/_gather_sparse_backward_native.h>
#include <ATen/ops/_gather_sparse_backward_ops.h>
#include <ATen/ops/linalg_vander_native.h>
#include <ATen/ops/linalg_vander_ops.h>
#include <ATen/ops/argsort_native.h>
#include <ATen/ops/argsort_ops.h>
#include <ATen/ops/argsort_native.h>
#include <ATen/ops/argsort_ops.h>
#include <ATen/ops/_pad_circular_native.h>
#include <ATen/ops/_pad_circular_ops.h>
#include <ATen/ops/upsample_trilinear3d_native.h>
#include <ATen/ops/upsample_trilinear3d_ops.h>
#include <ATen/ops/_upsample_bicubic2d_aa_native.h>
#include <ATen/ops/_upsample_bicubic2d_aa_ops.h>
#include <ATen/ops/record_stream_native.h>
#include <ATen/ops/record_stream_ops.h>
#include <ATen/ops/special_softmax_native.h>
#include <ATen/ops/special_softmax_ops.h>
#include <ATen/ops/fft_fftshift_native.h>
#include <ATen/ops/fft_fftshift_ops.h>
#include <ATen/ops/nested_to_padded_tensor_native.h>
#include <ATen/ops/nested_to_padded_tensor_ops.h>
#include <ATen/ops/_test_ambiguous_defaults_native.h>
#include <ATen/ops/_test_ambiguous_defaults_ops.h>
#include <ATen/ops/_test_ambiguous_defaults_native.h>
#include <ATen/ops/_test_ambiguous_defaults_ops.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_native.h>
#include <ATen/ops/_test_autograd_multiple_dispatch_ops.h>
#include <ATen/ops/pad_sequence_native.h>
#include <ATen/ops/pad_sequence_ops.h>
#include <ATen/ops/flatten_dense_tensors_native.h>
#include <ATen/ops/flatten_dense_tensors_ops.h>
#include <ATen/ops/_scaled_dot_product_flash_attention_for_cpu_native.h>
#include <ATen/ops/_scaled_dot_product_flash_attention_for_cpu_ops.h>
#include <ATen/ops/_scaled_dot_product_efficient_attention_backward_native.h>
#include <ATen/ops/_scaled_dot_product_efficient_attention_backward_ops.h>
#endif

namespace at {
namespace functionalization {

// This keyset is used by functionalization when it calls into meta kernels
// to accurately propagate stride metadata.
// Exclude any modes: the purpose of calling into meta kernels is only as an implementation
// detail to perform shape inference, and we don't want any modal keys to run.
// Specifically, we want to prevent functionalization and Python modes from running.
constexpr auto exclude_keys_for_meta_dispatch =
    c10::functorch_transforms_ks |
    c10::DispatchKeySet({
        c10::DispatchKey::FuncTorchDynamicLayerBackMode,
        c10::DispatchKey::FuncTorchDynamicLayerFrontMode,
        c10::DispatchKey::Python,
        c10::DispatchKey::PreDispatch,

    });

// Helper around at::has_internal_overlap.
// The ATen util is used in hot-path eager mode: it's always fast,
// but might return TOO_HARD sometimes.
// During functionalization, we're ok taking a bit longer
// to detect memory overlap.
inline bool has_internal_overlap_helper(const at::Tensor t) {
  auto has_overlap = at::has_internal_overlap(t);
  if (has_overlap == at::MemOverlap::Yes) return true;
  if (has_overlap == at::MemOverlap::No) return false;
  return false;
}


inline Tensor to_meta(const Tensor& t) {
    if (!t.defined()) return t;
    return at::native::empty_strided_meta_symint(t.sym_sizes(), t.sym_strides(),
/*dtype=*/c10::make_optional(t.scalar_type()), /*layout=*/c10::make_optional(t.layout()),
/*device=*/c10::make_optional(c10::Device(kMeta)), /*pin_memory=*/c10::nullopt);
}

inline c10::optional<Tensor> to_meta(const c10::optional<Tensor>& t) {
  if (t.has_value()) {
    return c10::make_optional<Tensor>(to_meta(*t));
  }
  return c10::nullopt;
}

inline std::vector<Tensor> to_meta(at::ITensorListRef t_list) {
  std::vector<Tensor> outputs;
  outputs.reserve(t_list.size());
  for (const auto& tensor : t_list) {
    outputs.push_back(to_meta(tensor));
  }
  return outputs;
}

inline c10::List<Tensor> to_meta(const c10::List<Tensor>& t_list) {
  c10::List<Tensor> outputs;
  outputs.reserve(t_list.size());
  for (const auto i : c10::irange(t_list.size())) {
    outputs.push_back(to_meta(t_list[i]));
  }
  return outputs;
}

inline c10::List<c10::optional<Tensor>> to_meta(const c10::List<c10::optional<Tensor>>& t_list) {
  c10::List<c10::optional<Tensor>> outputs;
  outputs.reserve(t_list.size());
  for (const auto i : c10::irange(t_list.size())) {
    outputs.push_back(to_meta(t_list[i]));
  }
  return outputs;
}



    ::std::tuple<at::Tensor &,at::Tensor &> _cudnn_ctc_loss_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto log_probs_meta = to_meta(log_probs);
        auto targets_meta = to_meta(targets);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_cudnn_ctc_loss_out::call(log_probs_meta, targets_meta, input_lengths, target_lengths, blank, deterministic, zero_infinity, out0_meta, out1_meta);
      }
      
      at::Tensor log_probs_;
      if (at::functionalization::impl::isFunctionalTensor(log_probs)) {
        at::functionalization::impl::sync(log_probs);
        log_probs_ = at::functionalization::impl::from_functional_tensor(log_probs);
      } else {
        log_probs_ = log_probs;
      }
      
      at::Tensor targets_;
      if (at::functionalization::impl::isFunctionalTensor(targets)) {
        at::functionalization::impl::sync(targets);
        targets_ = at::functionalization::impl::from_functional_tensor(targets);
      } else {
        targets_ = targets;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || log_probs.device().type() == c10::DeviceType::XLA || targets.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(log_probs) || at::functionalization::impl::isFunctionalTensor(targets))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_cudnn_ctc_loss_out::call(log_probs_, targets_, input_lengths, target_lengths, blank, deterministic, zero_infinity, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_cudnn_ctc_loss::call(log_probs_, targets_, input_lengths, target_lengths, blank, deterministic, zero_infinity);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> _cudnn_rnn_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const c10::optional<at::Tensor> & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, c10::SymIntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3, at::Tensor & out4) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto weight_buf_meta = to_meta(weight_buf);
        auto hx_meta = to_meta(hx);
        auto cx_meta = to_meta(cx);
        auto dropout_state_meta = to_meta(dropout_state);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        auto out3_meta = to_meta(out3);
        auto out4_meta = to_meta(out4);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_cudnn_rnn_out::call(input_meta, weight_meta, weight_stride0, weight_buf_meta, hx_meta, cx_meta, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state_meta, out0_meta, out1_meta, out2_meta, out3_meta, out4_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      ::std::vector<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight.vec();
      }
      
      c10::optional<at::Tensor> weight_buf_;
      if (at::functionalization::impl::isFunctionalTensor(weight_buf)) {
        at::functionalization::impl::sync(weight_buf);
        weight_buf_ = at::functionalization::impl::from_functional_tensor(weight_buf);
      } else {
        weight_buf_ = weight_buf;
      }
      
      at::Tensor hx_;
      if (at::functionalization::impl::isFunctionalTensor(hx)) {
        at::functionalization::impl::sync(hx);
        hx_ = at::functionalization::impl::from_functional_tensor(hx);
      } else {
        hx_ = hx;
      }
      
      c10::optional<at::Tensor> cx_;
      if (at::functionalization::impl::isFunctionalTensor(cx)) {
        at::functionalization::impl::sync(cx);
        cx_ = at::functionalization::impl::from_functional_tensor(cx);
      } else {
        cx_ = cx;
      }
      
      c10::optional<at::Tensor> dropout_state_;
      if (at::functionalization::impl::isFunctionalTensor(dropout_state)) {
        at::functionalization::impl::sync(dropout_state);
        dropout_state_ = at::functionalization::impl::from_functional_tensor(dropout_state);
      } else {
        dropout_state_ = dropout_state;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      
      at::Tensor out3_;
      if (at::functionalization::impl::isFunctionalTensor(out3)) {
        at::functionalization::impl::sync(out3);
        out3_ = at::functionalization::impl::from_functional_tensor(out3);
      } else {
        out3_ = out3;
      }
      
      at::Tensor out4_;
      if (at::functionalization::impl::isFunctionalTensor(out4)) {
        at::functionalization::impl::sync(out4);
        out4_ = at::functionalization::impl::from_functional_tensor(out4);
      } else {
        out4_ = out4;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2) && at::functionalization::impl::isFunctionalTensor(out3) && at::functionalization::impl::isFunctionalTensor(out4))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || hx.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(weight_buf) || at::functionalization::impl::isFunctionalTensor(hx) || at::functionalization::impl::isFunctionalTensor(cx) || at::functionalization::impl::isFunctionalTensor(dropout_state))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_cudnn_rnn_out::call(input_, weight_, weight_stride0, weight_buf_, hx_, cx_, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state_, out0_, out1_, out2_, out3_, out4_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3, out4);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_cudnn_rnn::call(input_, weight_, weight_stride0, weight_buf_, hx_, cx_, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state_);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
  at::functionalization::impl::propagate_xla_data(out3, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(out3, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(out3);
  at::functionalization::impl::sync(out3);
  at::functionalization::impl::propagate_xla_data(out4, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(out4, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(out4);
  at::functionalization::impl::sync(out4);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3, out4);
      }
    }

    void _cudnn_rnn_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, c10::SymIntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, ::std::array<bool,4> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::TensorList out3) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto weight_buf_meta = to_meta(weight_buf);
        auto hx_meta = to_meta(hx);
        auto cx_meta = to_meta(cx);
        auto output_meta = to_meta(output);
        auto grad_output_meta = to_meta(grad_output);
        auto grad_hy_meta = to_meta(grad_hy);
        auto grad_cy_meta = to_meta(grad_cy);
        auto dropout_state_meta = to_meta(dropout_state);
        auto reserve_meta = to_meta(reserve);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        auto out3_meta = to_meta(out3);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_cudnn_rnn_backward_out::call(input_meta, weight_meta, weight_stride0, weight_buf_meta, hx_meta, cx_meta, output_meta, grad_output_meta, grad_hy_meta, grad_cy_meta, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state_meta, reserve_meta, output_mask, out0_meta, out1_meta, out2_meta, out3_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      ::std::vector<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight.vec();
      }
      
      at::Tensor weight_buf_;
      if (at::functionalization::impl::isFunctionalTensor(weight_buf)) {
        at::functionalization::impl::sync(weight_buf);
        weight_buf_ = at::functionalization::impl::from_functional_tensor(weight_buf);
      } else {
        weight_buf_ = weight_buf;
      }
      
      at::Tensor hx_;
      if (at::functionalization::impl::isFunctionalTensor(hx)) {
        at::functionalization::impl::sync(hx);
        hx_ = at::functionalization::impl::from_functional_tensor(hx);
      } else {
        hx_ = hx;
      }
      
      c10::optional<at::Tensor> cx_;
      if (at::functionalization::impl::isFunctionalTensor(cx)) {
        at::functionalization::impl::sync(cx);
        cx_ = at::functionalization::impl::from_functional_tensor(cx);
      } else {
        cx_ = cx;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      c10::optional<at::Tensor> grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      c10::optional<at::Tensor> grad_hy_;
      if (at::functionalization::impl::isFunctionalTensor(grad_hy)) {
        at::functionalization::impl::sync(grad_hy);
        grad_hy_ = at::functionalization::impl::from_functional_tensor(grad_hy);
      } else {
        grad_hy_ = grad_hy;
      }
      
      c10::optional<at::Tensor> grad_cy_;
      if (at::functionalization::impl::isFunctionalTensor(grad_cy)) {
        at::functionalization::impl::sync(grad_cy);
        grad_cy_ = at::functionalization::impl::from_functional_tensor(grad_cy);
      } else {
        grad_cy_ = grad_cy;
      }
      
      c10::optional<at::Tensor> dropout_state_;
      if (at::functionalization::impl::isFunctionalTensor(dropout_state)) {
        at::functionalization::impl::sync(dropout_state);
        dropout_state_ = at::functionalization::impl::from_functional_tensor(dropout_state);
      } else {
        dropout_state_ = dropout_state;
      }
      
      at::Tensor reserve_;
      if (at::functionalization::impl::isFunctionalTensor(reserve)) {
        at::functionalization::impl::sync(reserve);
        reserve_ = at::functionalization::impl::from_functional_tensor(reserve);
      } else {
        reserve_ = reserve;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      
      ::std::vector<at::Tensor> out3_;
      if (at::functionalization::impl::isFunctionalTensor(out3)) {
        at::functionalization::impl::sync(out3);
        out3_ = at::functionalization::impl::from_functional_tensor(out3);
      } else {
        out3_ = out3.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2) && at::functionalization::impl::isFunctionalTensor(out3))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || weight_buf.device().type() == c10::DeviceType::XLA || hx.device().type() == c10::DeviceType::XLA || output.device().type() == c10::DeviceType::XLA || reserve.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(weight_buf) || at::functionalization::impl::isFunctionalTensor(hx) || at::functionalization::impl::isFunctionalTensor(cx) || at::functionalization::impl::isFunctionalTensor(output) || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(grad_hy) || at::functionalization::impl::isFunctionalTensor(grad_cy) || at::functionalization::impl::isFunctionalTensor(dropout_state) || at::functionalization::impl::isFunctionalTensor(reserve))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_cudnn_rnn_backward_out::call(input_, weight_, weight_stride0, weight_buf_, hx_, cx_, output_, grad_output_, grad_hy_, grad_cy_, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state_, reserve_, output_mask, out0_, out1_, out2_, out3_);
         ;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_cudnn_rnn_backward::call(input_, weight_, weight_stride0, weight_buf_, hx_, cx_, output_, grad_output_, grad_hy_, grad_cy_, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state_, reserve_, output_mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
  at::functionalization::impl::propagate_xla_data(out3, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(out3, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(out3);
  at::functionalization::impl::sync(out3);
    
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _fused_dropout_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double p, c10::optional<at::Generator> generator, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_dropout_out::call(self_meta, p, generator, out0_meta, out1_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_fused_dropout_out::call(self_, p, generator, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_dropout::call(self_, p, generator);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & conj_physical_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::conj_physical_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::conj_physical_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::conj_physical::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & conj_physical_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::conj_physical_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::conj_physical_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::conj_physical::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & add_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::add_out::call(self_meta, other_meta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::add_out::call(self_, other_, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::add_Tensor::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & add__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::add__Tensor::call(self_meta, other_meta, alpha);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::add__Tensor::call(self_, other_, alpha);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::add_Tensor::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & add_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::add_Scalar_out::call(self_meta, other, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::add_Scalar_out::call(self_, other, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::add_Scalar::call(self_, other, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & add__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::add__Scalar::call(self_meta, other, alpha);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::add__Scalar::call(self_, other, alpha);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::add_Scalar::call(self_, other, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & addmv_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mat_meta = to_meta(mat);
        auto vec_meta = to_meta(vec);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::addmv_out::call(self_meta, mat_meta, vec_meta, beta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mat_;
      if (at::functionalization::impl::isFunctionalTensor(mat)) {
        at::functionalization::impl::sync(mat);
        mat_ = at::functionalization::impl::from_functional_tensor(mat);
      } else {
        mat_ = mat;
      }
      
      at::Tensor vec_;
      if (at::functionalization::impl::isFunctionalTensor(vec)) {
        at::functionalization::impl::sync(vec);
        vec_ = at::functionalization::impl::from_functional_tensor(vec);
      } else {
        vec_ = vec;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mat.device().type() == c10::DeviceType::XLA || vec.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mat) || at::functionalization::impl::isFunctionalTensor(vec))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::addmv_out::call(self_, mat_, vec_, beta, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::addmv::call(self_, mat_, vec_, beta, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & addmv_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mat_meta = to_meta(mat);
        auto vec_meta = to_meta(vec);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::addmv_::call(self_meta, mat_meta, vec_meta, beta, alpha);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mat_;
      if (at::functionalization::impl::isFunctionalTensor(mat)) {
        at::functionalization::impl::sync(mat);
        mat_ = at::functionalization::impl::from_functional_tensor(mat);
      } else {
        mat_ = mat;
      }
      
      at::Tensor vec_;
      if (at::functionalization::impl::isFunctionalTensor(vec)) {
        at::functionalization::impl::sync(vec);
        vec_ = at::functionalization::impl::from_functional_tensor(vec);
      } else {
        vec_ = vec;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || mat.device().type() == c10::DeviceType::XLA || vec.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(mat) || at::functionalization::impl::isFunctionalTensor(vec))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::addmv_::call(self_, mat_, vec_, beta, alpha);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::addmv::call(self_, mat_, vec_, beta, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & addr_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto vec1_meta = to_meta(vec1);
        auto vec2_meta = to_meta(vec2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::addr_out::call(self_meta, vec1_meta, vec2_meta, beta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor vec1_;
      if (at::functionalization::impl::isFunctionalTensor(vec1)) {
        at::functionalization::impl::sync(vec1);
        vec1_ = at::functionalization::impl::from_functional_tensor(vec1);
      } else {
        vec1_ = vec1;
      }
      
      at::Tensor vec2_;
      if (at::functionalization::impl::isFunctionalTensor(vec2)) {
        at::functionalization::impl::sync(vec2);
        vec2_ = at::functionalization::impl::from_functional_tensor(vec2);
      } else {
        vec2_ = vec2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || vec1.device().type() == c10::DeviceType::XLA || vec2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(vec1) || at::functionalization::impl::isFunctionalTensor(vec2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::addr_out::call(self_, vec1_, vec2_, beta, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::addr::call(self_, vec1_, vec2_, beta, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & addr_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto vec1_meta = to_meta(vec1);
        auto vec2_meta = to_meta(vec2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::addr_::call(self_meta, vec1_meta, vec2_meta, beta, alpha);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor vec1_;
      if (at::functionalization::impl::isFunctionalTensor(vec1)) {
        at::functionalization::impl::sync(vec1);
        vec1_ = at::functionalization::impl::from_functional_tensor(vec1);
      } else {
        vec1_ = vec1;
      }
      
      at::Tensor vec2_;
      if (at::functionalization::impl::isFunctionalTensor(vec2)) {
        at::functionalization::impl::sync(vec2);
        vec2_ = at::functionalization::impl::from_functional_tensor(vec2);
      } else {
        vec2_ = vec2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || vec1.device().type() == c10::DeviceType::XLA || vec2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(vec1) || at::functionalization::impl::isFunctionalTensor(vec2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::addr_::call(self_, vec1_, vec2_, beta, alpha);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::addr::call(self_, vec1_, vec2_, beta, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & all_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::all_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::all_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::all_dim::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & all_out_dims_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::all_dims_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::all_dims_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::all_dims::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & all_out_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::all_dimname_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::all_dimname_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::all_dimname::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & argmax_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::argmax_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::argmax_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::argmax::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & atan_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::atan_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::atan_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::atan::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & atan_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::atan_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::atan_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::atan::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bartlett_window_out_out(c10::DispatchKeySet dispatchKeySet, int64_t window_length, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bartlett_window_out::call(window_length, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bartlett_window_out::call(window_length, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bartlett_window::call(window_length, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bartlett_window_out_periodic_out(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bartlett_window_periodic_out::call(window_length, periodic, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bartlett_window_periodic_out::call(window_length, periodic, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bartlett_window_periodic::call(window_length, periodic, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & binary_cross_entropy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto weight_meta = to_meta(weight);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::binary_cross_entropy_out::call(self_meta, target_meta, weight_meta, reduction, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::binary_cross_entropy_out::call(self_, target_, weight_, reduction, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::binary_cross_entropy::call(self_, target_, weight_, reduction);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_not_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_not_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_not_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_not::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_not_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_not_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_not_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_not::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & logical_xor_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logical_xor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logical_xor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logical_xor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & logical_xor_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logical_xor_::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logical_xor_::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logical_xor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & blackman_window_out_out(c10::DispatchKeySet dispatchKeySet, int64_t window_length, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::blackman_window_out::call(window_length, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::blackman_window_out::call(window_length, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::blackman_window::call(window_length, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & blackman_window_out_periodic_out(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::blackman_window_periodic_out::call(window_length, periodic, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::blackman_window_periodic_out::call(window_length, periodic, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::blackman_window_periodic::call(window_length, periodic, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & clamp_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clamp_out::call(self_meta, min, max, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clamp_out::call(self_, min, max, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clamp::call(self_, min, max);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & clamp_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clamp_::call(self_meta, min, max);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clamp_::call(self_, min, max);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clamp::call(self_, min, max);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & clamp_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto min_meta = to_meta(min);
        auto max_meta = to_meta(max);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clamp_Tensor_out::call(self_meta, min_meta, max_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::optional<at::Tensor> min_;
      if (at::functionalization::impl::isFunctionalTensor(min)) {
        at::functionalization::impl::sync(min);
        min_ = at::functionalization::impl::from_functional_tensor(min);
      } else {
        min_ = min;
      }
      
      c10::optional<at::Tensor> max_;
      if (at::functionalization::impl::isFunctionalTensor(max)) {
        at::functionalization::impl::sync(max);
        max_ = at::functionalization::impl::from_functional_tensor(max);
      } else {
        max_ = max;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(min) || at::functionalization::impl::isFunctionalTensor(max))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clamp_Tensor_out::call(self_, min_, max_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clamp_Tensor::call(self_, min_, max_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & clamp__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto min_meta = to_meta(min);
        auto max_meta = to_meta(max);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clamp__Tensor::call(self_meta, min_meta, max_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::optional<at::Tensor> min_;
      if (at::functionalization::impl::isFunctionalTensor(min)) {
        at::functionalization::impl::sync(min);
        min_ = at::functionalization::impl::from_functional_tensor(min);
      } else {
        min_ = min;
      }
      
      c10::optional<at::Tensor> max_;
      if (at::functionalization::impl::isFunctionalTensor(max)) {
        at::functionalization::impl::sync(max);
        max_ = at::functionalization::impl::from_functional_tensor(max);
      } else {
        max_ = max;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(min) || at::functionalization::impl::isFunctionalTensor(max))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clamp__Tensor::call(self_, min_, max_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clamp_Tensor::call(self_, min_, max_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & _convolution_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_convolution_out::call(input_meta, weight_meta, bias_meta, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32, out_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_convolution_out::call(input_, weight_, bias_, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_convolution::call(input_, weight_, bias_, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & src, bool non_blocking, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto src_meta = to_meta(src);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::copy_out::call(self_meta, src_meta, non_blocking, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::copy_out::call(self_, src_, non_blocking, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::copy::call(self_, src_, non_blocking);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & copy_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & src, bool non_blocking) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto src_meta = to_meta(src);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::copy_::call(self_meta, src_meta, non_blocking);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::copy_::call(self_, src_, non_blocking);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::copy::call(self_, src_, non_blocking);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & cudnn_convolution_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic, bool allow_tf32, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cudnn_convolution_out::call(self_meta, weight_meta, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cudnn_convolution_out::call(self_, weight_, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cudnn_convolution::call(self_, weight_, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & divide_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::divide_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::divide_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::divide_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & divide__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::divide__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::divide__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::divide_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & divide_out_out_mode(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::divide_out_mode::call(self_meta, other_meta, rounding_mode, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::divide_out_mode::call(self_, other_, rounding_mode, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::divide_Tensor_mode::call(self_, other_, rounding_mode);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & divide__Tensor_mode(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::divide__Tensor_mode::call(self_meta, other_meta, rounding_mode);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::divide__Tensor_mode::call(self_, other_, rounding_mode);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::divide_Tensor_mode::call(self_, other_, rounding_mode);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & true_divide_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::true_divide_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::true_divide_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::true_divide_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & true_divide__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::true_divide__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::true_divide__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::true_divide_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & dot_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor_meta = to_meta(tensor);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::dot_out::call(self_meta, tensor_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor tensor_;
      if (at::functionalization::impl::isFunctionalTensor(tensor)) {
        at::functionalization::impl::sync(tensor);
        tensor_ = at::functionalization::impl::from_functional_tensor(tensor);
      } else {
        tensor_ = tensor;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || tensor.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(tensor))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::dot_out::call(self_, tensor_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::dot::call(self_, tensor_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & vdot_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::vdot_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::vdot_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::vdot::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & row_stack_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::row_stack_out::call(tensors_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::row_stack_out::call(tensors_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::row_stack::call(tensors_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & new_empty_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::new_empty_out::call(self_meta, size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::new_empty_out::call(self_, size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::new_empty::call(self_, size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & new_empty_strided_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::new_empty_strided_out::call(self_meta, size, stride, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::new_empty_strided_out::call(self_, size, stride, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::new_empty_strided::call(self_, size, stride, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _empty_affine_quantized_out_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_empty_affine_quantized_out::call(size, scale, zero_point, memory_format, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_empty_affine_quantized_out::call(size, scale, zero_point, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_empty_affine_quantized::call(size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, scale, zero_point, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & _resize_output_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, at::Device device, const at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_resize_output_out::call(self_meta, size, device, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_resize_output_out::call(self_, size, device, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_resize_output::call(self_, size, device);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & _resize_output_(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, at::Device device) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_resize_output_::call(self_meta, size, device);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_resize_output_::call(self_, size, device);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_resize_output::call(self_, size, device);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & frac_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::frac_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::frac_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::frac::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & frac_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::frac_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::frac_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::frac::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & full_like_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & fill_value, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::full_like_out::call(self_meta, fill_value, memory_format, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::full_like_out::call(self_, fill_value, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::full_like::call(self_, fill_value, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & lcm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lcm_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lcm_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lcm::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & lcm_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lcm_::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lcm_::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lcm::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & hann_window_out_out(c10::DispatchKeySet dispatchKeySet, int64_t window_length, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hann_window_out::call(window_length, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hann_window_out::call(window_length, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hann_window::call(window_length, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hann_window_out_periodic_out(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hann_window_periodic_out::call(window_length, periodic, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hann_window_periodic_out::call(window_length, periodic, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hann_window_periodic::call(window_length, periodic, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & kaiser_window_out_out(c10::DispatchKeySet dispatchKeySet, int64_t window_length, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::kaiser_window_out::call(window_length, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::kaiser_window_out::call(window_length, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::kaiser_window::call(window_length, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & kaiser_window_out_periodic_out(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::kaiser_window_periodic_out::call(window_length, periodic, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::kaiser_window_periodic_out::call(window_length, periodic, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::kaiser_window_periodic::call(window_length, periodic, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & kaiser_window_out_beta_out(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, double beta, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::kaiser_window_beta_out::call(window_length, periodic, beta, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::kaiser_window_beta_out::call(window_length, periodic, beta, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::kaiser_window_beta::call(window_length, periodic, beta, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _index_put_impl_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate, bool unsafe, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto values_meta = to_meta(values);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_index_put_impl_out::call(self_meta, indices_meta, values_meta, accumulate, unsafe, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::List<c10::optional<at::Tensor>> indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || values.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(values))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_index_put_impl_out::call(self_, indices_, values_, accumulate, unsafe, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_index_put_impl::call(self_, indices_, values_, accumulate, unsafe);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _index_put_impl_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate, bool unsafe) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto values_meta = to_meta(values);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_index_put_impl_::call(self_meta, indices_meta, values_meta, accumulate, unsafe);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::List<c10::optional<at::Tensor>> indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || values.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(values))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_index_put_impl_::call(self_, indices_, values_, accumulate, unsafe);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_index_put_impl::call(self_, indices_, values_, accumulate, unsafe);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> kthvalue_out_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::kthvalue_values::call(self_meta, k, dim, keepdim, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::kthvalue_values::call(self_, k, dim, keepdim, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::kthvalue::call(self_, k, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> kthvalue_out_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::kthvalue_dimname_out::call(self_meta, k, dim, keepdim, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::kthvalue_dimname_out::call(self_, k, dim, keepdim, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::kthvalue_dimname::call(self_, k, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_layer_norm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::SymIntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::native_layer_norm_out::call(input_meta, normalized_shape, weight_meta, bias_meta, eps, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::native_layer_norm_out::call(input_, normalized_shape, weight_, bias_, eps, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::native_layer_norm::call(input_, normalized_shape, weight_, bias_, eps);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_layer_norm_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, c10::SymIntArrayRef normalized_shape, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_out_meta = to_meta(grad_out);
        auto input_meta = to_meta(input);
        auto mean_meta = to_meta(mean);
        auto rstd_meta = to_meta(rstd);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::native_layer_norm_backward_out::call(grad_out_meta, input_meta, normalized_shape, mean_meta, rstd_meta, weight_meta, bias_meta, output_mask, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor grad_out_;
      if (at::functionalization::impl::isFunctionalTensor(grad_out)) {
        at::functionalization::impl::sync(grad_out);
        grad_out_ = at::functionalization::impl::from_functional_tensor(grad_out);
      } else {
        grad_out_ = grad_out;
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor mean_;
      if (at::functionalization::impl::isFunctionalTensor(mean)) {
        at::functionalization::impl::sync(mean);
        mean_ = at::functionalization::impl::from_functional_tensor(mean);
      } else {
        mean_ = mean;
      }
      
      at::Tensor rstd_;
      if (at::functionalization::impl::isFunctionalTensor(rstd)) {
        at::functionalization::impl::sync(rstd);
        rstd_ = at::functionalization::impl::from_functional_tensor(rstd);
      } else {
        rstd_ = rstd;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_out.device().type() == c10::DeviceType::XLA || input.device().type() == c10::DeviceType::XLA || mean.device().type() == c10::DeviceType::XLA || rstd.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_out) || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(mean) || at::functionalization::impl::isFunctionalTensor(rstd) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::native_layer_norm_backward_out::call(grad_out_, input_, normalized_shape, mean_, rstd_, weight_, bias_, output_mask, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::native_layer_norm_backward::call(grad_out_, input_, normalized_shape, mean_, rstd_, weight_, bias_, output_mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & mkldnn_linear_backward_input_out_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef input_size, const at::Tensor & grad_output, const at::Tensor & weight, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto weight_meta = to_meta(weight);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_linear_backward_input_out::call(input_size, grad_output_meta, weight_meta, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mkldnn_linear_backward_input_out::call(input_size, grad_output_, weight_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_linear_backward_input::call(input_size, grad_output_, weight_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> mkldnn_linear_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grad_output_meta = to_meta(grad_output);
        auto weight_meta = to_meta(weight);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_linear_backward_out::call(self_meta, grad_output_meta, weight_meta, output_mask, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || grad_output.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::mkldnn_linear_backward_out::call(self_, grad_output_, weight_, output_mask, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_linear_backward::call(self_, grad_output_, weight_, output_mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & log10_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::log10_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::log10_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::log10::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & log10_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::log10_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::log10_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::log10::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & log1p_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::log1p_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::log1p_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::log1p::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & log1p_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::log1p_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::log1p_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::log1p::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & logsumexp_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logsumexp_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logsumexp_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logsumexp::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & logsumexp_out_names_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logsumexp_names_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logsumexp_names_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logsumexp_names::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & matmul_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::matmul_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::matmul_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::matmul::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & max_pool2d_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::max_pool2d_backward_out::call(grad_output_meta, self_meta, kernel_size, stride, padding, dilation, ceil_mode, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::max_pool2d_backward_out::call(grad_output_, self_, kernel_size, stride, padding, dilation, ceil_mode, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::max_pool2d_backward::call(grad_output_, self_, kernel_size, stride, padding, dilation, ceil_mode);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mkldnn_max_pool2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_max_pool2d_out::call(self_meta, kernel_size, stride, padding, dilation, ceil_mode, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mkldnn_max_pool2d_out::call(self_, kernel_size, stride, padding, dilation, ceil_mode, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_max_pool2d::call(self_, kernel_size, stride, padding, dilation, ceil_mode);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & quantized_max_pool1d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::quantized_max_pool1d_out::call(self_meta, kernel_size, stride, padding, dilation, ceil_mode, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::quantized_max_pool1d_out::call(self_, kernel_size, stride, padding, dilation, ceil_mode, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::quantized_max_pool1d::call(self_, kernel_size, stride, padding, dilation, ceil_mode);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & nanmean_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nanmean_out::call(self_meta, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nanmean_out::call(self_, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nanmean::call(self_, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _mps_convolution_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_mps_convolution_out::call(self_meta, weight_meta, bias_meta, padding, stride, dilation, groups, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_mps_convolution_out::call(self_, weight_, bias_, padding, stride, dilation, groups, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_mps_convolution::call(self_, weight_, bias_, padding, stride, dilation, groups);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mkldnn_convolution_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_convolution_out::call(self_meta, weight_meta, bias_meta, padding, stride, dilation, groups, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mkldnn_convolution_out::call(self_, weight_, bias_, padding, stride, dilation, groups, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_convolution::call(self_, weight_, bias_, padding, stride, dilation, groups);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> mkldnn_rnn_layer_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight0, const at::Tensor & weight1, const at::Tensor & weight2, const at::Tensor & weight3, const at::Tensor & hx_, const at::Tensor & cx_, bool reverse, at::IntArrayRef batch_sizes, int64_t mode, int64_t hidden_size, int64_t num_layers, bool has_biases, bool bidirectional, bool batch_first, bool train, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight0_meta = to_meta(weight0);
        auto weight1_meta = to_meta(weight1);
        auto weight2_meta = to_meta(weight2);
        auto weight3_meta = to_meta(weight3);
        auto hx__meta = to_meta(hx_);
        auto cx__meta = to_meta(cx_);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        auto out3_meta = to_meta(out3);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_rnn_layer_out::call(input_meta, weight0_meta, weight1_meta, weight2_meta, weight3_meta, hx__meta, cx__meta, reverse, batch_sizes, mode, hidden_size, num_layers, has_biases, bidirectional, batch_first, train, out0_meta, out1_meta, out2_meta, out3_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor weight0_;
      if (at::functionalization::impl::isFunctionalTensor(weight0)) {
        at::functionalization::impl::sync(weight0);
        weight0_ = at::functionalization::impl::from_functional_tensor(weight0);
      } else {
        weight0_ = weight0;
      }
      
      at::Tensor weight1_;
      if (at::functionalization::impl::isFunctionalTensor(weight1)) {
        at::functionalization::impl::sync(weight1);
        weight1_ = at::functionalization::impl::from_functional_tensor(weight1);
      } else {
        weight1_ = weight1;
      }
      
      at::Tensor weight2_;
      if (at::functionalization::impl::isFunctionalTensor(weight2)) {
        at::functionalization::impl::sync(weight2);
        weight2_ = at::functionalization::impl::from_functional_tensor(weight2);
      } else {
        weight2_ = weight2;
      }
      
      at::Tensor weight3_;
      if (at::functionalization::impl::isFunctionalTensor(weight3)) {
        at::functionalization::impl::sync(weight3);
        weight3_ = at::functionalization::impl::from_functional_tensor(weight3);
      } else {
        weight3_ = weight3;
      }
      
      at::Tensor hx__;
      if (at::functionalization::impl::isFunctionalTensor(hx_)) {
        at::functionalization::impl::sync(hx_);
        hx__ = at::functionalization::impl::from_functional_tensor(hx_);
      } else {
        hx__ = hx_;
      }
      
      at::Tensor cx__;
      if (at::functionalization::impl::isFunctionalTensor(cx_)) {
        at::functionalization::impl::sync(cx_);
        cx__ = at::functionalization::impl::from_functional_tensor(cx_);
      } else {
        cx__ = cx_;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      
      at::Tensor out3_;
      if (at::functionalization::impl::isFunctionalTensor(out3)) {
        at::functionalization::impl::sync(out3);
        out3_ = at::functionalization::impl::from_functional_tensor(out3);
      } else {
        out3_ = out3;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2) && at::functionalization::impl::isFunctionalTensor(out3))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || weight0.device().type() == c10::DeviceType::XLA || weight1.device().type() == c10::DeviceType::XLA || weight2.device().type() == c10::DeviceType::XLA || weight3.device().type() == c10::DeviceType::XLA || hx_.device().type() == c10::DeviceType::XLA || cx_.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight0) || at::functionalization::impl::isFunctionalTensor(weight1) || at::functionalization::impl::isFunctionalTensor(weight2) || at::functionalization::impl::isFunctionalTensor(weight3) || at::functionalization::impl::isFunctionalTensor(hx_) || at::functionalization::impl::isFunctionalTensor(cx_))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::mkldnn_rnn_layer_out::call(input_, weight0_, weight1_, weight2_, weight3_, hx__, cx__, reverse, batch_sizes, mode, hidden_size, num_layers, has_biases, bidirectional, batch_first, train, out0_, out1_, out2_, out3_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_rnn_layer::call(input_, weight0_, weight1_, weight2_, weight3_, hx__, cx__, reverse, batch_sizes, mode, hidden_size, num_layers, has_biases, bidirectional, batch_first, train);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
  at::functionalization::impl::propagate_xla_data(out3, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(out3, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(out3);
  at::functionalization::impl::sync(out3);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> miopen_batch_norm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double exponential_average_factor, double epsilon, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto running_mean_meta = to_meta(running_mean);
        auto running_var_meta = to_meta(running_var);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::miopen_batch_norm_out::call(input_meta, weight_meta, bias_meta, running_mean_meta, running_var_meta, training, exponential_average_factor, epsilon, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      c10::optional<at::Tensor> running_mean_;
      if (at::functionalization::impl::isFunctionalTensor(running_mean)) {
        at::functionalization::impl::sync(running_mean);
        running_mean_ = at::functionalization::impl::from_functional_tensor(running_mean);
      } else {
        running_mean_ = running_mean;
      }
      
      c10::optional<at::Tensor> running_var_;
      if (at::functionalization::impl::isFunctionalTensor(running_var)) {
        at::functionalization::impl::sync(running_var);
        running_var_ = at::functionalization::impl::from_functional_tensor(running_var);
      } else {
        running_var_ = running_var;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias) || at::functionalization::impl::isFunctionalTensor(running_mean) || at::functionalization::impl::isFunctionalTensor(running_var))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::miopen_batch_norm_out::call(input_, weight_, bias_, running_mean_, running_var_, training, exponential_average_factor, epsilon, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::miopen_batch_norm::call(input_, weight_, bias_, running_mean_, running_var_, training, exponential_average_factor, epsilon);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> miopen_batch_norm_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var, double epsilon, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto grad_output_meta = to_meta(grad_output);
        auto weight_meta = to_meta(weight);
        auto running_mean_meta = to_meta(running_mean);
        auto running_var_meta = to_meta(running_var);
        auto save_mean_meta = to_meta(save_mean);
        auto save_var_meta = to_meta(save_var);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::miopen_batch_norm_backward_out::call(input_meta, grad_output_meta, weight_meta, running_mean_meta, running_var_meta, save_mean_meta, save_var_meta, epsilon, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> running_mean_;
      if (at::functionalization::impl::isFunctionalTensor(running_mean)) {
        at::functionalization::impl::sync(running_mean);
        running_mean_ = at::functionalization::impl::from_functional_tensor(running_mean);
      } else {
        running_mean_ = running_mean;
      }
      
      c10::optional<at::Tensor> running_var_;
      if (at::functionalization::impl::isFunctionalTensor(running_var)) {
        at::functionalization::impl::sync(running_var);
        running_var_ = at::functionalization::impl::from_functional_tensor(running_var);
      } else {
        running_var_ = running_var;
      }
      
      c10::optional<at::Tensor> save_mean_;
      if (at::functionalization::impl::isFunctionalTensor(save_mean)) {
        at::functionalization::impl::sync(save_mean);
        save_mean_ = at::functionalization::impl::from_functional_tensor(save_mean);
      } else {
        save_mean_ = save_mean;
      }
      
      c10::optional<at::Tensor> save_var_;
      if (at::functionalization::impl::isFunctionalTensor(save_var)) {
        at::functionalization::impl::sync(save_var);
        save_var_ = at::functionalization::impl::from_functional_tensor(save_var);
      } else {
        save_var_ = save_var;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || grad_output.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(running_mean) || at::functionalization::impl::isFunctionalTensor(running_var) || at::functionalization::impl::isFunctionalTensor(save_mean) || at::functionalization::impl::isFunctionalTensor(save_var))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::miopen_batch_norm_backward_out::call(input_, grad_output_, weight_, running_mean_, running_var_, save_mean_, save_var_, epsilon, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::miopen_batch_norm_backward::call(input_, grad_output_, weight_, running_mean_, running_var_, save_mean_, save_var_, epsilon);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & miopen_convolution_transpose_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::miopen_convolution_transpose_out::call(self_meta, weight_meta, bias_meta, padding, output_padding, stride, dilation, groups, benchmark, deterministic, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::miopen_convolution_transpose_out::call(self_, weight_, bias_, padding, output_padding, stride, dilation, groups, benchmark, deterministic, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::miopen_convolution_transpose::call(self_, weight_, bias_, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mat2_meta = to_meta(mat2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mm_out::call(self_meta, mat2_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mat2_;
      if (at::functionalization::impl::isFunctionalTensor(mat2)) {
        at::functionalization::impl::sync(mat2);
        mat2_ = at::functionalization::impl::from_functional_tensor(mat2);
      } else {
        mat2_ = mat2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mat2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mat2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mm_out::call(self_, mat2_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mm::call(self_, mat2_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _sparse_sparse_matmul_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sparse_sparse_matmul_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sparse_sparse_matmul_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sparse_sparse_matmul::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mul_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mul_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mul_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mul_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mul__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mul__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mul__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mul_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & mul_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mul_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mul_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mul_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mul__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mul__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mul__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mul_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & mvlgamma_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t p, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mvlgamma_out::call(self_meta, p, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mvlgamma_out::call(self_, p, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mvlgamma::call(self_, p);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mvlgamma_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t p) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mvlgamma_::call(self_meta, p);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mvlgamma_::call(self_, p);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mvlgamma::call(self_, p);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> batch_norm_backward_reduce_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & weight, bool input_g, bool weight_g, bool bias_g, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_out_meta = to_meta(grad_out);
        auto input_meta = to_meta(input);
        auto mean_meta = to_meta(mean);
        auto invstd_meta = to_meta(invstd);
        auto weight_meta = to_meta(weight);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        auto out3_meta = to_meta(out3);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::batch_norm_backward_reduce_out::call(grad_out_meta, input_meta, mean_meta, invstd_meta, weight_meta, input_g, weight_g, bias_g, out0_meta, out1_meta, out2_meta, out3_meta);
      }
      
      at::Tensor grad_out_;
      if (at::functionalization::impl::isFunctionalTensor(grad_out)) {
        at::functionalization::impl::sync(grad_out);
        grad_out_ = at::functionalization::impl::from_functional_tensor(grad_out);
      } else {
        grad_out_ = grad_out;
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor mean_;
      if (at::functionalization::impl::isFunctionalTensor(mean)) {
        at::functionalization::impl::sync(mean);
        mean_ = at::functionalization::impl::from_functional_tensor(mean);
      } else {
        mean_ = mean;
      }
      
      at::Tensor invstd_;
      if (at::functionalization::impl::isFunctionalTensor(invstd)) {
        at::functionalization::impl::sync(invstd);
        invstd_ = at::functionalization::impl::from_functional_tensor(invstd);
      } else {
        invstd_ = invstd;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      
      at::Tensor out3_;
      if (at::functionalization::impl::isFunctionalTensor(out3)) {
        at::functionalization::impl::sync(out3);
        out3_ = at::functionalization::impl::from_functional_tensor(out3);
      } else {
        out3_ = out3;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2) && at::functionalization::impl::isFunctionalTensor(out3))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_out.device().type() == c10::DeviceType::XLA || input.device().type() == c10::DeviceType::XLA || mean.device().type() == c10::DeviceType::XLA || invstd.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_out) || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(mean) || at::functionalization::impl::isFunctionalTensor(invstd) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::batch_norm_backward_reduce_out::call(grad_out_, input_, mean_, invstd_, weight_, input_g, weight_g, bias_g, out0_, out1_, out2_, out3_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::batch_norm_backward_reduce::call(grad_out_, input_, mean_, invstd_, weight_, input_g, weight_g, bias_g);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
  at::functionalization::impl::propagate_xla_data(out3, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(out3, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(out3);
  at::functionalization::impl::sync(out3);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3);
      }
    }

    at::Tensor & deg2rad_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::deg2rad_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::deg2rad_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::deg2rad::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & deg2rad_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::deg2rad_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::deg2rad_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::deg2rad::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & randint_like_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt high, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::randint_like_out::call(self_meta, high, memory_format, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::randint_like_out::call(self_, high, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::randint_like::call(self_, high, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & randint_like_out_low_dtype_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt low, c10::SymInt high, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::randint_like_low_dtype_out::call(self_meta, low, high, memory_format, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::randint_like_low_dtype_out::call(self_, low, high, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::randint_like_low_dtype::call(self_, low, high, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & repeat_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef repeats, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::repeat_out::call(self_meta, repeats, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::repeat_out::call(self_, repeats, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::repeat::call(self_, repeats);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _mkldnn_reshape_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef shape, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_mkldnn_reshape_out::call(self_meta, shape, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_mkldnn_reshape_out::call(self_, shape, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_mkldnn_reshape::call(self_, shape);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & round_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::round_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::round_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::round::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & round_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::round_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::round_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::round::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & round_out_decimals_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t decimals, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::round_decimals_out::call(self_meta, decimals, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::round_decimals_out::call(self_, decimals, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::round_decimals::call(self_, decimals);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & round__decimals(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t decimals) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::round__decimals::call(self_meta, decimals);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::round__decimals::call(self_, decimals);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::round_decimals::call(self_, decimals);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & rsqrt_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rsqrt_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rsqrt_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rsqrt::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & rsqrt_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rsqrt_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rsqrt_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rsqrt::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & celu_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::celu_out::call(self_meta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::celu_out::call(self_, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::celu::call(self_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & celu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::celu_::call(self_meta, alpha);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::celu_::call(self_, alpha);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::celu::call(self_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & sigmoid_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sigmoid_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sigmoid_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sigmoid::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sigmoid_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sigmoid_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sigmoid_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sigmoid::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & sinc_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sinc_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sinc_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sinc::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sinc_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sinc_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sinc_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sinc::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & sinh_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sinh_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sinh_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sinh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sinh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sinh_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sinh_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sinh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & slice_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt start, c10::SymInt end, c10::SymInt step, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::slice_backward_out::call(grad_output_meta, input_sizes, dim, start, end, step, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::slice_backward_out::call(grad_output_, input_sizes, dim, start, end, step, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::slice_backward::call(grad_output_, input_sizes, dim, start, end, step);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & as_strided_scatter_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & src, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto src_meta = to_meta(src);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::as_strided_scatter_out::call(self_meta, src_meta, size, stride, storage_offset, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::as_strided_scatter_out::call(self_, src_, size, stride, storage_offset, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::as_strided_scatter::call(self_, src_, size, stride, storage_offset);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> std_mean_out_correction_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::std_mean_correction_out::call(self_meta, dim, correction, keepdim, out0_meta, out1_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::std_mean_correction_out::call(self_, dim, correction, keepdim, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::std_mean_correction::call(self_, dim, correction, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & _mkldnn_transpose_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim0, int64_t dim1, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_mkldnn_transpose_out::call(self_meta, dim0, dim1, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_mkldnn_transpose_out::call(self_, dim0, dim1, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_mkldnn_transpose::call(self_, dim0, dim1);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _mkldnn_transpose_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim0, int64_t dim1) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_mkldnn_transpose_::call(self_meta, dim0, dim1);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_mkldnn_transpose_::call(self_, dim0, dim1);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_mkldnn_transpose::call(self_, dim0, dim1);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & flip_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dims, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::flip_out::call(self_meta, dims, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::flip_out::call(self_, dims, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::flip::call(self_, dims);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _nested_tensor_from_mask_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & t, const at::Tensor & mask, bool mask_check, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto t_meta = to_meta(t);
        auto mask_meta = to_meta(mask);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_nested_tensor_from_mask_out::call(t_meta, mask_meta, mask_check, out_meta);
      }
      
      at::Tensor t_;
      if (at::functionalization::impl::isFunctionalTensor(t)) {
        at::functionalization::impl::sync(t);
        t_ = at::functionalization::impl::from_functional_tensor(t);
      } else {
        t_ = t;
      }
      
      at::Tensor mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || t.device().type() == c10::DeviceType::XLA || mask.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(t) || at::functionalization::impl::isFunctionalTensor(mask))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_nested_tensor_from_mask_out::call(t_, mask_, mask_check, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_nested_tensor_from_mask::call(t_, mask_, mask_check);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _nested_from_padded_and_nested_example_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & padded, const at::Tensor & nt_example, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto padded_meta = to_meta(padded);
        auto nt_example_meta = to_meta(nt_example);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_nested_from_padded_and_nested_example_out::call(padded_meta, nt_example_meta, out_meta);
      }
      
      at::Tensor padded_;
      if (at::functionalization::impl::isFunctionalTensor(padded)) {
        at::functionalization::impl::sync(padded);
        padded_ = at::functionalization::impl::from_functional_tensor(padded);
      } else {
        padded_ = padded;
      }
      
      at::Tensor nt_example_;
      if (at::functionalization::impl::isFunctionalTensor(nt_example)) {
        at::functionalization::impl::sync(nt_example);
        nt_example_ = at::functionalization::impl::from_functional_tensor(nt_example);
      } else {
        nt_example_ = nt_example;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || padded.device().type() == c10::DeviceType::XLA || nt_example.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(padded) || at::functionalization::impl::isFunctionalTensor(nt_example))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_nested_from_padded_and_nested_example_out::call(padded_, nt_example_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_nested_from_padded_and_nested_example::call(padded_, nt_example_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> unique_dim_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool sorted, bool return_inverse, bool return_counts, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::unique_dim_out::call(self_meta, dim, sorted, return_inverse, return_counts, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::unique_dim_out::call(self_, dim, sorted, return_inverse, return_counts, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::unique_dim::call(self_, dim, sorted, return_inverse, return_counts);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & _unsafe_view_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_unsafe_view_out::call(self_meta, size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_unsafe_view_out::call(self_, size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_unsafe_view::call(self_, size);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> var_mean_out_correction_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::var_mean_correction_out::call(self_meta, dim, correction, keepdim, out0_meta, out1_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::var_mean_correction_out::call(self_, dim, correction, keepdim, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::var_mean_correction::call(self_, dim, correction, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & zeros_out_names_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::zeros_names_out::call(size, names, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::zeros_names_out::call(size, names, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::zeros_names::call(size, names, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & zeros_out_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::zeros_out::call(size, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::zeros_out::call(size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::zeros::call(size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & zeros_like_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::zeros_like_out::call(self_meta, memory_format, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::zeros_like_out::call(self_, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::zeros_like::call(self_, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _sparse_csr_prod_out_dim_dtype_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sparse_csr_prod_dim_dtype_out::call(self_meta, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sparse_csr_prod_dim_dtype_out::call(self_, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sparse_csr_prod_dim_dtype::call(self_, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _spdiags_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & diagonals, const at::Tensor & offsets, at::IntArrayRef shape, c10::optional<at::Layout> layout, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto diagonals_meta = to_meta(diagonals);
        auto offsets_meta = to_meta(offsets);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_spdiags_out::call(diagonals_meta, offsets_meta, shape, layout, out_meta);
      }
      
      at::Tensor diagonals_;
      if (at::functionalization::impl::isFunctionalTensor(diagonals)) {
        at::functionalization::impl::sync(diagonals);
        diagonals_ = at::functionalization::impl::from_functional_tensor(diagonals);
      } else {
        diagonals_ = diagonals;
      }
      
      at::Tensor offsets_;
      if (at::functionalization::impl::isFunctionalTensor(offsets)) {
        at::functionalization::impl::sync(offsets);
        offsets_ = at::functionalization::impl::from_functional_tensor(offsets);
      } else {
        offsets_ = offsets;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || diagonals.device().type() == c10::DeviceType::XLA || offsets.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(diagonals) || at::functionalization::impl::isFunctionalTensor(offsets))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_spdiags_out::call(diagonals_, offsets_, shape, layout, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_spdiags::call(diagonals_, offsets_, shape, layout);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & rsub_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rsub_Tensor_out::call(self_meta, other_meta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rsub_Tensor_out::call(self_, other_, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rsub_Tensor::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & rsub_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rsub_Scalar_out::call(self_meta, other, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rsub_Scalar_out::call(self_, other, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rsub_Scalar::call(self_, other, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _sparse_addmm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mat1_meta = to_meta(mat1);
        auto mat2_meta = to_meta(mat2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sparse_addmm_out::call(self_meta, mat1_meta, mat2_meta, beta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mat1_;
      if (at::functionalization::impl::isFunctionalTensor(mat1)) {
        at::functionalization::impl::sync(mat1);
        mat1_ = at::functionalization::impl::from_functional_tensor(mat1);
      } else {
        mat1_ = mat1;
      }
      
      at::Tensor mat2_;
      if (at::functionalization::impl::isFunctionalTensor(mat2)) {
        at::functionalization::impl::sync(mat2);
        mat2_ = at::functionalization::impl::from_functional_tensor(mat2);
      } else {
        mat2_ = mat2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mat1.device().type() == c10::DeviceType::XLA || mat2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mat1) || at::functionalization::impl::isFunctionalTensor(mat2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sparse_addmm_out::call(self_, mat1_, mat2_, beta, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sparse_addmm::call(self_, mat1_, mat2_, beta, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sparse_coo_tensor_out_size_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sparse_coo_tensor_size_out::call(size, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sparse_coo_tensor_size_out::call(size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sparse_coo_tensor_size::call(size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & sparse_resize_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim, const at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sparse_resize_out::call(self_meta, size, sparse_dim, dense_dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sparse_resize_out::call(self_, size, sparse_dim, dense_dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sparse_resize::call(self_, size, sparse_dim, dense_dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & sparse_resize_(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sparse_resize_::call(self_meta, size, sparse_dim, dense_dim);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sparse_resize_::call(self_, size, sparse_dim, dense_dim);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sparse_resize::call(self_, size, sparse_dim, dense_dim);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & sparse_mask_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mask_meta = to_meta(mask);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sparse_mask_out::call(self_meta, mask_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mask.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mask))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sparse_mask_out::call(self_, mask_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sparse_mask::call(self_, mask_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _coalesce_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_coalesce_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_coalesce_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_coalesce::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _to_sparse_csr_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dense_dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_to_sparse_csr_out::call(self_meta, dense_dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_to_sparse_csr_out::call(self_, dense_dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_to_sparse_csr::call(self_, dense_dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _to_sparse_csc_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dense_dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_to_sparse_csc_out::call(self_meta, dense_dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_to_sparse_csc_out::call(self_, dense_dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_to_sparse_csc::call(self_, dense_dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & dequantize_out_self_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::dequantize_self_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::dequantize_self_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::dequantize_self::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    void dequantize_out_tensors_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::dequantize_tensors_out::call(tensors_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::dequantize_tensors_out::call(tensors_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::dequantize_tensors::call(tensors_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    at::Tensor & q_per_channel_zero_points_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::q_per_channel_zero_points_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::q_per_channel_zero_points_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::q_per_channel_zero_points::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _fake_quantize_learnable_per_channel_affine_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto scale_meta = to_meta(scale);
        auto zero_point_meta = to_meta(zero_point);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fake_quantize_learnable_per_channel_affine_out::call(self_meta, scale_meta, zero_point_meta, axis, quant_min, quant_max, grad_factor, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor scale_;
      if (at::functionalization::impl::isFunctionalTensor(scale)) {
        at::functionalization::impl::sync(scale);
        scale_ = at::functionalization::impl::from_functional_tensor(scale);
      } else {
        scale_ = scale;
      }
      
      at::Tensor zero_point_;
      if (at::functionalization::impl::isFunctionalTensor(zero_point)) {
        at::functionalization::impl::sync(zero_point);
        zero_point_ = at::functionalization::impl::from_functional_tensor(zero_point);
      } else {
        zero_point_ = zero_point;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || scale.device().type() == c10::DeviceType::XLA || zero_point.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(scale) || at::functionalization::impl::isFunctionalTensor(zero_point))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_fake_quantize_learnable_per_channel_affine_out::call(self_, scale_, zero_point_, axis, quant_min, quant_max, grad_factor, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fake_quantize_learnable_per_channel_affine::call(self_, scale_, zero_point_, axis, quant_min, quant_max, grad_factor);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _fused_moving_avg_obs_fq_helper_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & observer_on, const at::Tensor & fake_quant_on, at::Tensor & running_min, at::Tensor & running_max, at::Tensor & scale, at::Tensor & zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, bool per_row_fake_quant, bool symmetric_quant, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto observer_on_meta = to_meta(observer_on);
        auto fake_quant_on_meta = to_meta(fake_quant_on);
        auto running_min_meta = to_meta(running_min);
        auto running_max_meta = to_meta(running_max);
        auto scale_meta = to_meta(scale);
        auto zero_point_meta = to_meta(zero_point);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_moving_avg_obs_fq_helper_out::call(self_meta, observer_on_meta, fake_quant_on_meta, running_min_meta, running_max_meta, scale_meta, zero_point_meta, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant, out0_meta, out1_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor observer_on_;
      if (at::functionalization::impl::isFunctionalTensor(observer_on)) {
        at::functionalization::impl::sync(observer_on);
        observer_on_ = at::functionalization::impl::from_functional_tensor(observer_on);
      } else {
        observer_on_ = observer_on;
      }
      
      at::Tensor fake_quant_on_;
      if (at::functionalization::impl::isFunctionalTensor(fake_quant_on)) {
        at::functionalization::impl::sync(fake_quant_on);
        fake_quant_on_ = at::functionalization::impl::from_functional_tensor(fake_quant_on);
      } else {
        fake_quant_on_ = fake_quant_on;
      }
      
      at::Tensor running_min_;
      if (at::functionalization::impl::isFunctionalTensor(running_min)) {
        at::functionalization::impl::sync(running_min);
        running_min_ = at::functionalization::impl::from_functional_tensor(running_min);
      } else {
        running_min_ = running_min;
      }
      
      at::Tensor running_max_;
      if (at::functionalization::impl::isFunctionalTensor(running_max)) {
        at::functionalization::impl::sync(running_max);
        running_max_ = at::functionalization::impl::from_functional_tensor(running_max);
      } else {
        running_max_ = running_max;
      }
      
      at::Tensor scale_;
      if (at::functionalization::impl::isFunctionalTensor(scale)) {
        at::functionalization::impl::sync(scale);
        scale_ = at::functionalization::impl::from_functional_tensor(scale);
      } else {
        scale_ = scale;
      }
      
      at::Tensor zero_point_;
      if (at::functionalization::impl::isFunctionalTensor(zero_point)) {
        at::functionalization::impl::sync(zero_point);
        zero_point_ = at::functionalization::impl::from_functional_tensor(zero_point);
      } else {
        zero_point_ = zero_point;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(running_min) && at::functionalization::impl::isFunctionalTensor(running_max) && at::functionalization::impl::isFunctionalTensor(scale) && at::functionalization::impl::isFunctionalTensor(zero_point) && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || observer_on.device().type() == c10::DeviceType::XLA || fake_quant_on.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(observer_on) || at::functionalization::impl::isFunctionalTensor(fake_quant_on))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_fused_moving_avg_obs_fq_helper_out::call(self_, observer_on_, fake_quant_on_, running_min_, running_max_, scale_, zero_point_, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_moving_avg_obs_fq_helper_functional::call(self_, observer_on_, fake_quant_on_, running_min_, running_max_, scale_, zero_point_, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
        }
          at::functionalization::impl::propagate_xla_data(running_min, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(running_min, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(running_min);
  at::functionalization::impl::sync(running_min);
  at::functionalization::impl::propagate_xla_data(running_max, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(running_max, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(running_max);
  at::functionalization::impl::sync(running_max);
  at::functionalization::impl::propagate_xla_data(scale, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(scale, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(scale);
  at::functionalization::impl::sync(scale);
  at::functionalization::impl::propagate_xla_data(zero_point, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(zero_point, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(zero_point);
  at::functionalization::impl::sync(zero_point);
  at::functionalization::impl::propagate_xla_data(out0, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<5>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<5>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    ::std::tuple<at::Tensor,at::Tensor> _fused_moving_avg_obs_fq_helper(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & observer_on, const at::Tensor & fake_quant_on, at::Tensor & running_min, at::Tensor & running_max, at::Tensor & scale, at::Tensor & zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, bool per_row_fake_quant, bool symmetric_quant) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto observer_on_meta = to_meta(observer_on);
        auto fake_quant_on_meta = to_meta(fake_quant_on);
        auto running_min_meta = to_meta(running_min);
        auto running_max_meta = to_meta(running_max);
        auto scale_meta = to_meta(scale);
        auto zero_point_meta = to_meta(zero_point);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_moving_avg_obs_fq_helper::call(self_meta, observer_on_meta, fake_quant_on_meta, running_min_meta, running_max_meta, scale_meta, zero_point_meta, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor observer_on_;
      if (at::functionalization::impl::isFunctionalTensor(observer_on)) {
        at::functionalization::impl::sync(observer_on);
        observer_on_ = at::functionalization::impl::from_functional_tensor(observer_on);
      } else {
        observer_on_ = observer_on;
      }
      
      at::Tensor fake_quant_on_;
      if (at::functionalization::impl::isFunctionalTensor(fake_quant_on)) {
        at::functionalization::impl::sync(fake_quant_on);
        fake_quant_on_ = at::functionalization::impl::from_functional_tensor(fake_quant_on);
      } else {
        fake_quant_on_ = fake_quant_on;
      }
      
      at::Tensor running_min_;
      if (at::functionalization::impl::isFunctionalTensor(running_min)) {
        at::functionalization::impl::sync(running_min);
        running_min_ = at::functionalization::impl::from_functional_tensor(running_min);
      } else {
        running_min_ = running_min;
      }
      
      at::Tensor running_max_;
      if (at::functionalization::impl::isFunctionalTensor(running_max)) {
        at::functionalization::impl::sync(running_max);
        running_max_ = at::functionalization::impl::from_functional_tensor(running_max);
      } else {
        running_max_ = running_max;
      }
      
      at::Tensor scale_;
      if (at::functionalization::impl::isFunctionalTensor(scale)) {
        at::functionalization::impl::sync(scale);
        scale_ = at::functionalization::impl::from_functional_tensor(scale);
      } else {
        scale_ = scale;
      }
      
      at::Tensor zero_point_;
      if (at::functionalization::impl::isFunctionalTensor(zero_point)) {
        at::functionalization::impl::sync(zero_point);
        zero_point_ = at::functionalization::impl::from_functional_tensor(zero_point);
      } else {
        zero_point_ = zero_point;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(running_min) && at::functionalization::impl::isFunctionalTensor(running_max) && at::functionalization::impl::isFunctionalTensor(scale) && at::functionalization::impl::isFunctionalTensor(zero_point))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || observer_on.device().type() == c10::DeviceType::XLA || fake_quant_on.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(observer_on) || at::functionalization::impl::isFunctionalTensor(fake_quant_on))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_fused_moving_avg_obs_fq_helper::call(self_, observer_on_, fake_quant_on_, running_min_, running_max_, scale_, zero_point_, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
         return ::std::tuple<at::Tensor,at::Tensor>(std::get<0>(tmp_output), std::get<1>(tmp_output));;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_moving_avg_obs_fq_helper_functional::call(self_, observer_on_, fake_quant_on_, running_min_, running_max_, scale_, zero_point_, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
        }
          auto output_0 = at::functionalization::impl::to_functional_tensor(std::get<0>(tmp_output));
  auto output_1 = at::functionalization::impl::to_functional_tensor(std::get<1>(tmp_output));
  at::functionalization::impl::propagate_xla_data(running_min, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(running_min, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(running_min);
  at::functionalization::impl::sync(running_min);
  at::functionalization::impl::propagate_xla_data(running_max, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(running_max, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(running_max);
  at::functionalization::impl::sync(running_max);
  at::functionalization::impl::propagate_xla_data(scale, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(scale, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(scale);
  at::functionalization::impl::sync(scale);
  at::functionalization::impl::propagate_xla_data(zero_point, std::get<5>(tmp_output));
  at::functionalization::impl::replace_(zero_point, std::get<5>(tmp_output));
  at::functionalization::impl::commit_update(zero_point);
  at::functionalization::impl::sync(zero_point);
    return ::std::tuple<at::Tensor,at::Tensor>(output_0, output_1);
      }
    }

    at::Tensor & _to_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_to_copy_out::call(self_meta, non_blocking, memory_format, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_to_copy_out::call(self_, non_blocking, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_to_copy::call(self_, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, non_blocking, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _thnn_fused_gru_cell_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_gates_meta = to_meta(input_gates);
        auto hidden_gates_meta = to_meta(hidden_gates);
        auto hx_meta = to_meta(hx);
        auto input_bias_meta = to_meta(input_bias);
        auto hidden_bias_meta = to_meta(hidden_bias);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_thnn_fused_gru_cell_out::call(input_gates_meta, hidden_gates_meta, hx_meta, input_bias_meta, hidden_bias_meta, out0_meta, out1_meta);
      }
      
      at::Tensor input_gates_;
      if (at::functionalization::impl::isFunctionalTensor(input_gates)) {
        at::functionalization::impl::sync(input_gates);
        input_gates_ = at::functionalization::impl::from_functional_tensor(input_gates);
      } else {
        input_gates_ = input_gates;
      }
      
      at::Tensor hidden_gates_;
      if (at::functionalization::impl::isFunctionalTensor(hidden_gates)) {
        at::functionalization::impl::sync(hidden_gates);
        hidden_gates_ = at::functionalization::impl::from_functional_tensor(hidden_gates);
      } else {
        hidden_gates_ = hidden_gates;
      }
      
      at::Tensor hx_;
      if (at::functionalization::impl::isFunctionalTensor(hx)) {
        at::functionalization::impl::sync(hx);
        hx_ = at::functionalization::impl::from_functional_tensor(hx);
      } else {
        hx_ = hx;
      }
      
      c10::optional<at::Tensor> input_bias_;
      if (at::functionalization::impl::isFunctionalTensor(input_bias)) {
        at::functionalization::impl::sync(input_bias);
        input_bias_ = at::functionalization::impl::from_functional_tensor(input_bias);
      } else {
        input_bias_ = input_bias;
      }
      
      c10::optional<at::Tensor> hidden_bias_;
      if (at::functionalization::impl::isFunctionalTensor(hidden_bias)) {
        at::functionalization::impl::sync(hidden_bias);
        hidden_bias_ = at::functionalization::impl::from_functional_tensor(hidden_bias);
      } else {
        hidden_bias_ = hidden_bias;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input_gates.device().type() == c10::DeviceType::XLA || hidden_gates.device().type() == c10::DeviceType::XLA || hx.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input_gates) || at::functionalization::impl::isFunctionalTensor(hidden_gates) || at::functionalization::impl::isFunctionalTensor(hx) || at::functionalization::impl::isFunctionalTensor(input_bias) || at::functionalization::impl::isFunctionalTensor(hidden_bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_thnn_fused_gru_cell_out::call(input_gates_, hidden_gates_, hx_, input_bias_, hidden_bias_, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_thnn_fused_gru_cell::call(input_gates_, hidden_gates_, hx_, input_bias_, hidden_bias_);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _pack_padded_sequence_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & lengths, bool batch_first, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto lengths_meta = to_meta(lengths);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_pack_padded_sequence_out::call(input_meta, lengths_meta, batch_first, out0_meta, out1_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor lengths_;
      if (at::functionalization::impl::isFunctionalTensor(lengths)) {
        at::functionalization::impl::sync(lengths);
        lengths_ = at::functionalization::impl::from_functional_tensor(lengths);
      } else {
        lengths_ = lengths;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || lengths.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(lengths))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_pack_padded_sequence_out::call(input_, lengths_, batch_first, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_pack_padded_sequence::call(input_, lengths_, batch_first);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & scatter_reduce_out_two_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce, bool include_self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto src_meta = to_meta(src);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::scatter_reduce_two_out::call(self_meta, dim, index_meta, src_meta, reduce, include_self, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::scatter_reduce_two_out::call(self_, dim, index_, src_, reduce, include_self, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::scatter_reduce_two::call(self_, dim, index_, src_, reduce, include_self);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & scatter_reduce__two(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce, bool include_self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto src_meta = to_meta(src);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::scatter_reduce__two::call(self_meta, dim, index_meta, src_meta, reduce, include_self);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || index.device().type() == c10::DeviceType::XLA || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::scatter_reduce__two::call(self_, dim, index_, src_, reduce, include_self);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::scatter_reduce_two::call(self_, dim, index_, src_, reduce, include_self);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bitwise_xor_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_xor_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_xor_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_xor_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_xor__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_xor__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_xor__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_xor_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bitwise_xor_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_xor_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_xor_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_xor_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_xor__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_xor__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_xor__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_xor_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bitwise_xor_out_Scalar_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_xor_Scalar_Tensor_out::call(self, other_meta, out_meta);
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_xor_Scalar_Tensor_out::call(self, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_xor_Scalar_Tensor::call(self, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & addbmm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto batch1_meta = to_meta(batch1);
        auto batch2_meta = to_meta(batch2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::addbmm_out::call(self_meta, batch1_meta, batch2_meta, beta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor batch1_;
      if (at::functionalization::impl::isFunctionalTensor(batch1)) {
        at::functionalization::impl::sync(batch1);
        batch1_ = at::functionalization::impl::from_functional_tensor(batch1);
      } else {
        batch1_ = batch1;
      }
      
      at::Tensor batch2_;
      if (at::functionalization::impl::isFunctionalTensor(batch2)) {
        at::functionalization::impl::sync(batch2);
        batch2_ = at::functionalization::impl::from_functional_tensor(batch2);
      } else {
        batch2_ = batch2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || batch1.device().type() == c10::DeviceType::XLA || batch2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(batch1) || at::functionalization::impl::isFunctionalTensor(batch2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::addbmm_out::call(self_, batch1_, batch2_, beta, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::addbmm::call(self_, batch1_, batch2_, beta, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & addbmm_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto batch1_meta = to_meta(batch1);
        auto batch2_meta = to_meta(batch2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::addbmm_::call(self_meta, batch1_meta, batch2_meta, beta, alpha);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor batch1_;
      if (at::functionalization::impl::isFunctionalTensor(batch1)) {
        at::functionalization::impl::sync(batch1);
        batch1_ = at::functionalization::impl::from_functional_tensor(batch1);
      } else {
        batch1_ = batch1;
      }
      
      at::Tensor batch2_;
      if (at::functionalization::impl::isFunctionalTensor(batch2)) {
        at::functionalization::impl::sync(batch2);
        batch2_ = at::functionalization::impl::from_functional_tensor(batch2);
      } else {
        batch2_ = batch2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || batch1.device().type() == c10::DeviceType::XLA || batch2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(batch1) || at::functionalization::impl::isFunctionalTensor(batch2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::addbmm_::call(self_, batch1_, batch2_, beta, alpha);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::addbmm::call(self_, batch1_, batch2_, beta, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & random_out_from_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::random_from_out::call(self_meta, from, to, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::random_from_out::call(self_, from, to, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::random_from::call(self_, from, to, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & random__from(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::random__from::call(self_meta, from, to, generator);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::random__from::call(self_, from, to, generator);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::random_from::call(self_, from, to, generator);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & random_out_to_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t to, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::random_to_out::call(self_meta, to, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::random_to_out::call(self_, to, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::random_to::call(self_, to, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & random__to(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t to, c10::optional<at::Generator> generator) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::random__to::call(self_meta, to, generator);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::random__to::call(self_, to, generator);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::random_to::call(self_, to, generator);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & random_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::random_out::call(self_meta, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::random_out::call(self_, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::random::call(self_, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & random_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<at::Generator> generator) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::random_::call(self_meta, generator);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::random_::call(self_, generator);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::random::call(self_, generator);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & exponential_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double lambd, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::exponential_out::call(self_meta, lambd, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::exponential_out::call(self_, lambd, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::exponential::call(self_, lambd, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & exponential_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double lambd, c10::optional<at::Generator> generator) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::exponential_::call(self_meta, lambd, generator);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::exponential_::call(self_, lambd, generator);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::exponential::call(self_, lambd, generator);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & geometric_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double p, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::geometric_out::call(self_meta, p, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::geometric_out::call(self_, p, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::geometric::call(self_, p, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & geometric_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, c10::optional<at::Generator> generator) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::geometric_::call(self_meta, p, generator);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::geometric_::call(self_, p, generator);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::geometric::call(self_, p, generator);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & cross_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cross_out::call(self_meta, other_meta, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cross_out::call(self_, other_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cross::call(self_, other_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & trace_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::trace_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::trace_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::trace::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & take_along_dim_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::take_along_dim_out::call(self_meta, indices_meta, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::take_along_dim_out::call(self_, indices_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::take_along_dim::call(self_, indices_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & index_select_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_select_out::call(self_meta, dim, index_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_select_out::call(self_, dim, index_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_select::call(self_, dim, index_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & index_select_out_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_select_dimname_out::call(self_meta, dim, index_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_select_dimname_out::call(self_, dim, index_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_select_dimname::call(self_, dim, index_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & masked_select_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mask_meta = to_meta(mask);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::masked_select_out::call(self_meta, mask_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mask.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mask))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::masked_select_out::call(self_, mask_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::masked_select::call(self_, mask_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_solve_triangular_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & B, bool upper, bool left, bool unitriangular, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto B_meta = to_meta(B);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_solve_triangular_out::call(self_meta, B_meta, upper, left, unitriangular, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor B_;
      if (at::functionalization::impl::isFunctionalTensor(B)) {
        at::functionalization::impl::sync(B);
        B_ = at::functionalization::impl::from_functional_tensor(B);
      } else {
        B_ = B;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || B.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(B))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_solve_triangular_out::call(self_, B_, upper, left, unitriangular, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_solve_triangular::call(self_, B_, upper, left, unitriangular);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> svd_out_U(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool some, bool compute_uv, at::Tensor & U, at::Tensor & S, at::Tensor & V) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto U_meta = to_meta(U);
        auto S_meta = to_meta(S);
        auto V_meta = to_meta(V);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::svd_U::call(self_meta, some, compute_uv, U_meta, S_meta, V_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor U_;
      if (at::functionalization::impl::isFunctionalTensor(U)) {
        at::functionalization::impl::sync(U);
        U_ = at::functionalization::impl::from_functional_tensor(U);
      } else {
        U_ = U;
      }
      
      at::Tensor S_;
      if (at::functionalization::impl::isFunctionalTensor(S)) {
        at::functionalization::impl::sync(S);
        S_ = at::functionalization::impl::from_functional_tensor(S);
      } else {
        S_ = S;
      }
      
      at::Tensor V_;
      if (at::functionalization::impl::isFunctionalTensor(V)) {
        at::functionalization::impl::sync(V);
        V_ = at::functionalization::impl::from_functional_tensor(V);
      } else {
        V_ = V;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(U) && at::functionalization::impl::isFunctionalTensor(S) && at::functionalization::impl::isFunctionalTensor(V))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::svd_U::call(self_, some, compute_uv, U_, S_, V_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(U, S, V);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::svd::call(self_, some, compute_uv);
        }
          at::functionalization::impl::propagate_xla_data(U, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(U, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(U);
  at::functionalization::impl::sync(U);
  at::functionalization::impl::propagate_xla_data(S, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(S, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(S);
  at::functionalization::impl::sync(S);
  at::functionalization::impl::propagate_xla_data(V, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(V, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(V);
  at::functionalization::impl::sync(V);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(U, S, V);
      }
    }

    at::Tensor & multinomial_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t num_samples, bool replacement, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::multinomial_out::call(self_meta, num_samples, replacement, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::multinomial_out::call(self_, num_samples, replacement, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::multinomial::call(self_, num_samples, replacement, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> histogram_out_bins_tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & bins, const c10::optional<at::Tensor> & weight, bool density, at::Tensor & hist, at::Tensor & bin_edges) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto bins_meta = to_meta(bins);
        auto weight_meta = to_meta(weight);
        auto hist_meta = to_meta(hist);
        auto bin_edges_meta = to_meta(bin_edges);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::histogram_bins_tensor_out::call(self_meta, bins_meta, weight_meta, density, hist_meta, bin_edges_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor bins_;
      if (at::functionalization::impl::isFunctionalTensor(bins)) {
        at::functionalization::impl::sync(bins);
        bins_ = at::functionalization::impl::from_functional_tensor(bins);
      } else {
        bins_ = bins;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor hist_;
      if (at::functionalization::impl::isFunctionalTensor(hist)) {
        at::functionalization::impl::sync(hist);
        hist_ = at::functionalization::impl::from_functional_tensor(hist);
      } else {
        hist_ = hist;
      }
      
      at::Tensor bin_edges_;
      if (at::functionalization::impl::isFunctionalTensor(bin_edges)) {
        at::functionalization::impl::sync(bin_edges);
        bin_edges_ = at::functionalization::impl::from_functional_tensor(bin_edges);
      } else {
        bin_edges_ = bin_edges;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(hist) && at::functionalization::impl::isFunctionalTensor(bin_edges))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || bins.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(bins) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::histogram_bins_tensor_out::call(self_, bins_, weight_, density, hist_, bin_edges_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(hist, bin_edges);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::histogram_bins_tensor::call(self_, bins_, weight_, density);
        }
          at::functionalization::impl::propagate_xla_data(hist, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(hist, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(hist);
  at::functionalization::impl::sync(hist);
  at::functionalization::impl::propagate_xla_data(bin_edges, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(bin_edges, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(bin_edges);
  at::functionalization::impl::sync(bin_edges);
    return ::std::tuple<at::Tensor &,at::Tensor &>(hist, bin_edges);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> histogram_out_bin_ct_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density, at::Tensor & hist, at::Tensor & bin_edges) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto hist_meta = to_meta(hist);
        auto bin_edges_meta = to_meta(bin_edges);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::histogram_bin_ct_out::call(self_meta, bins, range, weight_meta, density, hist_meta, bin_edges_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor hist_;
      if (at::functionalization::impl::isFunctionalTensor(hist)) {
        at::functionalization::impl::sync(hist);
        hist_ = at::functionalization::impl::from_functional_tensor(hist);
      } else {
        hist_ = hist;
      }
      
      at::Tensor bin_edges_;
      if (at::functionalization::impl::isFunctionalTensor(bin_edges)) {
        at::functionalization::impl::sync(bin_edges);
        bin_edges_ = at::functionalization::impl::from_functional_tensor(bin_edges);
      } else {
        bin_edges_ = bin_edges;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(hist) && at::functionalization::impl::isFunctionalTensor(bin_edges))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::histogram_bin_ct_out::call(self_, bins, range, weight_, density, hist_, bin_edges_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(hist, bin_edges);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::histogram_bin_ct::call(self_, bins, range, weight_, density);
        }
          at::functionalization::impl::propagate_xla_data(hist, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(hist, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(hist);
  at::functionalization::impl::sync(hist);
  at::functionalization::impl::propagate_xla_data(bin_edges, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(bin_edges, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(bin_edges);
  at::functionalization::impl::sync(bin_edges);
    return ::std::tuple<at::Tensor &,at::Tensor &>(hist, bin_edges);
      }
    }

    at::Tensor & igammac_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::igammac_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::igammac_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::igammac::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & igammac_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::igammac_::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::igammac_::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::igammac::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & remainder_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::remainder_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::remainder_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::remainder_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & remainder__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::remainder__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::remainder__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::remainder_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & remainder_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::remainder_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::remainder_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::remainder_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & remainder__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::remainder__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::remainder__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::remainder_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & remainder_out_Scalar_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::remainder_Scalar_Tensor_out::call(self, other_meta, out_meta);
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::remainder_Scalar_Tensor_out::call(self, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::remainder_Scalar_Tensor::call(self, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & quantile_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto q_meta = to_meta(q);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::quantile_out::call(self_meta, q_meta, dim, keepdim, interpolation, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor q_;
      if (at::functionalization::impl::isFunctionalTensor(q)) {
        at::functionalization::impl::sync(q);
        q_ = at::functionalization::impl::from_functional_tensor(q);
      } else {
        q_ = q;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || q.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(q))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::quantile_out::call(self_, q_, dim, keepdim, interpolation, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::quantile::call(self_, q_, dim, keepdim, interpolation);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & quantile_out_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::quantile_scalar_out::call(self_meta, q, dim, keepdim, interpolation, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::quantile_scalar_out::call(self_, q, dim, keepdim, interpolation, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::quantile_scalar::call(self_, q, dim, keepdim, interpolation);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & nanquantile_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto q_meta = to_meta(q);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nanquantile_out::call(self_meta, q_meta, dim, keepdim, interpolation, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor q_;
      if (at::functionalization::impl::isFunctionalTensor(q)) {
        at::functionalization::impl::sync(q);
        q_ = at::functionalization::impl::from_functional_tensor(q);
      } else {
        q_ = q;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || q.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(q))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nanquantile_out::call(self_, q_, dim, keepdim, interpolation, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nanquantile::call(self_, q_, dim, keepdim, interpolation);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & nanquantile_out_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nanquantile_scalar_out::call(self_meta, q, dim, keepdim, interpolation, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nanquantile_scalar_out::call(self_, q, dim, keepdim, interpolation, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nanquantile_scalar::call(self_, q, dim, keepdim, interpolation);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> sort_out_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sort_values::call(self_meta, dim, descending, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::sort_values::call(self_, dim, descending, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sort::call(self_, dim, descending);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> sort_out_values_stable(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<bool> stable, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sort_values_stable::call(self_meta, stable, dim, descending, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::sort_values_stable::call(self_, stable, dim, descending, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sort_stable::call(self_, stable, dim, descending);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> sort_out_dimname_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sort_dimname_values::call(self_meta, dim, descending, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::sort_dimname_values::call(self_, dim, descending, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sort_dimname::call(self_, dim, descending);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> sort_out_dimname_values_stable(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sort_dimname_values_stable::call(self_meta, stable, dim, descending, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::sort_dimname_values_stable::call(self_, stable, dim, descending, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sort_dimname_stable::call(self_, stable, dim, descending);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    at::Tensor & argsort_out_stable_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool stable, int64_t dim, bool descending, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::argsort_stable_out::call(self_meta, stable, dim, descending, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::argsort_stable_out::call(self_, stable, dim, descending, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::argsort_stable::call(self_, stable, dim, descending);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & all_out_all_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::all_all_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::all_all_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::all::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & renorm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::renorm_out::call(self_meta, p, dim, maxnorm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::renorm_out::call(self_, p, dim, maxnorm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::renorm::call(self_, p, dim, maxnorm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & renorm_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::renorm_::call(self_meta, p, dim, maxnorm);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::renorm_::call(self_, p, dim, maxnorm);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::renorm::call(self_, p, dim, maxnorm);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & unfold_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_in, c10::SymIntArrayRef input_sizes, int64_t dim, int64_t size, int64_t step, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_in_meta = to_meta(grad_in);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::unfold_backward_out::call(grad_in_meta, input_sizes, dim, size, step, out_meta);
      }
      
      at::Tensor grad_in_;
      if (at::functionalization::impl::isFunctionalTensor(grad_in)) {
        at::functionalization::impl::sync(grad_in);
        grad_in_ = at::functionalization::impl::from_functional_tensor(grad_in);
      } else {
        grad_in_ = grad_in;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_in.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_in))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::unfold_backward_out::call(grad_in_, input_sizes, dim, size, step, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::unfold_backward::call(grad_in_, input_sizes, dim, size, step);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & pow_out_Tensor_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto exponent_meta = to_meta(exponent);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::pow_Tensor_Tensor_out::call(self_meta, exponent_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor exponent_;
      if (at::functionalization::impl::isFunctionalTensor(exponent)) {
        at::functionalization::impl::sync(exponent);
        exponent_ = at::functionalization::impl::from_functional_tensor(exponent);
      } else {
        exponent_ = exponent;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || exponent.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(exponent))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::pow_Tensor_Tensor_out::call(self_, exponent_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::pow_Tensor_Tensor::call(self_, exponent_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & pow__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & exponent) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto exponent_meta = to_meta(exponent);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::pow__Tensor::call(self_meta, exponent_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor exponent_;
      if (at::functionalization::impl::isFunctionalTensor(exponent)) {
        at::functionalization::impl::sync(exponent);
        exponent_ = at::functionalization::impl::from_functional_tensor(exponent);
      } else {
        exponent_ = exponent;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || exponent.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(exponent))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::pow__Tensor::call(self_, exponent_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::pow_Tensor_Tensor::call(self_, exponent_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & pow_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto exponent_meta = to_meta(exponent);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::pow_Scalar_out::call(self, exponent_meta, out_meta);
      }
      
      at::Tensor exponent_;
      if (at::functionalization::impl::isFunctionalTensor(exponent)) {
        at::functionalization::impl::sync(exponent);
        exponent_ = at::functionalization::impl::from_functional_tensor(exponent);
      } else {
        exponent_ = exponent;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || exponent.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(exponent))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::pow_Scalar_out::call(self, exponent_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::pow_Scalar::call(self, exponent_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & pow_out_Tensor_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::pow_Tensor_Scalar_out::call(self_meta, exponent, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::pow_Tensor_Scalar_out::call(self_, exponent, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::pow_Tensor_Scalar::call(self_, exponent);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & pow__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & exponent) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::pow__Scalar::call(self_meta, exponent);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::pow__Scalar::call(self_, exponent);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::pow_Tensor_Scalar::call(self_, exponent);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & float_power_out_Tensor_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto exponent_meta = to_meta(exponent);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::float_power_Tensor_Tensor_out::call(self_meta, exponent_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor exponent_;
      if (at::functionalization::impl::isFunctionalTensor(exponent)) {
        at::functionalization::impl::sync(exponent);
        exponent_ = at::functionalization::impl::from_functional_tensor(exponent);
      } else {
        exponent_ = exponent;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || exponent.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(exponent))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::float_power_Tensor_Tensor_out::call(self_, exponent_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::float_power_Tensor_Tensor::call(self_, exponent_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & float_power__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & exponent) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto exponent_meta = to_meta(exponent);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::float_power__Tensor::call(self_meta, exponent_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor exponent_;
      if (at::functionalization::impl::isFunctionalTensor(exponent)) {
        at::functionalization::impl::sync(exponent);
        exponent_ = at::functionalization::impl::from_functional_tensor(exponent);
      } else {
        exponent_ = exponent;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || exponent.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(exponent))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::float_power__Tensor::call(self_, exponent_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::float_power_Tensor_Tensor::call(self_, exponent_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & float_power_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto exponent_meta = to_meta(exponent);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::float_power_Scalar_out::call(self, exponent_meta, out_meta);
      }
      
      at::Tensor exponent_;
      if (at::functionalization::impl::isFunctionalTensor(exponent)) {
        at::functionalization::impl::sync(exponent);
        exponent_ = at::functionalization::impl::from_functional_tensor(exponent);
      } else {
        exponent_ = exponent;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || exponent.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(exponent))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::float_power_Scalar_out::call(self, exponent_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::float_power_Scalar::call(self, exponent_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & float_power_out_Tensor_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::float_power_Tensor_Scalar_out::call(self_meta, exponent, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::float_power_Tensor_Scalar_out::call(self_, exponent, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::float_power_Tensor_Scalar::call(self_, exponent);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & float_power__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & exponent) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::float_power__Scalar::call(self_meta, exponent);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::float_power__Scalar::call(self_, exponent);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::float_power_Tensor_Scalar::call(self_, exponent);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & normal_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::normal_out::call(self_meta, mean, std, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::normal_out::call(self_, mean, std, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::normal_functional::call(self_, mean, std, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & normal_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::normal_::call(self_meta, mean, std, generator);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::normal_::call(self_, mean, std, generator);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::normal_functional::call(self_, mean, std, generator);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & normal_out_Tensor_float_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mean, double std, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto mean_meta = to_meta(mean);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::normal_Tensor_float_out::call(mean_meta, std, generator, out_meta);
      }
      
      at::Tensor mean_;
      if (at::functionalization::impl::isFunctionalTensor(mean)) {
        at::functionalization::impl::sync(mean);
        mean_ = at::functionalization::impl::from_functional_tensor(mean);
      } else {
        mean_ = mean;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || mean.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(mean))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::normal_Tensor_float_out::call(mean_, std, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::normal_Tensor_float::call(mean_, std, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & normal_out_float_Tensor_out(c10::DispatchKeySet dispatchKeySet, double mean, const at::Tensor & std, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto std_meta = to_meta(std);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::normal_float_Tensor_out::call(mean, std_meta, generator, out_meta);
      }
      
      at::Tensor std_;
      if (at::functionalization::impl::isFunctionalTensor(std)) {
        at::functionalization::impl::sync(std);
        std_ = at::functionalization::impl::from_functional_tensor(std);
      } else {
        std_ = std;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || std.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(std))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::normal_float_Tensor_out::call(mean, std_, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::normal_float_Tensor::call(mean, std_, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & normal_out_Tensor_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mean, const at::Tensor & std, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto mean_meta = to_meta(mean);
        auto std_meta = to_meta(std);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::normal_Tensor_Tensor_out::call(mean_meta, std_meta, generator, out_meta);
      }
      
      at::Tensor mean_;
      if (at::functionalization::impl::isFunctionalTensor(mean)) {
        at::functionalization::impl::sync(mean);
        mean_ = at::functionalization::impl::from_functional_tensor(mean);
      } else {
        mean_ = mean;
      }
      
      at::Tensor std_;
      if (at::functionalization::impl::isFunctionalTensor(std)) {
        at::functionalization::impl::sync(std);
        std_ = at::functionalization::impl::from_functional_tensor(std);
      } else {
        std_ = std;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || mean.device().type() == c10::DeviceType::XLA || std.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(mean) || at::functionalization::impl::isFunctionalTensor(std))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::normal_Tensor_Tensor_out::call(mean_, std_, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::normal_Tensor_Tensor::call(mean_, std_, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & normal_out_float_float_out(c10::DispatchKeySet dispatchKeySet, double mean, double std, c10::SymIntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::normal_float_float_out::call(mean, std, size, generator, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::normal_float_float_out::call(mean, std, size, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::normal_float_float::call(mean, std, size, generator, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _amp_update_scale_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & growth_tracker, const at::Tensor & found_inf, double scale_growth_factor, double scale_backoff_factor, int64_t growth_interval, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto growth_tracker_meta = to_meta(growth_tracker);
        auto found_inf_meta = to_meta(found_inf);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_amp_update_scale_out::call(self_meta, growth_tracker_meta, found_inf_meta, scale_growth_factor, scale_backoff_factor, growth_interval, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor growth_tracker_;
      if (at::functionalization::impl::isFunctionalTensor(growth_tracker)) {
        at::functionalization::impl::sync(growth_tracker);
        growth_tracker_ = at::functionalization::impl::from_functional_tensor(growth_tracker);
      } else {
        growth_tracker_ = growth_tracker;
      }
      
      at::Tensor found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(growth_tracker) && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || found_inf.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(found_inf))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_amp_update_scale_out::call(self_, growth_tracker_, found_inf_, scale_growth_factor, scale_backoff_factor, growth_interval, out_);
         return out;;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_amp_update_scale::call(self_, growth_tracker_, found_inf_, scale_growth_factor, scale_backoff_factor, growth_interval);
        }
          at::functionalization::impl::propagate_xla_data(growth_tracker, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(growth_tracker, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(growth_tracker);
  at::functionalization::impl::sync(growth_tracker);
  at::functionalization::impl::propagate_xla_data(out, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _amp_update_scale_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Tensor & growth_tracker, const at::Tensor & found_inf, double scale_growth_factor, double scale_backoff_factor, int64_t growth_interval) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto growth_tracker_meta = to_meta(growth_tracker);
        auto found_inf_meta = to_meta(found_inf);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_amp_update_scale_::call(self_meta, growth_tracker_meta, found_inf_meta, scale_growth_factor, scale_backoff_factor, growth_interval);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor growth_tracker_;
      if (at::functionalization::impl::isFunctionalTensor(growth_tracker)) {
        at::functionalization::impl::sync(growth_tracker);
        growth_tracker_ = at::functionalization::impl::from_functional_tensor(growth_tracker);
      } else {
        growth_tracker_ = growth_tracker;
      }
      
      at::Tensor found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self) && at::functionalization::impl::isFunctionalTensor(growth_tracker))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || found_inf.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(found_inf))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_amp_update_scale_::call(self_, growth_tracker_, found_inf_, scale_growth_factor, scale_backoff_factor, growth_interval);
         return self;;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_amp_update_scale::call(self_, growth_tracker_, found_inf_, scale_growth_factor, scale_backoff_factor, growth_interval);
        }
          at::functionalization::impl::propagate_xla_data(self, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(self, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
  at::functionalization::impl::propagate_xla_data(growth_tracker, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(growth_tracker, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(growth_tracker);
  at::functionalization::impl::sync(growth_tracker);
    return self;
      }
    }

    void _foreach_atan_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_atan_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_atan_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_atan::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_atan_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_atan_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_atan_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_atan::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_erf_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_erf_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_erf_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_erf::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_erf_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_erf_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_erf_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_erf::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_erfc_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_erfc_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_erfc_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_erfc::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_erfc_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_erfc_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_erfc_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_erfc::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_lerp_out_List_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensors1, at::TensorList weights, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensors1_meta = to_meta(tensors1);
        auto weights_meta = to_meta(weights);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_lerp_List_out::call(self_meta, tensors1_meta, weights_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensors1_;
      if (at::functionalization::impl::isFunctionalTensor(tensors1)) {
        at::functionalization::impl::sync(tensors1);
        tensors1_ = at::functionalization::impl::from_functional_tensor(tensors1);
      } else {
        tensors1_ = tensors1.vec();
      }
      
      ::std::vector<at::Tensor> weights_;
      if (at::functionalization::impl::isFunctionalTensor(weights)) {
        at::functionalization::impl::sync(weights);
        weights_ = at::functionalization::impl::from_functional_tensor(weights);
      } else {
        weights_ = weights.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(tensors1) || at::functionalization::impl::isFunctionalTensor(weights))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_lerp_List_out::call(self_, tensors1_, weights_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_lerp_List::call(self_, tensors1_, weights_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_lerp__List(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensors1, at::TensorList weights) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensors1_meta = to_meta(tensors1);
        auto weights_meta = to_meta(weights);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_lerp__List::call(self_meta, tensors1_meta, weights_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensors1_;
      if (at::functionalization::impl::isFunctionalTensor(tensors1)) {
        at::functionalization::impl::sync(tensors1);
        tensors1_ = at::functionalization::impl::from_functional_tensor(tensors1);
      } else {
        tensors1_ = tensors1.vec();
      }
      
      ::std::vector<at::Tensor> weights_;
      if (at::functionalization::impl::isFunctionalTensor(weights)) {
        at::functionalization::impl::sync(weights);
        weights_ = at::functionalization::impl::from_functional_tensor(weights);
      } else {
        weights_ = weights.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors1) || at::functionalization::impl::isFunctionalTensor(weights))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_lerp__List::call(self_, tensors1_, weights_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_lerp_List::call(self_, tensors1_, weights_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_lerp_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensors1, const at::Scalar & weight, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensors1_meta = to_meta(tensors1);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_lerp_Scalar_out::call(self_meta, tensors1_meta, weight, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensors1_;
      if (at::functionalization::impl::isFunctionalTensor(tensors1)) {
        at::functionalization::impl::sync(tensors1);
        tensors1_ = at::functionalization::impl::from_functional_tensor(tensors1);
      } else {
        tensors1_ = tensors1.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(tensors1))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_lerp_Scalar_out::call(self_, tensors1_, weight, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_lerp_Scalar::call(self_, tensors1_, weight);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_lerp__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensors1, const at::Scalar & weight) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensors1_meta = to_meta(tensors1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_lerp__Scalar::call(self_meta, tensors1_meta, weight);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensors1_;
      if (at::functionalization::impl::isFunctionalTensor(tensors1)) {
        at::functionalization::impl::sync(tensors1);
        tensors1_ = at::functionalization::impl::from_functional_tensor(tensors1);
      } else {
        tensors1_ = tensors1.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors1))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_lerp__Scalar::call(self_, tensors1_, weight);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_lerp_Scalar::call(self_, tensors1_, weight);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_lgamma_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_lgamma_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_lgamma_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_lgamma::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_lgamma_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_lgamma_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_lgamma_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_lgamma::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_log_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_log_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_log_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_log::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_log_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_log_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_log_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_log::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_sinh_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sinh_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sinh_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sinh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_sinh_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sinh_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sinh_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sinh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    at::Tensor & _convert_indices_from_coo_to_csr_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t size, bool out_int32, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_convert_indices_from_coo_to_csr_out::call(self_meta, size, out_int32, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_convert_indices_from_coo_to_csr_out::call(self_, size, out_int32, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_convert_indices_from_coo_to_csr::call(self_, size, out_int32);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & multi_margin_loss_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto weight_meta = to_meta(weight);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::multi_margin_loss_backward_grad_input::call(grad_output_meta, self_meta, target_meta, p, margin, weight_meta, reduction, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::multi_margin_loss_backward_grad_input::call(grad_output_, self_, target_, p, margin, weight_, reduction, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::multi_margin_loss_backward::call(grad_output_, self_, target_, p, margin, weight_, reduction);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> nll_loss_forward_out_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & output, at::Tensor & total_weight) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto weight_meta = to_meta(weight);
        auto output_meta = to_meta(output);
        auto total_weight_meta = to_meta(total_weight);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nll_loss_forward_output::call(self_meta, target_meta, weight_meta, reduction, ignore_index, output_meta, total_weight_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor total_weight_;
      if (at::functionalization::impl::isFunctionalTensor(total_weight)) {
        at::functionalization::impl::sync(total_weight);
        total_weight_ = at::functionalization::impl::from_functional_tensor(total_weight);
      } else {
        total_weight_ = total_weight;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(output) && at::functionalization::impl::isFunctionalTensor(total_weight))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::nll_loss_forward_output::call(self_, target_, weight_, reduction, ignore_index, output_, total_weight_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(output, total_weight);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nll_loss_forward::call(self_, target_, weight_, reduction, ignore_index);
        }
          at::functionalization::impl::propagate_xla_data(output, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(output, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(output);
  at::functionalization::impl::sync(output);
  at::functionalization::impl::propagate_xla_data(total_weight, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(total_weight, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(total_weight);
  at::functionalization::impl::sync(total_weight);
    return ::std::tuple<at::Tensor &,at::Tensor &>(output, total_weight);
      }
    }

    at::Tensor & hardswish_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hardswish_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hardswish_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hardswish::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hardswish_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hardswish_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hardswish_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hardswish::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & hardswish_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hardswish_backward_out::call(grad_output_meta, self_meta, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hardswish_backward_out::call(grad_output_, self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hardswish_backward::call(grad_output_, self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & softshrink_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::softshrink_backward_grad_input::call(grad_output_meta, self_meta, lambd, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::softshrink_backward_grad_input::call(grad_output_, self_, lambd, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::softshrink_backward::call(grad_output_, self_, lambd);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & mkldnn_adaptive_avg_pool2d_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_adaptive_avg_pool2d_backward_out::call(grad_output_meta, self_meta, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mkldnn_adaptive_avg_pool2d_backward_out::call(grad_output_, self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_adaptive_avg_pool2d_backward::call(grad_output_, self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _adaptive_avg_pool2d_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_adaptive_avg_pool2d_backward_out::call(grad_output_meta, self_meta, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_adaptive_avg_pool2d_backward_out::call(grad_output_, self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_adaptive_avg_pool2d_backward::call(grad_output_, self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & adaptive_avg_pool3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::adaptive_avg_pool3d_out::call(self_meta, output_size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::adaptive_avg_pool3d_out::call(self_, output_size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::adaptive_avg_pool3d::call(self_, output_size);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & adaptive_max_pool3d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::adaptive_max_pool3d_backward_grad_input::call(grad_output_meta, self_meta, indices_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::adaptive_max_pool3d_backward_grad_input::call(grad_output_, self_, indices_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::adaptive_max_pool3d_backward::call(grad_output_, self_, indices_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & avg_pool2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::avg_pool2d_out::call(self_meta, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::avg_pool2d_out::call(self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::avg_pool2d::call(self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & avg_pool3d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::avg_pool3d_backward_grad_input::call(grad_output_meta, self_meta, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::avg_pool3d_backward_grad_input::call(grad_output_, self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::avg_pool3d_backward::call(grad_output_, self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> fractional_max_pool2d_out_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto random_samples_meta = to_meta(random_samples);
        auto output_meta = to_meta(output);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fractional_max_pool2d_output::call(self_meta, kernel_size, output_size, random_samples_meta, output_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor random_samples_;
      if (at::functionalization::impl::isFunctionalTensor(random_samples)) {
        at::functionalization::impl::sync(random_samples);
        random_samples_ = at::functionalization::impl::from_functional_tensor(random_samples);
      } else {
        random_samples_ = random_samples;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(output) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || random_samples.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(random_samples))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::fractional_max_pool2d_output::call(self_, kernel_size, output_size, random_samples_, output_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(output, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fractional_max_pool2d::call(self_, kernel_size, output_size, random_samples_);
        }
          at::functionalization::impl::propagate_xla_data(output, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(output, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(output);
  at::functionalization::impl::sync(output);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(output, indices);
      }
    }

    at::Tensor & reflection_pad3d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::reflection_pad3d_backward_grad_input::call(grad_output_meta, self_meta, padding, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::reflection_pad3d_backward_grad_input::call(grad_output_, self_, padding, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::reflection_pad3d_backward::call(grad_output_, self_, padding);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & replication_pad2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::replication_pad2d_out::call(self_meta, padding, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::replication_pad2d_out::call(self_, padding, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::replication_pad2d::call(self_, padding);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _upsample_bilinear2d_aa_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_upsample_bilinear2d_aa_backward_grad_input::call(grad_output_meta, output_size, input_size, align_corners, scales_h, scales_w, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_upsample_bilinear2d_aa_backward_grad_input::call(grad_output_, output_size, input_size, align_corners, scales_h, scales_w, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_upsample_bilinear2d_aa_backward::call(grad_output_, output_size, input_size, align_corners, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & _upsample_bicubic2d_aa_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_upsample_bicubic2d_aa_out::call(self_meta, output_size, align_corners, scales_h, scales_w, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_upsample_bicubic2d_aa_out::call(self_, output_size, align_corners, scales_h, scales_w, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_upsample_bicubic2d_aa::call(self_, output_size, align_corners, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & upsample_trilinear3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::upsample_trilinear3d_out::call(self_meta, output_size, align_corners, scales_d, scales_h, scales_w, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::upsample_trilinear3d_out::call(self_, output_size, align_corners, scales_d, scales_h, scales_w, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::upsample_trilinear3d::call(self_, output_size, align_corners, scales_d, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _upsample_nearest_exact1d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_upsample_nearest_exact1d_backward_grad_input::call(grad_output_meta, output_size, input_size, scales, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_upsample_nearest_exact1d_backward_grad_input::call(grad_output_, output_size, input_size, scales, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_upsample_nearest_exact1d_backward::call(grad_output_, output_size, input_size, scales);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & upsample_nearest2d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::upsample_nearest2d_backward_grad_input::call(grad_output_meta, output_size, input_size, scales_h, scales_w, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::upsample_nearest2d_backward_grad_input::call(grad_output_, output_size, input_size, scales_h, scales_w, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::upsample_nearest2d_backward::call(grad_output_, output_size, input_size, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & tanh_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto output_meta = to_meta(output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::tanh_backward_grad_input::call(grad_output_meta, output_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::tanh_backward_grad_input::call(grad_output_, output_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::tanh_backward::call(grad_output_, output_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    const at::Tensor & _conv_depthwise2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, const at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_conv_depthwise2d_out::call(self_meta, weight_meta, kernel_size, bias_meta, stride, padding, dilation, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_conv_depthwise2d_out::call(self_, weight_, kernel_size, bias_, stride, padding, dilation, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_conv_depthwise2d::call(self_, weight_, kernel_size, bias_, stride, padding, dilation);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & col2im_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::col2im_out::call(self_meta, output_size, kernel_size, dilation, padding, stride, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::col2im_out::call(self_, output_size, kernel_size, dilation, padding, stride, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::col2im::call(self_, output_size, kernel_size, dilation, padding, stride);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & column_stack_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::column_stack_out::call(tensors_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::column_stack_out::call(tensors_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::column_stack::call(tensors_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & im2col_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::im2col_out::call(self_meta, kernel_size, dilation, padding, stride, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::im2col_out::call(self_, kernel_size, dilation, padding, stride, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::im2col::call(self_, kernel_size, dilation, padding, stride);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & isinf_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::isinf_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::isinf_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::isinf::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & isneginf_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::isneginf_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::isneginf_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::isneginf::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_expm1_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_expm1_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_expm1_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_expm1::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_erf_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_erf_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_erf_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_erf::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_logsumexp_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_logsumexp_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_logsumexp_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_logsumexp::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_log1p_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_log1p_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_log1p_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_log1p::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_gammaincc_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_gammaincc_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_gammaincc_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_gammaincc::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_multigammaln_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t p, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_multigammaln_out::call(self_meta, p, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_multigammaln_out::call(self_, p, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_multigammaln::call(self_, p);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_rfft2_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_rfft2_out::call(self_meta, s, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_rfft2_out::call(self_, s, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_rfft2::call(self_, s, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_irfft2_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_irfft2_out::call(self_meta, s, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_irfft2_out::call(self_, s, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_irfft2::call(self_, s, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & fft_ihfft2_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_ihfft2_out::call(self_meta, s, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_ihfft2_out::call(self_, s, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_ihfft2::call(self_, s, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_fftn_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_fftn_out::call(self_meta, s, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_fftn_out::call(self_, s, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_fftn::call(self_, s, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_irfftn_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_irfftn_out::call(self_meta, s, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_irfftn_out::call(self_, s, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_irfftn::call(self_, s, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> linalg_lu_factor_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, bool pivot, at::Tensor & LU, at::Tensor & pivots) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto LU_meta = to_meta(LU);
        auto pivots_meta = to_meta(pivots);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_lu_factor_out::call(A_meta, pivot, LU_meta, pivots_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor LU_;
      if (at::functionalization::impl::isFunctionalTensor(LU)) {
        at::functionalization::impl::sync(LU);
        LU_ = at::functionalization::impl::from_functional_tensor(LU);
      } else {
        LU_ = LU;
      }
      
      at::Tensor pivots_;
      if (at::functionalization::impl::isFunctionalTensor(pivots)) {
        at::functionalization::impl::sync(pivots);
        pivots_ = at::functionalization::impl::from_functional_tensor(pivots);
      } else {
        pivots_ = pivots;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(LU) && at::functionalization::impl::isFunctionalTensor(pivots))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::linalg_lu_factor_out::call(A_, pivot, LU_, pivots_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(LU, pivots);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_lu_factor::call(A_, pivot);
        }
          at::functionalization::impl::propagate_xla_data(LU, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(LU, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(LU);
  at::functionalization::impl::sync(LU);
  at::functionalization::impl::propagate_xla_data(pivots, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(pivots, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(pivots);
  at::functionalization::impl::sync(pivots);
    return ::std::tuple<at::Tensor &,at::Tensor &>(LU, pivots);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_lu_factor_ex_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, bool pivot, bool check_errors, at::Tensor & LU, at::Tensor & pivots, at::Tensor & info) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto LU_meta = to_meta(LU);
        auto pivots_meta = to_meta(pivots);
        auto info_meta = to_meta(info);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_lu_factor_ex_out::call(A_meta, pivot, check_errors, LU_meta, pivots_meta, info_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor LU_;
      if (at::functionalization::impl::isFunctionalTensor(LU)) {
        at::functionalization::impl::sync(LU);
        LU_ = at::functionalization::impl::from_functional_tensor(LU);
      } else {
        LU_ = LU;
      }
      
      at::Tensor pivots_;
      if (at::functionalization::impl::isFunctionalTensor(pivots)) {
        at::functionalization::impl::sync(pivots);
        pivots_ = at::functionalization::impl::from_functional_tensor(pivots);
      } else {
        pivots_ = pivots;
      }
      
      at::Tensor info_;
      if (at::functionalization::impl::isFunctionalTensor(info)) {
        at::functionalization::impl::sync(info);
        info_ = at::functionalization::impl::from_functional_tensor(info);
      } else {
        info_ = info;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(LU) && at::functionalization::impl::isFunctionalTensor(pivots) && at::functionalization::impl::isFunctionalTensor(info))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::linalg_lu_factor_ex_out::call(A_, pivot, check_errors, LU_, pivots_, info_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(LU, pivots, info);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_lu_factor_ex::call(A_, pivot, check_errors);
        }
          at::functionalization::impl::propagate_xla_data(LU, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(LU, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(LU);
  at::functionalization::impl::sync(LU);
  at::functionalization::impl::propagate_xla_data(pivots, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(pivots, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(pivots);
  at::functionalization::impl::sync(pivots);
  at::functionalization::impl::propagate_xla_data(info, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(info, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(info);
  at::functionalization::impl::sync(info);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(LU, pivots, info);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> linalg_ldl_factor_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool hermitian, at::Tensor & LD, at::Tensor & pivots) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto LD_meta = to_meta(LD);
        auto pivots_meta = to_meta(pivots);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_ldl_factor_out::call(self_meta, hermitian, LD_meta, pivots_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor LD_;
      if (at::functionalization::impl::isFunctionalTensor(LD)) {
        at::functionalization::impl::sync(LD);
        LD_ = at::functionalization::impl::from_functional_tensor(LD);
      } else {
        LD_ = LD;
      }
      
      at::Tensor pivots_;
      if (at::functionalization::impl::isFunctionalTensor(pivots)) {
        at::functionalization::impl::sync(pivots);
        pivots_ = at::functionalization::impl::from_functional_tensor(pivots);
      } else {
        pivots_ = pivots;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(LD) && at::functionalization::impl::isFunctionalTensor(pivots))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::linalg_ldl_factor_out::call(self_, hermitian, LD_, pivots_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(LD, pivots);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_ldl_factor::call(self_, hermitian);
        }
          at::functionalization::impl::propagate_xla_data(LD, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(LD, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(LD);
  at::functionalization::impl::sync(LD);
  at::functionalization::impl::propagate_xla_data(pivots, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(pivots, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(pivots);
  at::functionalization::impl::sync(pivots);
    return ::std::tuple<at::Tensor &,at::Tensor &>(LD, pivots);
      }
    }

    at::Tensor & linalg_ldl_solve_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & LD, const at::Tensor & pivots, const at::Tensor & B, bool hermitian, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto LD_meta = to_meta(LD);
        auto pivots_meta = to_meta(pivots);
        auto B_meta = to_meta(B);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_ldl_solve_out::call(LD_meta, pivots_meta, B_meta, hermitian, out_meta);
      }
      
      at::Tensor LD_;
      if (at::functionalization::impl::isFunctionalTensor(LD)) {
        at::functionalization::impl::sync(LD);
        LD_ = at::functionalization::impl::from_functional_tensor(LD);
      } else {
        LD_ = LD;
      }
      
      at::Tensor pivots_;
      if (at::functionalization::impl::isFunctionalTensor(pivots)) {
        at::functionalization::impl::sync(pivots);
        pivots_ = at::functionalization::impl::from_functional_tensor(pivots);
      } else {
        pivots_ = pivots;
      }
      
      at::Tensor B_;
      if (at::functionalization::impl::isFunctionalTensor(B)) {
        at::functionalization::impl::sync(B);
        B_ = at::functionalization::impl::from_functional_tensor(B);
      } else {
        B_ = B;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || LD.device().type() == c10::DeviceType::XLA || pivots.device().type() == c10::DeviceType::XLA || B.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(LD) || at::functionalization::impl::isFunctionalTensor(pivots) || at::functionalization::impl::isFunctionalTensor(B))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_ldl_solve_out::call(LD_, pivots_, B_, hermitian, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_ldl_solve::call(LD_, pivots_, B_, hermitian);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> _linalg_slogdet_out_sign(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, at::Tensor & sign, at::Tensor & logabsdet, at::Tensor & LU, at::Tensor & pivots) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto sign_meta = to_meta(sign);
        auto logabsdet_meta = to_meta(logabsdet);
        auto LU_meta = to_meta(LU);
        auto pivots_meta = to_meta(pivots);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_linalg_slogdet_sign::call(A_meta, sign_meta, logabsdet_meta, LU_meta, pivots_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor sign_;
      if (at::functionalization::impl::isFunctionalTensor(sign)) {
        at::functionalization::impl::sync(sign);
        sign_ = at::functionalization::impl::from_functional_tensor(sign);
      } else {
        sign_ = sign;
      }
      
      at::Tensor logabsdet_;
      if (at::functionalization::impl::isFunctionalTensor(logabsdet)) {
        at::functionalization::impl::sync(logabsdet);
        logabsdet_ = at::functionalization::impl::from_functional_tensor(logabsdet);
      } else {
        logabsdet_ = logabsdet;
      }
      
      at::Tensor LU_;
      if (at::functionalization::impl::isFunctionalTensor(LU)) {
        at::functionalization::impl::sync(LU);
        LU_ = at::functionalization::impl::from_functional_tensor(LU);
      } else {
        LU_ = LU;
      }
      
      at::Tensor pivots_;
      if (at::functionalization::impl::isFunctionalTensor(pivots)) {
        at::functionalization::impl::sync(pivots);
        pivots_ = at::functionalization::impl::from_functional_tensor(pivots);
      } else {
        pivots_ = pivots;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(sign) && at::functionalization::impl::isFunctionalTensor(logabsdet) && at::functionalization::impl::isFunctionalTensor(LU) && at::functionalization::impl::isFunctionalTensor(pivots))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_linalg_slogdet_sign::call(A_, sign_, logabsdet_, LU_, pivots_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(sign, logabsdet, LU, pivots);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_linalg_slogdet::call(A_);
        }
          at::functionalization::impl::propagate_xla_data(sign, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(sign, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(sign);
  at::functionalization::impl::sync(sign);
  at::functionalization::impl::propagate_xla_data(logabsdet, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(logabsdet, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(logabsdet);
  at::functionalization::impl::sync(logabsdet);
  at::functionalization::impl::propagate_xla_data(LU, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(LU, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(LU);
  at::functionalization::impl::sync(LU);
  at::functionalization::impl::propagate_xla_data(pivots, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(pivots, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(pivots);
  at::functionalization::impl::sync(pivots);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(sign, logabsdet, LU, pivots);
      }
    }

    at::Tensor & linalg_eigvals_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_eigvals_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_eigvals_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_eigvals::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> linalg_eigh_out_eigvals(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view UPLO, at::Tensor & eigvals, at::Tensor & eigvecs) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto eigvals_meta = to_meta(eigvals);
        auto eigvecs_meta = to_meta(eigvecs);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_eigh_eigvals::call(self_meta, UPLO, eigvals_meta, eigvecs_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor eigvals_;
      if (at::functionalization::impl::isFunctionalTensor(eigvals)) {
        at::functionalization::impl::sync(eigvals);
        eigvals_ = at::functionalization::impl::from_functional_tensor(eigvals);
      } else {
        eigvals_ = eigvals;
      }
      
      at::Tensor eigvecs_;
      if (at::functionalization::impl::isFunctionalTensor(eigvecs)) {
        at::functionalization::impl::sync(eigvecs);
        eigvecs_ = at::functionalization::impl::from_functional_tensor(eigvecs);
      } else {
        eigvecs_ = eigvecs;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(eigvals) && at::functionalization::impl::isFunctionalTensor(eigvecs))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::linalg_eigh_eigvals::call(self_, UPLO, eigvals_, eigvecs_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(eigvals, eigvecs);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_eigh::call(self_, UPLO);
        }
          at::functionalization::impl::propagate_xla_data(eigvals, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(eigvals, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(eigvals);
  at::functionalization::impl::sync(eigvals);
  at::functionalization::impl::propagate_xla_data(eigvecs, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(eigvecs, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(eigvecs);
  at::functionalization::impl::sync(eigvecs);
    return ::std::tuple<at::Tensor &,at::Tensor &>(eigvals, eigvecs);
      }
    }

    at::Tensor & linalg_householder_product_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & tau, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto tau_meta = to_meta(tau);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_householder_product_out::call(input_meta, tau_meta, out_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor tau_;
      if (at::functionalization::impl::isFunctionalTensor(tau)) {
        at::functionalization::impl::sync(tau);
        tau_ = at::functionalization::impl::from_functional_tensor(tau);
      } else {
        tau_ = tau;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || tau.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(tau))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_householder_product_out::call(input_, tau_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_householder_product::call(input_, tau_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & ger_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto vec2_meta = to_meta(vec2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ger_out::call(self_meta, vec2_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor vec2_;
      if (at::functionalization::impl::isFunctionalTensor(vec2)) {
        at::functionalization::impl::sync(vec2);
        vec2_ = at::functionalization::impl::from_functional_tensor(vec2);
      } else {
        vec2_ = vec2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || vec2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(vec2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ger_out::call(self_, vec2_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ger::call(self_, vec2_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_norm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_norm_out::call(self_meta, ord, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_norm_out::call(self_, ord, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_norm::call(self_, ord, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_norm_out_ord_str_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_norm_ord_str_out::call(self_meta, ord, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_norm_ord_str_out::call(self_, ord, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_norm_ord_str::call(self_, ord, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_vector_norm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_vector_norm_out::call(self_meta, ord, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_vector_norm_out::call(self_, ord, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_vector_norm::call(self_, ord, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> _linalg_solve_ex_out_result(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, const at::Tensor & B, bool left, bool check_errors, at::Tensor & result, at::Tensor & LU, at::Tensor & pivots, at::Tensor & info) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto B_meta = to_meta(B);
        auto result_meta = to_meta(result);
        auto LU_meta = to_meta(LU);
        auto pivots_meta = to_meta(pivots);
        auto info_meta = to_meta(info);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_linalg_solve_ex_result::call(A_meta, B_meta, left, check_errors, result_meta, LU_meta, pivots_meta, info_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor B_;
      if (at::functionalization::impl::isFunctionalTensor(B)) {
        at::functionalization::impl::sync(B);
        B_ = at::functionalization::impl::from_functional_tensor(B);
      } else {
        B_ = B;
      }
      
      at::Tensor result_;
      if (at::functionalization::impl::isFunctionalTensor(result)) {
        at::functionalization::impl::sync(result);
        result_ = at::functionalization::impl::from_functional_tensor(result);
      } else {
        result_ = result;
      }
      
      at::Tensor LU_;
      if (at::functionalization::impl::isFunctionalTensor(LU)) {
        at::functionalization::impl::sync(LU);
        LU_ = at::functionalization::impl::from_functional_tensor(LU);
      } else {
        LU_ = LU;
      }
      
      at::Tensor pivots_;
      if (at::functionalization::impl::isFunctionalTensor(pivots)) {
        at::functionalization::impl::sync(pivots);
        pivots_ = at::functionalization::impl::from_functional_tensor(pivots);
      } else {
        pivots_ = pivots;
      }
      
      at::Tensor info_;
      if (at::functionalization::impl::isFunctionalTensor(info)) {
        at::functionalization::impl::sync(info);
        info_ = at::functionalization::impl::from_functional_tensor(info);
      } else {
        info_ = info;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(result) && at::functionalization::impl::isFunctionalTensor(LU) && at::functionalization::impl::isFunctionalTensor(pivots) && at::functionalization::impl::isFunctionalTensor(info))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA || B.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A) || at::functionalization::impl::isFunctionalTensor(B))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_linalg_solve_ex_result::call(A_, B_, left, check_errors, result_, LU_, pivots_, info_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(result, LU, pivots, info);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_linalg_solve_ex::call(A_, B_, left, check_errors);
        }
          at::functionalization::impl::propagate_xla_data(result, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(result, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(result);
  at::functionalization::impl::sync(result);
  at::functionalization::impl::propagate_xla_data(LU, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(LU, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(LU);
  at::functionalization::impl::sync(LU);
  at::functionalization::impl::propagate_xla_data(pivots, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(pivots, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(pivots);
  at::functionalization::impl::sync(pivots);
  at::functionalization::impl::propagate_xla_data(info, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(info, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(info);
  at::functionalization::impl::sync(info);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(result, LU, pivots, info);
      }
    }

    at::Tensor & linalg_solve_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, const at::Tensor & B, bool left, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto B_meta = to_meta(B);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_solve_out::call(A_meta, B_meta, left, out_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor B_;
      if (at::functionalization::impl::isFunctionalTensor(B)) {
        at::functionalization::impl::sync(B);
        B_ = at::functionalization::impl::from_functional_tensor(B);
      } else {
        B_ = B;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA || B.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A) || at::functionalization::impl::isFunctionalTensor(B))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_solve_out::call(A_, B_, left, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_solve::call(A_, B_, left);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_multi_dot_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_multi_dot_out::call(tensors_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_multi_dot_out::call(tensors_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_multi_dot::call(tensors_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _test_optional_filled_intlist_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & values, at::OptionalIntArrayRef addends, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto values_meta = to_meta(values);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_test_optional_filled_intlist_out::call(values_meta, addends, out_meta);
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || values.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(values))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_test_optional_filled_intlist_out::call(values_, addends, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_test_optional_filled_intlist::call(values_, addends);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _test_autograd_multiple_dispatch_out_fullcoverage_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_test_autograd_multiple_dispatch_fullcoverage_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_test_autograd_multiple_dispatch_fullcoverage_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_test_autograd_multiple_dispatch_fullcoverage::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _test_autograd_multiple_dispatch_view_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_test_autograd_multiple_dispatch_view_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_test_autograd_multiple_dispatch_view_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_test_autograd_multiple_dispatch_view_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & segment_reduce_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, c10::string_view reduce, const c10::optional<at::Tensor> & lengths, const c10::optional<at::Tensor> & indices, const c10::optional<at::Tensor> & offsets, int64_t axis, bool unsafe, const c10::optional<at::Scalar> & initial, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto data_meta = to_meta(data);
        auto lengths_meta = to_meta(lengths);
        auto indices_meta = to_meta(indices);
        auto offsets_meta = to_meta(offsets);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::segment_reduce_out::call(data_meta, reduce, lengths_meta, indices_meta, offsets_meta, axis, unsafe, initial, out_meta);
      }
      
      at::Tensor data_;
      if (at::functionalization::impl::isFunctionalTensor(data)) {
        at::functionalization::impl::sync(data);
        data_ = at::functionalization::impl::from_functional_tensor(data);
      } else {
        data_ = data;
      }
      
      c10::optional<at::Tensor> lengths_;
      if (at::functionalization::impl::isFunctionalTensor(lengths)) {
        at::functionalization::impl::sync(lengths);
        lengths_ = at::functionalization::impl::from_functional_tensor(lengths);
      } else {
        lengths_ = lengths;
      }
      
      c10::optional<at::Tensor> indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      c10::optional<at::Tensor> offsets_;
      if (at::functionalization::impl::isFunctionalTensor(offsets)) {
        at::functionalization::impl::sync(offsets);
        offsets_ = at::functionalization::impl::from_functional_tensor(offsets);
      } else {
        offsets_ = offsets;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || data.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(data) || at::functionalization::impl::isFunctionalTensor(lengths) || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(offsets))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::segment_reduce_out::call(data_, reduce, lengths_, indices_, offsets_, axis, unsafe, initial, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::segment_reduce::call(data_, reduce, lengths_, indices_, offsets_, axis, unsafe, initial);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _nested_tensor_from_tensor_list_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList list, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto list_meta = to_meta(list);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_nested_tensor_from_tensor_list_out::call(list_meta, dtype, layout, device, pin_memory, out_meta);
      }
      
      ::std::vector<at::Tensor> list_;
      if (at::functionalization::impl::isFunctionalTensor(list)) {
        at::functionalization::impl::sync(list);
        list_ = at::functionalization::impl::from_functional_tensor(list);
      } else {
        list_ = list.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(list))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_nested_tensor_from_tensor_list_out::call(list_, dtype, layout, device, pin_memory, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_nested_tensor_from_tensor_list::call(list_, dtype, layout, device, pin_memory);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & diagonal_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::diagonal_copy_out::call(self_meta, offset, dim1, dim2, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::diagonal_copy_out::call(self_, offset, dim1, dim2, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::diagonal_copy::call(self_, offset, dim1, dim2);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & detach_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::detach_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::detach_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::detach_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & slice_copy_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<c10::SymInt> start, c10::optional<c10::SymInt> end, c10::SymInt step, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::slice_copy_Tensor_out::call(self_meta, dim, start, end, step, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::slice_copy_Tensor_out::call(self_, dim, start, end, step, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::slice_copy_Tensor::call(self_, dim, start, end, step);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & transpose_copy_out_int_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim0, int64_t dim1, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::transpose_copy_int_out::call(self_meta, dim0, dim1, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::transpose_copy_int_out::call(self_, dim0, dim1, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::transpose_copy_int::call(self_, dim0, dim1);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & indices_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::indices_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::indices_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::indices_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & row_indices_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::row_indices_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::row_indices_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::row_indices_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _triton_multi_head_attention_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, int64_t embed_dim, int64_t num_head, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, const c10::optional<at::Tensor> & mask, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto query_meta = to_meta(query);
        auto key_meta = to_meta(key);
        auto value_meta = to_meta(value);
        auto qkv_weight_meta = to_meta(qkv_weight);
        auto qkv_bias_meta = to_meta(qkv_bias);
        auto proj_weight_meta = to_meta(proj_weight);
        auto proj_bias_meta = to_meta(proj_bias);
        auto mask_meta = to_meta(mask);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_triton_multi_head_attention_out::call(query_meta, key_meta, value_meta, embed_dim, num_head, qkv_weight_meta, qkv_bias_meta, proj_weight_meta, proj_bias_meta, mask_meta, out_meta);
      }
      
      at::Tensor query_;
      if (at::functionalization::impl::isFunctionalTensor(query)) {
        at::functionalization::impl::sync(query);
        query_ = at::functionalization::impl::from_functional_tensor(query);
      } else {
        query_ = query;
      }
      
      at::Tensor key_;
      if (at::functionalization::impl::isFunctionalTensor(key)) {
        at::functionalization::impl::sync(key);
        key_ = at::functionalization::impl::from_functional_tensor(key);
      } else {
        key_ = key;
      }
      
      at::Tensor value_;
      if (at::functionalization::impl::isFunctionalTensor(value)) {
        at::functionalization::impl::sync(value);
        value_ = at::functionalization::impl::from_functional_tensor(value);
      } else {
        value_ = value;
      }
      
      at::Tensor qkv_weight_;
      if (at::functionalization::impl::isFunctionalTensor(qkv_weight)) {
        at::functionalization::impl::sync(qkv_weight);
        qkv_weight_ = at::functionalization::impl::from_functional_tensor(qkv_weight);
      } else {
        qkv_weight_ = qkv_weight;
      }
      
      at::Tensor qkv_bias_;
      if (at::functionalization::impl::isFunctionalTensor(qkv_bias)) {
        at::functionalization::impl::sync(qkv_bias);
        qkv_bias_ = at::functionalization::impl::from_functional_tensor(qkv_bias);
      } else {
        qkv_bias_ = qkv_bias;
      }
      
      at::Tensor proj_weight_;
      if (at::functionalization::impl::isFunctionalTensor(proj_weight)) {
        at::functionalization::impl::sync(proj_weight);
        proj_weight_ = at::functionalization::impl::from_functional_tensor(proj_weight);
      } else {
        proj_weight_ = proj_weight;
      }
      
      at::Tensor proj_bias_;
      if (at::functionalization::impl::isFunctionalTensor(proj_bias)) {
        at::functionalization::impl::sync(proj_bias);
        proj_bias_ = at::functionalization::impl::from_functional_tensor(proj_bias);
      } else {
        proj_bias_ = proj_bias;
      }
      
      c10::optional<at::Tensor> mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || query.device().type() == c10::DeviceType::XLA || key.device().type() == c10::DeviceType::XLA || value.device().type() == c10::DeviceType::XLA || qkv_weight.device().type() == c10::DeviceType::XLA || qkv_bias.device().type() == c10::DeviceType::XLA || proj_weight.device().type() == c10::DeviceType::XLA || proj_bias.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(query) || at::functionalization::impl::isFunctionalTensor(key) || at::functionalization::impl::isFunctionalTensor(value) || at::functionalization::impl::isFunctionalTensor(qkv_weight) || at::functionalization::impl::isFunctionalTensor(qkv_bias) || at::functionalization::impl::isFunctionalTensor(proj_weight) || at::functionalization::impl::isFunctionalTensor(proj_bias) || at::functionalization::impl::isFunctionalTensor(mask))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_triton_multi_head_attention_out::call(query_, key_, value_, embed_dim, num_head, qkv_weight_, qkv_bias_, proj_weight_, proj_bias_, mask_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_triton_multi_head_attention::call(query_, key_, value_, embed_dim, num_head, qkv_weight_, qkv_bias_, proj_weight_, proj_bias_, mask_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_bessel_j1_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_bessel_j1_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_bessel_j1_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_bessel_j1::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_bessel_y1_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_bessel_y1_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_bessel_y1_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_bessel_y1::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_legendre_polynomial_p_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_legendre_polynomial_p_out::call(x_meta, n_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_legendre_polynomial_p_out::call(x_, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_legendre_polynomial_p::call(x_, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_legendre_polynomial_p_out_x_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_legendre_polynomial_p_x_scalar_out::call(x, n_meta, out_meta);
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_legendre_polynomial_p_x_scalar_out::call(x, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_legendre_polynomial_p_x_scalar::call(x, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_legendre_polynomial_p_out_n_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_legendre_polynomial_p_n_scalar_out::call(x_meta, n, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_legendre_polynomial_p_n_scalar_out::call(x_, n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_legendre_polynomial_p_n_scalar::call(x_, n);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_modified_bessel_i0_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_modified_bessel_i0_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_modified_bessel_i0_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_modified_bessel_i0::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_shifted_chebyshev_polynomial_t_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_shifted_chebyshev_polynomial_t_out::call(x_meta, n_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_shifted_chebyshev_polynomial_t_out::call(x_, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_shifted_chebyshev_polynomial_t::call(x_, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_shifted_chebyshev_polynomial_t_out_x_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_shifted_chebyshev_polynomial_t_x_scalar_out::call(x, n_meta, out_meta);
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_shifted_chebyshev_polynomial_t_x_scalar_out::call(x, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_shifted_chebyshev_polynomial_t_x_scalar::call(x, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_shifted_chebyshev_polynomial_t_out_n_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_shifted_chebyshev_polynomial_t_n_scalar_out::call(x_meta, n, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_shifted_chebyshev_polynomial_t_n_scalar_out::call(x_, n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_shifted_chebyshev_polynomial_t_n_scalar::call(x_, n);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_shifted_chebyshev_polynomial_u_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_shifted_chebyshev_polynomial_u_out::call(x_meta, n_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_shifted_chebyshev_polynomial_u_out::call(x_, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_shifted_chebyshev_polynomial_u::call(x_, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_shifted_chebyshev_polynomial_u_out_x_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_shifted_chebyshev_polynomial_u_x_scalar_out::call(x, n_meta, out_meta);
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_shifted_chebyshev_polynomial_u_x_scalar_out::call(x, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_shifted_chebyshev_polynomial_u_x_scalar::call(x, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_shifted_chebyshev_polynomial_u_out_n_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_shifted_chebyshev_polynomial_u_n_scalar_out::call(x_meta, n, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_shifted_chebyshev_polynomial_u_n_scalar_out::call(x_, n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_shifted_chebyshev_polynomial_u_n_scalar::call(x_, n);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_shifted_chebyshev_polynomial_w_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_shifted_chebyshev_polynomial_w_out::call(x_meta, n_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_shifted_chebyshev_polynomial_w_out::call(x_, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_shifted_chebyshev_polynomial_w::call(x_, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_shifted_chebyshev_polynomial_w_out_x_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_shifted_chebyshev_polynomial_w_x_scalar_out::call(x, n_meta, out_meta);
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_shifted_chebyshev_polynomial_w_x_scalar_out::call(x, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_shifted_chebyshev_polynomial_w_x_scalar::call(x, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_shifted_chebyshev_polynomial_w_out_n_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_shifted_chebyshev_polynomial_w_n_scalar_out::call(x_meta, n, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_shifted_chebyshev_polynomial_w_n_scalar_out::call(x_, n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_shifted_chebyshev_polynomial_w_n_scalar::call(x_, n);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    void _fused_adam_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grads_meta = to_meta(grads);
        auto exp_avgs_meta = to_meta(exp_avgs);
        auto exp_avg_sqs_meta = to_meta(exp_avg_sqs);
        auto max_exp_avg_sqs_meta = to_meta(max_exp_avg_sqs);
        auto state_steps_meta = to_meta(state_steps);
        auto grad_scale_meta = to_meta(grad_scale);
        auto found_inf_meta = to_meta(found_inf);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_adam_out::call(self_meta, grads_meta, exp_avgs_meta, exp_avg_sqs_meta, max_exp_avg_sqs_meta, state_steps_meta, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_meta, found_inf_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> grads_;
      if (at::functionalization::impl::isFunctionalTensor(grads)) {
        at::functionalization::impl::sync(grads);
        grads_ = at::functionalization::impl::from_functional_tensor(grads);
      } else {
        grads_ = grads.vec();
      }
      
      ::std::vector<at::Tensor> exp_avgs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avgs)) {
        at::functionalization::impl::sync(exp_avgs);
        exp_avgs_ = at::functionalization::impl::from_functional_tensor(exp_avgs);
      } else {
        exp_avgs_ = exp_avgs.vec();
      }
      
      ::std::vector<at::Tensor> exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avg_sqs)) {
        at::functionalization::impl::sync(exp_avg_sqs);
        exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(exp_avg_sqs);
      } else {
        exp_avg_sqs_ = exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> max_exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs)) {
        at::functionalization::impl::sync(max_exp_avg_sqs);
        max_exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(max_exp_avg_sqs);
      } else {
        max_exp_avg_sqs_ = max_exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> state_steps_;
      if (at::functionalization::impl::isFunctionalTensor(state_steps)) {
        at::functionalization::impl::sync(state_steps);
        state_steps_ = at::functionalization::impl::from_functional_tensor(state_steps);
      } else {
        state_steps_ = state_steps.vec();
      }
      
      c10::optional<at::Tensor> grad_scale_;
      if (at::functionalization::impl::isFunctionalTensor(grad_scale)) {
        at::functionalization::impl::sync(grad_scale);
        grad_scale_ = at::functionalization::impl::from_functional_tensor(grad_scale);
      } else {
        grad_scale_ = grad_scale;
      }
      
      c10::optional<at::Tensor> found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grads) && at::functionalization::impl::isFunctionalTensor(exp_avgs) && at::functionalization::impl::isFunctionalTensor(exp_avg_sqs) && at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs) && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(state_steps) || at::functionalization::impl::isFunctionalTensor(grad_scale) || at::functionalization::impl::isFunctionalTensor(found_inf))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_fused_adam_out::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_, out_);
         ;
        }
      } else {
        ::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_adam::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_);
        }
          at::functionalization::impl::propagate_xla_data(grads, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(grads, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(grads);
  at::functionalization::impl::sync(grads);
  at::functionalization::impl::propagate_xla_data(exp_avgs, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(exp_avgs, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(exp_avgs);
  at::functionalization::impl::sync(exp_avgs);
  at::functionalization::impl::propagate_xla_data(exp_avg_sqs, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(exp_avg_sqs, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(exp_avg_sqs);
  at::functionalization::impl::sync(exp_avg_sqs);
  at::functionalization::impl::propagate_xla_data(max_exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(max_exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(max_exp_avg_sqs);
  at::functionalization::impl::sync(max_exp_avg_sqs);
  at::functionalization::impl::propagate_xla_data(out, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _fused_adam_(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grads_meta = to_meta(grads);
        auto exp_avgs_meta = to_meta(exp_avgs);
        auto exp_avg_sqs_meta = to_meta(exp_avg_sqs);
        auto max_exp_avg_sqs_meta = to_meta(max_exp_avg_sqs);
        auto state_steps_meta = to_meta(state_steps);
        auto grad_scale_meta = to_meta(grad_scale);
        auto found_inf_meta = to_meta(found_inf);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_adam_::call(self_meta, grads_meta, exp_avgs_meta, exp_avg_sqs_meta, max_exp_avg_sqs_meta, state_steps_meta, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_meta, found_inf_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> grads_;
      if (at::functionalization::impl::isFunctionalTensor(grads)) {
        at::functionalization::impl::sync(grads);
        grads_ = at::functionalization::impl::from_functional_tensor(grads);
      } else {
        grads_ = grads.vec();
      }
      
      ::std::vector<at::Tensor> exp_avgs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avgs)) {
        at::functionalization::impl::sync(exp_avgs);
        exp_avgs_ = at::functionalization::impl::from_functional_tensor(exp_avgs);
      } else {
        exp_avgs_ = exp_avgs.vec();
      }
      
      ::std::vector<at::Tensor> exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avg_sqs)) {
        at::functionalization::impl::sync(exp_avg_sqs);
        exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(exp_avg_sqs);
      } else {
        exp_avg_sqs_ = exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> max_exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs)) {
        at::functionalization::impl::sync(max_exp_avg_sqs);
        max_exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(max_exp_avg_sqs);
      } else {
        max_exp_avg_sqs_ = max_exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> state_steps_;
      if (at::functionalization::impl::isFunctionalTensor(state_steps)) {
        at::functionalization::impl::sync(state_steps);
        state_steps_ = at::functionalization::impl::from_functional_tensor(state_steps);
      } else {
        state_steps_ = state_steps.vec();
      }
      
      c10::optional<at::Tensor> grad_scale_;
      if (at::functionalization::impl::isFunctionalTensor(grad_scale)) {
        at::functionalization::impl::sync(grad_scale);
        grad_scale_ = at::functionalization::impl::from_functional_tensor(grad_scale);
      } else {
        grad_scale_ = grad_scale;
      }
      
      c10::optional<at::Tensor> found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self) && at::functionalization::impl::isFunctionalTensor(grads) && at::functionalization::impl::isFunctionalTensor(exp_avgs) && at::functionalization::impl::isFunctionalTensor(exp_avg_sqs) && at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(state_steps) || at::functionalization::impl::isFunctionalTensor(grad_scale) || at::functionalization::impl::isFunctionalTensor(found_inf))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_fused_adam_::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_);
         ;
        }
      } else {
        ::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_adam::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_);
        }
          at::functionalization::impl::propagate_xla_data(self, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(self, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
  at::functionalization::impl::propagate_xla_data(grads, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(grads, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(grads);
  at::functionalization::impl::sync(grads);
  at::functionalization::impl::propagate_xla_data(exp_avgs, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(exp_avgs, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(exp_avgs);
  at::functionalization::impl::sync(exp_avgs);
  at::functionalization::impl::propagate_xla_data(exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(exp_avg_sqs);
  at::functionalization::impl::sync(exp_avg_sqs);
  at::functionalization::impl::propagate_xla_data(max_exp_avg_sqs, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(max_exp_avg_sqs, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(max_exp_avg_sqs);
  at::functionalization::impl::sync(max_exp_avg_sqs);
    
      }
    }

    void _fused_adam_out_tensor_lr_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, const at::Tensor & lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grads_meta = to_meta(grads);
        auto exp_avgs_meta = to_meta(exp_avgs);
        auto exp_avg_sqs_meta = to_meta(exp_avg_sqs);
        auto max_exp_avg_sqs_meta = to_meta(max_exp_avg_sqs);
        auto state_steps_meta = to_meta(state_steps);
        auto lr_meta = to_meta(lr);
        auto grad_scale_meta = to_meta(grad_scale);
        auto found_inf_meta = to_meta(found_inf);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_adam_tensor_lr_out::call(self_meta, grads_meta, exp_avgs_meta, exp_avg_sqs_meta, max_exp_avg_sqs_meta, state_steps_meta, lr_meta, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_meta, found_inf_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> grads_;
      if (at::functionalization::impl::isFunctionalTensor(grads)) {
        at::functionalization::impl::sync(grads);
        grads_ = at::functionalization::impl::from_functional_tensor(grads);
      } else {
        grads_ = grads.vec();
      }
      
      ::std::vector<at::Tensor> exp_avgs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avgs)) {
        at::functionalization::impl::sync(exp_avgs);
        exp_avgs_ = at::functionalization::impl::from_functional_tensor(exp_avgs);
      } else {
        exp_avgs_ = exp_avgs.vec();
      }
      
      ::std::vector<at::Tensor> exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avg_sqs)) {
        at::functionalization::impl::sync(exp_avg_sqs);
        exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(exp_avg_sqs);
      } else {
        exp_avg_sqs_ = exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> max_exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs)) {
        at::functionalization::impl::sync(max_exp_avg_sqs);
        max_exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(max_exp_avg_sqs);
      } else {
        max_exp_avg_sqs_ = max_exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> state_steps_;
      if (at::functionalization::impl::isFunctionalTensor(state_steps)) {
        at::functionalization::impl::sync(state_steps);
        state_steps_ = at::functionalization::impl::from_functional_tensor(state_steps);
      } else {
        state_steps_ = state_steps.vec();
      }
      
      at::Tensor lr_;
      if (at::functionalization::impl::isFunctionalTensor(lr)) {
        at::functionalization::impl::sync(lr);
        lr_ = at::functionalization::impl::from_functional_tensor(lr);
      } else {
        lr_ = lr;
      }
      
      c10::optional<at::Tensor> grad_scale_;
      if (at::functionalization::impl::isFunctionalTensor(grad_scale)) {
        at::functionalization::impl::sync(grad_scale);
        grad_scale_ = at::functionalization::impl::from_functional_tensor(grad_scale);
      } else {
        grad_scale_ = grad_scale;
      }
      
      c10::optional<at::Tensor> found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grads) && at::functionalization::impl::isFunctionalTensor(exp_avgs) && at::functionalization::impl::isFunctionalTensor(exp_avg_sqs) && at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs) && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || lr.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(state_steps) || at::functionalization::impl::isFunctionalTensor(lr) || at::functionalization::impl::isFunctionalTensor(grad_scale) || at::functionalization::impl::isFunctionalTensor(found_inf))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_fused_adam_tensor_lr_out::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr_, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_, out_);
         ;
        }
      } else {
        ::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_adam_tensor_lr::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr_, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_);
        }
          at::functionalization::impl::propagate_xla_data(grads, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(grads, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(grads);
  at::functionalization::impl::sync(grads);
  at::functionalization::impl::propagate_xla_data(exp_avgs, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(exp_avgs, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(exp_avgs);
  at::functionalization::impl::sync(exp_avgs);
  at::functionalization::impl::propagate_xla_data(exp_avg_sqs, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(exp_avg_sqs, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(exp_avg_sqs);
  at::functionalization::impl::sync(exp_avg_sqs);
  at::functionalization::impl::propagate_xla_data(max_exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(max_exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(max_exp_avg_sqs);
  at::functionalization::impl::sync(max_exp_avg_sqs);
  at::functionalization::impl::propagate_xla_data(out, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _fused_adam__tensor_lr(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, const at::Tensor & lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grads_meta = to_meta(grads);
        auto exp_avgs_meta = to_meta(exp_avgs);
        auto exp_avg_sqs_meta = to_meta(exp_avg_sqs);
        auto max_exp_avg_sqs_meta = to_meta(max_exp_avg_sqs);
        auto state_steps_meta = to_meta(state_steps);
        auto lr_meta = to_meta(lr);
        auto grad_scale_meta = to_meta(grad_scale);
        auto found_inf_meta = to_meta(found_inf);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_adam__tensor_lr::call(self_meta, grads_meta, exp_avgs_meta, exp_avg_sqs_meta, max_exp_avg_sqs_meta, state_steps_meta, lr_meta, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_meta, found_inf_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> grads_;
      if (at::functionalization::impl::isFunctionalTensor(grads)) {
        at::functionalization::impl::sync(grads);
        grads_ = at::functionalization::impl::from_functional_tensor(grads);
      } else {
        grads_ = grads.vec();
      }
      
      ::std::vector<at::Tensor> exp_avgs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avgs)) {
        at::functionalization::impl::sync(exp_avgs);
        exp_avgs_ = at::functionalization::impl::from_functional_tensor(exp_avgs);
      } else {
        exp_avgs_ = exp_avgs.vec();
      }
      
      ::std::vector<at::Tensor> exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avg_sqs)) {
        at::functionalization::impl::sync(exp_avg_sqs);
        exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(exp_avg_sqs);
      } else {
        exp_avg_sqs_ = exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> max_exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs)) {
        at::functionalization::impl::sync(max_exp_avg_sqs);
        max_exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(max_exp_avg_sqs);
      } else {
        max_exp_avg_sqs_ = max_exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> state_steps_;
      if (at::functionalization::impl::isFunctionalTensor(state_steps)) {
        at::functionalization::impl::sync(state_steps);
        state_steps_ = at::functionalization::impl::from_functional_tensor(state_steps);
      } else {
        state_steps_ = state_steps.vec();
      }
      
      at::Tensor lr_;
      if (at::functionalization::impl::isFunctionalTensor(lr)) {
        at::functionalization::impl::sync(lr);
        lr_ = at::functionalization::impl::from_functional_tensor(lr);
      } else {
        lr_ = lr;
      }
      
      c10::optional<at::Tensor> grad_scale_;
      if (at::functionalization::impl::isFunctionalTensor(grad_scale)) {
        at::functionalization::impl::sync(grad_scale);
        grad_scale_ = at::functionalization::impl::from_functional_tensor(grad_scale);
      } else {
        grad_scale_ = grad_scale;
      }
      
      c10::optional<at::Tensor> found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self) && at::functionalization::impl::isFunctionalTensor(grads) && at::functionalization::impl::isFunctionalTensor(exp_avgs) && at::functionalization::impl::isFunctionalTensor(exp_avg_sqs) && at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || lr.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(state_steps) || at::functionalization::impl::isFunctionalTensor(lr) || at::functionalization::impl::isFunctionalTensor(grad_scale) || at::functionalization::impl::isFunctionalTensor(found_inf))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_fused_adam__tensor_lr::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr_, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_);
         ;
        }
      } else {
        ::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_adam_tensor_lr::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr_, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_);
        }
          at::functionalization::impl::propagate_xla_data(self, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(self, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
  at::functionalization::impl::propagate_xla_data(grads, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(grads, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(grads);
  at::functionalization::impl::sync(grads);
  at::functionalization::impl::propagate_xla_data(exp_avgs, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(exp_avgs, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(exp_avgs);
  at::functionalization::impl::sync(exp_avgs);
  at::functionalization::impl::propagate_xla_data(exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(exp_avg_sqs);
  at::functionalization::impl::sync(exp_avg_sqs);
  at::functionalization::impl::propagate_xla_data(max_exp_avg_sqs, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(max_exp_avg_sqs, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(max_exp_avg_sqs);
  at::functionalization::impl::sync(max_exp_avg_sqs);
    
      }
    }

    at::Tensor _conj(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::_conj::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::_conj::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::_conj::call(self_);
        } else {
          tmp_output = at::_ops::_conj_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::_conj::call(base);
          } else {
            return at::_ops::_conj_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::_conj_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor as_strided(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::as_strided::call(self_, size, stride, storage_offset);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::as_strided::call(self_meta, size, stride, storage_offset);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::as_strided::call(self_, size, stride, storage_offset);
        } else {
          tmp_output = at::_ops::as_strided_copy::call(self_, size, stride, storage_offset);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, size = size.vec(), stride = stride.vec(), storage_offset = storage_offset](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::as_strided::call(base, size, stride, storage_offset);
          } else {
            return at::_ops::as_strided_copy::call(base, size, stride, storage_offset);
          }
        },
        [inverse_return_mode = inverse_return_mode, size = size.vec(), stride = stride.vec(), storage_offset = storage_offset](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::as_strided_inverse(base, mutated_view, inverse_return_mode, size, stride, storage_offset);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    const at::Tensor & as_strided_(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset) {
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::as_strided_::call(self_, size, stride, storage_offset);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, size = size.vec(), stride = stride.vec(), storage_offset = storage_offset](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::as_strided::call(base, size, stride, storage_offset);
          } else {
            return at::_ops::as_strided_copy::call(base, size, stride, storage_offset);
          }
        },
        [inverse_return_mode = inverse_return_mode, size = size.vec(), stride = stride.vec(), storage_offset = storage_offset](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::as_strided_inverse(base, mutated_view, inverse_return_mode, size, stride, storage_offset);
        }
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::as_strided_::call(self_meta, size, stride, storage_offset);
      }
      // This function adds the above view meta to the current tensor and replays them off the base,
      // mutating the size/stride info of the current FunctionalTensorWrapper.
      // Because of this, we need to make sure to run the reference shape function above,
      // BEFORE doing this (otherwise we'll end up runnin the reference function using the wrong sizes/strides)
      at::functionalization::impl::mutate_view_meta(self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      // XLA/LTC don't implement the logic to propagate strides correctly, so we need to rely
      // on a reference implementation here (instead of relying on the output from the forward lambda
      // having the correct stride info)
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(self, reference_tensor_output);
      }
      return self;
    }

    at::Tensor _sparse_broadcast_to(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::_sparse_broadcast_to::call(self_, size);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::_sparse_broadcast_to::call(self_meta, size);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::_sparse_broadcast_to::call(self_, size);
        } else {
          tmp_output = at::_ops::_sparse_broadcast_to_copy::call(self_, size);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, size = size.vec()](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::_sparse_broadcast_to::call(base, size);
          } else {
            return at::_ops::_sparse_broadcast_to_copy::call(base, size);
          }
        },
        [inverse_return_mode = inverse_return_mode, size = size.vec()](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::_sparse_broadcast_to_inverse(base, mutated_view, inverse_return_mode, size);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor permute(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dims) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::permute::call(self_, dims);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::permute::call(self_meta, dims);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::permute::call(self_, dims);
        } else {
          tmp_output = at::_ops::permute_copy::call(self_, dims);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, dims = dims.vec()](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::permute::call(base, dims);
          } else {
            return at::_ops::permute_copy::call(base, dims);
          }
        },
        [inverse_return_mode = inverse_return_mode, dims = dims.vec()](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::permute_inverse(base, mutated_view, inverse_return_mode, dims);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor _reshape_alias(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::_reshape_alias::call(self_, size, stride);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::_reshape_alias::call(self_meta, size, stride);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::_reshape_alias::call(self_, size, stride);
        } else {
          tmp_output = at::_ops::_reshape_alias_copy::call(self_, size, stride);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, size = size.vec(), stride = stride.vec()](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::_reshape_alias::call(base, size, stride);
          } else {
            return at::_ops::_reshape_alias_copy::call(base, size, stride);
          }
        },
        [inverse_return_mode = inverse_return_mode, size = size.vec(), stride = stride.vec()](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::_reshape_alias_inverse(base, mutated_view, inverse_return_mode, size, stride);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor detach(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::detach::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::detach::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::detach::call(self_);
        } else {
          tmp_output = at::_ops::detach_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::detach::call(base);
          } else {
            return at::_ops::detach_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::detach_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor & detach_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::detach_::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::detach::call(base);
          } else {
            return at::_ops::detach_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::detach_inverse(base, mutated_view, inverse_return_mode);
        }
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::detach_::call(self_meta);
      }
      // This function adds the above view meta to the current tensor and replays them off the base,
      // mutating the size/stride info of the current FunctionalTensorWrapper.
      // Because of this, we need to make sure to run the reference shape function above,
      // BEFORE doing this (otherwise we'll end up runnin the reference function using the wrong sizes/strides)
      at::functionalization::impl::mutate_view_meta(self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      // XLA/LTC don't implement the logic to propagate strides correctly, so we need to rely
      // on a reference implementation here (instead of relying on the output from the forward lambda
      // having the correct stride info)
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(self, reference_tensor_output);
      }
      return self;
    }

    ::std::vector<at::Tensor> split_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt split_size, int64_t dim) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::split_Tensor::call(self_, split_size, dim);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      ::std::vector<at::Tensor> reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::split_Tensor::call(self_meta, split_size, dim);
      }
      ::std::vector<at::Tensor> tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::split_Tensor::call(self_, split_size, dim);
        } else {
          tmp_output = at::_ops::split_copy_Tensor::call(self_, split_size, dim);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, split_size = split_size, dim = dim](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::split_Tensor::call(base, split_size, dim)[mutated_view_idx];
          } else {
            return at::_ops::split_copy_Tensor::call(base, split_size, dim)[mutated_view_idx];
          }
        },
        [inverse_return_mode = inverse_return_mode, split_size = split_size, dim = dim](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::split_Tensor_inverse(base, mutated_view, inverse_return_mode, mutated_view_idx, split_size, dim);
        },
        /*is_multi_output=*/true
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::values::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::values::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::values::call(self_);
        } else {
          tmp_output = at::_ops::values_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::values::call(base);
          } else {
            return at::_ops::values_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::values_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor row_indices(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::row_indices::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::row_indices::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::row_indices::call(self_);
        } else {
          tmp_output = at::_ops::row_indices_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::row_indices::call(base);
          } else {
            return at::_ops::row_indices_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::row_indices_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor lift_fresh(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::lift_fresh::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::lift_fresh::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::lift_fresh::call(self_);
        } else {
          tmp_output = at::_ops::lift_fresh_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::lift_fresh::call(base);
          } else {
            return at::_ops::lift_fresh_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::lift_fresh_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor _test_autograd_multiple_dispatch_view(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::_test_autograd_multiple_dispatch_view::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::_test_autograd_multiple_dispatch_view::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::_test_autograd_multiple_dispatch_view::call(self_);
        } else {
          tmp_output = at::_ops::_test_autograd_multiple_dispatch_view_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::_test_autograd_multiple_dispatch_view::call(base);
          } else {
            return at::_ops::_test_autograd_multiple_dispatch_view_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::_test_autograd_multiple_dispatch_view_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

}  // namespace functionalization

namespace {

TORCH_LIBRARY_IMPL(aten, Functionalize, m) {
  m.impl("_cudnn_ctc_loss.out", TORCH_FN(functionalization::_cudnn_ctc_loss_out_out));
  m.impl("_cudnn_rnn.out", TORCH_FN(functionalization::_cudnn_rnn_out_out));
  m.impl("_cudnn_rnn_backward.out", TORCH_FN(functionalization::_cudnn_rnn_backward_out_out));
  m.impl("_fused_dropout.out", TORCH_FN(functionalization::_fused_dropout_out_out));
  m.impl("conj_physical.out", TORCH_FN(functionalization::conj_physical_out_out));
  m.impl("conj_physical_", TORCH_FN(functionalization::conj_physical_));
  m.impl("add.out", TORCH_FN(functionalization::add_out_out));
  m.impl("add_.Tensor", TORCH_FN(functionalization::add__Tensor));
  m.impl("add.Scalar_out", TORCH_FN(functionalization::add_out_Scalar_out));
  m.impl("add_.Scalar", TORCH_FN(functionalization::add__Scalar));
  m.impl("addmv.out", TORCH_FN(functionalization::addmv_out_out));
  m.impl("addmv_", TORCH_FN(functionalization::addmv_));
  m.impl("addr.out", TORCH_FN(functionalization::addr_out_out));
  m.impl("addr_", TORCH_FN(functionalization::addr_));
  m.impl("all.out", TORCH_FN(functionalization::all_out_out));
  m.impl("all.dims_out", TORCH_FN(functionalization::all_out_dims_out));
  m.impl("argmax.out", TORCH_FN(functionalization::argmax_out_out));
  m.impl("atan.out", TORCH_FN(functionalization::atan_out_out));
  m.impl("atan_", TORCH_FN(functionalization::atan_));
  m.impl("bartlett_window.out", TORCH_FN(functionalization::bartlett_window_out_out));
  m.impl("bartlett_window.periodic_out", TORCH_FN(functionalization::bartlett_window_out_periodic_out));
  m.impl("binary_cross_entropy.out", TORCH_FN(functionalization::binary_cross_entropy_out_out));
  m.impl("bitwise_not.out", TORCH_FN(functionalization::bitwise_not_out_out));
  m.impl("bitwise_not_", TORCH_FN(functionalization::bitwise_not_));
  m.impl("logical_xor.out", TORCH_FN(functionalization::logical_xor_out_out));
  m.impl("logical_xor_", TORCH_FN(functionalization::logical_xor_));
  m.impl("blackman_window.out", TORCH_FN(functionalization::blackman_window_out_out));
  m.impl("blackman_window.periodic_out", TORCH_FN(functionalization::blackman_window_out_periodic_out));
  m.impl("clamp.out", TORCH_FN(functionalization::clamp_out_out));
  m.impl("clamp_", TORCH_FN(functionalization::clamp_));
  m.impl("clamp.Tensor_out", TORCH_FN(functionalization::clamp_out_Tensor_out));
  m.impl("clamp_.Tensor", TORCH_FN(functionalization::clamp__Tensor));
  m.impl("_convolution.out", TORCH_FN(functionalization::_convolution_out_out));
  m.impl("copy.out", TORCH_FN(functionalization::copy_out_out));
  m.impl("copy_", TORCH_FN(functionalization::copy_));
  m.impl("cudnn_convolution.out", TORCH_FN(functionalization::cudnn_convolution_out_out));
  m.impl("dot.out", TORCH_FN(functionalization::dot_out_out));
  m.impl("vdot.out", TORCH_FN(functionalization::vdot_out_out));
  m.impl("new_empty.out", TORCH_FN(functionalization::new_empty_out_out));
  m.impl("new_empty_strided.out", TORCH_FN(functionalization::new_empty_strided_out_out));
  m.impl("_empty_affine_quantized.out", TORCH_FN(functionalization::_empty_affine_quantized_out_out));
  m.impl("_resize_output.out", TORCH_FN(functionalization::_resize_output_out_out));
  m.impl("_resize_output_", TORCH_FN(functionalization::_resize_output_));
  m.impl("frac.out", TORCH_FN(functionalization::frac_out_out));
  m.impl("frac_", TORCH_FN(functionalization::frac_));
  m.impl("full_like.out", TORCH_FN(functionalization::full_like_out_out));
  m.impl("lcm.out", TORCH_FN(functionalization::lcm_out_out));
  m.impl("lcm_", TORCH_FN(functionalization::lcm_));
  m.impl("hann_window.out", TORCH_FN(functionalization::hann_window_out_out));
  m.impl("hann_window.periodic_out", TORCH_FN(functionalization::hann_window_out_periodic_out));
  m.impl("kaiser_window.out", TORCH_FN(functionalization::kaiser_window_out_out));
  m.impl("kaiser_window.periodic_out", TORCH_FN(functionalization::kaiser_window_out_periodic_out));
  m.impl("kaiser_window.beta_out", TORCH_FN(functionalization::kaiser_window_out_beta_out));
  m.impl("_index_put_impl.out", TORCH_FN(functionalization::_index_put_impl_out_out));
  m.impl("_index_put_impl_", TORCH_FN(functionalization::_index_put_impl_));
  m.impl("kthvalue.values", TORCH_FN(functionalization::kthvalue_out_values));
  m.impl("native_layer_norm.out", TORCH_FN(functionalization::native_layer_norm_out_out));
  m.impl("native_layer_norm_backward.out", TORCH_FN(functionalization::native_layer_norm_backward_out_out));
  m.impl("mkldnn_linear_backward_input.out", TORCH_FN(functionalization::mkldnn_linear_backward_input_out_out));
  m.impl("mkldnn_linear_backward.out", TORCH_FN(functionalization::mkldnn_linear_backward_out_out));
  m.impl("log10.out", TORCH_FN(functionalization::log10_out_out));
  m.impl("log10_", TORCH_FN(functionalization::log10_));
  m.impl("log1p.out", TORCH_FN(functionalization::log1p_out_out));
  m.impl("log1p_", TORCH_FN(functionalization::log1p_));
  m.impl("logsumexp.out", TORCH_FN(functionalization::logsumexp_out_out));
  m.impl("max_pool2d_backward.out", TORCH_FN(functionalization::max_pool2d_backward_out_out));
  m.impl("mkldnn_max_pool2d.out", TORCH_FN(functionalization::mkldnn_max_pool2d_out_out));
  m.impl("quantized_max_pool1d.out", TORCH_FN(functionalization::quantized_max_pool1d_out_out));
  m.impl("_mps_convolution.out", TORCH_FN(functionalization::_mps_convolution_out_out));
  m.impl("mkldnn_convolution.out", TORCH_FN(functionalization::mkldnn_convolution_out_out));
  m.impl("mkldnn_rnn_layer.out", TORCH_FN(functionalization::mkldnn_rnn_layer_out_out));
  m.impl("miopen_batch_norm.out", TORCH_FN(functionalization::miopen_batch_norm_out_out));
  m.impl("miopen_batch_norm_backward.out", TORCH_FN(functionalization::miopen_batch_norm_backward_out_out));
  m.impl("miopen_convolution_transpose.out", TORCH_FN(functionalization::miopen_convolution_transpose_out_out));
  m.impl("mm.out", TORCH_FN(functionalization::mm_out_out));
  m.impl("_sparse_sparse_matmul.out", TORCH_FN(functionalization::_sparse_sparse_matmul_out_out));
  m.impl("mul.out", TORCH_FN(functionalization::mul_out_out));
  m.impl("mul_.Tensor", TORCH_FN(functionalization::mul__Tensor));
  m.impl("mul.Scalar_out", TORCH_FN(functionalization::mul_out_Scalar_out));
  m.impl("mul_.Scalar", TORCH_FN(functionalization::mul__Scalar));
  m.impl("mvlgamma.out", TORCH_FN(functionalization::mvlgamma_out_out));
  m.impl("mvlgamma_", TORCH_FN(functionalization::mvlgamma_));
  m.impl("batch_norm_backward_reduce.out", TORCH_FN(functionalization::batch_norm_backward_reduce_out_out));
  m.impl("deg2rad.out", TORCH_FN(functionalization::deg2rad_out_out));
  m.impl("deg2rad_", TORCH_FN(functionalization::deg2rad_));
  m.impl("randint_like.out", TORCH_FN(functionalization::randint_like_out_out));
  m.impl("randint_like.low_dtype_out", TORCH_FN(functionalization::randint_like_out_low_dtype_out));
  m.impl("repeat.out", TORCH_FN(functionalization::repeat_out_out));
  m.impl("_mkldnn_reshape.out", TORCH_FN(functionalization::_mkldnn_reshape_out_out));
  m.impl("round.out", TORCH_FN(functionalization::round_out_out));
  m.impl("round_", TORCH_FN(functionalization::round_));
  m.impl("round.decimals_out", TORCH_FN(functionalization::round_out_decimals_out));
  m.impl("round_.decimals", TORCH_FN(functionalization::round__decimals));
  m.impl("rsqrt.out", TORCH_FN(functionalization::rsqrt_out_out));
  m.impl("rsqrt_", TORCH_FN(functionalization::rsqrt_));
  m.impl("celu.out", TORCH_FN(functionalization::celu_out_out));
  m.impl("celu_", TORCH_FN(functionalization::celu_));
  m.impl("sigmoid.out", TORCH_FN(functionalization::sigmoid_out_out));
  m.impl("sigmoid_", TORCH_FN(functionalization::sigmoid_));
  m.impl("sinc.out", TORCH_FN(functionalization::sinc_out_out));
  m.impl("sinc_", TORCH_FN(functionalization::sinc_));
  m.impl("sinh.out", TORCH_FN(functionalization::sinh_out_out));
  m.impl("sinh_", TORCH_FN(functionalization::sinh_));
  m.impl("slice_backward.out", TORCH_FN(functionalization::slice_backward_out_out));
  m.impl("as_strided_scatter.out", TORCH_FN(functionalization::as_strided_scatter_out_out));
  m.impl("std_mean.correction_out", TORCH_FN(functionalization::std_mean_out_correction_out));
  m.impl("_mkldnn_transpose.out", TORCH_FN(functionalization::_mkldnn_transpose_out_out));
  m.impl("_mkldnn_transpose_", TORCH_FN(functionalization::_mkldnn_transpose_));
  m.impl("flip.out", TORCH_FN(functionalization::flip_out_out));
  m.impl("_nested_tensor_from_mask.out", TORCH_FN(functionalization::_nested_tensor_from_mask_out_out));
  m.impl("_nested_from_padded_and_nested_example.out", TORCH_FN(functionalization::_nested_from_padded_and_nested_example_out_out));
  m.impl("unique_dim.out", TORCH_FN(functionalization::unique_dim_out_out));
  m.impl("_unsafe_view.out", TORCH_FN(functionalization::_unsafe_view_out_out));
  m.impl("var_mean.correction_out", TORCH_FN(functionalization::var_mean_out_correction_out));
  m.impl("zeros.names_out", TORCH_FN(functionalization::zeros_out_names_out));
  m.impl("zeros.out", TORCH_FN(functionalization::zeros_out_out));
  m.impl("zeros_like.out", TORCH_FN(functionalization::zeros_like_out_out));
  m.impl("_sparse_csr_prod.dim_dtype_out", TORCH_FN(functionalization::_sparse_csr_prod_out_dim_dtype_out));
  m.impl("_spdiags.out", TORCH_FN(functionalization::_spdiags_out_out));
  m.impl("rsub.Tensor_out", TORCH_FN(functionalization::rsub_out_Tensor_out));
  m.impl("rsub.Scalar_out", TORCH_FN(functionalization::rsub_out_Scalar_out));
  m.impl("_sparse_addmm.out", TORCH_FN(functionalization::_sparse_addmm_out_out));
  m.impl("sparse_coo_tensor.size_out", TORCH_FN(functionalization::sparse_coo_tensor_out_size_out));
  m.impl("sparse_resize.out", TORCH_FN(functionalization::sparse_resize_out_out));
  m.impl("sparse_resize_", TORCH_FN(functionalization::sparse_resize_));
  m.impl("sparse_mask.out", TORCH_FN(functionalization::sparse_mask_out_out));
  m.impl("_coalesce.out", TORCH_FN(functionalization::_coalesce_out_out));
  m.impl("_to_sparse_csr.out", TORCH_FN(functionalization::_to_sparse_csr_out_out));
  m.impl("_to_sparse_csc.out", TORCH_FN(functionalization::_to_sparse_csc_out_out));
  m.impl("dequantize.self_out", TORCH_FN(functionalization::dequantize_out_self_out));
  m.impl("dequantize.tensors_out", TORCH_FN(functionalization::dequantize_out_tensors_out));
  m.impl("q_per_channel_zero_points.out", TORCH_FN(functionalization::q_per_channel_zero_points_out_out));
  m.impl("_fake_quantize_learnable_per_channel_affine.out", TORCH_FN(functionalization::_fake_quantize_learnable_per_channel_affine_out_out));
  m.impl("_fused_moving_avg_obs_fq_helper.out", TORCH_FN(functionalization::_fused_moving_avg_obs_fq_helper_out_out));
  m.impl("_fused_moving_avg_obs_fq_helper", TORCH_FN(functionalization::_fused_moving_avg_obs_fq_helper));
  m.impl("_to_copy.out", TORCH_FN(functionalization::_to_copy_out_out));
  m.impl("_thnn_fused_gru_cell.out", TORCH_FN(functionalization::_thnn_fused_gru_cell_out_out));
  m.impl("_pack_padded_sequence.out", TORCH_FN(functionalization::_pack_padded_sequence_out_out));
  m.impl("scatter_reduce.two_out", TORCH_FN(functionalization::scatter_reduce_out_two_out));
  m.impl("scatter_reduce_.two", TORCH_FN(functionalization::scatter_reduce__two));
  m.impl("bitwise_xor.Tensor_out", TORCH_FN(functionalization::bitwise_xor_out_Tensor_out));
  m.impl("bitwise_xor_.Tensor", TORCH_FN(functionalization::bitwise_xor__Tensor));
  m.impl("bitwise_xor.Scalar_out", TORCH_FN(functionalization::bitwise_xor_out_Scalar_out));
  m.impl("bitwise_xor_.Scalar", TORCH_FN(functionalization::bitwise_xor__Scalar));
  m.impl("bitwise_xor.Scalar_Tensor_out", TORCH_FN(functionalization::bitwise_xor_out_Scalar_Tensor_out));
  m.impl("addbmm.out", TORCH_FN(functionalization::addbmm_out_out));
  m.impl("addbmm_", TORCH_FN(functionalization::addbmm_));
  m.impl("random.from_out", TORCH_FN(functionalization::random_out_from_out));
  m.impl("random_.from", TORCH_FN(functionalization::random__from));
  m.impl("random.to_out", TORCH_FN(functionalization::random_out_to_out));
  m.impl("random_.to", TORCH_FN(functionalization::random__to));
  m.impl("random.out", TORCH_FN(functionalization::random_out_out));
  m.impl("random_", TORCH_FN(functionalization::random_));
  m.impl("exponential.out", TORCH_FN(functionalization::exponential_out_out));
  m.impl("exponential_", TORCH_FN(functionalization::exponential_));
  m.impl("geometric.out", TORCH_FN(functionalization::geometric_out_out));
  m.impl("geometric_", TORCH_FN(functionalization::geometric_));
  m.impl("trace.out", TORCH_FN(functionalization::trace_out_out));
  m.impl("index_select.out", TORCH_FN(functionalization::index_select_out_out));
  m.impl("masked_select.out", TORCH_FN(functionalization::masked_select_out_out));
  m.impl("linalg_solve_triangular.out", TORCH_FN(functionalization::linalg_solve_triangular_out_out));
  m.impl("multinomial.out", TORCH_FN(functionalization::multinomial_out_out));
  m.impl("histogram.bins_tensor_out", TORCH_FN(functionalization::histogram_out_bins_tensor_out));
  m.impl("histogram.bin_ct_out", TORCH_FN(functionalization::histogram_out_bin_ct_out));
  m.impl("igammac.out", TORCH_FN(functionalization::igammac_out_out));
  m.impl("igammac_", TORCH_FN(functionalization::igammac_));
  m.impl("remainder.Scalar_out", TORCH_FN(functionalization::remainder_out_Scalar_out));
  m.impl("remainder_.Scalar", TORCH_FN(functionalization::remainder__Scalar));
  m.impl("remainder.Tensor_out", TORCH_FN(functionalization::remainder_out_Tensor_out));
  m.impl("remainder_.Tensor", TORCH_FN(functionalization::remainder__Tensor));
  m.impl("remainder.Scalar_Tensor_out", TORCH_FN(functionalization::remainder_out_Scalar_Tensor_out));
  m.impl("sort.values", TORCH_FN(functionalization::sort_out_values));
  m.impl("sort.values_stable", TORCH_FN(functionalization::sort_out_values_stable));
  m.impl("argsort.stable_out", TORCH_FN(functionalization::argsort_out_stable_out));
  m.impl("all.all_out", TORCH_FN(functionalization::all_out_all_out));
  m.impl("renorm.out", TORCH_FN(functionalization::renorm_out_out));
  m.impl("renorm_", TORCH_FN(functionalization::renorm_));
  m.impl("unfold_backward.out", TORCH_FN(functionalization::unfold_backward_out_out));
  m.impl("pow.Tensor_Tensor_out", TORCH_FN(functionalization::pow_out_Tensor_Tensor_out));
  m.impl("pow_.Tensor", TORCH_FN(functionalization::pow__Tensor));
  m.impl("pow.Scalar_out", TORCH_FN(functionalization::pow_out_Scalar_out));
  m.impl("pow.Tensor_Scalar_out", TORCH_FN(functionalization::pow_out_Tensor_Scalar_out));
  m.impl("pow_.Scalar", TORCH_FN(functionalization::pow__Scalar));
  m.impl("normal.out", TORCH_FN(functionalization::normal_out_out));
  m.impl("normal_", TORCH_FN(functionalization::normal_));
  m.impl("normal.Tensor_float_out", TORCH_FN(functionalization::normal_out_Tensor_float_out));
  m.impl("normal.float_Tensor_out", TORCH_FN(functionalization::normal_out_float_Tensor_out));
  m.impl("normal.Tensor_Tensor_out", TORCH_FN(functionalization::normal_out_Tensor_Tensor_out));
  m.impl("normal.float_float_out", TORCH_FN(functionalization::normal_out_float_float_out));
  m.impl("_amp_update_scale.out", TORCH_FN(functionalization::_amp_update_scale_out_out));
  m.impl("_amp_update_scale_", TORCH_FN(functionalization::_amp_update_scale_));
  m.impl("_foreach_atan.out", TORCH_FN(functionalization::_foreach_atan_out_out));
  m.impl("_foreach_atan_", TORCH_FN(functionalization::_foreach_atan_));
  m.impl("_foreach_erf.out", TORCH_FN(functionalization::_foreach_erf_out_out));
  m.impl("_foreach_erf_", TORCH_FN(functionalization::_foreach_erf_));
  m.impl("_foreach_erfc.out", TORCH_FN(functionalization::_foreach_erfc_out_out));
  m.impl("_foreach_erfc_", TORCH_FN(functionalization::_foreach_erfc_));
  m.impl("_foreach_lerp.List_out", TORCH_FN(functionalization::_foreach_lerp_out_List_out));
  m.impl("_foreach_lerp_.List", TORCH_FN(functionalization::_foreach_lerp__List));
  m.impl("_foreach_lerp.Scalar_out", TORCH_FN(functionalization::_foreach_lerp_out_Scalar_out));
  m.impl("_foreach_lerp_.Scalar", TORCH_FN(functionalization::_foreach_lerp__Scalar));
  m.impl("_foreach_lgamma.out", TORCH_FN(functionalization::_foreach_lgamma_out_out));
  m.impl("_foreach_lgamma_", TORCH_FN(functionalization::_foreach_lgamma_));
  m.impl("_foreach_log.out", TORCH_FN(functionalization::_foreach_log_out_out));
  m.impl("_foreach_log_", TORCH_FN(functionalization::_foreach_log_));
  m.impl("_foreach_sinh.out", TORCH_FN(functionalization::_foreach_sinh_out_out));
  m.impl("_foreach_sinh_", TORCH_FN(functionalization::_foreach_sinh_));
  m.impl("_convert_indices_from_coo_to_csr.out", TORCH_FN(functionalization::_convert_indices_from_coo_to_csr_out_out));
  m.impl("multi_margin_loss_backward.grad_input", TORCH_FN(functionalization::multi_margin_loss_backward_out_grad_input));
  m.impl("nll_loss_forward.output", TORCH_FN(functionalization::nll_loss_forward_out_output));
  m.impl("hardswish.out", TORCH_FN(functionalization::hardswish_out_out));
  m.impl("hardswish_", TORCH_FN(functionalization::hardswish_));
  m.impl("hardswish_backward.out", TORCH_FN(functionalization::hardswish_backward_out_out));
  m.impl("softshrink_backward.grad_input", TORCH_FN(functionalization::softshrink_backward_out_grad_input));
  m.impl("mkldnn_adaptive_avg_pool2d_backward.out", TORCH_FN(functionalization::mkldnn_adaptive_avg_pool2d_backward_out_out));
  m.impl("_adaptive_avg_pool2d_backward.out", TORCH_FN(functionalization::_adaptive_avg_pool2d_backward_out_out));
  m.impl("adaptive_avg_pool3d.out", TORCH_FN(functionalization::adaptive_avg_pool3d_out_out));
  m.impl("adaptive_max_pool3d_backward.grad_input", TORCH_FN(functionalization::adaptive_max_pool3d_backward_out_grad_input));
  m.impl("avg_pool2d.out", TORCH_FN(functionalization::avg_pool2d_out_out));
  m.impl("avg_pool3d_backward.grad_input", TORCH_FN(functionalization::avg_pool3d_backward_out_grad_input));
  m.impl("fractional_max_pool2d.output", TORCH_FN(functionalization::fractional_max_pool2d_out_output));
  m.impl("reflection_pad3d_backward.grad_input", TORCH_FN(functionalization::reflection_pad3d_backward_out_grad_input));
  m.impl("replication_pad2d.out", TORCH_FN(functionalization::replication_pad2d_out_out));
  m.impl("_upsample_bilinear2d_aa_backward.grad_input", TORCH_FN(functionalization::_upsample_bilinear2d_aa_backward_out_grad_input));
  m.impl("_upsample_bicubic2d_aa.out", TORCH_FN(functionalization::_upsample_bicubic2d_aa_out_out));
  m.impl("upsample_trilinear3d.out", TORCH_FN(functionalization::upsample_trilinear3d_out_out));
  m.impl("_upsample_nearest_exact1d_backward.grad_input", TORCH_FN(functionalization::_upsample_nearest_exact1d_backward_out_grad_input));
  m.impl("upsample_nearest2d_backward.grad_input", TORCH_FN(functionalization::upsample_nearest2d_backward_out_grad_input));
  m.impl("tanh_backward.grad_input", TORCH_FN(functionalization::tanh_backward_out_grad_input));
  m.impl("_conv_depthwise2d.out", TORCH_FN(functionalization::_conv_depthwise2d_out_out));
  m.impl("col2im.out", TORCH_FN(functionalization::col2im_out_out));
  m.impl("im2col.out", TORCH_FN(functionalization::im2col_out_out));
  m.impl("isinf.out", TORCH_FN(functionalization::isinf_out_out));
  m.impl("isneginf.out", TORCH_FN(functionalization::isneginf_out_out));
  m.impl("linalg_lu_factor_ex.out", TORCH_FN(functionalization::linalg_lu_factor_ex_out_out));
  m.impl("linalg_ldl_solve.out", TORCH_FN(functionalization::linalg_ldl_solve_out_out));
  m.impl("_linalg_slogdet.sign", TORCH_FN(functionalization::_linalg_slogdet_out_sign));
  m.impl("linalg_householder_product.out", TORCH_FN(functionalization::linalg_householder_product_out_out));
  m.impl("linalg_vector_norm.out", TORCH_FN(functionalization::linalg_vector_norm_out_out));
  m.impl("_linalg_solve_ex.result", TORCH_FN(functionalization::_linalg_solve_ex_out_result));
  m.impl("_test_optional_filled_intlist.out", TORCH_FN(functionalization::_test_optional_filled_intlist_out_out));
  m.impl("_test_autograd_multiple_dispatch.fullcoverage_out", TORCH_FN(functionalization::_test_autograd_multiple_dispatch_out_fullcoverage_out));
  m.impl("_test_autograd_multiple_dispatch_view_copy.out", TORCH_FN(functionalization::_test_autograd_multiple_dispatch_view_copy_out_out));
  m.impl("segment_reduce.out", TORCH_FN(functionalization::segment_reduce_out_out));
  m.impl("_nested_tensor_from_tensor_list.out", TORCH_FN(functionalization::_nested_tensor_from_tensor_list_out_out));
  m.impl("diagonal_copy.out", TORCH_FN(functionalization::diagonal_copy_out_out));
  m.impl("detach_copy.out", TORCH_FN(functionalization::detach_copy_out_out));
  m.impl("slice_copy.Tensor_out", TORCH_FN(functionalization::slice_copy_out_Tensor_out));
  m.impl("transpose_copy.int_out", TORCH_FN(functionalization::transpose_copy_out_int_out));
  m.impl("indices_copy.out", TORCH_FN(functionalization::indices_copy_out_out));
  m.impl("row_indices_copy.out", TORCH_FN(functionalization::row_indices_copy_out_out));
  m.impl("_triton_multi_head_attention.out", TORCH_FN(functionalization::_triton_multi_head_attention_out_out));
  m.impl("special_bessel_j1.out", TORCH_FN(functionalization::special_bessel_j1_out_out));
  m.impl("special_bessel_y1.out", TORCH_FN(functionalization::special_bessel_y1_out_out));
  m.impl("special_legendre_polynomial_p.out", TORCH_FN(functionalization::special_legendre_polynomial_p_out_out));
  m.impl("special_legendre_polynomial_p.x_scalar_out", TORCH_FN(functionalization::special_legendre_polynomial_p_out_x_scalar_out));
  m.impl("special_legendre_polynomial_p.n_scalar_out", TORCH_FN(functionalization::special_legendre_polynomial_p_out_n_scalar_out));
  m.impl("special_modified_bessel_i0.out", TORCH_FN(functionalization::special_modified_bessel_i0_out_out));
  m.impl("special_shifted_chebyshev_polynomial_t.out", TORCH_FN(functionalization::special_shifted_chebyshev_polynomial_t_out_out));
  m.impl("special_shifted_chebyshev_polynomial_t.x_scalar_out", TORCH_FN(functionalization::special_shifted_chebyshev_polynomial_t_out_x_scalar_out));
  m.impl("special_shifted_chebyshev_polynomial_t.n_scalar_out", TORCH_FN(functionalization::special_shifted_chebyshev_polynomial_t_out_n_scalar_out));
  m.impl("special_shifted_chebyshev_polynomial_u.out", TORCH_FN(functionalization::special_shifted_chebyshev_polynomial_u_out_out));
  m.impl("special_shifted_chebyshev_polynomial_u.x_scalar_out", TORCH_FN(functionalization::special_shifted_chebyshev_polynomial_u_out_x_scalar_out));
  m.impl("special_shifted_chebyshev_polynomial_u.n_scalar_out", TORCH_FN(functionalization::special_shifted_chebyshev_polynomial_u_out_n_scalar_out));
  m.impl("special_shifted_chebyshev_polynomial_w.out", TORCH_FN(functionalization::special_shifted_chebyshev_polynomial_w_out_out));
  m.impl("special_shifted_chebyshev_polynomial_w.x_scalar_out", TORCH_FN(functionalization::special_shifted_chebyshev_polynomial_w_out_x_scalar_out));
  m.impl("special_shifted_chebyshev_polynomial_w.n_scalar_out", TORCH_FN(functionalization::special_shifted_chebyshev_polynomial_w_out_n_scalar_out));
  m.impl("_fused_adam.out", TORCH_FN(functionalization::_fused_adam_out_out));
  m.impl("_fused_adam_", TORCH_FN(functionalization::_fused_adam_));
  m.impl("_fused_adam.tensor_lr_out", TORCH_FN(functionalization::_fused_adam_out_tensor_lr_out));
  m.impl("_fused_adam_.tensor_lr", TORCH_FN(functionalization::_fused_adam__tensor_lr));
  m.impl("_conj", TORCH_FN(functionalization::_conj));
  m.impl("as_strided", TORCH_FN(functionalization::as_strided));
  m.impl("as_strided_", TORCH_FN(functionalization::as_strided_));
  m.impl("_sparse_broadcast_to", TORCH_FN(functionalization::_sparse_broadcast_to));
  m.impl("permute", TORCH_FN(functionalization::permute));
  m.impl("_reshape_alias", TORCH_FN(functionalization::_reshape_alias));
  m.impl("detach", TORCH_FN(functionalization::detach));
  m.impl("detach_", TORCH_FN(functionalization::detach_));
  m.impl("split.Tensor", TORCH_FN(functionalization::split_Tensor));
  m.impl("values", TORCH_FN(functionalization::values));
  m.impl("row_indices", TORCH_FN(functionalization::row_indices));
  m.impl("_test_autograd_multiple_dispatch_view", TORCH_FN(functionalization::_test_autograd_multiple_dispatch_view));;
}

}  // namespace

} // namespace at
