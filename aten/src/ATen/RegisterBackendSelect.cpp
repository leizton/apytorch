// We register ops with a higher priority dispatch key (BackendSelect) than the usual backend-specific keys (e.g. CPU)
// which makes calls to the factory functions dispatch to here.
// We then 'manually' compute a lower-priority to re-dispatch to (e.g. CPU) to get to the eventually correct backend.
// @generated by torchgen/gen.py from RegisterBackendSelect.cpp

#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
#include <ATen/core/Tensor.h>
#include <ATen/core/dispatch/DispatchKeyExtractor.h>
#include <torch/library.h>

#ifndef AT_PER_OPERATOR_HEADERS
#include <ATen/Operators.h>
#else
#include <ATen/ops/is_pinned_ops.h>
#include <ATen/ops/_pin_memory_ops.h>

#include <ATen/ops/_make_dep_token_ops.h>
#include <ATen/ops/_cudnn_init_dropout_state_ops.h>
#include <ATen/ops/arange_ops.h>
#include <ATen/ops/arange_ops.h>
#include <ATen/ops/arange_ops.h>
#include <ATen/ops/bartlett_window_ops.h>
#include <ATen/ops/bartlett_window_ops.h>
#include <ATen/ops/blackman_window_ops.h>
#include <ATen/ops/blackman_window_ops.h>
#include <ATen/ops/empty_ops.h>
#include <ATen/ops/empty_ops.h>
#include <ATen/ops/empty_permuted_ops.h>
#include <ATen/ops/_empty_affine_quantized_ops.h>
#include <ATen/ops/_empty_per_channel_affine_quantized_ops.h>
#include <ATen/ops/empty_quantized_ops.h>
#include <ATen/ops/empty_strided_ops.h>
#include <ATen/ops/eye_ops.h>
#include <ATen/ops/eye_ops.h>
#include <ATen/ops/full_ops.h>
#include <ATen/ops/full_ops.h>
#include <ATen/ops/from_file_ops.h>
#include <ATen/ops/hann_window_ops.h>
#include <ATen/ops/hann_window_ops.h>
#include <ATen/ops/hamming_window_ops.h>
#include <ATen/ops/hamming_window_ops.h>
#include <ATen/ops/hamming_window_ops.h>
#include <ATen/ops/hamming_window_ops.h>
#include <ATen/ops/kaiser_window_ops.h>
#include <ATen/ops/kaiser_window_ops.h>
#include <ATen/ops/kaiser_window_ops.h>
#include <ATen/ops/linspace_ops.h>
#include <ATen/ops/linspace_ops.h>
#include <ATen/ops/linspace_ops.h>
#include <ATen/ops/linspace_ops.h>
#include <ATen/ops/logspace_ops.h>
#include <ATen/ops/logspace_ops.h>
#include <ATen/ops/logspace_ops.h>
#include <ATen/ops/logspace_ops.h>
#include <ATen/ops/ones_ops.h>
#include <ATen/ops/ones_ops.h>
#include <ATen/ops/scalar_tensor_ops.h>
#include <ATen/ops/rand_ops.h>
#include <ATen/ops/rand_ops.h>
#include <ATen/ops/rand_ops.h>
#include <ATen/ops/rand_ops.h>
#include <ATen/ops/randint_ops.h>
#include <ATen/ops/randint_ops.h>
#include <ATen/ops/randint_ops.h>
#include <ATen/ops/randint_ops.h>
#include <ATen/ops/randn_ops.h>
#include <ATen/ops/randn_ops.h>
#include <ATen/ops/randn_ops.h>
#include <ATen/ops/randn_ops.h>
#include <ATen/ops/randperm_ops.h>
#include <ATen/ops/randperm_ops.h>
#include <ATen/ops/range_ops.h>
#include <ATen/ops/range_ops.h>
#include <ATen/ops/zeros_ops.h>
#include <ATen/ops/_efficientzerotensor_ops.h>
#include <ATen/ops/zeros_ops.h>
#include <ATen/ops/sparse_compressed_tensor_ops.h>
#include <ATen/ops/sparse_csr_tensor_ops.h>
#include <ATen/ops/sparse_csc_tensor_ops.h>
#include <ATen/ops/sparse_bsr_tensor_ops.h>
#include <ATen/ops/sparse_bsc_tensor_ops.h>
#include <ATen/ops/sparse_compressed_tensor_ops.h>
#include <ATen/ops/sparse_csr_tensor_ops.h>
#include <ATen/ops/sparse_csc_tensor_ops.h>
#include <ATen/ops/sparse_bsr_tensor_ops.h>
#include <ATen/ops/sparse_bsc_tensor_ops.h>
#include <ATen/ops/_sparse_compressed_tensor_unsafe_ops.h>
#include <ATen/ops/_sparse_csr_tensor_unsafe_ops.h>
#include <ATen/ops/_sparse_csc_tensor_unsafe_ops.h>
#include <ATen/ops/_sparse_bsr_tensor_unsafe_ops.h>
#include <ATen/ops/_sparse_bsc_tensor_unsafe_ops.h>
#include <ATen/ops/sparse_coo_tensor_ops.h>
#include <ATen/ops/sparse_coo_tensor_ops.h>
#include <ATen/ops/sparse_coo_tensor_ops.h>
#include <ATen/ops/_sparse_coo_tensor_unsafe_ops.h>
#include <ATen/ops/_sparse_coo_tensor_with_dims_ops.h>
#include <ATen/ops/_sparse_coo_tensor_with_dims_and_tensors_ops.h>
#include <ATen/ops/_to_copy_ops.h>
#include <ATen/ops/to_ops.h>
#include <ATen/ops/tril_indices_ops.h>
#include <ATen/ops/triu_indices_ops.h>
#include <ATen/ops/normal_ops.h>
#include <ATen/ops/fft_fftfreq_ops.h>
#include <ATen/ops/fft_rfftfreq_ops.h>
#endif

namespace at {

namespace {

// aten::_make_dep_token(*, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _make_dep_token(c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::_make_dep_token::redispatch(
      _dk, dtype, layout, device, pin_memory, memory_format);
}
// aten::_cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::_cudnn_init_dropout_state::redispatch(
      _dk, dropout, train, dropout_seed, dtype, layout, device, pin_memory);
}
// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor arange(const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::arange::redispatch(
      _dk, end, dtype, layout, device, pin_memory);
}
// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor arange_start(const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::arange_start::redispatch(
      _dk, start, end, dtype, layout, device, pin_memory);
}
// aten::arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor arange_start_step(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::arange_start_step::redispatch(
      _dk, start, end, step, dtype, layout, device, pin_memory);
}
// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor bartlett_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::bartlett_window::redispatch(
      _dk, window_length, dtype, layout, device, pin_memory);
}
// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor bartlett_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::bartlett_window_periodic::redispatch(
      _dk, window_length, periodic, dtype, layout, device, pin_memory);
}
// aten::blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor blackman_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::blackman_window::redispatch(
      _dk, window_length, dtype, layout, device, pin_memory);
}
// aten::blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor blackman_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::blackman_window_periodic::redispatch(
      _dk, window_length, periodic, dtype, layout, device, pin_memory);
}
// aten::empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor empty_names(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::empty_names::redispatch(
      _dk, size, names, dtype, layout, device, pin_memory, memory_format);
}
// aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor empty_memory_format(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::empty_memory_format::redispatch(
      _dk, size, dtype, layout, device, pin_memory, memory_format);
}
// aten::empty_permuted(SymInt[] size, int[] physical_layout, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor empty_permuted(c10::SymIntArrayRef size, at::IntArrayRef physical_layout, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::empty_permuted::redispatch(
      _dk, size, physical_layout, dtype, layout, device, pin_memory);
}
// aten::_empty_affine_quantized(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _empty_affine_quantized(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::_empty_affine_quantized::redispatch(
      _dk, size, dtype, layout, device, pin_memory, scale, zero_point, memory_format);
}
// aten::_empty_per_channel_affine_quantized(SymInt[] size, *, Tensor scales, Tensor zero_points, int axis, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=contiguous_format) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _empty_per_channel_affine_quantized(c10::SymIntArrayRef size, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(scales, zero_points);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::_empty_per_channel_affine_quantized::redispatch(
      _dk, size, scales, zero_points, axis, dtype, layout, device, pin_memory, memory_format);
}
// aten::empty_quantized(int[] size, Tensor qtensor, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor empty_quantized(at::IntArrayRef size, const at::Tensor & qtensor, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(qtensor);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::empty_quantized::redispatch(
      _dk, size, qtensor, dtype, layout, device, pin_memory, memory_format);
}
// aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor empty_strided(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::empty_strided::redispatch(
      _dk, size, stride, dtype, layout, device, pin_memory);
}
// aten::eye(SymInt n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor eye(c10::SymInt n, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::eye::redispatch(
      _dk, n, dtype, layout, device, pin_memory);
}
// aten::eye.m(SymInt n, SymInt m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor eye_m(c10::SymInt n, c10::SymInt m, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::eye_m::redispatch(
      _dk, n, m, dtype, layout, device, pin_memory);
}
// aten::full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor full_names(at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::full_names::redispatch(
      _dk, size, fill_value, names, dtype, layout, device, pin_memory);
}
// aten::full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor full(c10::SymIntArrayRef size, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::full::redispatch(
      _dk, size, fill_value, dtype, layout, device, pin_memory);
}
// aten::from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor from_file(c10::string_view filename, c10::optional<bool> shared, c10::optional<int64_t> size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::from_file::redispatch(
      _dk, filename, shared, size, dtype, layout, device, pin_memory);
}
// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor hann_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::hann_window::redispatch(
      _dk, window_length, dtype, layout, device, pin_memory);
}
// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor hann_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::hann_window_periodic::redispatch(
      _dk, window_length, periodic, dtype, layout, device, pin_memory);
}
// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor hamming_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::hamming_window::redispatch(
      _dk, window_length, dtype, layout, device, pin_memory);
}
// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor hamming_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::hamming_window_periodic::redispatch(
      _dk, window_length, periodic, dtype, layout, device, pin_memory);
}
// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor hamming_window_periodic_alpha(int64_t window_length, bool periodic, double alpha, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::hamming_window_periodic_alpha::redispatch(
      _dk, window_length, periodic, alpha, dtype, layout, device, pin_memory);
}
// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor hamming_window_periodic_alpha_beta(int64_t window_length, bool periodic, double alpha, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::hamming_window_periodic_alpha_beta::redispatch(
      _dk, window_length, periodic, alpha, beta, dtype, layout, device, pin_memory);
}
// aten::kaiser_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor kaiser_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::kaiser_window::redispatch(
      _dk, window_length, dtype, layout, device, pin_memory);
}
// aten::kaiser_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor kaiser_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::kaiser_window_periodic::redispatch(
      _dk, window_length, periodic, dtype, layout, device, pin_memory);
}
// aten::kaiser_window.beta(int window_length, bool periodic, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor kaiser_window_beta(int64_t window_length, bool periodic, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::kaiser_window_beta::redispatch(
      _dk, window_length, periodic, beta, dtype, layout, device, pin_memory);
}
// aten::linspace(Scalar start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor linspace(const at::Scalar & start, const at::Scalar & end, int64_t steps, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::linspace::redispatch(
      _dk, start, end, steps, dtype, layout, device, pin_memory);
}
// aten::linspace.Tensor_Tensor(Tensor start, Tensor end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor linspace_Tensor_Tensor(const at::Tensor & start, const at::Tensor & end, int64_t steps, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(start, end);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::linspace_Tensor_Tensor::redispatch(
      _dk, start, end, steps, dtype, layout, device, pin_memory);
}
// aten::linspace.Tensor_Scalar(Tensor start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor linspace_Tensor_Scalar(const at::Tensor & start, const at::Scalar & end, int64_t steps, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(start);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::linspace_Tensor_Scalar::redispatch(
      _dk, start, end, steps, dtype, layout, device, pin_memory);
}
// aten::linspace.Scalar_Tensor(Scalar start, Tensor end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor linspace_Scalar_Tensor(const at::Scalar & start, const at::Tensor & end, int64_t steps, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(end);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::linspace_Scalar_Tensor::redispatch(
      _dk, start, end, steps, dtype, layout, device, pin_memory);
}
// aten::logspace(Scalar start, Scalar end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor logspace(const at::Scalar & start, const at::Scalar & end, int64_t steps, double base, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::logspace::redispatch(
      _dk, start, end, steps, base, dtype, layout, device, pin_memory);
}
// aten::logspace.Tensor_Tensor(Tensor start, Tensor end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor logspace_Tensor_Tensor(const at::Tensor & start, const at::Tensor & end, int64_t steps, double base, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(start, end);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::logspace_Tensor_Tensor::redispatch(
      _dk, start, end, steps, base, dtype, layout, device, pin_memory);
}
// aten::logspace.Tensor_Scalar(Tensor start, Scalar end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor logspace_Tensor_Scalar(const at::Tensor & start, const at::Scalar & end, int64_t steps, double base, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(start);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::logspace_Tensor_Scalar::redispatch(
      _dk, start, end, steps, base, dtype, layout, device, pin_memory);
}
// aten::logspace.Scalar_Tensor(Scalar start, Tensor end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor logspace_Scalar_Tensor(const at::Scalar & start, const at::Tensor & end, int64_t steps, double base, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(end);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::logspace_Scalar_Tensor::redispatch(
      _dk, start, end, steps, base, dtype, layout, device, pin_memory);
}
// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor ones_names(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::ones_names::redispatch(
      _dk, size, names, dtype, layout, device, pin_memory);
}
// aten::ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor ones(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::ones::redispatch(
      _dk, size, dtype, layout, device, pin_memory);
}
// aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor scalar_tensor(const at::Scalar & s, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::scalar_tensor::redispatch(
      _dk, s, dtype, layout, device, pin_memory);
}
// aten::rand.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor rand_names(c10::SymIntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::rand_names::redispatch(
      _dk, size, names, dtype, layout, device, pin_memory);
}
// aten::rand.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor rand_generator_with_names(c10::SymIntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::rand_generator_with_names::redispatch(
      _dk, size, generator, names, dtype, layout, device, pin_memory);
}
// aten::rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor rand(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::rand::redispatch(
      _dk, size, dtype, layout, device, pin_memory);
}
// aten::rand.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor rand_generator(c10::SymIntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::rand_generator::redispatch(
      _dk, size, generator, dtype, layout, device, pin_memory);
}
// aten::randint(SymInt high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randint(c10::SymInt high, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::randint::redispatch(
      _dk, high, size, dtype, layout, device, pin_memory);
}
// aten::randint.generator(SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randint_generator(c10::SymInt high, c10::SymIntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::randint_generator::redispatch(
      _dk, high, size, generator, dtype, layout, device, pin_memory);
}
// aten::randint.low(SymInt low, SymInt high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randint_low(c10::SymInt low, c10::SymInt high, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::randint_low::redispatch(
      _dk, low, high, size, dtype, layout, device, pin_memory);
}
// aten::randint.low_generator(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randint_low_generator(c10::SymInt low, c10::SymInt high, c10::SymIntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::randint_low_generator::redispatch(
      _dk, low, high, size, generator, dtype, layout, device, pin_memory);
}
// aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randn(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::randn::redispatch(
      _dk, size, dtype, layout, device, pin_memory);
}
// aten::randn.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randn_generator(c10::SymIntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::randn_generator::redispatch(
      _dk, size, generator, dtype, layout, device, pin_memory);
}
// aten::randn.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randn_names(c10::SymIntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::randn_names::redispatch(
      _dk, size, names, dtype, layout, device, pin_memory);
}
// aten::randn.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randn_generator_with_names(c10::SymIntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::randn_generator_with_names::redispatch(
      _dk, size, generator, names, dtype, layout, device, pin_memory);
}
// aten::randperm(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randperm(c10::SymInt n, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::randperm::redispatch(
      _dk, n, dtype, layout, device, pin_memory);
}
// aten::randperm.generator(SymInt n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randperm_generator(c10::SymInt n, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::randperm_generator::redispatch(
      _dk, n, generator, dtype, layout, device, pin_memory);
}
// aten::range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor range_step(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::range_step::redispatch(
      _dk, start, end, step, dtype, layout, device, pin_memory);
}
// aten::range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor range(const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::range::redispatch(
      _dk, start, end, dtype, layout, device, pin_memory);
}
// aten::zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor zeros_names(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::zeros_names::redispatch(
      _dk, size, names, dtype, layout, device, pin_memory);
}
// aten::_efficientzerotensor(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _efficientzerotensor(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::_efficientzerotensor::redispatch(
      _dk, size, dtype, layout, device, pin_memory);
}
// aten::zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor zeros(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::zeros::redispatch(
      _dk, size, dtype, layout, device, pin_memory);
}
// aten::sparse_compressed_tensor.comp_plain_value_size(Tensor compressed_indices, Tensor plain_indices, Tensor values, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_compressed_tensor_comp_plain_value_size(const at::Tensor & compressed_indices, const at::Tensor & plain_indices, const at::Tensor & values, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(compressed_indices, plain_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::sparse_compressed_tensor_comp_plain_value_size::redispatch(
      _dk, compressed_indices, plain_indices, values, size, dtype, layout, device, pin_memory);
}
// aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_csr_tensor_crow_col_value_size(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(crow_indices, col_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::sparse_csr_tensor_crow_col_value_size::redispatch(
      _dk, crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}
// aten::sparse_csc_tensor.ccol_row_value_size(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_csc_tensor_ccol_row_value_size(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(ccol_indices, row_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::sparse_csc_tensor_ccol_row_value_size::redispatch(
      _dk, ccol_indices, row_indices, values, size, dtype, layout, device, pin_memory);
}
// aten::sparse_bsr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_bsr_tensor_crow_col_value_size(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(crow_indices, col_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::sparse_bsr_tensor_crow_col_value_size::redispatch(
      _dk, crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}
// aten::sparse_bsc_tensor.ccol_row_value_size(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_bsc_tensor_ccol_row_value_size(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(ccol_indices, row_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::sparse_bsc_tensor_ccol_row_value_size::redispatch(
      _dk, ccol_indices, row_indices, values, size, dtype, layout, device, pin_memory);
}
// aten::sparse_compressed_tensor.comp_plain_value(Tensor compressed_indices, Tensor plain_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_compressed_tensor_comp_plain_value(const at::Tensor & compressed_indices, const at::Tensor & plain_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(compressed_indices, plain_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::sparse_compressed_tensor_comp_plain_value::redispatch(
      _dk, compressed_indices, plain_indices, values, dtype, layout, device, pin_memory);
}
// aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_csr_tensor_crow_col_value(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(crow_indices, col_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::sparse_csr_tensor_crow_col_value::redispatch(
      _dk, crow_indices, col_indices, values, dtype, layout, device, pin_memory);
}
// aten::sparse_csc_tensor.ccol_row_value(Tensor ccol_indices, Tensor row_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_csc_tensor_ccol_row_value(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(ccol_indices, row_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::sparse_csc_tensor_ccol_row_value::redispatch(
      _dk, ccol_indices, row_indices, values, dtype, layout, device, pin_memory);
}
// aten::sparse_bsr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_bsr_tensor_crow_col_value(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(crow_indices, col_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::sparse_bsr_tensor_crow_col_value::redispatch(
      _dk, crow_indices, col_indices, values, dtype, layout, device, pin_memory);
}
// aten::sparse_bsc_tensor.ccol_row_value(Tensor ccol_indices, Tensor row_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_bsc_tensor_ccol_row_value(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(ccol_indices, row_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::sparse_bsc_tensor_ccol_row_value::redispatch(
      _dk, ccol_indices, row_indices, values, dtype, layout, device, pin_memory);
}
// aten::_sparse_compressed_tensor_unsafe(Tensor compressed_indices, Tensor plain_indices, Tensor values, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _sparse_compressed_tensor_unsafe(const at::Tensor & compressed_indices, const at::Tensor & plain_indices, const at::Tensor & values, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(compressed_indices, plain_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::_sparse_compressed_tensor_unsafe::redispatch(
      _dk, compressed_indices, plain_indices, values, size, dtype, layout, device, pin_memory);
}
// aten::_sparse_csr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _sparse_csr_tensor_unsafe(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(crow_indices, col_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::_sparse_csr_tensor_unsafe::redispatch(
      _dk, crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}
// aten::_sparse_csc_tensor_unsafe(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _sparse_csc_tensor_unsafe(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(ccol_indices, row_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::_sparse_csc_tensor_unsafe::redispatch(
      _dk, ccol_indices, row_indices, values, size, dtype, layout, device, pin_memory);
}
// aten::_sparse_bsr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _sparse_bsr_tensor_unsafe(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(crow_indices, col_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::_sparse_bsr_tensor_unsafe::redispatch(
      _dk, crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}
// aten::_sparse_bsc_tensor_unsafe(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _sparse_bsc_tensor_unsafe(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(ccol_indices, row_indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::_sparse_bsc_tensor_unsafe::redispatch(
      _dk, ccol_indices, row_indices, values, size, dtype, layout, device, pin_memory);
}
// aten::sparse_coo_tensor.size(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_coo_tensor_size(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::sparse_coo_tensor_size::redispatch(
      _dk, size, dtype, layout, device, pin_memory);
}
// aten::sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool? is_coalesced=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_coo_tensor_indices(const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<bool> is_coalesced) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::sparse_coo_tensor_indices::redispatch(
      _dk, indices, values, dtype, layout, device, pin_memory, is_coalesced);
}
// aten::sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool? is_coalesced=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_coo_tensor_indices_size(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<bool> is_coalesced) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::sparse_coo_tensor_indices_size::redispatch(
      _dk, indices, values, size, dtype, layout, device, pin_memory, is_coalesced);
}
// aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool? is_coalesced=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _sparse_coo_tensor_unsafe(const at::Tensor & indices, const at::Tensor & values, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<bool> is_coalesced) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::_sparse_coo_tensor_unsafe::redispatch(
      _dk, indices, values, size, dtype, layout, device, pin_memory, is_coalesced);
}
// aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::_sparse_coo_tensor_with_dims::redispatch(
      _dk, sparse_dim, dense_dim, size, dtype, layout, device, pin_memory);
}
// aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, SymInt[] size, Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? is_coalesced=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, c10::SymIntArrayRef size, const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<bool> is_coalesced) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(indices, values);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::_sparse_coo_tensor_with_dims_and_tensors::redispatch(
      _dk, sparse_dim, dense_dim, size, indices, values, dtype, layout, device, pin_memory, is_coalesced);
}
// aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _to_copy(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, c10::optional<at::MemoryFormat> memory_format) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(self);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::_to_copy::redispatch(
      _dk, self, dtype, layout, device, pin_memory, non_blocking, memory_format);
}
// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
C10_ALWAYS_INLINE
at::Tensor to_dtype_layout(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(self);
DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return at::_ops::to_dtype_layout::redispatch(
      _dk, self, dtype, layout, device, pin_memory, non_blocking, copy, memory_format);
}
// aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor tril_indices(int64_t row, int64_t col, int64_t offset, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::tril_indices::redispatch(
      _dk, row, col, offset, dtype, layout, device, pin_memory);
}
// aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor triu_indices(int64_t row, int64_t col, int64_t offset, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::triu_indices::redispatch(
      _dk, row, col, offset, dtype, layout, device, pin_memory);
}
// aten::normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor normal_float_float(double mean, double std, c10::SymIntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::normal_float_float::redispatch(
      _dk, mean, std, size, generator, dtype, layout, device, pin_memory);
}
// aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor fft_fftfreq(int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::fft_fftfreq::redispatch(
      _dk, n, d, dtype, layout, device, pin_memory);
}
// aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor fft_rfftfreq(int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return at::_ops::fft_rfftfreq::redispatch(
      _dk, n, d, dtype, layout, device, pin_memory);
}

bool is_pinned(const Tensor& self, c10::optional<at::Device> device) {
  // Only CPU tensors can be pinned
  if (!self.is_cpu()) {
    return false;
  }
  // TODO: fetch scalar type from Tensor? But it doesn't really matter...
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(c10::nullopt, self.layout(), device.value_or(at::kCUDA)));
  return at::_ops::is_pinned::redispatch(_dk, self, device);
}

at::Tensor _pin_memory(const Tensor& self, c10::optional<at::Device> device) {
  TORCH_CHECK(self.device().is_cpu(), "cannot pin '", self.toString(), "' only dense CPU tensors can be pinned");
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(c10::nullopt, self.layout(), device.value_or(at::kCUDA)));
  if (self.is_nested()) {
    constexpr auto nested_key_set = c10::DispatchKeySet(
        {c10::DispatchKey::NestedTensor, c10::DispatchKey::AutogradNestedTensor});
    _dk = _dk.add(self.key_set() & nested_key_set);
  }
  return at::_ops::_pin_memory::redispatch(_dk, self, device);
}

TORCH_LIBRARY_IMPL(aten, BackendSelect, m) {
  m.impl("aten::_make_dep_token", TORCH_FN(_make_dep_token));
  m.impl("aten::_cudnn_init_dropout_state", TORCH_FN(_cudnn_init_dropout_state));
  m.impl("aten::arange", TORCH_FN(arange));
  m.impl("aten::arange.start", TORCH_FN(arange_start));
  m.impl("aten::arange.start_step", TORCH_FN(arange_start_step));
  m.impl("aten::bartlett_window", TORCH_FN(bartlett_window));
  m.impl("aten::bartlett_window.periodic", TORCH_FN(bartlett_window_periodic));
  m.impl("aten::blackman_window", TORCH_FN(blackman_window));
  m.impl("aten::blackman_window.periodic", TORCH_FN(blackman_window_periodic));
  m.impl("aten::empty.names", TORCH_FN(empty_names));
  m.impl("aten::empty.memory_format", TORCH_FN(empty_memory_format));
  m.impl("aten::empty_permuted", TORCH_FN(empty_permuted));
  m.impl("aten::_empty_affine_quantized", TORCH_FN(_empty_affine_quantized));
  m.impl("aten::_empty_per_channel_affine_quantized", TORCH_FN(_empty_per_channel_affine_quantized));
  m.impl("aten::empty_quantized", TORCH_FN(empty_quantized));
  m.impl("aten::empty_strided", TORCH_FN(empty_strided));
  m.impl("aten::eye", TORCH_FN(eye));
  m.impl("aten::eye.m", TORCH_FN(eye_m));
  m.impl("aten::full.names", TORCH_FN(full_names));
  m.impl("aten::full", TORCH_FN(full));
  m.impl("aten::from_file", TORCH_FN(from_file));
  m.impl("aten::hann_window", TORCH_FN(hann_window));
  m.impl("aten::hann_window.periodic", TORCH_FN(hann_window_periodic));
  m.impl("aten::hamming_window", TORCH_FN(hamming_window));
  m.impl("aten::hamming_window.periodic", TORCH_FN(hamming_window_periodic));
  m.impl("aten::hamming_window.periodic_alpha", TORCH_FN(hamming_window_periodic_alpha));
  m.impl("aten::hamming_window.periodic_alpha_beta", TORCH_FN(hamming_window_periodic_alpha_beta));
  m.impl("aten::kaiser_window", TORCH_FN(kaiser_window));
  m.impl("aten::kaiser_window.periodic", TORCH_FN(kaiser_window_periodic));
  m.impl("aten::kaiser_window.beta", TORCH_FN(kaiser_window_beta));
  m.impl("aten::linspace", TORCH_FN(linspace));
  m.impl("aten::linspace.Tensor_Tensor", TORCH_FN(linspace_Tensor_Tensor));
  m.impl("aten::linspace.Tensor_Scalar", TORCH_FN(linspace_Tensor_Scalar));
  m.impl("aten::linspace.Scalar_Tensor", TORCH_FN(linspace_Scalar_Tensor));
  m.impl("aten::logspace", TORCH_FN(logspace));
  m.impl("aten::logspace.Tensor_Tensor", TORCH_FN(logspace_Tensor_Tensor));
  m.impl("aten::logspace.Tensor_Scalar", TORCH_FN(logspace_Tensor_Scalar));
  m.impl("aten::logspace.Scalar_Tensor", TORCH_FN(logspace_Scalar_Tensor));
  m.impl("aten::ones.names", TORCH_FN(ones_names));
  m.impl("aten::ones", TORCH_FN(ones));
  m.impl("aten::scalar_tensor", TORCH_FN(scalar_tensor));
  m.impl("aten::rand.names", TORCH_FN(rand_names));
  m.impl("aten::rand.generator_with_names", TORCH_FN(rand_generator_with_names));
  m.impl("aten::rand", TORCH_FN(rand));
  m.impl("aten::rand.generator", TORCH_FN(rand_generator));
  m.impl("aten::randint", TORCH_FN(randint));
  m.impl("aten::randint.generator", TORCH_FN(randint_generator));
  m.impl("aten::randint.low", TORCH_FN(randint_low));
  m.impl("aten::randint.low_generator", TORCH_FN(randint_low_generator));
  m.impl("aten::randn", TORCH_FN(randn));
  m.impl("aten::randn.generator", TORCH_FN(randn_generator));
  m.impl("aten::randn.names", TORCH_FN(randn_names));
  m.impl("aten::randn.generator_with_names", TORCH_FN(randn_generator_with_names));
  m.impl("aten::randperm", TORCH_FN(randperm));
  m.impl("aten::randperm.generator", TORCH_FN(randperm_generator));
  m.impl("aten::range.step", TORCH_FN(range_step));
  m.impl("aten::range", TORCH_FN(range));
  m.impl("aten::zeros.names", TORCH_FN(zeros_names));
  m.impl("aten::_efficientzerotensor", TORCH_FN(_efficientzerotensor));
  m.impl("aten::zeros", TORCH_FN(zeros));
  m.impl("aten::sparse_compressed_tensor.comp_plain_value_size", TORCH_FN(sparse_compressed_tensor_comp_plain_value_size));
  m.impl("aten::sparse_csr_tensor.crow_col_value_size", TORCH_FN(sparse_csr_tensor_crow_col_value_size));
  m.impl("aten::sparse_csc_tensor.ccol_row_value_size", TORCH_FN(sparse_csc_tensor_ccol_row_value_size));
  m.impl("aten::sparse_bsr_tensor.crow_col_value_size", TORCH_FN(sparse_bsr_tensor_crow_col_value_size));
  m.impl("aten::sparse_bsc_tensor.ccol_row_value_size", TORCH_FN(sparse_bsc_tensor_ccol_row_value_size));
  m.impl("aten::sparse_compressed_tensor.comp_plain_value", TORCH_FN(sparse_compressed_tensor_comp_plain_value));
  m.impl("aten::sparse_csr_tensor.crow_col_value", TORCH_FN(sparse_csr_tensor_crow_col_value));
  m.impl("aten::sparse_csc_tensor.ccol_row_value", TORCH_FN(sparse_csc_tensor_ccol_row_value));
  m.impl("aten::sparse_bsr_tensor.crow_col_value", TORCH_FN(sparse_bsr_tensor_crow_col_value));
  m.impl("aten::sparse_bsc_tensor.ccol_row_value", TORCH_FN(sparse_bsc_tensor_ccol_row_value));
  m.impl("aten::_sparse_compressed_tensor_unsafe", TORCH_FN(_sparse_compressed_tensor_unsafe));
  m.impl("aten::_sparse_csr_tensor_unsafe", TORCH_FN(_sparse_csr_tensor_unsafe));
  m.impl("aten::_sparse_csc_tensor_unsafe", TORCH_FN(_sparse_csc_tensor_unsafe));
  m.impl("aten::_sparse_bsr_tensor_unsafe", TORCH_FN(_sparse_bsr_tensor_unsafe));
  m.impl("aten::_sparse_bsc_tensor_unsafe", TORCH_FN(_sparse_bsc_tensor_unsafe));
  m.impl("aten::sparse_coo_tensor.size", TORCH_FN(sparse_coo_tensor_size));
  m.impl("aten::sparse_coo_tensor.indices", TORCH_FN(sparse_coo_tensor_indices));
  m.impl("aten::sparse_coo_tensor.indices_size", TORCH_FN(sparse_coo_tensor_indices_size));
  m.impl("aten::_sparse_coo_tensor_unsafe", TORCH_FN(_sparse_coo_tensor_unsafe));
  m.impl("aten::_sparse_coo_tensor_with_dims", TORCH_FN(_sparse_coo_tensor_with_dims));
  m.impl("aten::_sparse_coo_tensor_with_dims_and_tensors", TORCH_FN(_sparse_coo_tensor_with_dims_and_tensors));
  m.impl("aten::_to_copy", TORCH_FN(_to_copy));
  m.impl("aten::to.dtype_layout", TORCH_FN(to_dtype_layout));
  m.impl("aten::tril_indices", TORCH_FN(tril_indices));
  m.impl("aten::triu_indices", TORCH_FN(triu_indices));
  m.impl("aten::normal.float_float", TORCH_FN(normal_float_float));
  m.impl("aten::fft_fftfreq", TORCH_FN(fft_fftfreq));
  m.impl("aten::fft_rfftfreq", TORCH_FN(fft_rfftfreq));;
  m.impl(TORCH_SELECTIVE_NAME("aten::is_pinned"), TORCH_FN(is_pinned));
  m.impl(TORCH_SELECTIVE_NAME("aten::_pin_memory"), TORCH_FN(_pin_memory));
}

} // namespace
} // at
