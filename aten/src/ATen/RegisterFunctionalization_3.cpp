#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
// @generated by torchgen/gen.py from RegisterFunctionalization.cpp

#include <ATen/core/LegacyTypeDispatch.h>
#include <ATen/EmptyTensor.h>
#include <ATen/FunctionalTensorWrapper.h>
#include <ATen/FunctionalInverses.h>
#include <ATen/MemoryOverlap.h>
#include <torch/library.h>

#ifndef AT_PER_OPERATOR_HEADERS
#include <ATen/Operators.h>
#include <ATen/NativeFunctions.h>
#else
// needed for the meta tensor calls to get stride info in functionalization
#include <ATen/ops/empty_strided_native.h>
// needed for special handling of copy_().
// See Note [functionalizating copy_() and not preserving strides]
#include <ATen/ops/to_ops.h>
#include <ATen/ops/expand_copy_ops.h>

#include <ATen/ops/_masked_scale_native.h>
#include <ATen/ops/_masked_scale_ops.h>
#include <ATen/ops/_masked_scale_native.h>
#include <ATen/ops/_masked_scale_ops.h>
#include <ATen/ops/native_dropout_native.h>
#include <ATen/ops/native_dropout_ops.h>
#include <ATen/ops/native_dropout_native.h>
#include <ATen/ops/native_dropout_ops.h>
#include <ATen/ops/native_dropout_backward_native.h>
#include <ATen/ops/native_dropout_backward_ops.h>
#include <ATen/ops/native_dropout_backward_native.h>
#include <ATen/ops/native_dropout_backward_ops.h>
#include <ATen/ops/asinh_native.h>
#include <ATen/ops/asinh_ops.h>
#include <ATen/ops/asinh_native.h>
#include <ATen/ops/asinh_ops.h>
#include <ATen/ops/asinh_native.h>
#include <ATen/ops/asinh_ops.h>
#include <ATen/ops/arctanh_native.h>
#include <ATen/ops/arctanh_ops.h>
#include <ATen/ops/arctanh_native.h>
#include <ATen/ops/arctanh_ops.h>
#include <ATen/ops/arctanh_native.h>
#include <ATen/ops/arctanh_ops.h>
#include <ATen/ops/baddbmm_native.h>
#include <ATen/ops/baddbmm_ops.h>
#include <ATen/ops/baddbmm_native.h>
#include <ATen/ops/baddbmm_ops.h>
#include <ATen/ops/baddbmm_native.h>
#include <ATen/ops/baddbmm_ops.h>
#include <ATen/ops/quantized_batch_norm_native.h>
#include <ATen/ops/quantized_batch_norm_ops.h>
#include <ATen/ops/quantized_batch_norm_native.h>
#include <ATen/ops/quantized_batch_norm_ops.h>
#include <ATen/ops/bernoulli_native.h>
#include <ATen/ops/bernoulli_ops.h>
#include <ATen/ops/bernoulli_native.h>
#include <ATen/ops/bernoulli_ops.h>
#include <ATen/ops/bernoulli_native.h>
#include <ATen/ops/bernoulli_ops.h>
#include <ATen/ops/bernoulli_native.h>
#include <ATen/ops/bernoulli_ops.h>
#include <ATen/ops/bernoulli_native.h>
#include <ATen/ops/bernoulli_ops.h>
#include <ATen/ops/bernoulli_native.h>
#include <ATen/ops/bernoulli_ops.h>
#include <ATen/ops/bernoulli_native.h>
#include <ATen/ops/bernoulli_ops.h>
#include <ATen/ops/bernoulli_native.h>
#include <ATen/ops/bernoulli_ops.h>
#include <ATen/ops/bmm_native.h>
#include <ATen/ops/bmm_ops.h>
#include <ATen/ops/bmm_native.h>
#include <ATen/ops/bmm_ops.h>
#include <ATen/ops/clamp_max_native.h>
#include <ATen/ops/clamp_max_ops.h>
#include <ATen/ops/clamp_max_native.h>
#include <ATen/ops/clamp_max_ops.h>
#include <ATen/ops/clamp_max_native.h>
#include <ATen/ops/clamp_max_ops.h>
#include <ATen/ops/clamp_max_native.h>
#include <ATen/ops/clamp_max_ops.h>
#include <ATen/ops/clamp_max_native.h>
#include <ATen/ops/clamp_max_ops.h>
#include <ATen/ops/clamp_max_native.h>
#include <ATen/ops/clamp_max_ops.h>
#include <ATen/ops/clamp_min_native.h>
#include <ATen/ops/clamp_min_ops.h>
#include <ATen/ops/clamp_min_native.h>
#include <ATen/ops/clamp_min_ops.h>
#include <ATen/ops/clamp_min_native.h>
#include <ATen/ops/clamp_min_ops.h>
#include <ATen/ops/clamp_min_native.h>
#include <ATen/ops/clamp_min_ops.h>
#include <ATen/ops/clamp_min_native.h>
#include <ATen/ops/clamp_min_ops.h>
#include <ATen/ops/clamp_min_native.h>
#include <ATen/ops/clamp_min_ops.h>
#include <ATen/ops/clip_native.h>
#include <ATen/ops/clip_ops.h>
#include <ATen/ops/clip_native.h>
#include <ATen/ops/clip_ops.h>
#include <ATen/ops/clip_native.h>
#include <ATen/ops/clip_ops.h>
#include <ATen/ops/clip_native.h>
#include <ATen/ops/clip_ops.h>
#include <ATen/ops/clip_native.h>
#include <ATen/ops/clip_ops.h>
#include <ATen/ops/clip_native.h>
#include <ATen/ops/clip_ops.h>
#include <ATen/ops/complex_native.h>
#include <ATen/ops/complex_ops.h>
#include <ATen/ops/complex_native.h>
#include <ATen/ops/complex_ops.h>
#include <ATen/ops/constant_pad_nd_native.h>
#include <ATen/ops/constant_pad_nd_ops.h>
#include <ATen/ops/constant_pad_nd_native.h>
#include <ATen/ops/constant_pad_nd_ops.h>
#include <ATen/ops/conv_tbc_native.h>
#include <ATen/ops/conv_tbc_ops.h>
#include <ATen/ops/conv_tbc_native.h>
#include <ATen/ops/conv_tbc_ops.h>
#include <ATen/ops/_copy_from_and_resize_native.h>
#include <ATen/ops/_copy_from_and_resize_ops.h>
#include <ATen/ops/_copy_from_and_resize_native.h>
#include <ATen/ops/_copy_from_and_resize_ops.h>
#include <ATen/ops/cos_native.h>
#include <ATen/ops/cos_ops.h>
#include <ATen/ops/cos_native.h>
#include <ATen/ops/cos_ops.h>
#include <ATen/ops/cos_native.h>
#include <ATen/ops/cos_ops.h>
#include <ATen/ops/count_nonzero_native.h>
#include <ATen/ops/count_nonzero_ops.h>
#include <ATen/ops/count_nonzero_native.h>
#include <ATen/ops/count_nonzero_ops.h>
#include <ATen/ops/count_nonzero_native.h>
#include <ATen/ops/count_nonzero_ops.h>
#include <ATen/ops/count_nonzero_native.h>
#include <ATen/ops/count_nonzero_ops.h>
#include <ATen/ops/cudnn_affine_grid_generator_backward_native.h>
#include <ATen/ops/cudnn_affine_grid_generator_backward_ops.h>
#include <ATen/ops/cudnn_affine_grid_generator_backward_native.h>
#include <ATen/ops/cudnn_affine_grid_generator_backward_ops.h>
#include <ATen/ops/cudnn_batch_norm_native.h>
#include <ATen/ops/cudnn_batch_norm_ops.h>
#include <ATen/ops/cudnn_batch_norm_native.h>
#include <ATen/ops/cudnn_batch_norm_ops.h>
#include <ATen/ops/mps_convolution_transpose_backward_native.h>
#include <ATen/ops/mps_convolution_transpose_backward_ops.h>
#include <ATen/ops/mps_convolution_transpose_backward_native.h>
#include <ATen/ops/mps_convolution_transpose_backward_ops.h>
#include <ATen/ops/cudnn_grid_sampler_backward_native.h>
#include <ATen/ops/cudnn_grid_sampler_backward_ops.h>
#include <ATen/ops/cudnn_grid_sampler_backward_native.h>
#include <ATen/ops/cudnn_grid_sampler_backward_ops.h>
#include <ATen/ops/cummin_native.h>
#include <ATen/ops/cummin_ops.h>
#include <ATen/ops/cummin_native.h>
#include <ATen/ops/cummin_ops.h>
#include <ATen/ops/cummin_native.h>
#include <ATen/ops/cummin_ops.h>
#include <ATen/ops/cummin_native.h>
#include <ATen/ops/cummin_ops.h>
#include <ATen/ops/cumsum_native.h>
#include <ATen/ops/cumsum_ops.h>
#include <ATen/ops/cumsum_native.h>
#include <ATen/ops/cumsum_ops.h>
#include <ATen/ops/cumsum_native.h>
#include <ATen/ops/cumsum_ops.h>
#include <ATen/ops/cumsum_native.h>
#include <ATen/ops/cumsum_ops.h>
#include <ATen/ops/cumsum_native.h>
#include <ATen/ops/cumsum_ops.h>
#include <ATen/ops/cumsum_native.h>
#include <ATen/ops/cumsum_ops.h>
#include <ATen/ops/_ctc_loss_native.h>
#include <ATen/ops/_ctc_loss_ops.h>
#include <ATen/ops/_ctc_loss_native.h>
#include <ATen/ops/_ctc_loss_ops.h>
#include <ATen/ops/_ctc_loss_native.h>
#include <ATen/ops/_ctc_loss_ops.h>
#include <ATen/ops/_ctc_loss_native.h>
#include <ATen/ops/_ctc_loss_ops.h>
#include <ATen/ops/_ctc_loss_backward_native.h>
#include <ATen/ops/_ctc_loss_backward_ops.h>
#include <ATen/ops/_ctc_loss_backward_native.h>
#include <ATen/ops/_ctc_loss_backward_ops.h>
#include <ATen/ops/embedding_native.h>
#include <ATen/ops/embedding_ops.h>
#include <ATen/ops/embedding_native.h>
#include <ATen/ops/embedding_ops.h>
#include <ATen/ops/embedding_dense_backward_native.h>
#include <ATen/ops/embedding_dense_backward_ops.h>
#include <ATen/ops/embedding_dense_backward_native.h>
#include <ATen/ops/embedding_dense_backward_ops.h>
#include <ATen/ops/empty_permuted_native.h>
#include <ATen/ops/empty_permuted_ops.h>
#include <ATen/ops/empty_permuted_native.h>
#include <ATen/ops/empty_permuted_ops.h>
#include <ATen/ops/new_zeros_native.h>
#include <ATen/ops/new_zeros_ops.h>
#include <ATen/ops/new_zeros_native.h>
#include <ATen/ops/new_zeros_ops.h>
#include <ATen/ops/new_ones_native.h>
#include <ATen/ops/new_ones_ops.h>
#include <ATen/ops/new_ones_native.h>
#include <ATen/ops/new_ones_ops.h>
#include <ATen/ops/_empty_per_channel_affine_quantized_native.h>
#include <ATen/ops/_empty_per_channel_affine_quantized_ops.h>
#include <ATen/ops/_empty_per_channel_affine_quantized_native.h>
#include <ATen/ops/_empty_per_channel_affine_quantized_ops.h>
#include <ATen/ops/resize_native.h>
#include <ATen/ops/resize_ops.h>
#include <ATen/ops/resize_native.h>
#include <ATen/ops/resize_ops.h>
#include <ATen/ops/resize_native.h>
#include <ATen/ops/resize_ops.h>
#include <ATen/ops/exp2_native.h>
#include <ATen/ops/exp2_ops.h>
#include <ATen/ops/exp2_native.h>
#include <ATen/ops/exp2_ops.h>
#include <ATen/ops/exp2_native.h>
#include <ATen/ops/exp2_ops.h>
#include <ATen/ops/_grid_sampler_2d_cpu_fallback_native.h>
#include <ATen/ops/_grid_sampler_2d_cpu_fallback_ops.h>
#include <ATen/ops/_grid_sampler_2d_cpu_fallback_native.h>
#include <ATen/ops/_grid_sampler_2d_cpu_fallback_ops.h>
#include <ATen/ops/grid_sampler_3d_backward_native.h>
#include <ATen/ops/grid_sampler_3d_backward_ops.h>
#include <ATen/ops/grid_sampler_3d_backward_native.h>
#include <ATen/ops/grid_sampler_3d_backward_ops.h>
#include <ATen/ops/_fft_c2c_native.h>
#include <ATen/ops/_fft_c2c_ops.h>
#include <ATen/ops/_fft_c2c_native.h>
#include <ATen/ops/_fft_c2c_ops.h>
#include <ATen/ops/index_copy_native.h>
#include <ATen/ops/index_copy_ops.h>
#include <ATen/ops/index_copy_native.h>
#include <ATen/ops/index_copy_ops.h>
#include <ATen/ops/index_copy_native.h>
#include <ATen/ops/index_copy_ops.h>
#include <ATen/ops/isin_native.h>
#include <ATen/ops/isin_ops.h>
#include <ATen/ops/isin_native.h>
#include <ATen/ops/isin_ops.h>
#include <ATen/ops/isin_native.h>
#include <ATen/ops/isin_ops.h>
#include <ATen/ops/isin_native.h>
#include <ATen/ops/isin_ops.h>
#include <ATen/ops/isin_native.h>
#include <ATen/ops/isin_ops.h>
#include <ATen/ops/isin_native.h>
#include <ATen/ops/isin_ops.h>
#include <ATen/ops/kron_native.h>
#include <ATen/ops/kron_ops.h>
#include <ATen/ops/kron_native.h>
#include <ATen/ops/kron_ops.h>
#include <ATen/ops/nan_to_num_native.h>
#include <ATen/ops/nan_to_num_ops.h>
#include <ATen/ops/nan_to_num_native.h>
#include <ATen/ops/nan_to_num_ops.h>
#include <ATen/ops/nan_to_num_native.h>
#include <ATen/ops/nan_to_num_ops.h>
#include <ATen/ops/linear_native.h>
#include <ATen/ops/linear_ops.h>
#include <ATen/ops/linear_native.h>
#include <ATen/ops/linear_ops.h>
#include <ATen/ops/mkldnn_linear_native.h>
#include <ATen/ops/mkldnn_linear_ops.h>
#include <ATen/ops/mkldnn_linear_native.h>
#include <ATen/ops/mkldnn_linear_ops.h>
#include <ATen/ops/linspace_native.h>
#include <ATen/ops/linspace_ops.h>
#include <ATen/ops/linspace_native.h>
#include <ATen/ops/linspace_ops.h>
#include <ATen/ops/linspace_native.h>
#include <ATen/ops/linspace_ops.h>
#include <ATen/ops/linspace_native.h>
#include <ATen/ops/linspace_ops.h>
#include <ATen/ops/linspace_native.h>
#include <ATen/ops/linspace_ops.h>
#include <ATen/ops/linspace_native.h>
#include <ATen/ops/linspace_ops.h>
#include <ATen/ops/linspace_native.h>
#include <ATen/ops/linspace_ops.h>
#include <ATen/ops/linspace_native.h>
#include <ATen/ops/linspace_ops.h>
#include <ATen/ops/log_native.h>
#include <ATen/ops/log_ops.h>
#include <ATen/ops/log_native.h>
#include <ATen/ops/log_ops.h>
#include <ATen/ops/log_native.h>
#include <ATen/ops/log_ops.h>
#include <ATen/ops/log_softmax_native.h>
#include <ATen/ops/log_softmax_ops.h>
#include <ATen/ops/log_softmax_native.h>
#include <ATen/ops/log_softmax_ops.h>
#include <ATen/ops/_log_softmax_native.h>
#include <ATen/ops/_log_softmax_ops.h>
#include <ATen/ops/_log_softmax_native.h>
#include <ATen/ops/_log_softmax_ops.h>
#include <ATen/ops/_log_softmax_backward_data_native.h>
#include <ATen/ops/_log_softmax_backward_data_ops.h>
#include <ATen/ops/_log_softmax_backward_data_native.h>
#include <ATen/ops/_log_softmax_backward_data_ops.h>
#include <ATen/ops/max_native.h>
#include <ATen/ops/max_ops.h>
#include <ATen/ops/max_native.h>
#include <ATen/ops/max_ops.h>
#include <ATen/ops/max_native.h>
#include <ATen/ops/max_ops.h>
#include <ATen/ops/max_native.h>
#include <ATen/ops/max_ops.h>
#include <ATen/ops/amax_native.h>
#include <ATen/ops/amax_ops.h>
#include <ATen/ops/amax_native.h>
#include <ATen/ops/amax_ops.h>
#include <ATen/ops/mkldnn_max_pool2d_backward_native.h>
#include <ATen/ops/mkldnn_max_pool2d_backward_ops.h>
#include <ATen/ops/mkldnn_max_pool2d_backward_native.h>
#include <ATen/ops/mkldnn_max_pool2d_backward_ops.h>
#include <ATen/ops/mkldnn_max_pool3d_native.h>
#include <ATen/ops/mkldnn_max_pool3d_ops.h>
#include <ATen/ops/mkldnn_max_pool3d_native.h>
#include <ATen/ops/mkldnn_max_pool3d_ops.h>
#include <ATen/ops/quantized_max_pool2d_native.h>
#include <ATen/ops/quantized_max_pool2d_ops.h>
#include <ATen/ops/quantized_max_pool2d_native.h>
#include <ATen/ops/quantized_max_pool2d_ops.h>
#include <ATen/ops/quantized_max_pool3d_native.h>
#include <ATen/ops/quantized_max_pool3d_ops.h>
#include <ATen/ops/quantized_max_pool3d_native.h>
#include <ATen/ops/quantized_max_pool3d_ops.h>
#include <ATen/ops/mean_native.h>
#include <ATen/ops/mean_ops.h>
#include <ATen/ops/mean_native.h>
#include <ATen/ops/mean_ops.h>
#include <ATen/ops/mean_native.h>
#include <ATen/ops/mean_ops.h>
#include <ATen/ops/mean_native.h>
#include <ATen/ops/mean_ops.h>
#include <ATen/ops/nanmedian_native.h>
#include <ATen/ops/nanmedian_ops.h>
#include <ATen/ops/nanmedian_native.h>
#include <ATen/ops/nanmedian_ops.h>
#include <ATen/ops/nanmedian_native.h>
#include <ATen/ops/nanmedian_ops.h>
#include <ATen/ops/nanmedian_native.h>
#include <ATen/ops/nanmedian_ops.h>
#include <ATen/ops/nanmedian_native.h>
#include <ATen/ops/nanmedian_ops.h>
#include <ATen/ops/nanmedian_native.h>
#include <ATen/ops/nanmedian_ops.h>
#include <ATen/ops/_int_mm_native.h>
#include <ATen/ops/_int_mm_ops.h>
#include <ATen/ops/_int_mm_native.h>
#include <ATen/ops/_int_mm_ops.h>
#include <ATen/ops/mode_native.h>
#include <ATen/ops/mode_ops.h>
#include <ATen/ops/mode_native.h>
#include <ATen/ops/mode_ops.h>
#include <ATen/ops/mode_native.h>
#include <ATen/ops/mode_ops.h>
#include <ATen/ops/mode_native.h>
#include <ATen/ops/mode_ops.h>
#include <ATen/ops/multiply_native.h>
#include <ATen/ops/multiply_ops.h>
#include <ATen/ops/multiply_native.h>
#include <ATen/ops/multiply_ops.h>
#include <ATen/ops/multiply_native.h>
#include <ATen/ops/multiply_ops.h>
#include <ATen/ops/narrow_copy_native.h>
#include <ATen/ops/narrow_copy_ops.h>
#include <ATen/ops/narrow_copy_native.h>
#include <ATen/ops/narrow_copy_ops.h>
#include <ATen/ops/batch_norm_gather_stats_native.h>
#include <ATen/ops/batch_norm_gather_stats_ops.h>
#include <ATen/ops/batch_norm_gather_stats_native.h>
#include <ATen/ops/batch_norm_gather_stats_ops.h>
#include <ATen/ops/batch_norm_gather_stats_with_counts_native.h>
#include <ATen/ops/batch_norm_gather_stats_with_counts_ops.h>
#include <ATen/ops/batch_norm_gather_stats_with_counts_native.h>
#include <ATen/ops/batch_norm_gather_stats_with_counts_ops.h>
#include <ATen/ops/native_batch_norm_backward_native.h>
#include <ATen/ops/native_batch_norm_backward_ops.h>
#include <ATen/ops/native_batch_norm_backward_native.h>
#include <ATen/ops/native_batch_norm_backward_ops.h>
#include <ATen/ops/ones_native.h>
#include <ATen/ops/ones_ops.h>
#include <ATen/ops/ones_native.h>
#include <ATen/ops/ones_ops.h>
#include <ATen/ops/ones_native.h>
#include <ATen/ops/ones_ops.h>
#include <ATen/ops/ones_native.h>
#include <ATen/ops/ones_ops.h>
#include <ATen/ops/_pdist_forward_native.h>
#include <ATen/ops/_pdist_forward_ops.h>
#include <ATen/ops/_pdist_forward_native.h>
#include <ATen/ops/_pdist_forward_ops.h>
#include <ATen/ops/_pdist_backward_native.h>
#include <ATen/ops/_pdist_backward_ops.h>
#include <ATen/ops/_pdist_backward_native.h>
#include <ATen/ops/_pdist_backward_ops.h>
#include <ATen/ops/pixel_shuffle_native.h>
#include <ATen/ops/pixel_shuffle_ops.h>
#include <ATen/ops/pixel_shuffle_native.h>
#include <ATen/ops/pixel_shuffle_ops.h>
#include <ATen/ops/_pin_memory_native.h>
#include <ATen/ops/_pin_memory_ops.h>
#include <ATen/ops/_pin_memory_native.h>
#include <ATen/ops/_pin_memory_ops.h>
#include <ATen/ops/randn_native.h>
#include <ATen/ops/randn_ops.h>
#include <ATen/ops/randn_native.h>
#include <ATen/ops/randn_ops.h>
#include <ATen/ops/randn_native.h>
#include <ATen/ops/randn_ops.h>
#include <ATen/ops/randn_native.h>
#include <ATen/ops/randn_ops.h>
#include <ATen/ops/randn_native.h>
#include <ATen/ops/randn_ops.h>
#include <ATen/ops/randn_native.h>
#include <ATen/ops/randn_ops.h>
#include <ATen/ops/randn_native.h>
#include <ATen/ops/randn_ops.h>
#include <ATen/ops/randn_native.h>
#include <ATen/ops/randn_ops.h>
#include <ATen/ops/randn_like_native.h>
#include <ATen/ops/randn_like_ops.h>
#include <ATen/ops/randn_like_native.h>
#include <ATen/ops/randn_like_ops.h>
#include <ATen/ops/neg_native.h>
#include <ATen/ops/neg_ops.h>
#include <ATen/ops/neg_native.h>
#include <ATen/ops/neg_ops.h>
#include <ATen/ops/neg_native.h>
#include <ATen/ops/neg_ops.h>
#include <ATen/ops/negative_native.h>
#include <ATen/ops/negative_ops.h>
#include <ATen/ops/negative_native.h>
#include <ATen/ops/negative_ops.h>
#include <ATen/ops/negative_native.h>
#include <ATen/ops/negative_ops.h>
#include <ATen/ops/repeat_interleave_native.h>
#include <ATen/ops/repeat_interleave_ops.h>
#include <ATen/ops/repeat_interleave_native.h>
#include <ATen/ops/repeat_interleave_ops.h>
#include <ATen/ops/gelu_native.h>
#include <ATen/ops/gelu_ops.h>
#include <ATen/ops/gelu_native.h>
#include <ATen/ops/gelu_ops.h>
#include <ATen/ops/gelu_native.h>
#include <ATen/ops/gelu_ops.h>
#include <ATen/ops/select_backward_native.h>
#include <ATen/ops/select_backward_ops.h>
#include <ATen/ops/select_backward_native.h>
#include <ATen/ops/select_backward_ops.h>
#include <ATen/ops/mish_native.h>
#include <ATen/ops/mish_ops.h>
#include <ATen/ops/mish_native.h>
#include <ATen/ops/mish_ops.h>
#include <ATen/ops/mish_native.h>
#include <ATen/ops/mish_ops.h>
#include <ATen/ops/slice_scatter_native.h>
#include <ATen/ops/slice_scatter_ops.h>
#include <ATen/ops/slice_scatter_native.h>
#include <ATen/ops/slice_scatter_ops.h>
#include <ATen/ops/diagonal_scatter_native.h>
#include <ATen/ops/diagonal_scatter_ops.h>
#include <ATen/ops/diagonal_scatter_native.h>
#include <ATen/ops/diagonal_scatter_ops.h>
#include <ATen/ops/_softmax_backward_data_native.h>
#include <ATen/ops/_softmax_backward_data_ops.h>
#include <ATen/ops/_softmax_backward_data_native.h>
#include <ATen/ops/_softmax_backward_data_ops.h>
#include <ATen/ops/unsafe_split_native.h>
#include <ATen/ops/unsafe_split_ops.h>
#include <ATen/ops/unsafe_split_native.h>
#include <ATen/ops/unsafe_split_ops.h>
#include <ATen/ops/unsafe_split_with_sizes_native.h>
#include <ATen/ops/unsafe_split_with_sizes_ops.h>
#include <ATen/ops/unsafe_split_with_sizes_native.h>
#include <ATen/ops/unsafe_split_with_sizes_ops.h>
#include <ATen/ops/square_native.h>
#include <ATen/ops/square_ops.h>
#include <ATen/ops/square_native.h>
#include <ATen/ops/square_ops.h>
#include <ATen/ops/square_native.h>
#include <ATen/ops/square_ops.h>
#include <ATen/ops/tanh_native.h>
#include <ATen/ops/tanh_ops.h>
#include <ATen/ops/tanh_native.h>
#include <ATen/ops/tanh_ops.h>
#include <ATen/ops/tanh_native.h>
#include <ATen/ops/tanh_ops.h>
#include <ATen/ops/roll_native.h>
#include <ATen/ops/roll_ops.h>
#include <ATen/ops/roll_native.h>
#include <ATen/ops/roll_ops.h>
#include <ATen/ops/rot90_native.h>
#include <ATen/ops/rot90_ops.h>
#include <ATen/ops/rot90_native.h>
#include <ATen/ops/rot90_ops.h>
#include <ATen/ops/_nested_tensor_storage_offsets_native.h>
#include <ATen/ops/_nested_tensor_storage_offsets_ops.h>
#include <ATen/ops/_nested_tensor_storage_offsets_native.h>
#include <ATen/ops/_nested_tensor_storage_offsets_ops.h>
#include <ATen/ops/_trilinear_native.h>
#include <ATen/ops/_trilinear_ops.h>
#include <ATen/ops/_trilinear_native.h>
#include <ATen/ops/_trilinear_ops.h>
#include <ATen/ops/_unique_native.h>
#include <ATen/ops/_unique_ops.h>
#include <ATen/ops/_unique_native.h>
#include <ATen/ops/_unique_ops.h>
#include <ATen/ops/_unique2_native.h>
#include <ATen/ops/_unique2_ops.h>
#include <ATen/ops/_unique2_native.h>
#include <ATen/ops/_unique2_ops.h>
#include <ATen/ops/_weight_norm_interface_native.h>
#include <ATen/ops/_weight_norm_interface_ops.h>
#include <ATen/ops/_weight_norm_interface_native.h>
#include <ATen/ops/_weight_norm_interface_ops.h>
#include <ATen/ops/_efficientzerotensor_native.h>
#include <ATen/ops/_efficientzerotensor_ops.h>
#include <ATen/ops/_efficientzerotensor_native.h>
#include <ATen/ops/_efficientzerotensor_ops.h>
#include <ATen/ops/_standard_gamma_native.h>
#include <ATen/ops/_standard_gamma_ops.h>
#include <ATen/ops/_standard_gamma_native.h>
#include <ATen/ops/_standard_gamma_ops.h>
#include <ATen/ops/_dirichlet_grad_native.h>
#include <ATen/ops/_dirichlet_grad_ops.h>
#include <ATen/ops/_dirichlet_grad_native.h>
#include <ATen/ops/_dirichlet_grad_ops.h>
#include <ATen/ops/norm_native.h>
#include <ATen/ops/norm_ops.h>
#include <ATen/ops/norm_native.h>
#include <ATen/ops/norm_ops.h>
#include <ATen/ops/norm_native.h>
#include <ATen/ops/norm_ops.h>
#include <ATen/ops/norm_native.h>
#include <ATen/ops/norm_ops.h>
#include <ATen/ops/norm_native.h>
#include <ATen/ops/norm_ops.h>
#include <ATen/ops/norm_native.h>
#include <ATen/ops/norm_ops.h>
#include <ATen/ops/norm_native.h>
#include <ATen/ops/norm_ops.h>
#include <ATen/ops/norm_native.h>
#include <ATen/ops/norm_ops.h>
#include <ATen/ops/norm_native.h>
#include <ATen/ops/norm_ops.h>
#include <ATen/ops/norm_native.h>
#include <ATen/ops/norm_ops.h>
#include <ATen/ops/norm_native.h>
#include <ATen/ops/norm_ops.h>
#include <ATen/ops/norm_native.h>
#include <ATen/ops/norm_ops.h>
#include <ATen/ops/frexp_native.h>
#include <ATen/ops/frexp_ops.h>
#include <ATen/ops/frexp_native.h>
#include <ATen/ops/frexp_ops.h>
#include <ATen/ops/frobenius_norm_native.h>
#include <ATen/ops/frobenius_norm_ops.h>
#include <ATen/ops/frobenius_norm_native.h>
#include <ATen/ops/frobenius_norm_ops.h>
#include <ATen/ops/nuclear_norm_native.h>
#include <ATen/ops/nuclear_norm_ops.h>
#include <ATen/ops/nuclear_norm_native.h>
#include <ATen/ops/nuclear_norm_ops.h>
#include <ATen/ops/nuclear_norm_native.h>
#include <ATen/ops/nuclear_norm_ops.h>
#include <ATen/ops/nuclear_norm_native.h>
#include <ATen/ops/nuclear_norm_ops.h>
#include <ATen/ops/subtract_native.h>
#include <ATen/ops/subtract_ops.h>
#include <ATen/ops/subtract_native.h>
#include <ATen/ops/subtract_ops.h>
#include <ATen/ops/subtract_native.h>
#include <ATen/ops/subtract_ops.h>
#include <ATen/ops/sparse_sampled_addmm_native.h>
#include <ATen/ops/sparse_sampled_addmm_ops.h>
#include <ATen/ops/sparse_sampled_addmm_native.h>
#include <ATen/ops/sparse_sampled_addmm_ops.h>
#include <ATen/ops/_addmm_activation_native.h>
#include <ATen/ops/_addmm_activation_ops.h>
#include <ATen/ops/_addmm_activation_native.h>
#include <ATen/ops/_addmm_activation_ops.h>
#include <ATen/ops/_to_dense_native.h>
#include <ATen/ops/_to_dense_ops.h>
#include <ATen/ops/_to_dense_native.h>
#include <ATen/ops/_to_dense_ops.h>
#include <ATen/ops/_coalesced_native.h>
#include <ATen/ops/_coalesced_ops.h>
#include <ATen/ops/_coalesced_native.h>
#include <ATen/ops/_coalesced_ops.h>
#include <ATen/ops/_coalesced_native.h>
#include <ATen/ops/_coalesced_ops.h>
#include <ATen/ops/quantize_per_tensor_dynamic_native.h>
#include <ATen/ops/quantize_per_tensor_dynamic_ops.h>
#include <ATen/ops/quantize_per_tensor_dynamic_native.h>
#include <ATen/ops/quantize_per_tensor_dynamic_ops.h>
#include <ATen/ops/quantize_per_tensor_native.h>
#include <ATen/ops/quantize_per_tensor_ops.h>
#include <ATen/ops/quantize_per_tensor_native.h>
#include <ATen/ops/quantize_per_tensor_ops.h>
#include <ATen/ops/quantize_per_tensor_native.h>
#include <ATen/ops/quantize_per_tensor_ops.h>
#include <ATen/ops/quantize_per_tensor_native.h>
#include <ATen/ops/quantize_per_tensor_ops.h>
#include <ATen/ops/quantize_per_tensor_native.h>
#include <ATen/ops/quantize_per_tensor_ops.h>
#include <ATen/ops/quantize_per_tensor_native.h>
#include <ATen/ops/quantize_per_tensor_ops.h>
#include <ATen/ops/quantize_per_channel_native.h>
#include <ATen/ops/quantize_per_channel_ops.h>
#include <ATen/ops/quantize_per_channel_native.h>
#include <ATen/ops/quantize_per_channel_ops.h>
#include <ATen/ops/_make_per_channel_quantized_tensor_native.h>
#include <ATen/ops/_make_per_channel_quantized_tensor_ops.h>
#include <ATen/ops/_make_per_channel_quantized_tensor_native.h>
#include <ATen/ops/_make_per_channel_quantized_tensor_ops.h>
#include <ATen/ops/fake_quantize_per_tensor_affine_cachemask_native.h>
#include <ATen/ops/fake_quantize_per_tensor_affine_cachemask_ops.h>
#include <ATen/ops/fake_quantize_per_tensor_affine_cachemask_native.h>
#include <ATen/ops/fake_quantize_per_tensor_affine_cachemask_ops.h>
#include <ATen/ops/_fake_quantize_per_tensor_affine_cachemask_tensor_qparams_native.h>
#include <ATen/ops/_fake_quantize_per_tensor_affine_cachemask_tensor_qparams_ops.h>
#include <ATen/ops/_fake_quantize_per_tensor_affine_cachemask_tensor_qparams_native.h>
#include <ATen/ops/_fake_quantize_per_tensor_affine_cachemask_tensor_qparams_ops.h>
#include <ATen/ops/_fake_quantize_learnable_per_tensor_affine_native.h>
#include <ATen/ops/_fake_quantize_learnable_per_tensor_affine_ops.h>
#include <ATen/ops/_fake_quantize_learnable_per_tensor_affine_native.h>
#include <ATen/ops/_fake_quantize_learnable_per_tensor_affine_ops.h>
#include <ATen/ops/fake_quantize_per_channel_affine_cachemask_native.h>
#include <ATen/ops/fake_quantize_per_channel_affine_cachemask_ops.h>
#include <ATen/ops/fake_quantize_per_channel_affine_cachemask_native.h>
#include <ATen/ops/fake_quantize_per_channel_affine_cachemask_ops.h>
#include <ATen/ops/lstm_mps_backward_native.h>
#include <ATen/ops/lstm_mps_backward_ops.h>
#include <ATen/ops/lstm_mps_backward_native.h>
#include <ATen/ops/lstm_mps_backward_ops.h>
#include <ATen/ops/lift_fresh_copy_native.h>
#include <ATen/ops/lift_fresh_copy_ops.h>
#include <ATen/ops/lift_fresh_copy_native.h>
#include <ATen/ops/lift_fresh_copy_ops.h>
#include <ATen/ops/_masked_softmax_backward_native.h>
#include <ATen/ops/_masked_softmax_backward_ops.h>
#include <ATen/ops/_masked_softmax_backward_native.h>
#include <ATen/ops/_masked_softmax_backward_ops.h>
#include <ATen/ops/put_native.h>
#include <ATen/ops/put_ops.h>
#include <ATen/ops/put_native.h>
#include <ATen/ops/put_ops.h>
#include <ATen/ops/put_native.h>
#include <ATen/ops/put_ops.h>
#include <ATen/ops/index_add_native.h>
#include <ATen/ops/index_add_ops.h>
#include <ATen/ops/index_add_native.h>
#include <ATen/ops/index_add_ops.h>
#include <ATen/ops/index_add_native.h>
#include <ATen/ops/index_add_ops.h>
#include <ATen/ops/index_fill_native.h>
#include <ATen/ops/index_fill_ops.h>
#include <ATen/ops/index_fill_native.h>
#include <ATen/ops/index_fill_ops.h>
#include <ATen/ops/index_fill_native.h>
#include <ATen/ops/index_fill_ops.h>
#include <ATen/ops/index_fill_native.h>
#include <ATen/ops/index_fill_ops.h>
#include <ATen/ops/index_fill_native.h>
#include <ATen/ops/index_fill_ops.h>
#include <ATen/ops/index_fill_native.h>
#include <ATen/ops/index_fill_ops.h>
#include <ATen/ops/scatter_native.h>
#include <ATen/ops/scatter_ops.h>
#include <ATen/ops/scatter_native.h>
#include <ATen/ops/scatter_ops.h>
#include <ATen/ops/scatter_native.h>
#include <ATen/ops/scatter_ops.h>
#include <ATen/ops/scatter_native.h>
#include <ATen/ops/scatter_ops.h>
#include <ATen/ops/scatter_native.h>
#include <ATen/ops/scatter_ops.h>
#include <ATen/ops/scatter_native.h>
#include <ATen/ops/scatter_ops.h>
#include <ATen/ops/scatter_native.h>
#include <ATen/ops/scatter_ops.h>
#include <ATen/ops/scatter_native.h>
#include <ATen/ops/scatter_ops.h>
#include <ATen/ops/scatter_native.h>
#include <ATen/ops/scatter_ops.h>
#include <ATen/ops/scatter_native.h>
#include <ATen/ops/scatter_ops.h>
#include <ATen/ops/scatter_native.h>
#include <ATen/ops/scatter_ops.h>
#include <ATen/ops/scatter_native.h>
#include <ATen/ops/scatter_ops.h>
#include <ATen/ops/scatter_add_native.h>
#include <ATen/ops/scatter_add_ops.h>
#include <ATen/ops/scatter_add_native.h>
#include <ATen/ops/scatter_add_ops.h>
#include <ATen/ops/scatter_add_native.h>
#include <ATen/ops/scatter_add_ops.h>
#include <ATen/ops/lshift_native.h>
#include <ATen/ops/lshift_ops.h>
#include <ATen/ops/lshift_native.h>
#include <ATen/ops/lshift_ops.h>
#include <ATen/ops/lshift_native.h>
#include <ATen/ops/lshift_ops.h>
#include <ATen/ops/lshift_native.h>
#include <ATen/ops/lshift_ops.h>
#include <ATen/ops/lshift_native.h>
#include <ATen/ops/lshift_ops.h>
#include <ATen/ops/lshift_native.h>
#include <ATen/ops/lshift_ops.h>
#include <ATen/ops/greater_equal_native.h>
#include <ATen/ops/greater_equal_ops.h>
#include <ATen/ops/greater_equal_native.h>
#include <ATen/ops/greater_equal_ops.h>
#include <ATen/ops/greater_equal_native.h>
#include <ATen/ops/greater_equal_ops.h>
#include <ATen/ops/greater_equal_native.h>
#include <ATen/ops/greater_equal_ops.h>
#include <ATen/ops/greater_equal_native.h>
#include <ATen/ops/greater_equal_ops.h>
#include <ATen/ops/greater_equal_native.h>
#include <ATen/ops/greater_equal_ops.h>
#include <ATen/ops/less_equal_native.h>
#include <ATen/ops/less_equal_ops.h>
#include <ATen/ops/less_equal_native.h>
#include <ATen/ops/less_equal_ops.h>
#include <ATen/ops/less_equal_native.h>
#include <ATen/ops/less_equal_ops.h>
#include <ATen/ops/less_equal_native.h>
#include <ATen/ops/less_equal_ops.h>
#include <ATen/ops/less_equal_native.h>
#include <ATen/ops/less_equal_ops.h>
#include <ATen/ops/less_equal_native.h>
#include <ATen/ops/less_equal_ops.h>
#include <ATen/ops/greater_native.h>
#include <ATen/ops/greater_ops.h>
#include <ATen/ops/greater_native.h>
#include <ATen/ops/greater_ops.h>
#include <ATen/ops/greater_native.h>
#include <ATen/ops/greater_ops.h>
#include <ATen/ops/greater_native.h>
#include <ATen/ops/greater_ops.h>
#include <ATen/ops/greater_native.h>
#include <ATen/ops/greater_ops.h>
#include <ATen/ops/greater_native.h>
#include <ATen/ops/greater_ops.h>
#include <ATen/ops/lt_native.h>
#include <ATen/ops/lt_ops.h>
#include <ATen/ops/lt_native.h>
#include <ATen/ops/lt_ops.h>
#include <ATen/ops/lt_native.h>
#include <ATen/ops/lt_ops.h>
#include <ATen/ops/lt_native.h>
#include <ATen/ops/lt_ops.h>
#include <ATen/ops/lt_native.h>
#include <ATen/ops/lt_ops.h>
#include <ATen/ops/lt_native.h>
#include <ATen/ops/lt_ops.h>
#include <ATen/ops/less_native.h>
#include <ATen/ops/less_ops.h>
#include <ATen/ops/less_native.h>
#include <ATen/ops/less_ops.h>
#include <ATen/ops/less_native.h>
#include <ATen/ops/less_ops.h>
#include <ATen/ops/less_native.h>
#include <ATen/ops/less_ops.h>
#include <ATen/ops/less_native.h>
#include <ATen/ops/less_ops.h>
#include <ATen/ops/less_native.h>
#include <ATen/ops/less_ops.h>
#include <ATen/ops/take_native.h>
#include <ATen/ops/take_ops.h>
#include <ATen/ops/take_native.h>
#include <ATen/ops/take_ops.h>
#include <ATen/ops/nonzero_static_native.h>
#include <ATen/ops/nonzero_static_ops.h>
#include <ATen/ops/nonzero_static_native.h>
#include <ATen/ops/nonzero_static_ops.h>
#include <ATen/ops/gather_native.h>
#include <ATen/ops/gather_ops.h>
#include <ATen/ops/gather_native.h>
#include <ATen/ops/gather_ops.h>
#include <ATen/ops/gather_native.h>
#include <ATen/ops/gather_ops.h>
#include <ATen/ops/gather_native.h>
#include <ATen/ops/gather_ops.h>
#include <ATen/ops/cholesky_native.h>
#include <ATen/ops/cholesky_ops.h>
#include <ATen/ops/cholesky_native.h>
#include <ATen/ops/cholesky_ops.h>
#include <ATen/ops/_cholesky_solve_helper_native.h>
#include <ATen/ops/_cholesky_solve_helper_ops.h>
#include <ATen/ops/_cholesky_solve_helper_native.h>
#include <ATen/ops/_cholesky_solve_helper_ops.h>
#include <ATen/ops/polygamma_native.h>
#include <ATen/ops/polygamma_ops.h>
#include <ATen/ops/polygamma_native.h>
#include <ATen/ops/polygamma_ops.h>
#include <ATen/ops/igamma_native.h>
#include <ATen/ops/igamma_ops.h>
#include <ATen/ops/igamma_native.h>
#include <ATen/ops/igamma_ops.h>
#include <ATen/ops/igamma_native.h>
#include <ATen/ops/igamma_ops.h>
#include <ATen/ops/fmin_native.h>
#include <ATen/ops/fmin_ops.h>
#include <ATen/ops/fmin_native.h>
#include <ATen/ops/fmin_ops.h>
#include <ATen/ops/max_native.h>
#include <ATen/ops/max_ops.h>
#include <ATen/ops/max_native.h>
#include <ATen/ops/max_ops.h>
#include <ATen/ops/fmax_native.h>
#include <ATen/ops/fmax_ops.h>
#include <ATen/ops/fmax_native.h>
#include <ATen/ops/fmax_ops.h>
#include <ATen/ops/maximum_native.h>
#include <ATen/ops/maximum_ops.h>
#include <ATen/ops/maximum_native.h>
#include <ATen/ops/maximum_ops.h>
#include <ATen/ops/max_native.h>
#include <ATen/ops/max_ops.h>
#include <ATen/ops/max_native.h>
#include <ATen/ops/max_ops.h>
#include <ATen/ops/_amp_foreach_non_finite_check_and_unscale_native.h>
#include <ATen/ops/_amp_foreach_non_finite_check_and_unscale_ops.h>
#include <ATen/ops/_amp_foreach_non_finite_check_and_unscale_native.h>
#include <ATen/ops/_amp_foreach_non_finite_check_and_unscale_ops.h>
#include <ATen/ops/_amp_foreach_non_finite_check_and_unscale_native.h>
#include <ATen/ops/_amp_foreach_non_finite_check_and_unscale_ops.h>
#include <ATen/ops/_foreach_sub_native.h>
#include <ATen/ops/_foreach_sub_ops.h>
#include <ATen/ops/_foreach_sub_native.h>
#include <ATen/ops/_foreach_sub_ops.h>
#include <ATen/ops/_foreach_sub_native.h>
#include <ATen/ops/_foreach_sub_ops.h>
#include <ATen/ops/_foreach_sub_native.h>
#include <ATen/ops/_foreach_sub_ops.h>
#include <ATen/ops/_foreach_sub_native.h>
#include <ATen/ops/_foreach_sub_ops.h>
#include <ATen/ops/_foreach_sub_native.h>
#include <ATen/ops/_foreach_sub_ops.h>
#include <ATen/ops/_foreach_sub_native.h>
#include <ATen/ops/_foreach_sub_ops.h>
#include <ATen/ops/_foreach_sub_native.h>
#include <ATen/ops/_foreach_sub_ops.h>
#include <ATen/ops/_foreach_sub_native.h>
#include <ATen/ops/_foreach_sub_ops.h>
#include <ATen/ops/_foreach_maximum_native.h>
#include <ATen/ops/_foreach_maximum_ops.h>
#include <ATen/ops/_foreach_maximum_native.h>
#include <ATen/ops/_foreach_maximum_ops.h>
#include <ATen/ops/_foreach_maximum_native.h>
#include <ATen/ops/_foreach_maximum_ops.h>
#include <ATen/ops/_foreach_maximum_native.h>
#include <ATen/ops/_foreach_maximum_ops.h>
#include <ATen/ops/_foreach_maximum_native.h>
#include <ATen/ops/_foreach_maximum_ops.h>
#include <ATen/ops/_foreach_maximum_native.h>
#include <ATen/ops/_foreach_maximum_ops.h>
#include <ATen/ops/_foreach_maximum_native.h>
#include <ATen/ops/_foreach_maximum_ops.h>
#include <ATen/ops/_foreach_maximum_native.h>
#include <ATen/ops/_foreach_maximum_ops.h>
#include <ATen/ops/_foreach_maximum_native.h>
#include <ATen/ops/_foreach_maximum_ops.h>
#include <ATen/ops/_foreach_abs_native.h>
#include <ATen/ops/_foreach_abs_ops.h>
#include <ATen/ops/_foreach_abs_native.h>
#include <ATen/ops/_foreach_abs_ops.h>
#include <ATen/ops/_foreach_abs_native.h>
#include <ATen/ops/_foreach_abs_ops.h>
#include <ATen/ops/_foreach_acos_native.h>
#include <ATen/ops/_foreach_acos_ops.h>
#include <ATen/ops/_foreach_acos_native.h>
#include <ATen/ops/_foreach_acos_ops.h>
#include <ATen/ops/_foreach_acos_native.h>
#include <ATen/ops/_foreach_acos_ops.h>
#include <ATen/ops/_foreach_cos_native.h>
#include <ATen/ops/_foreach_cos_ops.h>
#include <ATen/ops/_foreach_cos_native.h>
#include <ATen/ops/_foreach_cos_ops.h>
#include <ATen/ops/_foreach_cos_native.h>
#include <ATen/ops/_foreach_cos_ops.h>
#include <ATen/ops/_foreach_exp_native.h>
#include <ATen/ops/_foreach_exp_ops.h>
#include <ATen/ops/_foreach_exp_native.h>
#include <ATen/ops/_foreach_exp_ops.h>
#include <ATen/ops/_foreach_exp_native.h>
#include <ATen/ops/_foreach_exp_ops.h>
#include <ATen/ops/_foreach_floor_native.h>
#include <ATen/ops/_foreach_floor_ops.h>
#include <ATen/ops/_foreach_floor_native.h>
#include <ATen/ops/_foreach_floor_ops.h>
#include <ATen/ops/_foreach_floor_native.h>
#include <ATen/ops/_foreach_floor_ops.h>
#include <ATen/ops/_foreach_log10_native.h>
#include <ATen/ops/_foreach_log10_ops.h>
#include <ATen/ops/_foreach_log10_native.h>
#include <ATen/ops/_foreach_log10_ops.h>
#include <ATen/ops/_foreach_log10_native.h>
#include <ATen/ops/_foreach_log10_ops.h>
#include <ATen/ops/_foreach_neg_native.h>
#include <ATen/ops/_foreach_neg_ops.h>
#include <ATen/ops/_foreach_neg_native.h>
#include <ATen/ops/_foreach_neg_ops.h>
#include <ATen/ops/_foreach_neg_native.h>
#include <ATen/ops/_foreach_neg_ops.h>
#include <ATen/ops/_foreach_norm_native.h>
#include <ATen/ops/_foreach_norm_ops.h>
#include <ATen/ops/_foreach_norm_native.h>
#include <ATen/ops/_foreach_norm_ops.h>
#include <ATen/ops/_foreach_sigmoid_native.h>
#include <ATen/ops/_foreach_sigmoid_ops.h>
#include <ATen/ops/_foreach_sigmoid_native.h>
#include <ATen/ops/_foreach_sigmoid_ops.h>
#include <ATen/ops/_foreach_sigmoid_native.h>
#include <ATen/ops/_foreach_sigmoid_ops.h>
#include <ATen/ops/_foreach_sign_native.h>
#include <ATen/ops/_foreach_sign_ops.h>
#include <ATen/ops/_foreach_sign_native.h>
#include <ATen/ops/_foreach_sign_ops.h>
#include <ATen/ops/_foreach_sign_native.h>
#include <ATen/ops/_foreach_sign_ops.h>
#include <ATen/ops/_foreach_sqrt_native.h>
#include <ATen/ops/_foreach_sqrt_ops.h>
#include <ATen/ops/_foreach_sqrt_native.h>
#include <ATen/ops/_foreach_sqrt_ops.h>
#include <ATen/ops/_foreach_sqrt_native.h>
#include <ATen/ops/_foreach_sqrt_ops.h>
#include <ATen/ops/_foreach_tan_native.h>
#include <ATen/ops/_foreach_tan_ops.h>
#include <ATen/ops/_foreach_tan_native.h>
#include <ATen/ops/_foreach_tan_ops.h>
#include <ATen/ops/_foreach_tan_native.h>
#include <ATen/ops/_foreach_tan_ops.h>
#include <ATen/ops/searchsorted_native.h>
#include <ATen/ops/searchsorted_ops.h>
#include <ATen/ops/searchsorted_native.h>
#include <ATen/ops/searchsorted_ops.h>
#include <ATen/ops/searchsorted_native.h>
#include <ATen/ops/searchsorted_ops.h>
#include <ATen/ops/searchsorted_native.h>
#include <ATen/ops/searchsorted_ops.h>
#include <ATen/ops/mse_loss_backward_native.h>
#include <ATen/ops/mse_loss_backward_ops.h>
#include <ATen/ops/mse_loss_backward_native.h>
#include <ATen/ops/mse_loss_backward_ops.h>
#include <ATen/ops/smooth_l1_loss_backward_native.h>
#include <ATen/ops/smooth_l1_loss_backward_ops.h>
#include <ATen/ops/smooth_l1_loss_backward_native.h>
#include <ATen/ops/smooth_l1_loss_backward_ops.h>
#include <ATen/ops/huber_loss_backward_native.h>
#include <ATen/ops/huber_loss_backward_ops.h>
#include <ATen/ops/huber_loss_backward_native.h>
#include <ATen/ops/huber_loss_backward_ops.h>
#include <ATen/ops/elu_backward_native.h>
#include <ATen/ops/elu_backward_ops.h>
#include <ATen/ops/elu_backward_native.h>
#include <ATen/ops/elu_backward_ops.h>
#include <ATen/ops/glu_jvp_native.h>
#include <ATen/ops/glu_jvp_ops.h>
#include <ATen/ops/glu_jvp_native.h>
#include <ATen/ops/glu_jvp_ops.h>
#include <ATen/ops/hardsigmoid_backward_native.h>
#include <ATen/ops/hardsigmoid_backward_ops.h>
#include <ATen/ops/hardsigmoid_backward_native.h>
#include <ATen/ops/hardsigmoid_backward_ops.h>
#include <ATen/ops/log_sigmoid_native.h>
#include <ATen/ops/log_sigmoid_ops.h>
#include <ATen/ops/log_sigmoid_native.h>
#include <ATen/ops/log_sigmoid_ops.h>
#include <ATen/ops/log_sigmoid_forward_native.h>
#include <ATen/ops/log_sigmoid_forward_ops.h>
#include <ATen/ops/log_sigmoid_forward_native.h>
#include <ATen/ops/log_sigmoid_forward_ops.h>
#include <ATen/ops/rrelu_with_noise_native.h>
#include <ATen/ops/rrelu_with_noise_ops.h>
#include <ATen/ops/rrelu_with_noise_native.h>
#include <ATen/ops/rrelu_with_noise_ops.h>
#include <ATen/ops/rrelu_with_noise_native.h>
#include <ATen/ops/rrelu_with_noise_ops.h>
#include <ATen/ops/rrelu_with_noise_backward_native.h>
#include <ATen/ops/rrelu_with_noise_backward_ops.h>
#include <ATen/ops/rrelu_with_noise_backward_native.h>
#include <ATen/ops/rrelu_with_noise_backward_ops.h>
#include <ATen/ops/softplus_backward_native.h>
#include <ATen/ops/softplus_backward_ops.h>
#include <ATen/ops/softplus_backward_native.h>
#include <ATen/ops/softplus_backward_ops.h>
#include <ATen/ops/mkldnn_adaptive_avg_pool2d_native.h>
#include <ATen/ops/mkldnn_adaptive_avg_pool2d_ops.h>
#include <ATen/ops/mkldnn_adaptive_avg_pool2d_native.h>
#include <ATen/ops/mkldnn_adaptive_avg_pool2d_ops.h>
#include <ATen/ops/adaptive_max_pool2d_native.h>
#include <ATen/ops/adaptive_max_pool2d_ops.h>
#include <ATen/ops/adaptive_max_pool2d_native.h>
#include <ATen/ops/adaptive_max_pool2d_ops.h>
#include <ATen/ops/adaptive_max_pool3d_native.h>
#include <ATen/ops/adaptive_max_pool3d_ops.h>
#include <ATen/ops/adaptive_max_pool3d_native.h>
#include <ATen/ops/adaptive_max_pool3d_ops.h>
#include <ATen/ops/avg_pool2d_backward_native.h>
#include <ATen/ops/avg_pool2d_backward_ops.h>
#include <ATen/ops/avg_pool2d_backward_native.h>
#include <ATen/ops/avg_pool2d_backward_ops.h>
#include <ATen/ops/max_pool2d_with_indices_native.h>
#include <ATen/ops/max_pool2d_with_indices_ops.h>
#include <ATen/ops/max_pool2d_with_indices_native.h>
#include <ATen/ops/max_pool2d_with_indices_ops.h>
#include <ATen/ops/upsample_linear1d_native.h>
#include <ATen/ops/upsample_linear1d_ops.h>
#include <ATen/ops/upsample_linear1d_native.h>
#include <ATen/ops/upsample_linear1d_ops.h>
#include <ATen/ops/upsample_linear1d_backward_native.h>
#include <ATen/ops/upsample_linear1d_backward_ops.h>
#include <ATen/ops/upsample_linear1d_backward_native.h>
#include <ATen/ops/upsample_linear1d_backward_ops.h>
#include <ATen/ops/upsample_bicubic2d_native.h>
#include <ATen/ops/upsample_bicubic2d_ops.h>
#include <ATen/ops/upsample_bicubic2d_native.h>
#include <ATen/ops/upsample_bicubic2d_ops.h>
#include <ATen/ops/upsample_bicubic2d_backward_native.h>
#include <ATen/ops/upsample_bicubic2d_backward_ops.h>
#include <ATen/ops/upsample_bicubic2d_backward_native.h>
#include <ATen/ops/upsample_bicubic2d_backward_ops.h>
#include <ATen/ops/_upsample_bicubic2d_aa_backward_native.h>
#include <ATen/ops/_upsample_bicubic2d_aa_backward_ops.h>
#include <ATen/ops/_upsample_bicubic2d_aa_backward_native.h>
#include <ATen/ops/_upsample_bicubic2d_aa_backward_ops.h>
#include <ATen/ops/upsample_nearest1d_native.h>
#include <ATen/ops/upsample_nearest1d_ops.h>
#include <ATen/ops/upsample_nearest1d_native.h>
#include <ATen/ops/upsample_nearest1d_ops.h>
#include <ATen/ops/_upsample_nearest_exact1d_native.h>
#include <ATen/ops/_upsample_nearest_exact1d_ops.h>
#include <ATen/ops/_upsample_nearest_exact1d_native.h>
#include <ATen/ops/_upsample_nearest_exact1d_ops.h>
#include <ATen/ops/upsample_nearest1d_backward_native.h>
#include <ATen/ops/upsample_nearest1d_backward_ops.h>
#include <ATen/ops/upsample_nearest1d_backward_native.h>
#include <ATen/ops/upsample_nearest1d_backward_ops.h>
#include <ATen/ops/upsample_nearest3d_native.h>
#include <ATen/ops/upsample_nearest3d_ops.h>
#include <ATen/ops/upsample_nearest3d_native.h>
#include <ATen/ops/upsample_nearest3d_ops.h>
#include <ATen/ops/slow_conv_transpose3d_native.h>
#include <ATen/ops/slow_conv_transpose3d_ops.h>
#include <ATen/ops/slow_conv_transpose3d_native.h>
#include <ATen/ops/slow_conv_transpose3d_ops.h>
#include <ATen/ops/isposinf_native.h>
#include <ATen/ops/isposinf_ops.h>
#include <ATen/ops/isposinf_native.h>
#include <ATen/ops/isposinf_ops.h>
#include <ATen/ops/special_entr_native.h>
#include <ATen/ops/special_entr_ops.h>
#include <ATen/ops/special_entr_native.h>
#include <ATen/ops/special_entr_ops.h>
#include <ATen/ops/special_psi_native.h>
#include <ATen/ops/special_psi_ops.h>
#include <ATen/ops/special_psi_native.h>
#include <ATen/ops/special_psi_ops.h>
#include <ATen/ops/special_erfinv_native.h>
#include <ATen/ops/special_erfinv_ops.h>
#include <ATen/ops/special_erfinv_native.h>
#include <ATen/ops/special_erfinv_ops.h>
#include <ATen/ops/special_ndtr_native.h>
#include <ATen/ops/special_ndtr_ops.h>
#include <ATen/ops/special_ndtr_native.h>
#include <ATen/ops/special_ndtr_ops.h>
#include <ATen/ops/special_xlogy_native.h>
#include <ATen/ops/special_xlogy_ops.h>
#include <ATen/ops/special_xlogy_native.h>
#include <ATen/ops/special_xlogy_ops.h>
#include <ATen/ops/special_xlogy_native.h>
#include <ATen/ops/special_xlogy_ops.h>
#include <ATen/ops/special_xlogy_native.h>
#include <ATen/ops/special_xlogy_ops.h>
#include <ATen/ops/special_xlogy_native.h>
#include <ATen/ops/special_xlogy_ops.h>
#include <ATen/ops/special_xlogy_native.h>
#include <ATen/ops/special_xlogy_ops.h>
#include <ATen/ops/special_zeta_native.h>
#include <ATen/ops/special_zeta_ops.h>
#include <ATen/ops/special_zeta_native.h>
#include <ATen/ops/special_zeta_ops.h>
#include <ATen/ops/special_zeta_native.h>
#include <ATen/ops/special_zeta_ops.h>
#include <ATen/ops/special_zeta_native.h>
#include <ATen/ops/special_zeta_ops.h>
#include <ATen/ops/special_zeta_native.h>
#include <ATen/ops/special_zeta_ops.h>
#include <ATen/ops/special_zeta_native.h>
#include <ATen/ops/special_zeta_ops.h>
#include <ATen/ops/special_i0_native.h>
#include <ATen/ops/special_i0_ops.h>
#include <ATen/ops/special_i0_native.h>
#include <ATen/ops/special_i0_ops.h>
#include <ATen/ops/special_i0e_native.h>
#include <ATen/ops/special_i0e_ops.h>
#include <ATen/ops/special_i0e_native.h>
#include <ATen/ops/special_i0e_ops.h>
#include <ATen/ops/special_expit_native.h>
#include <ATen/ops/special_expit_ops.h>
#include <ATen/ops/special_expit_native.h>
#include <ATen/ops/special_expit_ops.h>
#include <ATen/ops/special_round_native.h>
#include <ATen/ops/special_round_ops.h>
#include <ATen/ops/special_round_native.h>
#include <ATen/ops/special_round_ops.h>
#include <ATen/ops/special_gammainc_native.h>
#include <ATen/ops/special_gammainc_ops.h>
#include <ATen/ops/special_gammainc_native.h>
#include <ATen/ops/special_gammainc_ops.h>
#include <ATen/ops/fft_irfft_native.h>
#include <ATen/ops/fft_irfft_ops.h>
#include <ATen/ops/fft_irfft_native.h>
#include <ATen/ops/fft_irfft_ops.h>
#include <ATen/ops/fft_fft2_native.h>
#include <ATen/ops/fft_fft2_ops.h>
#include <ATen/ops/fft_fft2_native.h>
#include <ATen/ops/fft_fft2_ops.h>
#include <ATen/ops/fft_rfftfreq_native.h>
#include <ATen/ops/fft_rfftfreq_ops.h>
#include <ATen/ops/fft_rfftfreq_native.h>
#include <ATen/ops/fft_rfftfreq_ops.h>
#include <ATen/ops/linalg_cholesky_native.h>
#include <ATen/ops/linalg_cholesky_ops.h>
#include <ATen/ops/linalg_cholesky_native.h>
#include <ATen/ops/linalg_cholesky_ops.h>
#include <ATen/ops/linalg_lu_native.h>
#include <ATen/ops/linalg_lu_ops.h>
#include <ATen/ops/linalg_lu_native.h>
#include <ATen/ops/linalg_lu_ops.h>
#include <ATen/ops/_linalg_det_native.h>
#include <ATen/ops/_linalg_det_ops.h>
#include <ATen/ops/_linalg_det_native.h>
#include <ATen/ops/_linalg_det_ops.h>
#include <ATen/ops/slogdet_native.h>
#include <ATen/ops/slogdet_ops.h>
#include <ATen/ops/slogdet_native.h>
#include <ATen/ops/slogdet_ops.h>
#include <ATen/ops/linalg_eig_native.h>
#include <ATen/ops/linalg_eig_ops.h>
#include <ATen/ops/linalg_eig_native.h>
#include <ATen/ops/linalg_eig_ops.h>
#include <ATen/ops/linalg_inv_native.h>
#include <ATen/ops/linalg_inv_ops.h>
#include <ATen/ops/linalg_inv_native.h>
#include <ATen/ops/linalg_inv_ops.h>
#include <ATen/ops/_linalg_svd_native.h>
#include <ATen/ops/_linalg_svd_ops.h>
#include <ATen/ops/_linalg_svd_native.h>
#include <ATen/ops/_linalg_svd_ops.h>
#include <ATen/ops/linalg_svdvals_native.h>
#include <ATen/ops/linalg_svdvals_ops.h>
#include <ATen/ops/linalg_svdvals_native.h>
#include <ATen/ops/linalg_svdvals_ops.h>
#include <ATen/ops/linalg_pinv_native.h>
#include <ATen/ops/linalg_pinv_ops.h>
#include <ATen/ops/linalg_pinv_native.h>
#include <ATen/ops/linalg_pinv_ops.h>
#include <ATen/ops/linalg_pinv_native.h>
#include <ATen/ops/linalg_pinv_ops.h>
#include <ATen/ops/linalg_pinv_native.h>
#include <ATen/ops/linalg_pinv_ops.h>
#include <ATen/ops/linalg_pinv_native.h>
#include <ATen/ops/linalg_pinv_ops.h>
#include <ATen/ops/linalg_pinv_native.h>
#include <ATen/ops/linalg_pinv_ops.h>
#include <ATen/ops/linalg_pinv_native.h>
#include <ATen/ops/linalg_pinv_ops.h>
#include <ATen/ops/linalg_pinv_native.h>
#include <ATen/ops/linalg_pinv_ops.h>
#include <ATen/ops/linalg_qr_native.h>
#include <ATen/ops/linalg_qr_ops.h>
#include <ATen/ops/linalg_qr_native.h>
#include <ATen/ops/linalg_qr_ops.h>
#include <ATen/ops/_test_optional_intlist_native.h>
#include <ATen/ops/_test_optional_intlist_ops.h>
#include <ATen/ops/_test_optional_intlist_native.h>
#include <ATen/ops/_test_optional_intlist_ops.h>
#include <ATen/ops/_fw_primal_copy_native.h>
#include <ATen/ops/_fw_primal_copy_ops.h>
#include <ATen/ops/_fw_primal_copy_native.h>
#include <ATen/ops/_fw_primal_copy_ops.h>
#include <ATen/ops/as_strided_copy_native.h>
#include <ATen/ops/as_strided_copy_ops.h>
#include <ATen/ops/as_strided_copy_native.h>
#include <ATen/ops/as_strided_copy_ops.h>
#include <ATen/ops/expand_copy_native.h>
#include <ATen/ops/expand_copy_ops.h>
#include <ATen/ops/expand_copy_native.h>
#include <ATen/ops/expand_copy_ops.h>
#include <ATen/ops/_reshape_alias_copy_native.h>
#include <ATen/ops/_reshape_alias_copy_ops.h>
#include <ATen/ops/_reshape_alias_copy_native.h>
#include <ATen/ops/_reshape_alias_copy_ops.h>
#include <ATen/ops/select_copy_native.h>
#include <ATen/ops/select_copy_ops.h>
#include <ATen/ops/select_copy_native.h>
#include <ATen/ops/select_copy_ops.h>
#include <ATen/ops/split_with_sizes_copy_native.h>
#include <ATen/ops/split_with_sizes_copy_ops.h>
#include <ATen/ops/split_with_sizes_copy_native.h>
#include <ATen/ops/split_with_sizes_copy_ops.h>
#include <ATen/ops/squeeze_copy_native.h>
#include <ATen/ops/squeeze_copy_ops.h>
#include <ATen/ops/squeeze_copy_native.h>
#include <ATen/ops/squeeze_copy_ops.h>
#include <ATen/ops/squeeze_copy_native.h>
#include <ATen/ops/squeeze_copy_ops.h>
#include <ATen/ops/squeeze_copy_native.h>
#include <ATen/ops/squeeze_copy_ops.h>
#include <ATen/ops/squeeze_copy_native.h>
#include <ATen/ops/squeeze_copy_ops.h>
#include <ATen/ops/squeeze_copy_native.h>
#include <ATen/ops/squeeze_copy_ops.h>
#include <ATen/ops/_indices_copy_native.h>
#include <ATen/ops/_indices_copy_ops.h>
#include <ATen/ops/_indices_copy_native.h>
#include <ATen/ops/_indices_copy_ops.h>
#include <ATen/ops/_values_copy_native.h>
#include <ATen/ops/_values_copy_ops.h>
#include <ATen/ops/_values_copy_native.h>
#include <ATen/ops/_values_copy_ops.h>
#include <ATen/ops/crow_indices_copy_native.h>
#include <ATen/ops/crow_indices_copy_ops.h>
#include <ATen/ops/crow_indices_copy_native.h>
#include <ATen/ops/crow_indices_copy_ops.h>
#include <ATen/ops/col_indices_copy_native.h>
#include <ATen/ops/col_indices_copy_ops.h>
#include <ATen/ops/col_indices_copy_native.h>
#include <ATen/ops/col_indices_copy_ops.h>
#include <ATen/ops/unbind_copy_native.h>
#include <ATen/ops/unbind_copy_ops.h>
#include <ATen/ops/unbind_copy_native.h>
#include <ATen/ops/unbind_copy_ops.h>
#include <ATen/ops/view_copy_native.h>
#include <ATen/ops/view_copy_ops.h>
#include <ATen/ops/view_copy_native.h>
#include <ATen/ops/view_copy_ops.h>
#include <ATen/ops/view_copy_native.h>
#include <ATen/ops/view_copy_ops.h>
#include <ATen/ops/view_copy_native.h>
#include <ATen/ops/view_copy_ops.h>
#include <ATen/ops/alias_copy_native.h>
#include <ATen/ops/alias_copy_ops.h>
#include <ATen/ops/alias_copy_native.h>
#include <ATen/ops/alias_copy_ops.h>
#include <ATen/ops/special_airy_ai_native.h>
#include <ATen/ops/special_airy_ai_ops.h>
#include <ATen/ops/special_airy_ai_native.h>
#include <ATen/ops/special_airy_ai_ops.h>
#include <ATen/ops/special_bessel_j0_native.h>
#include <ATen/ops/special_bessel_j0_ops.h>
#include <ATen/ops/special_bessel_j0_native.h>
#include <ATen/ops/special_bessel_j0_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_v_native.h>
#include <ATen/ops/special_chebyshev_polynomial_v_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_v_native.h>
#include <ATen/ops/special_chebyshev_polynomial_v_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_v_native.h>
#include <ATen/ops/special_chebyshev_polynomial_v_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_v_native.h>
#include <ATen/ops/special_chebyshev_polynomial_v_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_v_native.h>
#include <ATen/ops/special_chebyshev_polynomial_v_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_v_native.h>
#include <ATen/ops/special_chebyshev_polynomial_v_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_w_native.h>
#include <ATen/ops/special_chebyshev_polynomial_w_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_w_native.h>
#include <ATen/ops/special_chebyshev_polynomial_w_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_w_native.h>
#include <ATen/ops/special_chebyshev_polynomial_w_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_w_native.h>
#include <ATen/ops/special_chebyshev_polynomial_w_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_w_native.h>
#include <ATen/ops/special_chebyshev_polynomial_w_ops.h>
#include <ATen/ops/special_chebyshev_polynomial_w_native.h>
#include <ATen/ops/special_chebyshev_polynomial_w_ops.h>
#include <ATen/ops/special_hermite_polynomial_he_native.h>
#include <ATen/ops/special_hermite_polynomial_he_ops.h>
#include <ATen/ops/special_hermite_polynomial_he_native.h>
#include <ATen/ops/special_hermite_polynomial_he_ops.h>
#include <ATen/ops/special_hermite_polynomial_he_native.h>
#include <ATen/ops/special_hermite_polynomial_he_ops.h>
#include <ATen/ops/special_hermite_polynomial_he_native.h>
#include <ATen/ops/special_hermite_polynomial_he_ops.h>
#include <ATen/ops/special_hermite_polynomial_he_native.h>
#include <ATen/ops/special_hermite_polynomial_he_ops.h>
#include <ATen/ops/special_hermite_polynomial_he_native.h>
#include <ATen/ops/special_hermite_polynomial_he_ops.h>
#include <ATen/ops/special_laguerre_polynomial_l_native.h>
#include <ATen/ops/special_laguerre_polynomial_l_ops.h>
#include <ATen/ops/special_laguerre_polynomial_l_native.h>
#include <ATen/ops/special_laguerre_polynomial_l_ops.h>
#include <ATen/ops/special_laguerre_polynomial_l_native.h>
#include <ATen/ops/special_laguerre_polynomial_l_ops.h>
#include <ATen/ops/special_laguerre_polynomial_l_native.h>
#include <ATen/ops/special_laguerre_polynomial_l_ops.h>
#include <ATen/ops/special_laguerre_polynomial_l_native.h>
#include <ATen/ops/special_laguerre_polynomial_l_ops.h>
#include <ATen/ops/special_laguerre_polynomial_l_native.h>
#include <ATen/ops/special_laguerre_polynomial_l_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_v_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_v_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_v_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_v_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_v_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_v_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_v_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_v_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_v_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_v_ops.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_v_native.h>
#include <ATen/ops/special_shifted_chebyshev_polynomial_v_ops.h>
#include <ATen/ops/_fused_adamw_native.h>
#include <ATen/ops/_fused_adamw_ops.h>
#include <ATen/ops/_fused_adamw_native.h>
#include <ATen/ops/_fused_adamw_ops.h>
#include <ATen/ops/_fused_adamw_native.h>
#include <ATen/ops/_fused_adamw_ops.h>
#include <ATen/ops/_fused_adamw_native.h>
#include <ATen/ops/_fused_adamw_ops.h>
#include <ATen/ops/_fused_adamw_native.h>
#include <ATen/ops/_fused_adamw_ops.h>
#include <ATen/ops/_fused_adamw_native.h>
#include <ATen/ops/_fused_adamw_ops.h>
#include <ATen/ops/_fw_primal_native.h>
#include <ATen/ops/_fw_primal_ops.h>
#include <ATen/ops/_fw_primal_copy_native.h>
#include <ATen/ops/_fw_primal_copy_ops.h>
#include <ATen/ops/_make_dual_native.h>
#include <ATen/ops/_make_dual_ops.h>
#include <ATen/ops/_make_dual_copy_native.h>
#include <ATen/ops/_make_dual_copy_ops.h>
#include <ATen/ops/_unpack_dual_native.h>
#include <ATen/ops/_unpack_dual_ops.h>
#include <ATen/ops/align_to_native.h>
#include <ATen/ops/align_to_ops.h>
#include <ATen/ops/align_to_native.h>
#include <ATen/ops/align_to_ops.h>
#include <ATen/ops/view_as_complex_native.h>
#include <ATen/ops/view_as_complex_ops.h>
#include <ATen/ops/view_as_complex_copy_native.h>
#include <ATen/ops/view_as_complex_copy_ops.h>
#include <ATen/ops/resolve_neg_native.h>
#include <ATen/ops/resolve_neg_ops.h>
#include <ATen/ops/linalg_diagonal_native.h>
#include <ATen/ops/linalg_diagonal_ops.h>
#include <ATen/ops/expand_native.h>
#include <ATen/ops/expand_ops.h>
#include <ATen/ops/expand_copy_native.h>
#include <ATen/ops/expand_copy_ops.h>
#include <ATen/ops/matrix_H_native.h>
#include <ATen/ops/matrix_H_ops.h>
#include <ATen/ops/adjoint_native.h>
#include <ATen/ops/adjoint_ops.h>
#include <ATen/ops/reshape_as_native.h>
#include <ATen/ops/reshape_as_ops.h>
#include <ATen/ops/transpose_native.h>
#include <ATen/ops/transpose_ops.h>
#include <ATen/ops/transpose_copy_native.h>
#include <ATen/ops/transpose_copy_ops.h>
#include <ATen/ops/transpose_native.h>
#include <ATen/ops/transpose_ops.h>
#include <ATen/ops/_nested_view_from_buffer_native.h>
#include <ATen/ops/_nested_view_from_buffer_ops.h>
#include <ATen/ops/_nested_view_from_buffer_copy_native.h>
#include <ATen/ops/_nested_view_from_buffer_copy_ops.h>
#include <ATen/ops/unsqueeze_native.h>
#include <ATen/ops/unsqueeze_ops.h>
#include <ATen/ops/unsqueeze_copy_native.h>
#include <ATen/ops/unsqueeze_copy_ops.h>
#include <ATen/ops/_values_native.h>
#include <ATen/ops/_values_ops.h>
#include <ATen/ops/_values_copy_native.h>
#include <ATen/ops/_values_copy_ops.h>
#include <ATen/ops/ccol_indices_native.h>
#include <ATen/ops/ccol_indices_ops.h>
#include <ATen/ops/ccol_indices_copy_native.h>
#include <ATen/ops/ccol_indices_copy_ops.h>
#include <ATen/ops/_autocast_to_full_precision_native.h>
#include <ATen/ops/_autocast_to_full_precision_ops.h>
#include <ATen/ops/to_native.h>
#include <ATen/ops/to_ops.h>
#include <ATen/ops/to_native.h>
#include <ATen/ops/to_ops.h>
#include <ATen/ops/to_native.h>
#include <ATen/ops/to_ops.h>
#include <ATen/ops/to_native.h>
#include <ATen/ops/to_ops.h>
#include <ATen/ops/view_native.h>
#include <ATen/ops/view_ops.h>
#include <ATen/ops/view_copy_native.h>
#include <ATen/ops/view_copy_ops.h>
#include <ATen/ops/view_native.h>
#include <ATen/ops/view_ops.h>
#include <ATen/ops/view_copy_native.h>
#include <ATen/ops/view_copy_ops.h>
#include <ATen/ops/_cast_Long_native.h>
#include <ATen/ops/_cast_Long_ops.h>
#include <ATen/ops/_version_native.h>
#include <ATen/ops/_version_ops.h>
#include <ATen/ops/retain_grad_native.h>
#include <ATen/ops/retain_grad_ops.h>
#include <ATen/ops/retains_grad_native.h>
#include <ATen/ops/retains_grad_ops.h>
#include <ATen/ops/_has_same_storage_numel_native.h>
#include <ATen/ops/_has_same_storage_numel_ops.h>
#include <ATen/ops/align_tensors_native.h>
#include <ATen/ops/align_tensors_ops.h>
#include <ATen/ops/_assert_tensor_metadata_native.h>
#include <ATen/ops/_assert_tensor_metadata_ops.h>
#include <ATen/ops/_debug_has_internal_overlap_native.h>
#include <ATen/ops/_debug_has_internal_overlap_ops.h>
#include <ATen/ops/_sobol_engine_draw_native.h>
#include <ATen/ops/_sobol_engine_draw_ops.h>
#include <ATen/ops/_sobol_engine_scramble_native.h>
#include <ATen/ops/_sobol_engine_scramble_ops.h>
#include <ATen/ops/feature_dropout_native.h>
#include <ATen/ops/feature_dropout_ops.h>
#include <ATen/ops/feature_dropout_native.h>
#include <ATen/ops/feature_dropout_ops.h>
#include <ATen/ops/alpha_dropout_native.h>
#include <ATen/ops/alpha_dropout_ops.h>
#include <ATen/ops/alpha_dropout_native.h>
#include <ATen/ops/alpha_dropout_ops.h>
#include <ATen/ops/chalf_native.h>
#include <ATen/ops/chalf_ops.h>
#include <ATen/ops/avg_pool1d_native.h>
#include <ATen/ops/avg_pool1d_ops.h>
#include <ATen/ops/adaptive_avg_pool1d_native.h>
#include <ATen/ops/adaptive_avg_pool1d_ops.h>
#include <ATen/ops/affine_grid_generator_backward_native.h>
#include <ATen/ops/affine_grid_generator_backward_ops.h>
#include <ATen/ops/_test_check_tensor_native.h>
#include <ATen/ops/_test_check_tensor_ops.h>
#include <ATen/ops/atleast_3d_native.h>
#include <ATen/ops/atleast_3d_ops.h>
#include <ATen/ops/atleast_3d_native.h>
#include <ATen/ops/atleast_3d_ops.h>
#include <ATen/ops/batch_norm_native.h>
#include <ATen/ops/batch_norm_ops.h>
#include <ATen/ops/broadcast_tensors_native.h>
#include <ATen/ops/broadcast_tensors_ops.h>
#include <ATen/ops/_convolution_mode_native.h>
#include <ATen/ops/_convolution_mode_ops.h>
#include <ATen/ops/conv3d_native.h>
#include <ATen/ops/conv3d_ops.h>
#include <ATen/ops/conv3d_native.h>
#include <ATen/ops/conv3d_ops.h>
#include <ATen/ops/_cummax_helper_native.h>
#include <ATen/ops/_cummax_helper_ops.h>
#include <ATen/ops/_ctc_loss_backward_native.h>
#include <ATen/ops/_ctc_loss_backward_ops.h>
#include <ATen/ops/diagflat_native.h>
#include <ATen/ops/diagflat_ops.h>
#include <ATen/ops/fill_diagonal_native.h>
#include <ATen/ops/fill_diagonal_ops.h>
#include <ATen/ops/index_copy_native.h>
#include <ATen/ops/index_copy_ops.h>
#include <ATen/ops/index_copy_native.h>
#include <ATen/ops/index_copy_ops.h>
#include <ATen/ops/instance_norm_native.h>
#include <ATen/ops/instance_norm_ops.h>
#include <ATen/ops/is_complex_native.h>
#include <ATen/ops/is_complex_ops.h>
#include <ATen/ops/fbgemm_linear_int8_weight_fp32_activation_native.h>
#include <ATen/ops/fbgemm_linear_int8_weight_fp32_activation_ops.h>
#include <ATen/ops/fbgemm_linear_int8_weight_native.h>
#include <ATen/ops/fbgemm_linear_int8_weight_ops.h>
#include <ATen/ops/fbgemm_linear_quantize_weight_native.h>
#include <ATen/ops/fbgemm_linear_quantize_weight_ops.h>
#include <ATen/ops/fbgemm_linear_fp16_weight_native.h>
#include <ATen/ops/fbgemm_linear_fp16_weight_ops.h>
#include <ATen/ops/log_softmax_native.h>
#include <ATen/ops/log_softmax_ops.h>
#include <ATen/ops/max_pool1d_with_indices_native.h>
#include <ATen/ops/max_pool1d_with_indices_ops.h>
#include <ATen/ops/max_pool3d_native.h>
#include <ATen/ops/max_pool3d_ops.h>
#include <ATen/ops/mean_native.h>
#include <ATen/ops/mean_ops.h>
#include <ATen/ops/multiply_native.h>
#include <ATen/ops/multiply_ops.h>
#include <ATen/ops/multiply_native.h>
#include <ATen/ops/multiply_ops.h>
#include <ATen/ops/is_vulkan_available_native.h>
#include <ATen/ops/is_vulkan_available_ops.h>
#include <ATen/ops/cdist_native.h>
#include <ATen/ops/cdist_ops.h>
#include <ATen/ops/is_pinned_native.h>
#include <ATen/ops/is_pinned_ops.h>
#include <ATen/ops/poisson_nll_loss_native.h>
#include <ATen/ops/poisson_nll_loss_ops.h>
#include <ATen/ops/repeat_interleave_native.h>
#include <ATen/ops/repeat_interleave_ops.h>
#include <ATen/ops/repeat_interleave_native.h>
#include <ATen/ops/repeat_interleave_ops.h>
#include <ATen/ops/_prelu_kernel_native.h>
#include <ATen/ops/_prelu_kernel_ops.h>
#include <ATen/ops/infinitely_differentiable_gelu_backward_native.h>
#include <ATen/ops/infinitely_differentiable_gelu_backward_ops.h>
#include <ATen/ops/mish_backward_native.h>
#include <ATen/ops/mish_backward_ops.h>
#include <ATen/ops/sym_numel_native.h>
#include <ATen/ops/sym_numel_ops.h>
#include <ATen/ops/subtract_native.h>
#include <ATen/ops/subtract_ops.h>
#include <ATen/ops/subtract_native.h>
#include <ATen/ops/subtract_ops.h>
#include <ATen/ops/_sparse_mm_reduce_impl_backward_native.h>
#include <ATen/ops/_sparse_mm_reduce_impl_backward_ops.h>
#include <ATen/ops/sparse_compressed_tensor_native.h>
#include <ATen/ops/sparse_compressed_tensor_ops.h>
#include <ATen/ops/sparse_bsr_tensor_native.h>
#include <ATen/ops/sparse_bsr_tensor_ops.h>
#include <ATen/ops/sparse_compressed_tensor_native.h>
#include <ATen/ops/sparse_compressed_tensor_ops.h>
#include <ATen/ops/sparse_bsr_tensor_native.h>
#include <ATen/ops/sparse_bsr_tensor_ops.h>
#include <ATen/ops/_sparse_csc_tensor_unsafe_native.h>
#include <ATen/ops/_sparse_csc_tensor_unsafe_ops.h>
#include <ATen/ops/_sparse_coo_tensor_unsafe_native.h>
#include <ATen/ops/_sparse_coo_tensor_unsafe_ops.h>
#include <ATen/ops/_validate_sparse_coo_tensor_args_native.h>
#include <ATen/ops/_validate_sparse_coo_tensor_args_ops.h>
#include <ATen/ops/to_dense_native.h>
#include <ATen/ops/to_dense_ops.h>
#include <ATen/ops/to_dense_backward_native.h>
#include <ATen/ops/to_dense_backward_ops.h>
#include <ATen/ops/_nnz_native.h>
#include <ATen/ops/_nnz_ops.h>
#include <ATen/ops/to_sparse_csr_native.h>
#include <ATen/ops/to_sparse_csr_ops.h>
#include <ATen/ops/to_sparse_csc_native.h>
#include <ATen/ops/to_sparse_csc_ops.h>
#include <ATen/ops/to_sparse_bsc_native.h>
#include <ATen/ops/to_sparse_bsc_ops.h>
#include <ATen/ops/_fake_quantize_learnable_per_tensor_affine_backward_native.h>
#include <ATen/ops/_fake_quantize_learnable_per_tensor_affine_backward_ops.h>
#include <ATen/ops/fake_quantize_per_channel_affine_native.h>
#include <ATen/ops/fake_quantize_per_channel_affine_ops.h>
#include <ATen/ops/_fake_quantize_learnable_per_channel_affine_backward_native.h>
#include <ATen/ops/_fake_quantize_learnable_per_channel_affine_backward_ops.h>
#include <ATen/ops/choose_qparams_optimized_native.h>
#include <ATen/ops/choose_qparams_optimized_ops.h>
#include <ATen/ops/combinations_native.h>
#include <ATen/ops/combinations_ops.h>
#include <ATen/ops/result_type_native.h>
#include <ATen/ops/result_type_ops.h>
#include <ATen/ops/result_type_native.h>
#include <ATen/ops/result_type_ops.h>
#include <ATen/ops/result_type_native.h>
#include <ATen/ops/result_type_ops.h>
#include <ATen/ops/result_type_native.h>
#include <ATen/ops/result_type_ops.h>
#include <ATen/ops/_thnn_differentiable_gru_cell_backward_native.h>
#include <ATen/ops/_thnn_differentiable_gru_cell_backward_ops.h>
#include <ATen/ops/lstm_native.h>
#include <ATen/ops/lstm_ops.h>
#include <ATen/ops/lstm_native.h>
#include <ATen/ops/lstm_ops.h>
#include <ATen/ops/gru_native.h>
#include <ATen/ops/gru_ops.h>
#include <ATen/ops/gru_native.h>
#include <ATen/ops/gru_ops.h>
#include <ATen/ops/lstm_cell_native.h>
#include <ATen/ops/lstm_cell_ops.h>
#include <ATen/ops/quantized_gru_cell_native.h>
#include <ATen/ops/quantized_gru_cell_ops.h>
#include <ATen/ops/quantized_rnn_tanh_cell_native.h>
#include <ATen/ops/quantized_rnn_tanh_cell_ops.h>
#include <ATen/ops/index_add_native.h>
#include <ATen/ops/index_add_ops.h>
#include <ATen/ops/index_fill_native.h>
#include <ATen/ops/index_fill_ops.h>
#include <ATen/ops/index_fill_native.h>
#include <ATen/ops/index_fill_ops.h>
#include <ATen/ops/index_fill_native.h>
#include <ATen/ops/index_fill_ops.h>
#include <ATen/ops/index_fill_native.h>
#include <ATen/ops/index_fill_ops.h>
#include <ATen/ops/scatter_native.h>
#include <ATen/ops/scatter_ops.h>
#include <ATen/ops/scatter_native.h>
#include <ATen/ops/scatter_ops.h>
#include <ATen/ops/scatter_add_native.h>
#include <ATen/ops/scatter_add_ops.h>
#include <ATen/ops/and_native.h>
#include <ATen/ops/and_ops.h>
#include <ATen/ops/and_native.h>
#include <ATen/ops/and_ops.h>
#include <ATen/ops/and_native.h>
#include <ATen/ops/and_ops.h>
#include <ATen/ops/and_native.h>
#include <ATen/ops/and_ops.h>
#include <ATen/ops/or_native.h>
#include <ATen/ops/or_ops.h>
#include <ATen/ops/or_native.h>
#include <ATen/ops/or_ops.h>
#include <ATen/ops/or_native.h>
#include <ATen/ops/or_ops.h>
#include <ATen/ops/or_native.h>
#include <ATen/ops/or_ops.h>
#include <ATen/ops/index_select_backward_native.h>
#include <ATen/ops/index_select_backward_ops.h>
#include <ATen/ops/nonzero_numpy_native.h>
#include <ATen/ops/nonzero_numpy_ops.h>
#include <ATen/ops/_linalg_check_errors_native.h>
#include <ATen/ops/_linalg_check_errors_ops.h>
#include <ATen/ops/_lu_with_info_native.h>
#include <ATen/ops/_lu_with_info_ops.h>
#include <ATen/ops/polygamma_native.h>
#include <ATen/ops/polygamma_ops.h>
#include <ATen/ops/nll_loss_nd_native.h>
#include <ATen/ops/nll_loss_nd_ops.h>
#include <ATen/ops/upsample_linear1d_native.h>
#include <ATen/ops/upsample_linear1d_ops.h>
#include <ATen/ops/upsample_bicubic2d_native.h>
#include <ATen/ops/upsample_bicubic2d_ops.h>
#include <ATen/ops/upsample_nearest1d_native.h>
#include <ATen/ops/upsample_nearest1d_ops.h>
#include <ATen/ops/_upsample_nearest_exact1d_native.h>
#include <ATen/ops/_upsample_nearest_exact1d_ops.h>
#include <ATen/ops/upsample_nearest3d_native.h>
#include <ATen/ops/upsample_nearest3d_ops.h>
#include <ATen/ops/det_native.h>
#include <ATen/ops/det_ops.h>
#include <ATen/ops/logdet_native.h>
#include <ATen/ops/logdet_ops.h>
#include <ATen/ops/_test_string_default_native.h>
#include <ATen/ops/_test_string_default_ops.h>
#include <ATen/ops/_scaled_dot_product_flash_attention_backward_native.h>
#include <ATen/ops/_scaled_dot_product_flash_attention_backward_ops.h>
#include <ATen/ops/_scaled_dot_product_flash_attention_for_cpu_backward_native.h>
#include <ATen/ops/_scaled_dot_product_flash_attention_for_cpu_backward_ops.h>
#include <ATen/ops/_flash_attention_forward_native.h>
#include <ATen/ops/_flash_attention_forward_ops.h>
#include <ATen/ops/_efficient_attention_backward_native.h>
#include <ATen/ops/_efficient_attention_backward_ops.h>
#endif

namespace at {
namespace functionalization {

// This keyset is used by functionalization when it calls into meta kernels
// to accurately propagate stride metadata.
// Exclude any modes: the purpose of calling into meta kernels is only as an implementation
// detail to perform shape inference, and we don't want any modal keys to run.
// Specifically, we want to prevent functionalization and Python modes from running.
constexpr auto exclude_keys_for_meta_dispatch =
    c10::functorch_transforms_ks |
    c10::DispatchKeySet({
        c10::DispatchKey::FuncTorchDynamicLayerBackMode,
        c10::DispatchKey::FuncTorchDynamicLayerFrontMode,
        c10::DispatchKey::Python,
        c10::DispatchKey::PreDispatch,

    });

// Helper around at::has_internal_overlap.
// The ATen util is used in hot-path eager mode: it's always fast,
// but might return TOO_HARD sometimes.
// During functionalization, we're ok taking a bit longer
// to detect memory overlap.
inline bool has_internal_overlap_helper(const at::Tensor t) {
  auto has_overlap = at::has_internal_overlap(t);
  if (has_overlap == at::MemOverlap::Yes) return true;
  if (has_overlap == at::MemOverlap::No) return false;
  return false;
}


inline Tensor to_meta(const Tensor& t) {
    if (!t.defined()) return t;
    return at::native::empty_strided_meta_symint(t.sym_sizes(), t.sym_strides(),
/*dtype=*/c10::make_optional(t.scalar_type()), /*layout=*/c10::make_optional(t.layout()),
/*device=*/c10::make_optional(c10::Device(kMeta)), /*pin_memory=*/c10::nullopt);
}

inline c10::optional<Tensor> to_meta(const c10::optional<Tensor>& t) {
  if (t.has_value()) {
    return c10::make_optional<Tensor>(to_meta(*t));
  }
  return c10::nullopt;
}

inline std::vector<Tensor> to_meta(at::ITensorListRef t_list) {
  std::vector<Tensor> outputs;
  outputs.reserve(t_list.size());
  for (const auto& tensor : t_list) {
    outputs.push_back(to_meta(tensor));
  }
  return outputs;
}

inline c10::List<Tensor> to_meta(const c10::List<Tensor>& t_list) {
  c10::List<Tensor> outputs;
  outputs.reserve(t_list.size());
  for (const auto i : c10::irange(t_list.size())) {
    outputs.push_back(to_meta(t_list[i]));
  }
  return outputs;
}

inline c10::List<c10::optional<Tensor>> to_meta(const c10::List<c10::optional<Tensor>>& t_list) {
  c10::List<c10::optional<Tensor>> outputs;
  outputs.reserve(t_list.size());
  for (const auto i : c10::irange(t_list.size())) {
    outputs.push_back(to_meta(t_list[i]));
  }
  return outputs;
}



    at::Tensor & _masked_scale_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, double scale, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mask_meta = to_meta(mask);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_masked_scale_out::call(self_meta, mask_meta, scale, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mask.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mask))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_masked_scale_out::call(self_, mask_, scale, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_masked_scale::call(self_, mask_, scale);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> native_dropout_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double p, c10::optional<bool> train, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::native_dropout_out::call(input_meta, p, train, out0_meta, out1_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::native_dropout_out::call(input_, p, train, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::native_dropout::call(input_, p, train);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & native_dropout_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & mask, double scale, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto mask_meta = to_meta(mask);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::native_dropout_backward_out::call(grad_output_meta, mask_meta, scale, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || mask.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(mask))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::native_dropout_backward_out::call(grad_output_, mask_, scale, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::native_dropout_backward::call(grad_output_, mask_, scale);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & asinh_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::asinh_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::asinh_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::asinh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & asinh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::asinh_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::asinh_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::asinh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & arctanh_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arctanh_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arctanh_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arctanh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & arctanh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arctanh_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arctanh_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arctanh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & baddbmm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto batch1_meta = to_meta(batch1);
        auto batch2_meta = to_meta(batch2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::baddbmm_out::call(self_meta, batch1_meta, batch2_meta, beta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor batch1_;
      if (at::functionalization::impl::isFunctionalTensor(batch1)) {
        at::functionalization::impl::sync(batch1);
        batch1_ = at::functionalization::impl::from_functional_tensor(batch1);
      } else {
        batch1_ = batch1;
      }
      
      at::Tensor batch2_;
      if (at::functionalization::impl::isFunctionalTensor(batch2)) {
        at::functionalization::impl::sync(batch2);
        batch2_ = at::functionalization::impl::from_functional_tensor(batch2);
      } else {
        batch2_ = batch2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || batch1.device().type() == c10::DeviceType::XLA || batch2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(batch1) || at::functionalization::impl::isFunctionalTensor(batch2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::baddbmm_out::call(self_, batch1_, batch2_, beta, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::baddbmm::call(self_, batch1_, batch2_, beta, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & baddbmm_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto batch1_meta = to_meta(batch1);
        auto batch2_meta = to_meta(batch2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::baddbmm_::call(self_meta, batch1_meta, batch2_meta, beta, alpha);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor batch1_;
      if (at::functionalization::impl::isFunctionalTensor(batch1)) {
        at::functionalization::impl::sync(batch1);
        batch1_ = at::functionalization::impl::from_functional_tensor(batch1);
      } else {
        batch1_ = batch1;
      }
      
      at::Tensor batch2_;
      if (at::functionalization::impl::isFunctionalTensor(batch2)) {
        at::functionalization::impl::sync(batch2);
        batch2_ = at::functionalization::impl::from_functional_tensor(batch2);
      } else {
        batch2_ = batch2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || batch1.device().type() == c10::DeviceType::XLA || batch2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(batch1) || at::functionalization::impl::isFunctionalTensor(batch2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::baddbmm_::call(self_, batch1_, batch2_, beta, alpha);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::baddbmm::call(self_, batch1_, batch2_, beta, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & quantized_batch_norm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & var, double eps, double output_scale, int64_t output_zero_point, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto mean_meta = to_meta(mean);
        auto var_meta = to_meta(var);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::quantized_batch_norm_out::call(input_meta, weight_meta, bias_meta, mean_meta, var_meta, eps, output_scale, output_zero_point, out_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor mean_;
      if (at::functionalization::impl::isFunctionalTensor(mean)) {
        at::functionalization::impl::sync(mean);
        mean_ = at::functionalization::impl::from_functional_tensor(mean);
      } else {
        mean_ = mean;
      }
      
      at::Tensor var_;
      if (at::functionalization::impl::isFunctionalTensor(var)) {
        at::functionalization::impl::sync(var);
        var_ = at::functionalization::impl::from_functional_tensor(var);
      } else {
        var_ = var;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || mean.device().type() == c10::DeviceType::XLA || var.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias) || at::functionalization::impl::isFunctionalTensor(mean) || at::functionalization::impl::isFunctionalTensor(var))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::quantized_batch_norm_out::call(input_, weight_, bias_, mean_, var_, eps, output_scale, output_zero_point, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::quantized_batch_norm::call(input_, weight_, bias_, mean_, var_, eps, output_scale, output_zero_point);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bernoulli_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bernoulli_out::call(self_meta, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bernoulli_out::call(self_, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bernoulli::call(self_, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bernoulli_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & p, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto p_meta = to_meta(p);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bernoulli_Tensor_out::call(self_meta, p_meta, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor p_;
      if (at::functionalization::impl::isFunctionalTensor(p)) {
        at::functionalization::impl::sync(p);
        p_ = at::functionalization::impl::from_functional_tensor(p);
      } else {
        p_ = p;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || p.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(p))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bernoulli_Tensor_out::call(self_, p_, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bernoulli_Tensor::call(self_, p_, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bernoulli__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & p, c10::optional<at::Generator> generator) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto p_meta = to_meta(p);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bernoulli__Tensor::call(self_meta, p_meta, generator);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor p_;
      if (at::functionalization::impl::isFunctionalTensor(p)) {
        at::functionalization::impl::sync(p);
        p_ = at::functionalization::impl::from_functional_tensor(p);
      } else {
        p_ = p;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || p.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(p))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bernoulli__Tensor::call(self_, p_, generator);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bernoulli_Tensor::call(self_, p_, generator);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bernoulli_out_float_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double p, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bernoulli_float_out::call(self_meta, p, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bernoulli_float_out::call(self_, p, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bernoulli_p::call(self_, p, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bernoulli__float(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, c10::optional<at::Generator> generator) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bernoulli__float::call(self_meta, p, generator);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bernoulli__float::call(self_, p, generator);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bernoulli_p::call(self_, p, generator);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bmm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mat2_meta = to_meta(mat2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bmm_out::call(self_meta, mat2_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mat2_;
      if (at::functionalization::impl::isFunctionalTensor(mat2)) {
        at::functionalization::impl::sync(mat2);
        mat2_ = at::functionalization::impl::from_functional_tensor(mat2);
      } else {
        mat2_ = mat2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mat2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mat2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bmm_out::call(self_, mat2_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bmm::call(self_, mat2_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & clamp_max_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & max, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clamp_max_out::call(self_meta, max, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clamp_max_out::call(self_, max, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clamp_max::call(self_, max);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & clamp_max_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & max) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clamp_max_::call(self_meta, max);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clamp_max_::call(self_, max);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clamp_max::call(self_, max);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & clamp_max_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & max, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto max_meta = to_meta(max);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clamp_max_Tensor_out::call(self_meta, max_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor max_;
      if (at::functionalization::impl::isFunctionalTensor(max)) {
        at::functionalization::impl::sync(max);
        max_ = at::functionalization::impl::from_functional_tensor(max);
      } else {
        max_ = max;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || max.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(max))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clamp_max_Tensor_out::call(self_, max_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clamp_max_Tensor::call(self_, max_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & clamp_max__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & max) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto max_meta = to_meta(max);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clamp_max__Tensor::call(self_meta, max_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor max_;
      if (at::functionalization::impl::isFunctionalTensor(max)) {
        at::functionalization::impl::sync(max);
        max_ = at::functionalization::impl::from_functional_tensor(max);
      } else {
        max_ = max;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || max.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(max))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clamp_max__Tensor::call(self_, max_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clamp_max_Tensor::call(self_, max_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & clamp_min_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & min, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clamp_min_out::call(self_meta, min, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clamp_min_out::call(self_, min, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clamp_min::call(self_, min);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & clamp_min_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & min) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clamp_min_::call(self_meta, min);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clamp_min_::call(self_, min);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clamp_min::call(self_, min);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & clamp_min_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & min, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto min_meta = to_meta(min);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clamp_min_Tensor_out::call(self_meta, min_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor min_;
      if (at::functionalization::impl::isFunctionalTensor(min)) {
        at::functionalization::impl::sync(min);
        min_ = at::functionalization::impl::from_functional_tensor(min);
      } else {
        min_ = min;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || min.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(min))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clamp_min_Tensor_out::call(self_, min_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clamp_min_Tensor::call(self_, min_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & clamp_min__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & min) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto min_meta = to_meta(min);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clamp_min__Tensor::call(self_meta, min_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor min_;
      if (at::functionalization::impl::isFunctionalTensor(min)) {
        at::functionalization::impl::sync(min);
        min_ = at::functionalization::impl::from_functional_tensor(min);
      } else {
        min_ = min;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || min.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(min))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clamp_min__Tensor::call(self_, min_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clamp_min_Tensor::call(self_, min_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & clip_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clip_out::call(self_meta, min, max, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clip_out::call(self_, min, max, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clip::call(self_, min, max);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & clip_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clip_::call(self_meta, min, max);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clip_::call(self_, min, max);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clip::call(self_, min, max);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & clip_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto min_meta = to_meta(min);
        auto max_meta = to_meta(max);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clip_Tensor_out::call(self_meta, min_meta, max_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::optional<at::Tensor> min_;
      if (at::functionalization::impl::isFunctionalTensor(min)) {
        at::functionalization::impl::sync(min);
        min_ = at::functionalization::impl::from_functional_tensor(min);
      } else {
        min_ = min;
      }
      
      c10::optional<at::Tensor> max_;
      if (at::functionalization::impl::isFunctionalTensor(max)) {
        at::functionalization::impl::sync(max);
        max_ = at::functionalization::impl::from_functional_tensor(max);
      } else {
        max_ = max;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(min) || at::functionalization::impl::isFunctionalTensor(max))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clip_Tensor_out::call(self_, min_, max_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clip_Tensor::call(self_, min_, max_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & clip__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto min_meta = to_meta(min);
        auto max_meta = to_meta(max);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clip__Tensor::call(self_meta, min_meta, max_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::optional<at::Tensor> min_;
      if (at::functionalization::impl::isFunctionalTensor(min)) {
        at::functionalization::impl::sync(min);
        min_ = at::functionalization::impl::from_functional_tensor(min);
      } else {
        min_ = min;
      }
      
      c10::optional<at::Tensor> max_;
      if (at::functionalization::impl::isFunctionalTensor(max)) {
        at::functionalization::impl::sync(max);
        max_ = at::functionalization::impl::from_functional_tensor(max);
      } else {
        max_ = max;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(min) || at::functionalization::impl::isFunctionalTensor(max))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clip__Tensor::call(self_, min_, max_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clip_Tensor::call(self_, min_, max_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & complex_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & real, const at::Tensor & imag, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto real_meta = to_meta(real);
        auto imag_meta = to_meta(imag);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::complex_out::call(real_meta, imag_meta, out_meta);
      }
      
      at::Tensor real_;
      if (at::functionalization::impl::isFunctionalTensor(real)) {
        at::functionalization::impl::sync(real);
        real_ = at::functionalization::impl::from_functional_tensor(real);
      } else {
        real_ = real;
      }
      
      at::Tensor imag_;
      if (at::functionalization::impl::isFunctionalTensor(imag)) {
        at::functionalization::impl::sync(imag);
        imag_ = at::functionalization::impl::from_functional_tensor(imag);
      } else {
        imag_ = imag;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || real.device().type() == c10::DeviceType::XLA || imag.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(real) || at::functionalization::impl::isFunctionalTensor(imag))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::complex_out::call(real_, imag_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::complex::call(real_, imag_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & constant_pad_nd_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef pad, const at::Scalar & value, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::constant_pad_nd_out::call(self_meta, pad, value, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::constant_pad_nd_out::call(self_, pad, value, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::constant_pad_nd::call(self_, pad, value);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & conv_tbc_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const at::Tensor & bias, int64_t pad, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::conv_tbc_out::call(self_meta, weight_meta, bias_meta, pad, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA || bias.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::conv_tbc_out::call(self_, weight_, bias_, pad, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::conv_tbc::call(self_, weight_, bias_, pad);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _copy_from_and_resize_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & dst, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto dst_meta = to_meta(dst);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_copy_from_and_resize_out::call(self_meta, dst_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor dst_;
      if (at::functionalization::impl::isFunctionalTensor(dst)) {
        at::functionalization::impl::sync(dst);
        dst_ = at::functionalization::impl::from_functional_tensor(dst);
      } else {
        dst_ = dst;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || dst.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(dst))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_copy_from_and_resize_out::call(self_, dst_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_copy_from_and_resize::call(self_, dst_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cos_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cos_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cos_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cos::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cos_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cos_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cos_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cos::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & count_nonzero_out_dim_IntList_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::count_nonzero_dim_IntList_out::call(self_meta, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::count_nonzero_dim_IntList_out::call(self_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::count_nonzero_dim_IntList::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & count_nonzero_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::count_nonzero_out::call(self_meta, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::count_nonzero_out::call(self_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::count_nonzero::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cudnn_affine_grid_generator_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_meta = to_meta(grad);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cudnn_affine_grid_generator_backward_out::call(grad_meta, N, C, H, W, out_meta);
      }
      
      at::Tensor grad_;
      if (at::functionalization::impl::isFunctionalTensor(grad)) {
        at::functionalization::impl::sync(grad);
        grad_ = at::functionalization::impl::from_functional_tensor(grad);
      } else {
        grad_ = grad;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cudnn_affine_grid_generator_backward_out::call(grad_, N, C, H, W, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cudnn_affine_grid_generator_backward::call(grad_, N, C, H, W);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> cudnn_batch_norm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double exponential_average_factor, double epsilon, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto running_mean_meta = to_meta(running_mean);
        auto running_var_meta = to_meta(running_var);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        auto out3_meta = to_meta(out3);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cudnn_batch_norm_out::call(input_meta, weight_meta, bias_meta, running_mean_meta, running_var_meta, training, exponential_average_factor, epsilon, out0_meta, out1_meta, out2_meta, out3_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      c10::optional<at::Tensor> running_mean_;
      if (at::functionalization::impl::isFunctionalTensor(running_mean)) {
        at::functionalization::impl::sync(running_mean);
        running_mean_ = at::functionalization::impl::from_functional_tensor(running_mean);
      } else {
        running_mean_ = running_mean;
      }
      
      c10::optional<at::Tensor> running_var_;
      if (at::functionalization::impl::isFunctionalTensor(running_var)) {
        at::functionalization::impl::sync(running_var);
        running_var_ = at::functionalization::impl::from_functional_tensor(running_var);
      } else {
        running_var_ = running_var;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      
      at::Tensor out3_;
      if (at::functionalization::impl::isFunctionalTensor(out3)) {
        at::functionalization::impl::sync(out3);
        out3_ = at::functionalization::impl::from_functional_tensor(out3);
      } else {
        out3_ = out3;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2) && at::functionalization::impl::isFunctionalTensor(out3))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias) || at::functionalization::impl::isFunctionalTensor(running_mean) || at::functionalization::impl::isFunctionalTensor(running_var))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::cudnn_batch_norm_out::call(input_, weight_, bias_, running_mean_, running_var_, training, exponential_average_factor, epsilon, out0_, out1_, out2_, out3_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cudnn_batch_norm::call(input_, weight_, bias_, running_mean_, running_var_, training, exponential_average_factor, epsilon);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
  at::functionalization::impl::propagate_xla_data(out3, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(out3, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(out3);
  at::functionalization::impl::sync(out3);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> mps_convolution_transpose_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, ::std::array<bool,2> output_mask, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grad_output_meta = to_meta(grad_output);
        auto weight_meta = to_meta(weight);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mps_convolution_transpose_backward_out::call(self_meta, grad_output_meta, weight_meta, padding, output_padding, stride, dilation, groups, output_mask, out0_meta, out1_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || grad_output.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::mps_convolution_transpose_backward_out::call(self_, grad_output_, weight_, padding, output_padding, stride, dilation, groups, output_mask, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mps_convolution_transpose_backward::call(self_, grad_output_, weight_, padding, output_padding, stride, dilation, groups, output_mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> cudnn_grid_sampler_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grid, const at::Tensor & grad_output, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grid_meta = to_meta(grid);
        auto grad_output_meta = to_meta(grad_output);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cudnn_grid_sampler_backward_out::call(self_meta, grid_meta, grad_output_meta, out0_meta, out1_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grid_;
      if (at::functionalization::impl::isFunctionalTensor(grid)) {
        at::functionalization::impl::sync(grid);
        grid_ = at::functionalization::impl::from_functional_tensor(grid);
      } else {
        grid_ = grid;
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || grid.device().type() == c10::DeviceType::XLA || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(grid) || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::cudnn_grid_sampler_backward_out::call(self_, grid_, grad_output_, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cudnn_grid_sampler_backward::call(self_, grid_, grad_output_);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> cummin_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cummin_out::call(self_meta, dim, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::cummin_out::call(self_, dim, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cummin::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> cummin_out_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cummin_dimname_out::call(self_meta, dim, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::cummin_dimname_out::call(self_, dim, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cummin_dimname::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    at::Tensor & cumsum_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cumsum_out::call(self_meta, dim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cumsum_out::call(self_, dim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cumsum::call(self_, dim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cumsum_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cumsum_::call(self_meta, dim, dtype);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cumsum_::call(self_, dim, dtype);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cumsum::call(self_, dim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & cumsum_out_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cumsum_dimname_out::call(self_meta, dim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cumsum_dimname_out::call(self_, dim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cumsum_dimname::call(self_, dim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cumsum__dimname(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cumsum__dimname::call(self_meta, dim, dtype);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cumsum__dimname::call(self_, dim, dtype);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cumsum_dimname::call(self_, dim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _ctc_loss_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, bool zero_infinity, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto log_probs_meta = to_meta(log_probs);
        auto targets_meta = to_meta(targets);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_ctc_loss_out::call(log_probs_meta, targets_meta, input_lengths, target_lengths, blank, zero_infinity, out0_meta, out1_meta);
      }
      
      at::Tensor log_probs_;
      if (at::functionalization::impl::isFunctionalTensor(log_probs)) {
        at::functionalization::impl::sync(log_probs);
        log_probs_ = at::functionalization::impl::from_functional_tensor(log_probs);
      } else {
        log_probs_ = log_probs;
      }
      
      at::Tensor targets_;
      if (at::functionalization::impl::isFunctionalTensor(targets)) {
        at::functionalization::impl::sync(targets);
        targets_ = at::functionalization::impl::from_functional_tensor(targets);
      } else {
        targets_ = targets;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || log_probs.device().type() == c10::DeviceType::XLA || targets.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(log_probs) || at::functionalization::impl::isFunctionalTensor(targets))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_ctc_loss_out::call(log_probs_, targets_, input_lengths, target_lengths, blank, zero_infinity, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_ctc_loss::call(log_probs_, targets_, input_lengths, target_lengths, blank, zero_infinity);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _ctc_loss_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, const at::Tensor & input_lengths, const at::Tensor & target_lengths, int64_t blank, bool zero_infinity, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto log_probs_meta = to_meta(log_probs);
        auto targets_meta = to_meta(targets);
        auto input_lengths_meta = to_meta(input_lengths);
        auto target_lengths_meta = to_meta(target_lengths);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_ctc_loss_Tensor_out::call(log_probs_meta, targets_meta, input_lengths_meta, target_lengths_meta, blank, zero_infinity, out0_meta, out1_meta);
      }
      
      at::Tensor log_probs_;
      if (at::functionalization::impl::isFunctionalTensor(log_probs)) {
        at::functionalization::impl::sync(log_probs);
        log_probs_ = at::functionalization::impl::from_functional_tensor(log_probs);
      } else {
        log_probs_ = log_probs;
      }
      
      at::Tensor targets_;
      if (at::functionalization::impl::isFunctionalTensor(targets)) {
        at::functionalization::impl::sync(targets);
        targets_ = at::functionalization::impl::from_functional_tensor(targets);
      } else {
        targets_ = targets;
      }
      
      at::Tensor input_lengths_;
      if (at::functionalization::impl::isFunctionalTensor(input_lengths)) {
        at::functionalization::impl::sync(input_lengths);
        input_lengths_ = at::functionalization::impl::from_functional_tensor(input_lengths);
      } else {
        input_lengths_ = input_lengths;
      }
      
      at::Tensor target_lengths_;
      if (at::functionalization::impl::isFunctionalTensor(target_lengths)) {
        at::functionalization::impl::sync(target_lengths);
        target_lengths_ = at::functionalization::impl::from_functional_tensor(target_lengths);
      } else {
        target_lengths_ = target_lengths;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || log_probs.device().type() == c10::DeviceType::XLA || targets.device().type() == c10::DeviceType::XLA || input_lengths.device().type() == c10::DeviceType::XLA || target_lengths.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(log_probs) || at::functionalization::impl::isFunctionalTensor(targets) || at::functionalization::impl::isFunctionalTensor(input_lengths) || at::functionalization::impl::isFunctionalTensor(target_lengths))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_ctc_loss_Tensor_out::call(log_probs_, targets_, input_lengths_, target_lengths_, blank, zero_infinity, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_ctc_loss_Tensor::call(log_probs_, targets_, input_lengths_, target_lengths_, blank, zero_infinity);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & _ctc_loss_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, const at::Tensor & neg_log_likelihood, const at::Tensor & log_alpha, int64_t blank, bool zero_infinity, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_meta = to_meta(grad);
        auto log_probs_meta = to_meta(log_probs);
        auto targets_meta = to_meta(targets);
        auto neg_log_likelihood_meta = to_meta(neg_log_likelihood);
        auto log_alpha_meta = to_meta(log_alpha);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_ctc_loss_backward_out::call(grad_meta, log_probs_meta, targets_meta, input_lengths, target_lengths, neg_log_likelihood_meta, log_alpha_meta, blank, zero_infinity, out_meta);
      }
      
      at::Tensor grad_;
      if (at::functionalization::impl::isFunctionalTensor(grad)) {
        at::functionalization::impl::sync(grad);
        grad_ = at::functionalization::impl::from_functional_tensor(grad);
      } else {
        grad_ = grad;
      }
      
      at::Tensor log_probs_;
      if (at::functionalization::impl::isFunctionalTensor(log_probs)) {
        at::functionalization::impl::sync(log_probs);
        log_probs_ = at::functionalization::impl::from_functional_tensor(log_probs);
      } else {
        log_probs_ = log_probs;
      }
      
      at::Tensor targets_;
      if (at::functionalization::impl::isFunctionalTensor(targets)) {
        at::functionalization::impl::sync(targets);
        targets_ = at::functionalization::impl::from_functional_tensor(targets);
      } else {
        targets_ = targets;
      }
      
      at::Tensor neg_log_likelihood_;
      if (at::functionalization::impl::isFunctionalTensor(neg_log_likelihood)) {
        at::functionalization::impl::sync(neg_log_likelihood);
        neg_log_likelihood_ = at::functionalization::impl::from_functional_tensor(neg_log_likelihood);
      } else {
        neg_log_likelihood_ = neg_log_likelihood;
      }
      
      at::Tensor log_alpha_;
      if (at::functionalization::impl::isFunctionalTensor(log_alpha)) {
        at::functionalization::impl::sync(log_alpha);
        log_alpha_ = at::functionalization::impl::from_functional_tensor(log_alpha);
      } else {
        log_alpha_ = log_alpha;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad.device().type() == c10::DeviceType::XLA || log_probs.device().type() == c10::DeviceType::XLA || targets.device().type() == c10::DeviceType::XLA || neg_log_likelihood.device().type() == c10::DeviceType::XLA || log_alpha.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad) || at::functionalization::impl::isFunctionalTensor(log_probs) || at::functionalization::impl::isFunctionalTensor(targets) || at::functionalization::impl::isFunctionalTensor(neg_log_likelihood) || at::functionalization::impl::isFunctionalTensor(log_alpha))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_ctc_loss_backward_out::call(grad_, log_probs_, targets_, input_lengths, target_lengths, neg_log_likelihood_, log_alpha_, blank, zero_infinity, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_ctc_loss_backward::call(grad_, log_probs_, targets_, input_lengths, target_lengths, neg_log_likelihood_, log_alpha_, blank, zero_infinity);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & embedding_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, c10::SymInt padding_idx, bool scale_grad_by_freq, bool sparse, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto weight_meta = to_meta(weight);
        auto indices_meta = to_meta(indices);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::embedding_out::call(weight_meta, indices_meta, padding_idx, scale_grad_by_freq, sparse, out_meta);
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || weight.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::embedding_out::call(weight_, indices_, padding_idx, scale_grad_by_freq, sparse, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::embedding::call(weight_, indices_, padding_idx, scale_grad_by_freq, sparse);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & embedding_dense_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & indices, c10::SymInt num_weights, c10::SymInt padding_idx, bool scale_grad_by_freq, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto indices_meta = to_meta(indices);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::embedding_dense_backward_out::call(grad_output_meta, indices_meta, num_weights, padding_idx, scale_grad_by_freq, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::embedding_dense_backward_out::call(grad_output_, indices_, num_weights, padding_idx, scale_grad_by_freq, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::embedding_dense_backward::call(grad_output_, indices_, num_weights, padding_idx, scale_grad_by_freq);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & empty_permuted_out_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, at::IntArrayRef physical_layout, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::empty_permuted_out::call(size, physical_layout, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::empty_permuted_out::call(size, physical_layout, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::empty_permuted::call(size, physical_layout, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & new_zeros_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::new_zeros_out::call(self_meta, size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::new_zeros_out::call(self_, size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::new_zeros::call(self_, size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & new_ones_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::new_ones_out::call(self_meta, size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::new_ones_out::call(self_, size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::new_ones::call(self_, size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _empty_per_channel_affine_quantized_out_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto scales_meta = to_meta(scales);
        auto zero_points_meta = to_meta(zero_points);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_empty_per_channel_affine_quantized_out::call(size, scales_meta, zero_points_meta, axis, memory_format, out_meta);
      }
      
      at::Tensor scales_;
      if (at::functionalization::impl::isFunctionalTensor(scales)) {
        at::functionalization::impl::sync(scales);
        scales_ = at::functionalization::impl::from_functional_tensor(scales);
      } else {
        scales_ = scales;
      }
      
      at::Tensor zero_points_;
      if (at::functionalization::impl::isFunctionalTensor(zero_points)) {
        at::functionalization::impl::sync(zero_points);
        zero_points_ = at::functionalization::impl::from_functional_tensor(zero_points);
      } else {
        zero_points_ = zero_points;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || scales.device().type() == c10::DeviceType::XLA || zero_points.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(scales) || at::functionalization::impl::isFunctionalTensor(zero_points))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_empty_per_channel_affine_quantized_out::call(size, scales_, zero_points_, axis, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_empty_per_channel_affine_quantized::call(size, scales_, zero_points_, axis, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & resize_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, c10::optional<at::MemoryFormat> memory_format, const at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::resize_out::call(self_meta, size, memory_format, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::resize_out::call(self_, size, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::resize::call(self_, size, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & resize_(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, c10::optional<at::MemoryFormat> memory_format) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::resize_::call(self_meta, size, memory_format);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::resize_::call(self_, size, memory_format);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::resize::call(self_, size, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & exp2_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::exp2_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::exp2_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::exp2::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & exp2_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::exp2_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::exp2_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::exp2::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & _grid_sampler_2d_cpu_fallback_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto grid_meta = to_meta(grid);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_grid_sampler_2d_cpu_fallback_out::call(input_meta, grid_meta, interpolation_mode, padding_mode, align_corners, out_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor grid_;
      if (at::functionalization::impl::isFunctionalTensor(grid)) {
        at::functionalization::impl::sync(grid);
        grid_ = at::functionalization::impl::from_functional_tensor(grid);
      } else {
        grid_ = grid;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || grid.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(grid))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_grid_sampler_2d_cpu_fallback_out::call(input_, grid_, interpolation_mode, padding_mode, align_corners, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_grid_sampler_2d_cpu_fallback::call(input_, grid_, interpolation_mode, padding_mode, align_corners);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> grid_sampler_3d_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, ::std::array<bool,2> output_mask, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto input_meta = to_meta(input);
        auto grid_meta = to_meta(grid);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::grid_sampler_3d_backward_out::call(grad_output_meta, input_meta, grid_meta, interpolation_mode, padding_mode, align_corners, output_mask, out0_meta, out1_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor grid_;
      if (at::functionalization::impl::isFunctionalTensor(grid)) {
        at::functionalization::impl::sync(grid);
        grid_ = at::functionalization::impl::from_functional_tensor(grid);
      } else {
        grid_ = grid;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || input.device().type() == c10::DeviceType::XLA || grid.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(grid))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::grid_sampler_3d_backward_out::call(grad_output_, input_, grid_, interpolation_mode, padding_mode, align_corners, output_mask, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::grid_sampler_3d_backward::call(grad_output_, input_, grid_, interpolation_mode, padding_mode, align_corners, output_mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & _fft_c2c_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef dim, int64_t normalization, bool forward, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fft_c2c_out::call(self_meta, dim, normalization, forward, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_fft_c2c_out::call(self_, dim, normalization, forward, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fft_c2c::call(self_, dim, normalization, forward);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & index_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto source_meta = to_meta(source);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_copy_out::call(self_meta, dim, index_meta, source_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor source_;
      if (at::functionalization::impl::isFunctionalTensor(source)) {
        at::functionalization::impl::sync(source);
        source_ = at::functionalization::impl::from_functional_tensor(source);
      } else {
        source_ = source;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA || source.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(source))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_copy_out::call(self_, dim, index_, source_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_copy::call(self_, dim, index_, source_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & index_copy_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto source_meta = to_meta(source);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_copy_::call(self_meta, dim, index_meta, source_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor source_;
      if (at::functionalization::impl::isFunctionalTensor(source)) {
        at::functionalization::impl::sync(source);
        source_ = at::functionalization::impl::from_functional_tensor(source);
      } else {
        source_ = source;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || index.device().type() == c10::DeviceType::XLA || source.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(source))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_copy_::call(self_, dim, index_, source_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_copy::call(self_, dim, index_, source_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & isin_out_Tensor_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & elements, const at::Tensor & test_elements, bool assume_unique, bool invert, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto elements_meta = to_meta(elements);
        auto test_elements_meta = to_meta(test_elements);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::isin_Tensor_Tensor_out::call(elements_meta, test_elements_meta, assume_unique, invert, out_meta);
      }
      
      at::Tensor elements_;
      if (at::functionalization::impl::isFunctionalTensor(elements)) {
        at::functionalization::impl::sync(elements);
        elements_ = at::functionalization::impl::from_functional_tensor(elements);
      } else {
        elements_ = elements;
      }
      
      at::Tensor test_elements_;
      if (at::functionalization::impl::isFunctionalTensor(test_elements)) {
        at::functionalization::impl::sync(test_elements);
        test_elements_ = at::functionalization::impl::from_functional_tensor(test_elements);
      } else {
        test_elements_ = test_elements;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || elements.device().type() == c10::DeviceType::XLA || test_elements.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(elements) || at::functionalization::impl::isFunctionalTensor(test_elements))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::isin_Tensor_Tensor_out::call(elements_, test_elements_, assume_unique, invert, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::isin_Tensor_Tensor::call(elements_, test_elements_, assume_unique, invert);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & isin_out_Tensor_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & elements, const at::Scalar & test_element, bool assume_unique, bool invert, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto elements_meta = to_meta(elements);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::isin_Tensor_Scalar_out::call(elements_meta, test_element, assume_unique, invert, out_meta);
      }
      
      at::Tensor elements_;
      if (at::functionalization::impl::isFunctionalTensor(elements)) {
        at::functionalization::impl::sync(elements);
        elements_ = at::functionalization::impl::from_functional_tensor(elements);
      } else {
        elements_ = elements;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || elements.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(elements))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::isin_Tensor_Scalar_out::call(elements_, test_element, assume_unique, invert, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::isin_Tensor_Scalar::call(elements_, test_element, assume_unique, invert);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & isin_out_Scalar_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & element, const at::Tensor & test_elements, bool assume_unique, bool invert, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto test_elements_meta = to_meta(test_elements);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::isin_Scalar_Tensor_out::call(element, test_elements_meta, assume_unique, invert, out_meta);
      }
      
      at::Tensor test_elements_;
      if (at::functionalization::impl::isFunctionalTensor(test_elements)) {
        at::functionalization::impl::sync(test_elements);
        test_elements_ = at::functionalization::impl::from_functional_tensor(test_elements);
      } else {
        test_elements_ = test_elements;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || test_elements.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(test_elements))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::isin_Scalar_Tensor_out::call(element, test_elements_, assume_unique, invert, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::isin_Scalar_Tensor::call(element, test_elements_, assume_unique, invert);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & kron_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::kron_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::kron_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::kron::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & nan_to_num_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nan_to_num_out::call(self_meta, nan, posinf, neginf, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nan_to_num_out::call(self_, nan, posinf, neginf, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nan_to_num::call(self_, nan, posinf, neginf);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & nan_to_num_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nan_to_num_::call(self_meta, nan, posinf, neginf);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nan_to_num_::call(self_, nan, posinf, neginf);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nan_to_num::call(self_, nan, posinf, neginf);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & linear_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linear_out::call(input_meta, weight_meta, bias_meta, out_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linear_out::call(input_, weight_, bias_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linear::call(input_, weight_, bias_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mkldnn_linear_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_linear_out::call(self_meta, weight_meta, bias_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mkldnn_linear_out::call(self_, weight_, bias_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_linear::call(self_, weight_, bias_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linspace_out_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, int64_t steps, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linspace_out::call(start, end, steps, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linspace_out::call(start, end, steps, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linspace::call(start, end, steps, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linspace_out_Tensor_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & start, const at::Tensor & end, int64_t steps, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto start_meta = to_meta(start);
        auto end_meta = to_meta(end);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linspace_Tensor_Tensor_out::call(start_meta, end_meta, steps, out_meta);
      }
      
      at::Tensor start_;
      if (at::functionalization::impl::isFunctionalTensor(start)) {
        at::functionalization::impl::sync(start);
        start_ = at::functionalization::impl::from_functional_tensor(start);
      } else {
        start_ = start;
      }
      
      at::Tensor end_;
      if (at::functionalization::impl::isFunctionalTensor(end)) {
        at::functionalization::impl::sync(end);
        end_ = at::functionalization::impl::from_functional_tensor(end);
      } else {
        end_ = end;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || start.device().type() == c10::DeviceType::XLA || end.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(start) || at::functionalization::impl::isFunctionalTensor(end))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linspace_Tensor_Tensor_out::call(start_, end_, steps, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linspace_Tensor_Tensor::call(start_, end_, steps, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linspace_out_Tensor_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & start, const at::Scalar & end, int64_t steps, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto start_meta = to_meta(start);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linspace_Tensor_Scalar_out::call(start_meta, end, steps, out_meta);
      }
      
      at::Tensor start_;
      if (at::functionalization::impl::isFunctionalTensor(start)) {
        at::functionalization::impl::sync(start);
        start_ = at::functionalization::impl::from_functional_tensor(start);
      } else {
        start_ = start;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || start.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(start))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linspace_Tensor_Scalar_out::call(start_, end, steps, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linspace_Tensor_Scalar::call(start_, end, steps, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linspace_out_Scalar_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Tensor & end, int64_t steps, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto end_meta = to_meta(end);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linspace_Scalar_Tensor_out::call(start, end_meta, steps, out_meta);
      }
      
      at::Tensor end_;
      if (at::functionalization::impl::isFunctionalTensor(end)) {
        at::functionalization::impl::sync(end);
        end_ = at::functionalization::impl::from_functional_tensor(end);
      } else {
        end_ = end;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || end.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(end))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linspace_Scalar_Tensor_out::call(start, end_, steps, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linspace_Scalar_Tensor::call(start, end_, steps, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & log_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::log_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::log_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::log::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & log_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::log_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::log_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::log::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & log_softmax_out_int_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::log_softmax_int_out::call(self_meta, dim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::log_softmax_int_out::call(self_, dim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::log_softmax_int::call(self_, dim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _log_softmax_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_log_softmax_out::call(self_meta, dim, half_to_float, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_log_softmax_out::call(self_, dim, half_to_float, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_log_softmax::call(self_, dim, half_to_float);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _log_softmax_backward_data_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto output_meta = to_meta(output);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_log_softmax_backward_data_out::call(grad_output_meta, output_meta, dim, input_dtype, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_log_softmax_backward_data_out::call(grad_output_, output_, dim, input_dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_log_softmax_backward_data::call(grad_output_, output_, dim, input_dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> max_out_dim_max(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto max_meta = to_meta(max);
        auto max_values_meta = to_meta(max_values);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::max_dim_max::call(self_meta, dim, keepdim, max_meta, max_values_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor max_;
      if (at::functionalization::impl::isFunctionalTensor(max)) {
        at::functionalization::impl::sync(max);
        max_ = at::functionalization::impl::from_functional_tensor(max);
      } else {
        max_ = max;
      }
      
      at::Tensor max_values_;
      if (at::functionalization::impl::isFunctionalTensor(max_values)) {
        at::functionalization::impl::sync(max_values);
        max_values_ = at::functionalization::impl::from_functional_tensor(max_values);
      } else {
        max_values_ = max_values;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(max) && at::functionalization::impl::isFunctionalTensor(max_values))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::max_dim_max::call(self_, dim, keepdim, max_, max_values_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(max, max_values);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::max_dim::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(max, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(max, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(max);
  at::functionalization::impl::sync(max);
  at::functionalization::impl::propagate_xla_data(max_values, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(max_values, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(max_values);
  at::functionalization::impl::sync(max_values);
    return ::std::tuple<at::Tensor &,at::Tensor &>(max, max_values);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> max_out_names_dim_max(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto max_meta = to_meta(max);
        auto max_values_meta = to_meta(max_values);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::max_names_dim_max::call(self_meta, dim, keepdim, max_meta, max_values_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor max_;
      if (at::functionalization::impl::isFunctionalTensor(max)) {
        at::functionalization::impl::sync(max);
        max_ = at::functionalization::impl::from_functional_tensor(max);
      } else {
        max_ = max;
      }
      
      at::Tensor max_values_;
      if (at::functionalization::impl::isFunctionalTensor(max_values)) {
        at::functionalization::impl::sync(max_values);
        max_values_ = at::functionalization::impl::from_functional_tensor(max_values);
      } else {
        max_values_ = max_values;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(max) && at::functionalization::impl::isFunctionalTensor(max_values))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::max_names_dim_max::call(self_, dim, keepdim, max_, max_values_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(max, max_values);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::max_names_dim::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(max, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(max, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(max);
  at::functionalization::impl::sync(max);
  at::functionalization::impl::propagate_xla_data(max_values, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(max_values, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(max_values);
  at::functionalization::impl::sync(max_values);
    return ::std::tuple<at::Tensor &,at::Tensor &>(max, max_values);
      }
    }

    at::Tensor & amax_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::amax_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::amax_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::amax::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mkldnn_max_pool2d_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto output_meta = to_meta(output);
        auto input_meta = to_meta(input);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_max_pool2d_backward_out::call(grad_output_meta, output_meta, input_meta, kernel_size, stride, padding, dilation, ceil_mode, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || output.device().type() == c10::DeviceType::XLA || input.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(output) || at::functionalization::impl::isFunctionalTensor(input))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mkldnn_max_pool2d_backward_out::call(grad_output_, output_, input_, kernel_size, stride, padding, dilation, ceil_mode, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_max_pool2d_backward::call(grad_output_, output_, input_, kernel_size, stride, padding, dilation, ceil_mode);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mkldnn_max_pool3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_max_pool3d_out::call(self_meta, kernel_size, stride, padding, dilation, ceil_mode, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mkldnn_max_pool3d_out::call(self_, kernel_size, stride, padding, dilation, ceil_mode, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_max_pool3d::call(self_, kernel_size, stride, padding, dilation, ceil_mode);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & quantized_max_pool2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::quantized_max_pool2d_out::call(self_meta, kernel_size, stride, padding, dilation, ceil_mode, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::quantized_max_pool2d_out::call(self_, kernel_size, stride, padding, dilation, ceil_mode, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::quantized_max_pool2d::call(self_, kernel_size, stride, padding, dilation, ceil_mode);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & quantized_max_pool3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::quantized_max_pool3d_out::call(self_meta, kernel_size, stride, padding, dilation, ceil_mode, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::quantized_max_pool3d_out::call(self_, kernel_size, stride, padding, dilation, ceil_mode, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::quantized_max_pool3d::call(self_, kernel_size, stride, padding, dilation, ceil_mode);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mean_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mean_out::call(self_meta, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mean_out::call(self_, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mean_dim::call(self_, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mean_out_names_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mean_names_out::call(self_meta, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mean_names_out::call(self_, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mean_names_dim::call(self_, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & nanmedian_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nanmedian_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nanmedian_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nanmedian::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> nanmedian_out_dim_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nanmedian_dim_values::call(self_meta, dim, keepdim, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::nanmedian_dim_values::call(self_, dim, keepdim, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nanmedian_dim::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> nanmedian_out_names_dim_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nanmedian_names_dim_values::call(self_meta, dim, keepdim, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::nanmedian_names_dim_values::call(self_, dim, keepdim, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nanmedian_names_dim::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    at::Tensor & _int_mm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mat2_meta = to_meta(mat2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_int_mm_out::call(self_meta, mat2_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mat2_;
      if (at::functionalization::impl::isFunctionalTensor(mat2)) {
        at::functionalization::impl::sync(mat2);
        mat2_ = at::functionalization::impl::from_functional_tensor(mat2);
      } else {
        mat2_ = mat2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mat2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mat2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_int_mm_out::call(self_, mat2_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_int_mm::call(self_, mat2_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> mode_out_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mode_values::call(self_meta, dim, keepdim, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::mode_values::call(self_, dim, keepdim, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mode::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> mode_out_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mode_dimname_out::call(self_meta, dim, keepdim, values_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(values) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::mode_dimname_out::call(self_, dim, keepdim, values_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mode_dimname::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(values, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(values, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(values);
  at::functionalization::impl::sync(values);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
      }
    }

    at::Tensor & multiply_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::multiply_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::multiply_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::multiply_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & multiply__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::multiply__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::multiply__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::multiply_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & narrow_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::SymInt start, c10::SymInt length, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::narrow_copy_out::call(self_meta, dim, start, length, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::narrow_copy_out::call(self_, dim, start, length, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::narrow_copy::call(self_, dim, start, length);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> batch_norm_gather_stats_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, int64_t count, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto mean_meta = to_meta(mean);
        auto invstd_meta = to_meta(invstd);
        auto running_mean_meta = to_meta(running_mean);
        auto running_var_meta = to_meta(running_var);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::batch_norm_gather_stats_out::call(input_meta, mean_meta, invstd_meta, running_mean_meta, running_var_meta, momentum, eps, count, out0_meta, out1_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor mean_;
      if (at::functionalization::impl::isFunctionalTensor(mean)) {
        at::functionalization::impl::sync(mean);
        mean_ = at::functionalization::impl::from_functional_tensor(mean);
      } else {
        mean_ = mean;
      }
      
      at::Tensor invstd_;
      if (at::functionalization::impl::isFunctionalTensor(invstd)) {
        at::functionalization::impl::sync(invstd);
        invstd_ = at::functionalization::impl::from_functional_tensor(invstd);
      } else {
        invstd_ = invstd;
      }
      
      c10::optional<at::Tensor> running_mean_;
      if (at::functionalization::impl::isFunctionalTensor(running_mean)) {
        at::functionalization::impl::sync(running_mean);
        running_mean_ = at::functionalization::impl::from_functional_tensor(running_mean);
      } else {
        running_mean_ = running_mean;
      }
      
      c10::optional<at::Tensor> running_var_;
      if (at::functionalization::impl::isFunctionalTensor(running_var)) {
        at::functionalization::impl::sync(running_var);
        running_var_ = at::functionalization::impl::from_functional_tensor(running_var);
      } else {
        running_var_ = running_var;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || mean.device().type() == c10::DeviceType::XLA || invstd.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(mean) || at::functionalization::impl::isFunctionalTensor(invstd) || at::functionalization::impl::isFunctionalTensor(running_mean) || at::functionalization::impl::isFunctionalTensor(running_var))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::batch_norm_gather_stats_out::call(input_, mean_, invstd_, running_mean_, running_var_, momentum, eps, count, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::batch_norm_gather_stats::call(input_, mean_, invstd_, running_mean_, running_var_, momentum, eps, count);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> batch_norm_gather_stats_with_counts_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, const at::Tensor & counts, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto mean_meta = to_meta(mean);
        auto invstd_meta = to_meta(invstd);
        auto running_mean_meta = to_meta(running_mean);
        auto running_var_meta = to_meta(running_var);
        auto counts_meta = to_meta(counts);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::batch_norm_gather_stats_with_counts_out::call(input_meta, mean_meta, invstd_meta, running_mean_meta, running_var_meta, momentum, eps, counts_meta, out0_meta, out1_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor mean_;
      if (at::functionalization::impl::isFunctionalTensor(mean)) {
        at::functionalization::impl::sync(mean);
        mean_ = at::functionalization::impl::from_functional_tensor(mean);
      } else {
        mean_ = mean;
      }
      
      at::Tensor invstd_;
      if (at::functionalization::impl::isFunctionalTensor(invstd)) {
        at::functionalization::impl::sync(invstd);
        invstd_ = at::functionalization::impl::from_functional_tensor(invstd);
      } else {
        invstd_ = invstd;
      }
      
      c10::optional<at::Tensor> running_mean_;
      if (at::functionalization::impl::isFunctionalTensor(running_mean)) {
        at::functionalization::impl::sync(running_mean);
        running_mean_ = at::functionalization::impl::from_functional_tensor(running_mean);
      } else {
        running_mean_ = running_mean;
      }
      
      c10::optional<at::Tensor> running_var_;
      if (at::functionalization::impl::isFunctionalTensor(running_var)) {
        at::functionalization::impl::sync(running_var);
        running_var_ = at::functionalization::impl::from_functional_tensor(running_var);
      } else {
        running_var_ = running_var;
      }
      
      at::Tensor counts_;
      if (at::functionalization::impl::isFunctionalTensor(counts)) {
        at::functionalization::impl::sync(counts);
        counts_ = at::functionalization::impl::from_functional_tensor(counts);
      } else {
        counts_ = counts;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || mean.device().type() == c10::DeviceType::XLA || invstd.device().type() == c10::DeviceType::XLA || counts.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(mean) || at::functionalization::impl::isFunctionalTensor(invstd) || at::functionalization::impl::isFunctionalTensor(running_mean) || at::functionalization::impl::isFunctionalTensor(running_var) || at::functionalization::impl::isFunctionalTensor(counts))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::batch_norm_gather_stats_with_counts_out::call(input_, mean_, invstd_, running_mean_, running_var_, momentum, eps, counts_, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::batch_norm_gather_stats_with_counts::call(input_, mean_, invstd_, running_mean_, running_var_, momentum, eps, counts_);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_batch_norm_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_out_meta = to_meta(grad_out);
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto running_mean_meta = to_meta(running_mean);
        auto running_var_meta = to_meta(running_var);
        auto save_mean_meta = to_meta(save_mean);
        auto save_invstd_meta = to_meta(save_invstd);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::native_batch_norm_backward_out::call(grad_out_meta, input_meta, weight_meta, running_mean_meta, running_var_meta, save_mean_meta, save_invstd_meta, train, eps, output_mask, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor grad_out_;
      if (at::functionalization::impl::isFunctionalTensor(grad_out)) {
        at::functionalization::impl::sync(grad_out);
        grad_out_ = at::functionalization::impl::from_functional_tensor(grad_out);
      } else {
        grad_out_ = grad_out;
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> running_mean_;
      if (at::functionalization::impl::isFunctionalTensor(running_mean)) {
        at::functionalization::impl::sync(running_mean);
        running_mean_ = at::functionalization::impl::from_functional_tensor(running_mean);
      } else {
        running_mean_ = running_mean;
      }
      
      c10::optional<at::Tensor> running_var_;
      if (at::functionalization::impl::isFunctionalTensor(running_var)) {
        at::functionalization::impl::sync(running_var);
        running_var_ = at::functionalization::impl::from_functional_tensor(running_var);
      } else {
        running_var_ = running_var;
      }
      
      c10::optional<at::Tensor> save_mean_;
      if (at::functionalization::impl::isFunctionalTensor(save_mean)) {
        at::functionalization::impl::sync(save_mean);
        save_mean_ = at::functionalization::impl::from_functional_tensor(save_mean);
      } else {
        save_mean_ = save_mean;
      }
      
      c10::optional<at::Tensor> save_invstd_;
      if (at::functionalization::impl::isFunctionalTensor(save_invstd)) {
        at::functionalization::impl::sync(save_invstd);
        save_invstd_ = at::functionalization::impl::from_functional_tensor(save_invstd);
      } else {
        save_invstd_ = save_invstd;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_out.device().type() == c10::DeviceType::XLA || input.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_out) || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(running_mean) || at::functionalization::impl::isFunctionalTensor(running_var) || at::functionalization::impl::isFunctionalTensor(save_mean) || at::functionalization::impl::isFunctionalTensor(save_invstd))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::native_batch_norm_backward_out::call(grad_out_, input_, weight_, running_mean_, running_var_, save_mean_, save_invstd_, train, eps, output_mask, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::native_batch_norm_backward::call(grad_out_, input_, weight_, running_mean_, running_var_, save_mean_, save_invstd_, train, eps, output_mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & ones_out_names_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ones_names_out::call(size, names, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ones_names_out::call(size, names, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ones_names::call(size, names, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & ones_out_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ones_out::call(size, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ones_out::call(size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ones::call(size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _pdist_forward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double p, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_pdist_forward_out::call(self_meta, p, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_pdist_forward_out::call(self_, p, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_pdist_forward::call(self_, p);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _pdist_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, double p, const at::Tensor & pdist, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_meta = to_meta(grad);
        auto self_meta = to_meta(self);
        auto pdist_meta = to_meta(pdist);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_pdist_backward_out::call(grad_meta, self_meta, p, pdist_meta, out_meta);
      }
      
      at::Tensor grad_;
      if (at::functionalization::impl::isFunctionalTensor(grad)) {
        at::functionalization::impl::sync(grad);
        grad_ = at::functionalization::impl::from_functional_tensor(grad);
      } else {
        grad_ = grad;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor pdist_;
      if (at::functionalization::impl::isFunctionalTensor(pdist)) {
        at::functionalization::impl::sync(pdist);
        pdist_ = at::functionalization::impl::from_functional_tensor(pdist);
      } else {
        pdist_ = pdist;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || pdist.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(pdist))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_pdist_backward_out::call(grad_, self_, p, pdist_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_pdist_backward::call(grad_, self_, p, pdist_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & pixel_shuffle_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t upscale_factor, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::pixel_shuffle_out::call(self_meta, upscale_factor, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::pixel_shuffle_out::call(self_, upscale_factor, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::pixel_shuffle::call(self_, upscale_factor);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _pin_memory_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Device> device, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_pin_memory_out::call(self_meta, device, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_pin_memory_out::call(self_, device, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_pin_memory::call(self_, device);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & randn_out_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::randn_out::call(size, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::randn_out::call(size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::randn::call(size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & randn_out_generator_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::randn_generator_out::call(size, generator, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::randn_generator_out::call(size, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::randn_generator::call(size, generator, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & randn_out_names_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, c10::optional<at::DimnameList> names, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::randn_names_out::call(size, names, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::randn_names_out::call(size, names, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::randn_names::call(size, names, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & randn_out_generator_with_names_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::randn_generator_with_names_out::call(size, generator, names, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::randn_generator_with_names_out::call(size, generator, names, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::randn_generator_with_names::call(size, generator, names, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & randn_like_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::randn_like_out::call(self_meta, memory_format, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::randn_like_out::call(self_, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::randn_like::call(self_, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & neg_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::neg_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::neg_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::neg::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & neg_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::neg_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::neg_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::neg::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & negative_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::negative_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::negative_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::negative::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & negative_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::negative_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::negative_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::negative::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & repeat_interleave_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & repeats, c10::optional<c10::SymInt> output_size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto repeats_meta = to_meta(repeats);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::repeat_interleave_Tensor_out::call(repeats_meta, output_size, out_meta);
      }
      
      at::Tensor repeats_;
      if (at::functionalization::impl::isFunctionalTensor(repeats)) {
        at::functionalization::impl::sync(repeats);
        repeats_ = at::functionalization::impl::from_functional_tensor(repeats);
      } else {
        repeats_ = repeats;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || repeats.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(repeats))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::repeat_interleave_Tensor_out::call(repeats_, output_size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::repeat_interleave_Tensor::call(repeats_, output_size);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & gelu_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view approximate, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::gelu_out::call(self_meta, approximate, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::gelu_out::call(self_, approximate, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::gelu::call(self_, approximate);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & gelu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::string_view approximate) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::gelu_::call(self_meta, approximate);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::gelu_::call(self_, approximate);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::gelu::call(self_, approximate);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & select_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt index, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::select_backward_out::call(grad_output_meta, input_sizes, dim, index, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::select_backward_out::call(grad_output_, input_sizes, dim, index, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::select_backward::call(grad_output_, input_sizes, dim, index);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mish_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mish_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mish_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mish::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mish_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mish_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mish_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mish::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & slice_scatter_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & src, int64_t dim, c10::optional<c10::SymInt> start, c10::optional<c10::SymInt> end, c10::SymInt step, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto src_meta = to_meta(src);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::slice_scatter_out::call(self_meta, src_meta, dim, start, end, step, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::slice_scatter_out::call(self_, src_, dim, start, end, step, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::slice_scatter::call(self_, src_, dim, start, end, step);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & diagonal_scatter_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & src, int64_t offset, int64_t dim1, int64_t dim2, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto src_meta = to_meta(src);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::diagonal_scatter_out::call(self_meta, src_meta, offset, dim1, dim2, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::diagonal_scatter_out::call(self_, src_, offset, dim1, dim2, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::diagonal_scatter::call(self_, src_, offset, dim1, dim2);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _softmax_backward_data_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto output_meta = to_meta(output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_softmax_backward_data_out::call(grad_output_meta, output_meta, dim, input_dtype, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_softmax_backward_data_out::call(grad_output_, output_, dim, input_dtype, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_softmax_backward_data::call(grad_output_, output_, dim, input_dtype);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    void unsafe_split_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt split_size, int64_t dim, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::unsafe_split_Tensor_out::call(self_meta, split_size, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::unsafe_split_Tensor_out::call(self_, split_size, dim, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::unsafe_split_Tensor::call(self_, split_size, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void unsafe_split_with_sizes_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::unsafe_split_with_sizes_out::call(self_meta, split_sizes, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::unsafe_split_with_sizes_out::call(self_, split_sizes, dim, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::unsafe_split_with_sizes::call(self_, split_sizes, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    at::Tensor & square_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::square_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::square_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::square::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & square_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::square_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::square_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::square::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & tanh_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::tanh_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::tanh_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::tanh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & tanh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::tanh_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::tanh_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::tanh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & roll_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef shifts, at::IntArrayRef dims, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::roll_out::call(self_meta, shifts, dims, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::roll_out::call(self_, shifts, dims, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::roll::call(self_, shifts, dims);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & rot90_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, at::IntArrayRef dims, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rot90_out::call(self_meta, k, dims, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rot90_out::call(self_, k, dims, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rot90::call(self_, k, dims);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _nested_tensor_storage_offsets_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_nested_tensor_storage_offsets_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_nested_tensor_storage_offsets_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_nested_tensor_storage_offsets::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _trilinear_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & i1, const at::Tensor & i2, const at::Tensor & i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto i1_meta = to_meta(i1);
        auto i2_meta = to_meta(i2);
        auto i3_meta = to_meta(i3);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_trilinear_out::call(i1_meta, i2_meta, i3_meta, expand1, expand2, expand3, sumdim, unroll_dim, out_meta);
      }
      
      at::Tensor i1_;
      if (at::functionalization::impl::isFunctionalTensor(i1)) {
        at::functionalization::impl::sync(i1);
        i1_ = at::functionalization::impl::from_functional_tensor(i1);
      } else {
        i1_ = i1;
      }
      
      at::Tensor i2_;
      if (at::functionalization::impl::isFunctionalTensor(i2)) {
        at::functionalization::impl::sync(i2);
        i2_ = at::functionalization::impl::from_functional_tensor(i2);
      } else {
        i2_ = i2;
      }
      
      at::Tensor i3_;
      if (at::functionalization::impl::isFunctionalTensor(i3)) {
        at::functionalization::impl::sync(i3);
        i3_ = at::functionalization::impl::from_functional_tensor(i3);
      } else {
        i3_ = i3;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || i1.device().type() == c10::DeviceType::XLA || i2.device().type() == c10::DeviceType::XLA || i3.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(i1) || at::functionalization::impl::isFunctionalTensor(i2) || at::functionalization::impl::isFunctionalTensor(i3))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_trilinear_out::call(i1_, i2_, i3_, expand1, expand2, expand3, sumdim, unroll_dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_trilinear::call(i1_, i2_, i3_, expand1, expand2, expand3, sumdim, unroll_dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _unique_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool sorted, bool return_inverse, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_unique_out::call(self_meta, sorted, return_inverse, out0_meta, out1_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_unique_out::call(self_, sorted, return_inverse, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_unique::call(self_, sorted, return_inverse);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _unique2_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool sorted, bool return_inverse, bool return_counts, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_unique2_out::call(self_meta, sorted, return_inverse, return_counts, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_unique2_out::call(self_, sorted, return_inverse, return_counts, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_unique2::call(self_, sorted, return_inverse, return_counts);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _weight_norm_interface_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & v, const at::Tensor & g, int64_t dim, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto v_meta = to_meta(v);
        auto g_meta = to_meta(g);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_weight_norm_interface_out::call(v_meta, g_meta, dim, out0_meta, out1_meta);
      }
      
      at::Tensor v_;
      if (at::functionalization::impl::isFunctionalTensor(v)) {
        at::functionalization::impl::sync(v);
        v_ = at::functionalization::impl::from_functional_tensor(v);
      } else {
        v_ = v;
      }
      
      at::Tensor g_;
      if (at::functionalization::impl::isFunctionalTensor(g)) {
        at::functionalization::impl::sync(g);
        g_ = at::functionalization::impl::from_functional_tensor(g);
      } else {
        g_ = g;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || v.device().type() == c10::DeviceType::XLA || g.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(v) || at::functionalization::impl::isFunctionalTensor(g))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_weight_norm_interface_out::call(v_, g_, dim, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_weight_norm_interface::call(v_, g_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & _efficientzerotensor_out_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_efficientzerotensor_out::call(size, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_efficientzerotensor_out::call(size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_efficientzerotensor::call(size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _standard_gamma_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_standard_gamma_out::call(self_meta, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_standard_gamma_out::call(self_, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_standard_gamma::call(self_, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _dirichlet_grad_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & alpha, const at::Tensor & total, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto alpha_meta = to_meta(alpha);
        auto total_meta = to_meta(total);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_dirichlet_grad_out::call(x_meta, alpha_meta, total_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor alpha_;
      if (at::functionalization::impl::isFunctionalTensor(alpha)) {
        at::functionalization::impl::sync(alpha);
        alpha_ = at::functionalization::impl::from_functional_tensor(alpha);
      } else {
        alpha_ = alpha;
      }
      
      at::Tensor total_;
      if (at::functionalization::impl::isFunctionalTensor(total)) {
        at::functionalization::impl::sync(total);
        total_ = at::functionalization::impl::from_functional_tensor(total);
      } else {
        total_ = total;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA || alpha.device().type() == c10::DeviceType::XLA || total.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(alpha) || at::functionalization::impl::isFunctionalTensor(total))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_dirichlet_grad_out::call(x_, alpha_, total_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_dirichlet_grad::call(x_, alpha_, total_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & norm_out_ScalarOpt_dtype_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::ScalarType dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::norm_ScalarOpt_dtype_out::call(self_meta, p, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::norm_ScalarOpt_dtype_out::call(self_, p, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::norm_ScalarOpt_dtype::call(self_, p, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & norm_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & p, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::norm_Scalar_out::call(self_meta, p, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::norm_Scalar_out::call(self_, p, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::norm_Scalar::call(self_, p);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & norm_out_dtype_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::norm_dtype_out::call(self_meta, p, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::norm_dtype_out::call(self_, p, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::norm_ScalarOpt_dim_dtype::call(self_, p, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & norm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::norm_out::call(self_meta, p, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::norm_out::call(self_, p, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::norm_ScalarOpt_dim::call(self_, p, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & norm_out_names_dtype_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::norm_names_dtype_out::call(self_meta, p, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::norm_names_dtype_out::call(self_, p, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::norm_names_ScalarOpt_dim_dtype::call(self_, p, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & norm_out_names_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::norm_names_out::call(self_meta, p, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::norm_names_out::call(self_, p, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::norm_names_ScalarOpt_dim::call(self_, p, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> frexp_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & mantissa, at::Tensor & exponent) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mantissa_meta = to_meta(mantissa);
        auto exponent_meta = to_meta(exponent);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::frexp_Tensor_out::call(self_meta, mantissa_meta, exponent_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mantissa_;
      if (at::functionalization::impl::isFunctionalTensor(mantissa)) {
        at::functionalization::impl::sync(mantissa);
        mantissa_ = at::functionalization::impl::from_functional_tensor(mantissa);
      } else {
        mantissa_ = mantissa;
      }
      
      at::Tensor exponent_;
      if (at::functionalization::impl::isFunctionalTensor(exponent)) {
        at::functionalization::impl::sync(exponent);
        exponent_ = at::functionalization::impl::from_functional_tensor(exponent);
      } else {
        exponent_ = exponent;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(mantissa) && at::functionalization::impl::isFunctionalTensor(exponent))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::frexp_Tensor_out::call(self_, mantissa_, exponent_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(mantissa, exponent);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::frexp_Tensor::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(mantissa, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(mantissa, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(mantissa);
  at::functionalization::impl::sync(mantissa);
  at::functionalization::impl::propagate_xla_data(exponent, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(exponent, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(exponent);
  at::functionalization::impl::sync(exponent);
    return ::std::tuple<at::Tensor &,at::Tensor &>(mantissa, exponent);
      }
    }

    at::Tensor & frobenius_norm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::frobenius_norm_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::frobenius_norm_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::frobenius_norm_dim::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & nuclear_norm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nuclear_norm_out::call(self_meta, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nuclear_norm_out::call(self_, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nuclear_norm::call(self_, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & nuclear_norm_out_dim_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nuclear_norm_dim_out::call(self_meta, dim, keepdim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nuclear_norm_dim_out::call(self_, dim, keepdim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nuclear_norm_dim::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & subtract_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::subtract_out::call(self_meta, other_meta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::subtract_out::call(self_, other_, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::subtract_Tensor::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & subtract__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::subtract__Tensor::call(self_meta, other_meta, alpha);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::subtract__Tensor::call(self_, other_, alpha);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::subtract_Tensor::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & sparse_sampled_addmm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mat1_meta = to_meta(mat1);
        auto mat2_meta = to_meta(mat2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sparse_sampled_addmm_out::call(self_meta, mat1_meta, mat2_meta, beta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mat1_;
      if (at::functionalization::impl::isFunctionalTensor(mat1)) {
        at::functionalization::impl::sync(mat1);
        mat1_ = at::functionalization::impl::from_functional_tensor(mat1);
      } else {
        mat1_ = mat1;
      }
      
      at::Tensor mat2_;
      if (at::functionalization::impl::isFunctionalTensor(mat2)) {
        at::functionalization::impl::sync(mat2);
        mat2_ = at::functionalization::impl::from_functional_tensor(mat2);
      } else {
        mat2_ = mat2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mat1.device().type() == c10::DeviceType::XLA || mat2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mat1) || at::functionalization::impl::isFunctionalTensor(mat2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sparse_sampled_addmm_out::call(self_, mat1_, mat2_, beta, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sparse_sampled_addmm::call(self_, mat1_, mat2_, beta, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _addmm_activation_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, bool use_gelu, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mat1_meta = to_meta(mat1);
        auto mat2_meta = to_meta(mat2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_addmm_activation_out::call(self_meta, mat1_meta, mat2_meta, beta, alpha, use_gelu, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mat1_;
      if (at::functionalization::impl::isFunctionalTensor(mat1)) {
        at::functionalization::impl::sync(mat1);
        mat1_ = at::functionalization::impl::from_functional_tensor(mat1);
      } else {
        mat1_ = mat1;
      }
      
      at::Tensor mat2_;
      if (at::functionalization::impl::isFunctionalTensor(mat2)) {
        at::functionalization::impl::sync(mat2);
        mat2_ = at::functionalization::impl::from_functional_tensor(mat2);
      } else {
        mat2_ = mat2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mat1.device().type() == c10::DeviceType::XLA || mat2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mat1) || at::functionalization::impl::isFunctionalTensor(mat2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_addmm_activation_out::call(self_, mat1_, mat2_, beta, alpha, use_gelu, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_addmm_activation::call(self_, mat1_, mat2_, beta, alpha, use_gelu);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _to_dense_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<bool> masked_grad, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_to_dense_out::call(self_meta, dtype, masked_grad, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_to_dense_out::call(self_, dtype, masked_grad, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_to_dense::call(self_, dtype, masked_grad);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _coalesced_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool coalesced, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_coalesced_out::call(self_meta, coalesced, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_coalesced_out::call(self_, coalesced, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_coalesced::call(self_, coalesced);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _coalesced_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, bool coalesced) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_coalesced_::call(self_meta, coalesced);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_coalesced_::call(self_, coalesced);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_coalesced::call(self_, coalesced);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & quantize_per_tensor_dynamic_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ScalarType dtype, bool reduce_range, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::quantize_per_tensor_dynamic_out::call(self_meta, dtype, reduce_range, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::quantize_per_tensor_dynamic_out::call(self_, dtype, reduce_range, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::quantize_per_tensor_dynamic::call(self_, dtype, reduce_range);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & quantize_per_tensor_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double scale, int64_t zero_point, at::ScalarType dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::quantize_per_tensor_out::call(self_meta, scale, zero_point, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::quantize_per_tensor_out::call(self_, scale, zero_point, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::quantize_per_tensor::call(self_, scale, zero_point, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & quantize_per_tensor_out_tensor_qparams_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, at::ScalarType dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto scale_meta = to_meta(scale);
        auto zero_point_meta = to_meta(zero_point);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::quantize_per_tensor_tensor_qparams_out::call(self_meta, scale_meta, zero_point_meta, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor scale_;
      if (at::functionalization::impl::isFunctionalTensor(scale)) {
        at::functionalization::impl::sync(scale);
        scale_ = at::functionalization::impl::from_functional_tensor(scale);
      } else {
        scale_ = scale;
      }
      
      at::Tensor zero_point_;
      if (at::functionalization::impl::isFunctionalTensor(zero_point)) {
        at::functionalization::impl::sync(zero_point);
        zero_point_ = at::functionalization::impl::from_functional_tensor(zero_point);
      } else {
        zero_point_ = zero_point;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || scale.device().type() == c10::DeviceType::XLA || zero_point.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(scale) || at::functionalization::impl::isFunctionalTensor(zero_point))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::quantize_per_tensor_tensor_qparams_out::call(self_, scale_, zero_point_, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::quantize_per_tensor_tensor_qparams::call(self_, scale_, zero_point_, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    void quantize_per_tensor_out_tensors_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, const at::Tensor & scales, const at::Tensor & zero_points, at::ScalarType dtype, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto scales_meta = to_meta(scales);
        auto zero_points_meta = to_meta(zero_points);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::quantize_per_tensor_tensors_out::call(tensors_meta, scales_meta, zero_points_meta, dtype, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor scales_;
      if (at::functionalization::impl::isFunctionalTensor(scales)) {
        at::functionalization::impl::sync(scales);
        scales_ = at::functionalization::impl::from_functional_tensor(scales);
      } else {
        scales_ = scales;
      }
      
      at::Tensor zero_points_;
      if (at::functionalization::impl::isFunctionalTensor(zero_points)) {
        at::functionalization::impl::sync(zero_points);
        zero_points_ = at::functionalization::impl::from_functional_tensor(zero_points);
      } else {
        zero_points_ = zero_points;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || scales.device().type() == c10::DeviceType::XLA || zero_points.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(tensors) || at::functionalization::impl::isFunctionalTensor(scales) || at::functionalization::impl::isFunctionalTensor(zero_points))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::quantize_per_tensor_tensors_out::call(tensors_, scales_, zero_points_, dtype, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::quantize_per_tensor_tensors::call(tensors_, scales_, zero_points_, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    at::Tensor & quantize_per_channel_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, at::ScalarType dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto scales_meta = to_meta(scales);
        auto zero_points_meta = to_meta(zero_points);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::quantize_per_channel_out::call(self_meta, scales_meta, zero_points_meta, axis, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor scales_;
      if (at::functionalization::impl::isFunctionalTensor(scales)) {
        at::functionalization::impl::sync(scales);
        scales_ = at::functionalization::impl::from_functional_tensor(scales);
      } else {
        scales_ = scales;
      }
      
      at::Tensor zero_points_;
      if (at::functionalization::impl::isFunctionalTensor(zero_points)) {
        at::functionalization::impl::sync(zero_points);
        zero_points_ = at::functionalization::impl::from_functional_tensor(zero_points);
      } else {
        zero_points_ = zero_points;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || scales.device().type() == c10::DeviceType::XLA || zero_points.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(scales) || at::functionalization::impl::isFunctionalTensor(zero_points))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::quantize_per_channel_out::call(self_, scales_, zero_points_, axis, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::quantize_per_channel::call(self_, scales_, zero_points_, axis, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _make_per_channel_quantized_tensor_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto scale_meta = to_meta(scale);
        auto zero_point_meta = to_meta(zero_point);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_make_per_channel_quantized_tensor_out::call(self_meta, scale_meta, zero_point_meta, axis, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor scale_;
      if (at::functionalization::impl::isFunctionalTensor(scale)) {
        at::functionalization::impl::sync(scale);
        scale_ = at::functionalization::impl::from_functional_tensor(scale);
      } else {
        scale_ = scale;
      }
      
      at::Tensor zero_point_;
      if (at::functionalization::impl::isFunctionalTensor(zero_point)) {
        at::functionalization::impl::sync(zero_point);
        zero_point_ = at::functionalization::impl::from_functional_tensor(zero_point);
      } else {
        zero_point_ = zero_point;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || scale.device().type() == c10::DeviceType::XLA || zero_point.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(scale) || at::functionalization::impl::isFunctionalTensor(zero_point))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_make_per_channel_quantized_tensor_out::call(self_, scale_, zero_point_, axis, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_make_per_channel_quantized_tensor::call(self_, scale_, zero_point_, axis);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> fake_quantize_per_tensor_affine_cachemask_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fake_quantize_per_tensor_affine_cachemask_out::call(self_meta, scale, zero_point, quant_min, quant_max, out0_meta, out1_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::fake_quantize_per_tensor_affine_cachemask_out::call(self_, scale, zero_point, quant_min, quant_max, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fake_quantize_per_tensor_affine_cachemask::call(self_, scale, zero_point, quant_min, quant_max);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _fake_quantize_per_tensor_affine_cachemask_tensor_qparams_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, const at::Tensor & fake_quant_enabled, int64_t quant_min, int64_t quant_max, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto scale_meta = to_meta(scale);
        auto zero_point_meta = to_meta(zero_point);
        auto fake_quant_enabled_meta = to_meta(fake_quant_enabled);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams_out::call(self_meta, scale_meta, zero_point_meta, fake_quant_enabled_meta, quant_min, quant_max, out0_meta, out1_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor scale_;
      if (at::functionalization::impl::isFunctionalTensor(scale)) {
        at::functionalization::impl::sync(scale);
        scale_ = at::functionalization::impl::from_functional_tensor(scale);
      } else {
        scale_ = scale;
      }
      
      at::Tensor zero_point_;
      if (at::functionalization::impl::isFunctionalTensor(zero_point)) {
        at::functionalization::impl::sync(zero_point);
        zero_point_ = at::functionalization::impl::from_functional_tensor(zero_point);
      } else {
        zero_point_ = zero_point;
      }
      
      at::Tensor fake_quant_enabled_;
      if (at::functionalization::impl::isFunctionalTensor(fake_quant_enabled)) {
        at::functionalization::impl::sync(fake_quant_enabled);
        fake_quant_enabled_ = at::functionalization::impl::from_functional_tensor(fake_quant_enabled);
      } else {
        fake_quant_enabled_ = fake_quant_enabled;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || scale.device().type() == c10::DeviceType::XLA || zero_point.device().type() == c10::DeviceType::XLA || fake_quant_enabled.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(scale) || at::functionalization::impl::isFunctionalTensor(zero_point) || at::functionalization::impl::isFunctionalTensor(fake_quant_enabled))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams_out::call(self_, scale_, zero_point_, fake_quant_enabled_, quant_min, quant_max, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams::call(self_, scale_, zero_point_, fake_quant_enabled_, quant_min, quant_max);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & _fake_quantize_learnable_per_tensor_affine_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max, double grad_factor, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto scale_meta = to_meta(scale);
        auto zero_point_meta = to_meta(zero_point);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fake_quantize_learnable_per_tensor_affine_out::call(self_meta, scale_meta, zero_point_meta, quant_min, quant_max, grad_factor, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor scale_;
      if (at::functionalization::impl::isFunctionalTensor(scale)) {
        at::functionalization::impl::sync(scale);
        scale_ = at::functionalization::impl::from_functional_tensor(scale);
      } else {
        scale_ = scale;
      }
      
      at::Tensor zero_point_;
      if (at::functionalization::impl::isFunctionalTensor(zero_point)) {
        at::functionalization::impl::sync(zero_point);
        zero_point_ = at::functionalization::impl::from_functional_tensor(zero_point);
      } else {
        zero_point_ = zero_point;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || scale.device().type() == c10::DeviceType::XLA || zero_point.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(scale) || at::functionalization::impl::isFunctionalTensor(zero_point))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_fake_quantize_learnable_per_tensor_affine_out::call(self_, scale_, zero_point_, quant_min, quant_max, grad_factor, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fake_quantize_learnable_per_tensor_affine::call(self_, scale_, zero_point_, quant_min, quant_max, grad_factor);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> fake_quantize_per_channel_affine_cachemask_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto scale_meta = to_meta(scale);
        auto zero_point_meta = to_meta(zero_point);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fake_quantize_per_channel_affine_cachemask_out::call(self_meta, scale_meta, zero_point_meta, axis, quant_min, quant_max, out0_meta, out1_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor scale_;
      if (at::functionalization::impl::isFunctionalTensor(scale)) {
        at::functionalization::impl::sync(scale);
        scale_ = at::functionalization::impl::from_functional_tensor(scale);
      } else {
        scale_ = scale;
      }
      
      at::Tensor zero_point_;
      if (at::functionalization::impl::isFunctionalTensor(zero_point)) {
        at::functionalization::impl::sync(zero_point);
        zero_point_ = at::functionalization::impl::from_functional_tensor(zero_point);
      } else {
        zero_point_ = zero_point;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || scale.device().type() == c10::DeviceType::XLA || zero_point.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(scale) || at::functionalization::impl::isFunctionalTensor(zero_point))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::fake_quantize_per_channel_affine_cachemask_out::call(self_, scale_, zero_point_, axis, quant_min, quant_max, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fake_quantize_per_channel_affine_cachemask::call(self_, scale_, zero_point_, axis, quant_min, quant_max);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    void lstm_mps_backward_out_out(c10::DispatchKeySet dispatchKeySet, const c10::optional<at::Tensor> & grad_y, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & z_state, const at::Tensor & cell_state_fwd, const at::Tensor & input, const at::Tensor & layersOutputs, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first, at::Tensor & out0, at::TensorList out1, at::TensorList out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_y_meta = to_meta(grad_y);
        auto grad_hy_meta = to_meta(grad_hy);
        auto grad_cy_meta = to_meta(grad_cy);
        auto z_state_meta = to_meta(z_state);
        auto cell_state_fwd_meta = to_meta(cell_state_fwd);
        auto input_meta = to_meta(input);
        auto layersOutputs_meta = to_meta(layersOutputs);
        auto hx_meta = to_meta(hx);
        auto params_meta = to_meta(params);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lstm_mps_backward_out::call(grad_y_meta, grad_hy_meta, grad_cy_meta, z_state_meta, cell_state_fwd_meta, input_meta, layersOutputs_meta, hx_meta, params_meta, has_biases, num_layers, dropout, train, bidirectional, batch_first, out0_meta, out1_meta, out2_meta);
      }
      
      c10::optional<at::Tensor> grad_y_;
      if (at::functionalization::impl::isFunctionalTensor(grad_y)) {
        at::functionalization::impl::sync(grad_y);
        grad_y_ = at::functionalization::impl::from_functional_tensor(grad_y);
      } else {
        grad_y_ = grad_y;
      }
      
      c10::optional<at::Tensor> grad_hy_;
      if (at::functionalization::impl::isFunctionalTensor(grad_hy)) {
        at::functionalization::impl::sync(grad_hy);
        grad_hy_ = at::functionalization::impl::from_functional_tensor(grad_hy);
      } else {
        grad_hy_ = grad_hy;
      }
      
      c10::optional<at::Tensor> grad_cy_;
      if (at::functionalization::impl::isFunctionalTensor(grad_cy)) {
        at::functionalization::impl::sync(grad_cy);
        grad_cy_ = at::functionalization::impl::from_functional_tensor(grad_cy);
      } else {
        grad_cy_ = grad_cy;
      }
      
      at::Tensor z_state_;
      if (at::functionalization::impl::isFunctionalTensor(z_state)) {
        at::functionalization::impl::sync(z_state);
        z_state_ = at::functionalization::impl::from_functional_tensor(z_state);
      } else {
        z_state_ = z_state;
      }
      
      at::Tensor cell_state_fwd_;
      if (at::functionalization::impl::isFunctionalTensor(cell_state_fwd)) {
        at::functionalization::impl::sync(cell_state_fwd);
        cell_state_fwd_ = at::functionalization::impl::from_functional_tensor(cell_state_fwd);
      } else {
        cell_state_fwd_ = cell_state_fwd;
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor layersOutputs_;
      if (at::functionalization::impl::isFunctionalTensor(layersOutputs)) {
        at::functionalization::impl::sync(layersOutputs);
        layersOutputs_ = at::functionalization::impl::from_functional_tensor(layersOutputs);
      } else {
        layersOutputs_ = layersOutputs;
      }
      
      ::std::vector<at::Tensor> hx_;
      if (at::functionalization::impl::isFunctionalTensor(hx)) {
        at::functionalization::impl::sync(hx);
        hx_ = at::functionalization::impl::from_functional_tensor(hx);
      } else {
        hx_ = hx.vec();
      }
      
      ::std::vector<at::Tensor> params_;
      if (at::functionalization::impl::isFunctionalTensor(params)) {
        at::functionalization::impl::sync(params);
        params_ = at::functionalization::impl::from_functional_tensor(params);
      } else {
        params_ = params.vec();
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      ::std::vector<at::Tensor> out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1.vec();
      }
      
      ::std::vector<at::Tensor> out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || z_state.device().type() == c10::DeviceType::XLA || cell_state_fwd.device().type() == c10::DeviceType::XLA || input.device().type() == c10::DeviceType::XLA || layersOutputs.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_y) || at::functionalization::impl::isFunctionalTensor(grad_hy) || at::functionalization::impl::isFunctionalTensor(grad_cy) || at::functionalization::impl::isFunctionalTensor(z_state) || at::functionalization::impl::isFunctionalTensor(cell_state_fwd) || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(layersOutputs) || at::functionalization::impl::isFunctionalTensor(hx) || at::functionalization::impl::isFunctionalTensor(params))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::lstm_mps_backward_out::call(grad_y_, grad_hy_, grad_cy_, z_state_, cell_state_fwd_, input_, layersOutputs_, hx_, params_, has_biases, num_layers, dropout, train, bidirectional, batch_first, out0_, out1_, out2_);
         ;
        }
      } else {
        ::std::tuple<at::Tensor,::std::vector<at::Tensor>,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lstm_mps_backward::call(grad_y_, grad_hy_, grad_cy_, z_state_, cell_state_fwd_, input_, layersOutputs_, hx_, params_, has_biases, num_layers, dropout, train, bidirectional, batch_first);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    
      }
    }

    at::Tensor & lift_fresh_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lift_fresh_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lift_fresh_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lift_fresh_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _masked_softmax_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & mask, c10::optional<int64_t> dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto output_meta = to_meta(output);
        auto mask_meta = to_meta(mask);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_masked_softmax_backward_out::call(grad_output_meta, output_meta, mask_meta, dim, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || output.device().type() == c10::DeviceType::XLA || mask.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(output) || at::functionalization::impl::isFunctionalTensor(mask))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_masked_softmax_backward_out::call(grad_output_, output_, mask_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_masked_softmax_backward::call(grad_output_, output_, mask_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & put_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & index, const at::Tensor & source, bool accumulate, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto source_meta = to_meta(source);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::put_out::call(self_meta, index_meta, source_meta, accumulate, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor source_;
      if (at::functionalization::impl::isFunctionalTensor(source)) {
        at::functionalization::impl::sync(source);
        source_ = at::functionalization::impl::from_functional_tensor(source);
      } else {
        source_ = source;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA || source.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(source))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::put_out::call(self_, index_, source_, accumulate, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::put::call(self_, index_, source_, accumulate);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & put_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & index, const at::Tensor & source, bool accumulate) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto source_meta = to_meta(source);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::put_::call(self_meta, index_meta, source_meta, accumulate);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor source_;
      if (at::functionalization::impl::isFunctionalTensor(source)) {
        at::functionalization::impl::sync(source);
        source_ = at::functionalization::impl::from_functional_tensor(source);
      } else {
        source_ = source;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || index.device().type() == c10::DeviceType::XLA || source.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(source))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::put_::call(self_, index_, source_, accumulate);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::put::call(self_, index_, source_, accumulate);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & index_add_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto source_meta = to_meta(source);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_add_out::call(self_meta, dim, index_meta, source_meta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor source_;
      if (at::functionalization::impl::isFunctionalTensor(source)) {
        at::functionalization::impl::sync(source);
        source_ = at::functionalization::impl::from_functional_tensor(source);
      } else {
        source_ = source;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA || source.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(source))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_add_out::call(self_, dim, index_, source_, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_add::call(self_, dim, index_, source_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & index_add_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto source_meta = to_meta(source);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_add_::call(self_meta, dim, index_meta, source_meta, alpha);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor source_;
      if (at::functionalization::impl::isFunctionalTensor(source)) {
        at::functionalization::impl::sync(source);
        source_ = at::functionalization::impl::from_functional_tensor(source);
      } else {
        source_ = source;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || index.device().type() == c10::DeviceType::XLA || source.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(source))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_add_::call(self_, dim, index_, source_, alpha);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_add::call(self_, dim, index_, source_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & index_fill_out_int_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_fill_int_Scalar_out::call(self_meta, dim, index_meta, value, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_fill_int_Scalar_out::call(self_, dim, index_, value, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_fill_int_Scalar::call(self_, dim, index_, value);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & index_fill__int_Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_fill__int_Scalar::call(self_meta, dim, index_meta, value);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || index.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(index))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_fill__int_Scalar::call(self_, dim, index_, value);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_fill_int_Scalar::call(self_, dim, index_, value);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & index_fill_out_int_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto value_meta = to_meta(value);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_fill_int_Tensor_out::call(self_meta, dim, index_meta, value_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor value_;
      if (at::functionalization::impl::isFunctionalTensor(value)) {
        at::functionalization::impl::sync(value);
        value_ = at::functionalization::impl::from_functional_tensor(value);
      } else {
        value_ = value;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA || value.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(value))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_fill_int_Tensor_out::call(self_, dim, index_, value_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_fill_int_Tensor::call(self_, dim, index_, value_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & index_fill__int_Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto value_meta = to_meta(value);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::index_fill__int_Tensor::call(self_meta, dim, index_meta, value_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor value_;
      if (at::functionalization::impl::isFunctionalTensor(value)) {
        at::functionalization::impl::sync(value);
        value_ = at::functionalization::impl::from_functional_tensor(value);
      } else {
        value_ = value;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || index.device().type() == c10::DeviceType::XLA || value.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(value))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::index_fill__int_Tensor::call(self_, dim, index_, value_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::index_fill_int_Tensor::call(self_, dim, index_, value_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & scatter_out_src_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto src_meta = to_meta(src);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::scatter_src_out::call(self_meta, dim, index_meta, src_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::scatter_src_out::call(self_, dim, index_, src_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::scatter_src::call(self_, dim, index_, src_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & scatter__src(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto src_meta = to_meta(src);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::scatter__src::call(self_meta, dim, index_meta, src_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || index.device().type() == c10::DeviceType::XLA || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::scatter__src::call(self_, dim, index_, src_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::scatter_src::call(self_, dim, index_, src_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & scatter_out_value_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::scatter_value_out::call(self_meta, dim, index_meta, value, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::scatter_value_out::call(self_, dim, index_, value, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::scatter_value::call(self_, dim, index_, value);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & scatter__value(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::scatter__value::call(self_meta, dim, index_meta, value);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || index.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(index))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::scatter__value::call(self_, dim, index_, value);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::scatter_value::call(self_, dim, index_, value);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & scatter_out_reduce_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto src_meta = to_meta(src);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::scatter_reduce_out::call(self_meta, dim, index_meta, src_meta, reduce, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::scatter_reduce_out::call(self_, dim, index_, src_, reduce, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::scatter_reduce::call(self_, dim, index_, src_, reduce);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & scatter__reduce(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto src_meta = to_meta(src);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::scatter__reduce::call(self_meta, dim, index_meta, src_meta, reduce);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || index.device().type() == c10::DeviceType::XLA || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::scatter__reduce::call(self_, dim, index_, src_, reduce);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::scatter_reduce::call(self_, dim, index_, src_, reduce);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & scatter_out_value_reduce_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::scatter_value_reduce_out::call(self_meta, dim, index_meta, value, reduce, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::scatter_value_reduce_out::call(self_, dim, index_, value, reduce, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::scatter_value_reduce::call(self_, dim, index_, value, reduce);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & scatter__value_reduce(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::scatter__value_reduce::call(self_meta, dim, index_meta, value, reduce);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || index.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(index))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::scatter__value_reduce::call(self_, dim, index_, value, reduce);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::scatter_value_reduce::call(self_, dim, index_, value, reduce);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & scatter_add_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto src_meta = to_meta(src);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::scatter_add_out::call(self_meta, dim, index_meta, src_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::scatter_add_out::call(self_, dim, index_, src_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::scatter_add::call(self_, dim, index_, src_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & scatter_add_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto src_meta = to_meta(src);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::scatter_add_::call(self_meta, dim, index_meta, src_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || index.device().type() == c10::DeviceType::XLA || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(index) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::scatter_add_::call(self_, dim, index_, src_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::scatter_add::call(self_, dim, index_, src_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & __lshift___out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::__lshift___Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::__lshift___Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::__lshift___Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & __ilshift___Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::__ilshift___Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::__ilshift___Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::__lshift___Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & __lshift___out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::__lshift___Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::__lshift___Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::__lshift___Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & __ilshift___Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::__ilshift___Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::__ilshift___Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::__lshift___Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & greater_equal_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::greater_equal_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::greater_equal_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::greater_equal_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & greater_equal__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::greater_equal__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::greater_equal__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::greater_equal_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & greater_equal_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::greater_equal_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::greater_equal_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::greater_equal_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & greater_equal__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::greater_equal__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::greater_equal__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::greater_equal_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & less_equal_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::less_equal_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::less_equal_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::less_equal_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & less_equal__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::less_equal__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::less_equal__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::less_equal_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & less_equal_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::less_equal_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::less_equal_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::less_equal_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & less_equal__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::less_equal__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::less_equal__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::less_equal_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & greater_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::greater_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::greater_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::greater_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & greater__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::greater__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::greater__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::greater_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & greater_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::greater_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::greater_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::greater_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & greater__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::greater__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::greater__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::greater_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & lt_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lt_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lt_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lt_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & lt__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lt__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lt__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lt_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & lt_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lt_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lt_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lt_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & lt__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lt__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::lt__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lt_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & less_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::less_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::less_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::less_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & less__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::less__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::less__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::less_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & less_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::less_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::less_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::less_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & less__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::less__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::less__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::less_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & take_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & index, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::take_out::call(self_meta, index_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::take_out::call(self_, index_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::take::call(self_, index_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & nonzero_static_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t size, int64_t fill_value, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nonzero_static_out::call(self_meta, size, fill_value, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nonzero_static_out::call(self_, size, fill_value, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nonzero_static::call(self_, size, fill_value);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & gather_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::gather_out::call(self_meta, dim, index_meta, sparse_grad, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::gather_out::call(self_, dim, index_, sparse_grad, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::gather::call(self_, dim, index_, sparse_grad);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & gather_out_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto index_meta = to_meta(index);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::gather_dimname_out::call(self_meta, dim, index_meta, sparse_grad, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor index_;
      if (at::functionalization::impl::isFunctionalTensor(index)) {
        at::functionalization::impl::sync(index);
        index_ = at::functionalization::impl::from_functional_tensor(index);
      } else {
        index_ = index;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || index.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(index))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::gather_dimname_out::call(self_, dim, index_, sparse_grad, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::gather_dimname::call(self_, dim, index_, sparse_grad);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cholesky_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cholesky_out::call(self_meta, upper, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cholesky_out::call(self_, upper, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cholesky::call(self_, upper);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _cholesky_solve_helper_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A, bool upper, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto A_meta = to_meta(A);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_cholesky_solve_helper_out::call(self_meta, A_meta, upper, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_cholesky_solve_helper_out::call(self_, A_, upper, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_cholesky_solve_helper::call(self_, A_, upper);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & polygamma_out_out(c10::DispatchKeySet dispatchKeySet, int64_t n, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::polygamma_out::call(n, self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::polygamma_out::call(n, self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::polygamma::call(n, self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & igamma_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::igamma_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::igamma_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::igamma::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & igamma_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::igamma_::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::igamma_::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::igamma::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & fmin_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fmin_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fmin_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fmin::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & max_out_unary_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::max_unary_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::max_unary_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::max::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fmax_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fmax_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fmax_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fmax::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & maximum_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::maximum_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::maximum_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::maximum::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & max_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::max_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::max_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::max_other::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    void _amp_foreach_non_finite_check_and_unscale_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::Tensor & found_inf, const at::Tensor & inv_scale, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto found_inf_meta = to_meta(found_inf);
        auto inv_scale_meta = to_meta(inv_scale);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_amp_foreach_non_finite_check_and_unscale_out::call(self_meta, found_inf_meta, inv_scale_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      at::Tensor found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      
      at::Tensor inv_scale_;
      if (at::functionalization::impl::isFunctionalTensor(inv_scale)) {
        at::functionalization::impl::sync(inv_scale);
        inv_scale_ = at::functionalization::impl::from_functional_tensor(inv_scale);
      } else {
        inv_scale_ = inv_scale;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(found_inf) && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || inv_scale.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(inv_scale))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_amp_foreach_non_finite_check_and_unscale_out::call(self_, found_inf_, inv_scale_, out_);
         ;
        }
      } else {
        ::std::tuple<::std::vector<at::Tensor>,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_amp_foreach_non_finite_check_and_unscale::call(self_, found_inf_, inv_scale_);
        }
          at::functionalization::impl::propagate_xla_data(found_inf, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(found_inf, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(found_inf);
  at::functionalization::impl::sync(found_inf);
  at::functionalization::impl::propagate_xla_data(out, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _amp_foreach_non_finite_check_and_unscale_(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::Tensor & found_inf, const at::Tensor & inv_scale) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto found_inf_meta = to_meta(found_inf);
        auto inv_scale_meta = to_meta(inv_scale);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_amp_foreach_non_finite_check_and_unscale_::call(self_meta, found_inf_meta, inv_scale_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      at::Tensor found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      
      at::Tensor inv_scale_;
      if (at::functionalization::impl::isFunctionalTensor(inv_scale)) {
        at::functionalization::impl::sync(inv_scale);
        inv_scale_ = at::functionalization::impl::from_functional_tensor(inv_scale);
      } else {
        inv_scale_ = inv_scale;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self) && at::functionalization::impl::isFunctionalTensor(found_inf))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || inv_scale.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(inv_scale))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_amp_foreach_non_finite_check_and_unscale_::call(self_, found_inf_, inv_scale_);
         ;
        }
      } else {
        ::std::tuple<::std::vector<at::Tensor>,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_amp_foreach_non_finite_check_and_unscale::call(self_, found_inf_, inv_scale_);
        }
          at::functionalization::impl::propagate_xla_data(self, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(self, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
  at::functionalization::impl::propagate_xla_data(found_inf, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(found_inf, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(found_inf);
  at::functionalization::impl::sync(found_inf);
    
      }
    }

    void _foreach_sub_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sub_Scalar_out::call(self_meta, scalar, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sub_Scalar_out::call(self_, scalar, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sub_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_sub__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sub__Scalar::call(self_meta, scalar);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sub__Scalar::call(self_, scalar);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sub_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_sub_out_List_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, const at::Scalar & alpha, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sub_List_out::call(self_meta, other_meta, alpha, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sub_List_out::call(self_, other_, alpha, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sub_List::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_sub__List(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sub__List::call(self_meta, other_meta, alpha);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sub__List::call(self_, other_, alpha);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sub_List::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_sub_out_ScalarList_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sub_ScalarList_out::call(self_meta, scalars, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sub_ScalarList_out::call(self_, scalars, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sub_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_sub__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sub__ScalarList::call(self_meta, scalars);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sub__ScalarList::call(self_, scalars);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sub_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_maximum_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_maximum_Scalar_out::call(self_meta, scalar, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_maximum_Scalar_out::call(self_, scalar, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_maximum_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_maximum__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_maximum__Scalar::call(self_meta, scalar);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_maximum__Scalar::call(self_, scalar);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_maximum_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_maximum_out_List_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_maximum_List_out::call(self_meta, other_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_maximum_List_out::call(self_, other_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_maximum_List::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_maximum__List(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_maximum__List::call(self_meta, other_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_maximum__List::call(self_, other_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_maximum_List::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_maximum_out_ScalarList_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_maximum_ScalarList_out::call(self_meta, scalars, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_maximum_ScalarList_out::call(self_, scalars, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_maximum_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_maximum__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_maximum__ScalarList::call(self_meta, scalars);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_maximum__ScalarList::call(self_, scalars);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_maximum_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_abs_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_abs_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_abs_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_abs::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_abs_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_abs_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_abs_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_abs::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_acos_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_acos_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_acos_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_acos::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_acos_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_acos_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_acos_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_acos::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_cos_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_cos_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_cos_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_cos::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_cos_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_cos_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_cos_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_cos::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_exp_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_exp_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_exp_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_exp::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_exp_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_exp_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_exp_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_exp::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_floor_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_floor_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_floor_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_floor::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_floor_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_floor_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_floor_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_floor::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_log10_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_log10_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_log10_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_log10::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_log10_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_log10_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_log10_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_log10::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_neg_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_neg_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_neg_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_neg::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_neg_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_neg_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_neg_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_neg::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_norm_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & ord, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_norm_Scalar_out::call(self_meta, ord, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_norm_Scalar_out::call(self_, ord, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_norm_Scalar::call(self_, ord);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_sigmoid_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sigmoid_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sigmoid_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sigmoid::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_sigmoid_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sigmoid_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sigmoid_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sigmoid::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_sign_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sign_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sign_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sign::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_sign_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sign_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sign_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sign::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_sqrt_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sqrt_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sqrt_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sqrt::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_sqrt_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_sqrt_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_sqrt_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_sqrt::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_tan_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_tan_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_tan_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_tan::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_tan_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_tan_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_tan_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_tan::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    at::Tensor & searchsorted_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & sorted_sequence, const at::Tensor & self, bool out_int32, bool right, c10::optional<c10::string_view> side, const c10::optional<at::Tensor> & sorter, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto sorted_sequence_meta = to_meta(sorted_sequence);
        auto self_meta = to_meta(self);
        auto sorter_meta = to_meta(sorter);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::searchsorted_Tensor_out::call(sorted_sequence_meta, self_meta, out_int32, right, side, sorter_meta, out_meta);
      }
      
      at::Tensor sorted_sequence_;
      if (at::functionalization::impl::isFunctionalTensor(sorted_sequence)) {
        at::functionalization::impl::sync(sorted_sequence);
        sorted_sequence_ = at::functionalization::impl::from_functional_tensor(sorted_sequence);
      } else {
        sorted_sequence_ = sorted_sequence;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::optional<at::Tensor> sorter_;
      if (at::functionalization::impl::isFunctionalTensor(sorter)) {
        at::functionalization::impl::sync(sorter);
        sorter_ = at::functionalization::impl::from_functional_tensor(sorter);
      } else {
        sorter_ = sorter;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || sorted_sequence.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(sorted_sequence) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(sorter))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::searchsorted_Tensor_out::call(sorted_sequence_, self_, out_int32, right, side, sorter_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::searchsorted_Tensor::call(sorted_sequence_, self_, out_int32, right, side, sorter_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & searchsorted_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & sorted_sequence, const at::Scalar & self, bool out_int32, bool right, c10::optional<c10::string_view> side, const c10::optional<at::Tensor> & sorter, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto sorted_sequence_meta = to_meta(sorted_sequence);
        auto sorter_meta = to_meta(sorter);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::searchsorted_Scalar_out::call(sorted_sequence_meta, self, out_int32, right, side, sorter_meta, out_meta);
      }
      
      at::Tensor sorted_sequence_;
      if (at::functionalization::impl::isFunctionalTensor(sorted_sequence)) {
        at::functionalization::impl::sync(sorted_sequence);
        sorted_sequence_ = at::functionalization::impl::from_functional_tensor(sorted_sequence);
      } else {
        sorted_sequence_ = sorted_sequence;
      }
      
      c10::optional<at::Tensor> sorter_;
      if (at::functionalization::impl::isFunctionalTensor(sorter)) {
        at::functionalization::impl::sync(sorter);
        sorter_ = at::functionalization::impl::from_functional_tensor(sorter);
      } else {
        sorter_ = sorter;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || sorted_sequence.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(sorted_sequence) || at::functionalization::impl::isFunctionalTensor(sorter))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::searchsorted_Scalar_out::call(sorted_sequence_, self, out_int32, right, side, sorter_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::searchsorted_Scalar::call(sorted_sequence_, self, out_int32, right, side, sorter_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mse_loss_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mse_loss_backward_grad_input::call(grad_output_meta, self_meta, target_meta, reduction, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mse_loss_backward_grad_input::call(grad_output_, self_, target_, reduction, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mse_loss_backward::call(grad_output_, self_, target_, reduction);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & smooth_l1_loss_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::smooth_l1_loss_backward_grad_input::call(grad_output_meta, self_meta, target_meta, reduction, beta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::smooth_l1_loss_backward_grad_input::call(grad_output_, self_, target_, reduction, beta, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::smooth_l1_loss_backward::call(grad_output_, self_, target_, reduction, beta);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & huber_loss_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::huber_loss_backward_out::call(grad_output_meta, self_meta, target_meta, reduction, delta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::huber_loss_backward_out::call(grad_output_, self_, target_, reduction, delta, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::huber_loss_backward::call(grad_output_, self_, target_, reduction, delta);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & elu_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_or_result_meta = to_meta(self_or_result);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::elu_backward_grad_input::call(grad_output_meta, alpha, scale, input_scale, is_result, self_or_result_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_or_result_;
      if (at::functionalization::impl::isFunctionalTensor(self_or_result)) {
        at::functionalization::impl::sync(self_or_result);
        self_or_result_ = at::functionalization::impl::from_functional_tensor(self_or_result);
      } else {
        self_or_result_ = self_or_result;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self_or_result.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self_or_result))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::elu_backward_grad_input::call(grad_output_, alpha, scale, input_scale, is_result, self_or_result_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::elu_backward::call(grad_output_, alpha, scale, input_scale, is_result, self_or_result_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & glu_jvp_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & glu, const at::Tensor & x, const at::Tensor & dx, int64_t dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto glu_meta = to_meta(glu);
        auto x_meta = to_meta(x);
        auto dx_meta = to_meta(dx);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::glu_jvp_out::call(glu_meta, x_meta, dx_meta, dim, out_meta);
      }
      
      at::Tensor glu_;
      if (at::functionalization::impl::isFunctionalTensor(glu)) {
        at::functionalization::impl::sync(glu);
        glu_ = at::functionalization::impl::from_functional_tensor(glu);
      } else {
        glu_ = glu;
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor dx_;
      if (at::functionalization::impl::isFunctionalTensor(dx)) {
        at::functionalization::impl::sync(dx);
        dx_ = at::functionalization::impl::from_functional_tensor(dx);
      } else {
        dx_ = dx;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || glu.device().type() == c10::DeviceType::XLA || x.device().type() == c10::DeviceType::XLA || dx.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(glu) || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(dx))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::glu_jvp_out::call(glu_, x_, dx_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::glu_jvp::call(glu_, x_, dx_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hardsigmoid_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hardsigmoid_backward_grad_input::call(grad_output_meta, self_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hardsigmoid_backward_grad_input::call(grad_output_, self_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hardsigmoid_backward::call(grad_output_, self_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & log_sigmoid_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::log_sigmoid_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::log_sigmoid_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::log_sigmoid::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> log_sigmoid_forward_out_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & output, at::Tensor & buffer) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto output_meta = to_meta(output);
        auto buffer_meta = to_meta(buffer);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::log_sigmoid_forward_output::call(self_meta, output_meta, buffer_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor buffer_;
      if (at::functionalization::impl::isFunctionalTensor(buffer)) {
        at::functionalization::impl::sync(buffer);
        buffer_ = at::functionalization::impl::from_functional_tensor(buffer);
      } else {
        buffer_ = buffer;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(output) && at::functionalization::impl::isFunctionalTensor(buffer))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::log_sigmoid_forward_output::call(self_, output_, buffer_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(output, buffer);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::log_sigmoid_forward::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(output, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(output, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(output);
  at::functionalization::impl::sync(output);
  at::functionalization::impl::propagate_xla_data(buffer, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(buffer, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(buffer);
  at::functionalization::impl::sync(buffer);
    return ::std::tuple<at::Tensor &,at::Tensor &>(output, buffer);
      }
    }

    at::Tensor & rrelu_with_noise_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto noise_meta = to_meta(noise);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rrelu_with_noise_out::call(self_meta, noise_meta, lower, upper, training, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor noise_;
      if (at::functionalization::impl::isFunctionalTensor(noise)) {
        at::functionalization::impl::sync(noise);
        noise_ = at::functionalization::impl::from_functional_tensor(noise);
      } else {
        noise_ = noise;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || noise.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(noise))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rrelu_with_noise_out::call(self_, noise_, lower, upper, training, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rrelu_with_noise::call(self_, noise_, lower, upper, training, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & rrelu_with_noise_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto noise_meta = to_meta(noise);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rrelu_with_noise_::call(self_meta, noise_meta, lower, upper, training, generator);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor noise_;
      if (at::functionalization::impl::isFunctionalTensor(noise)) {
        at::functionalization::impl::sync(noise);
        noise_ = at::functionalization::impl::from_functional_tensor(noise);
      } else {
        noise_ = noise;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || noise.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(noise))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rrelu_with_noise_::call(self_, noise_, lower, upper, training, generator);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rrelu_with_noise::call(self_, noise_, lower, upper, training, generator);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & rrelu_with_noise_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, bool self_is_result, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto noise_meta = to_meta(noise);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rrelu_with_noise_backward_out::call(grad_output_meta, self_meta, noise_meta, lower, upper, training, self_is_result, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor noise_;
      if (at::functionalization::impl::isFunctionalTensor(noise)) {
        at::functionalization::impl::sync(noise);
        noise_ = at::functionalization::impl::from_functional_tensor(noise);
      } else {
        noise_ = noise;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || noise.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(noise))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rrelu_with_noise_backward_out::call(grad_output_, self_, noise_, lower, upper, training, self_is_result, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rrelu_with_noise_backward::call(grad_output_, self_, noise_, lower, upper, training, self_is_result);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & softplus_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::softplus_backward_grad_input::call(grad_output_meta, self_meta, beta, threshold, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::softplus_backward_grad_input::call(grad_output_, self_, beta, threshold, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::softplus_backward::call(grad_output_, self_, beta, threshold);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & mkldnn_adaptive_avg_pool2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_adaptive_avg_pool2d_out::call(self_meta, output_size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mkldnn_adaptive_avg_pool2d_out::call(self_, output_size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_adaptive_avg_pool2d::call(self_, output_size);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> adaptive_max_pool2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::adaptive_max_pool2d_out::call(self_meta, output_size, out_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::adaptive_max_pool2d_out::call(self_, output_size, out_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::adaptive_max_pool2d::call(self_, output_size);
        }
          at::functionalization::impl::propagate_xla_data(out, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out, indices);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> adaptive_max_pool3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::adaptive_max_pool3d_out::call(self_meta, output_size, out_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::adaptive_max_pool3d_out::call(self_, output_size, out_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::adaptive_max_pool3d::call(self_, output_size);
        }
          at::functionalization::impl::propagate_xla_data(out, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out, indices);
      }
    }

    at::Tensor & avg_pool2d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::avg_pool2d_backward_grad_input::call(grad_output_meta, self_meta, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::avg_pool2d_backward_grad_input::call(grad_output_, self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::avg_pool2d_backward::call(grad_output_, self_, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> max_pool2d_with_indices_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::max_pool2d_with_indices_out::call(self_meta, kernel_size, stride, padding, dilation, ceil_mode, out_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::max_pool2d_with_indices_out::call(self_, kernel_size, stride, padding, dilation, ceil_mode, out_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::max_pool2d_with_indices::call(self_, kernel_size, stride, padding, dilation, ceil_mode);
        }
          at::functionalization::impl::propagate_xla_data(out, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out, indices);
      }
    }

    at::Tensor & upsample_linear1d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::upsample_linear1d_out::call(self_meta, output_size, align_corners, scales, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::upsample_linear1d_out::call(self_, output_size, align_corners, scales, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::upsample_linear1d::call(self_, output_size, align_corners, scales);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & upsample_linear1d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::upsample_linear1d_backward_grad_input::call(grad_output_meta, output_size, input_size, align_corners, scales, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::upsample_linear1d_backward_grad_input::call(grad_output_, output_size, input_size, align_corners, scales, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::upsample_linear1d_backward::call(grad_output_, output_size, input_size, align_corners, scales);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & upsample_bicubic2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::upsample_bicubic2d_out::call(self_meta, output_size, align_corners, scales_h, scales_w, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::upsample_bicubic2d_out::call(self_, output_size, align_corners, scales_h, scales_w, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::upsample_bicubic2d::call(self_, output_size, align_corners, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & upsample_bicubic2d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::upsample_bicubic2d_backward_grad_input::call(grad_output_meta, output_size, input_size, align_corners, scales_h, scales_w, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::upsample_bicubic2d_backward_grad_input::call(grad_output_, output_size, input_size, align_corners, scales_h, scales_w, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::upsample_bicubic2d_backward::call(grad_output_, output_size, input_size, align_corners, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & _upsample_bicubic2d_aa_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_upsample_bicubic2d_aa_backward_grad_input::call(grad_output_meta, output_size, input_size, align_corners, scales_h, scales_w, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_upsample_bicubic2d_aa_backward_grad_input::call(grad_output_, output_size, input_size, align_corners, scales_h, scales_w, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_upsample_bicubic2d_aa_backward::call(grad_output_, output_size, input_size, align_corners, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & upsample_nearest1d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::upsample_nearest1d_out::call(self_meta, output_size, scales, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::upsample_nearest1d_out::call(self_, output_size, scales, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::upsample_nearest1d::call(self_, output_size, scales);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _upsample_nearest_exact1d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_upsample_nearest_exact1d_out::call(self_meta, output_size, scales, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_upsample_nearest_exact1d_out::call(self_, output_size, scales, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_upsample_nearest_exact1d::call(self_, output_size, scales);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & upsample_nearest1d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::upsample_nearest1d_backward_grad_input::call(grad_output_meta, output_size, input_size, scales, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::upsample_nearest1d_backward_grad_input::call(grad_output_, output_size, input_size, scales, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::upsample_nearest1d_backward::call(grad_output_, output_size, input_size, scales);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & upsample_nearest3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::upsample_nearest3d_out::call(self_meta, output_size, scales_d, scales_h, scales_w, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::upsample_nearest3d_out::call(self_, output_size, scales_d, scales_h, scales_w, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::upsample_nearest3d::call(self_, output_size, scales_d, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & slow_conv_transpose3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef dilation, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::slow_conv_transpose3d_out::call(self_meta, weight_meta, kernel_size, bias_meta, stride, padding, output_padding, dilation, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::slow_conv_transpose3d_out::call(self_, weight_, kernel_size, bias_, stride, padding, output_padding, dilation, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::slow_conv_transpose3d::call(self_, weight_, kernel_size, bias_, stride, padding, output_padding, dilation);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & isposinf_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::isposinf_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::isposinf_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::isposinf::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_entr_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_entr_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_entr_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_entr::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_psi_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_psi_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_psi_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_psi::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_erfinv_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_erfinv_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_erfinv_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_erfinv::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_ndtr_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_ndtr_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_ndtr_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_ndtr::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_xlogy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_xlogy_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_xlogy_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_xlogy::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_xlogy_out_self_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_xlogy_self_scalar_out::call(self, other_meta, out_meta);
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_xlogy_self_scalar_out::call(self, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_xlogy_self_scalar::call(self, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_xlogy_out_other_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_xlogy_other_scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_xlogy_other_scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_xlogy_other_scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_zeta_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_zeta_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_zeta_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_zeta::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_zeta_out_self_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_zeta_self_scalar_out::call(self, other_meta, out_meta);
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_zeta_self_scalar_out::call(self, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_zeta_self_scalar::call(self, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_zeta_out_other_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_zeta_other_scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_zeta_other_scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_zeta_other_scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_i0_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_i0_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_i0_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_i0::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_i0e_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_i0e_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_i0e_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_i0e::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_expit_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_expit_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_expit_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_expit::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_round_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t decimals, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_round_out::call(self_meta, decimals, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_round_out::call(self_, decimals, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_round::call(self_, decimals);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_gammainc_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_gammainc_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_gammainc_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_gammainc::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_irfft_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_irfft_out::call(self_meta, n, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_irfft_out::call(self_, n, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_irfft::call(self_, n, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_fft2_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_fft2_out::call(self_meta, s, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_fft2_out::call(self_, s, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_fft2::call(self_, s, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_rfftfreq_out_out(c10::DispatchKeySet dispatchKeySet, int64_t n, double d, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_rfftfreq_out::call(n, d, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_rfftfreq_out::call(n, d, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_rfftfreq::call(n, d, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_cholesky_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_cholesky_out::call(self_meta, upper, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_cholesky_out::call(self_, upper, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_cholesky::call(self_, upper);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_lu_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, bool pivot, at::Tensor & P, at::Tensor & L, at::Tensor & U) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto P_meta = to_meta(P);
        auto L_meta = to_meta(L);
        auto U_meta = to_meta(U);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_lu_out::call(A_meta, pivot, P_meta, L_meta, U_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor P_;
      if (at::functionalization::impl::isFunctionalTensor(P)) {
        at::functionalization::impl::sync(P);
        P_ = at::functionalization::impl::from_functional_tensor(P);
      } else {
        P_ = P;
      }
      
      at::Tensor L_;
      if (at::functionalization::impl::isFunctionalTensor(L)) {
        at::functionalization::impl::sync(L);
        L_ = at::functionalization::impl::from_functional_tensor(L);
      } else {
        L_ = L;
      }
      
      at::Tensor U_;
      if (at::functionalization::impl::isFunctionalTensor(U)) {
        at::functionalization::impl::sync(U);
        U_ = at::functionalization::impl::from_functional_tensor(U);
      } else {
        U_ = U;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(P) && at::functionalization::impl::isFunctionalTensor(L) && at::functionalization::impl::isFunctionalTensor(U))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::linalg_lu_out::call(A_, pivot, P_, L_, U_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(P, L, U);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_lu::call(A_, pivot);
        }
          at::functionalization::impl::propagate_xla_data(P, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(P, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(P);
  at::functionalization::impl::sync(P);
  at::functionalization::impl::propagate_xla_data(L, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(L, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(L);
  at::functionalization::impl::sync(L);
  at::functionalization::impl::propagate_xla_data(U, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(U, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(U);
  at::functionalization::impl::sync(U);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(P, L, U);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _linalg_det_out_result(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, at::Tensor & result, at::Tensor & LU, at::Tensor & pivots) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto result_meta = to_meta(result);
        auto LU_meta = to_meta(LU);
        auto pivots_meta = to_meta(pivots);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_linalg_det_result::call(A_meta, result_meta, LU_meta, pivots_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor result_;
      if (at::functionalization::impl::isFunctionalTensor(result)) {
        at::functionalization::impl::sync(result);
        result_ = at::functionalization::impl::from_functional_tensor(result);
      } else {
        result_ = result;
      }
      
      at::Tensor LU_;
      if (at::functionalization::impl::isFunctionalTensor(LU)) {
        at::functionalization::impl::sync(LU);
        LU_ = at::functionalization::impl::from_functional_tensor(LU);
      } else {
        LU_ = LU;
      }
      
      at::Tensor pivots_;
      if (at::functionalization::impl::isFunctionalTensor(pivots)) {
        at::functionalization::impl::sync(pivots);
        pivots_ = at::functionalization::impl::from_functional_tensor(pivots);
      } else {
        pivots_ = pivots;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(result) && at::functionalization::impl::isFunctionalTensor(LU) && at::functionalization::impl::isFunctionalTensor(pivots))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_linalg_det_result::call(A_, result_, LU_, pivots_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(result, LU, pivots);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_linalg_det::call(A_);
        }
          at::functionalization::impl::propagate_xla_data(result, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(result, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(result);
  at::functionalization::impl::sync(result);
  at::functionalization::impl::propagate_xla_data(LU, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(LU, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(LU);
  at::functionalization::impl::sync(LU);
  at::functionalization::impl::propagate_xla_data(pivots, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(pivots, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(pivots);
  at::functionalization::impl::sync(pivots);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(result, LU, pivots);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> slogdet_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & sign, at::Tensor & logabsdet) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto sign_meta = to_meta(sign);
        auto logabsdet_meta = to_meta(logabsdet);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::slogdet_out::call(self_meta, sign_meta, logabsdet_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor sign_;
      if (at::functionalization::impl::isFunctionalTensor(sign)) {
        at::functionalization::impl::sync(sign);
        sign_ = at::functionalization::impl::from_functional_tensor(sign);
      } else {
        sign_ = sign;
      }
      
      at::Tensor logabsdet_;
      if (at::functionalization::impl::isFunctionalTensor(logabsdet)) {
        at::functionalization::impl::sync(logabsdet);
        logabsdet_ = at::functionalization::impl::from_functional_tensor(logabsdet);
      } else {
        logabsdet_ = logabsdet;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(sign) && at::functionalization::impl::isFunctionalTensor(logabsdet))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::slogdet_out::call(self_, sign_, logabsdet_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(sign, logabsdet);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::slogdet::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(sign, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(sign, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(sign);
  at::functionalization::impl::sync(sign);
  at::functionalization::impl::propagate_xla_data(logabsdet, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(logabsdet, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(logabsdet);
  at::functionalization::impl::sync(logabsdet);
    return ::std::tuple<at::Tensor &,at::Tensor &>(sign, logabsdet);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> linalg_eig_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & eigenvalues, at::Tensor & eigenvectors) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto eigenvalues_meta = to_meta(eigenvalues);
        auto eigenvectors_meta = to_meta(eigenvectors);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_eig_out::call(self_meta, eigenvalues_meta, eigenvectors_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor eigenvalues_;
      if (at::functionalization::impl::isFunctionalTensor(eigenvalues)) {
        at::functionalization::impl::sync(eigenvalues);
        eigenvalues_ = at::functionalization::impl::from_functional_tensor(eigenvalues);
      } else {
        eigenvalues_ = eigenvalues;
      }
      
      at::Tensor eigenvectors_;
      if (at::functionalization::impl::isFunctionalTensor(eigenvectors)) {
        at::functionalization::impl::sync(eigenvectors);
        eigenvectors_ = at::functionalization::impl::from_functional_tensor(eigenvectors);
      } else {
        eigenvectors_ = eigenvectors;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(eigenvalues) && at::functionalization::impl::isFunctionalTensor(eigenvectors))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::linalg_eig_out::call(self_, eigenvalues_, eigenvectors_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(eigenvalues, eigenvectors);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_eig::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(eigenvalues, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(eigenvalues, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(eigenvalues);
  at::functionalization::impl::sync(eigenvalues);
  at::functionalization::impl::propagate_xla_data(eigenvectors, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(eigenvectors, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(eigenvectors);
  at::functionalization::impl::sync(eigenvectors);
    return ::std::tuple<at::Tensor &,at::Tensor &>(eigenvalues, eigenvectors);
      }
    }

    at::Tensor & linalg_inv_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_inv_out::call(A_meta, out_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_inv_out::call(A_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_inv::call(A_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _linalg_svd_out_U(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, bool full_matrices, bool compute_uv, c10::optional<c10::string_view> driver, at::Tensor & U, at::Tensor & S, at::Tensor & Vh) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto U_meta = to_meta(U);
        auto S_meta = to_meta(S);
        auto Vh_meta = to_meta(Vh);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_linalg_svd_U::call(A_meta, full_matrices, compute_uv, driver, U_meta, S_meta, Vh_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor U_;
      if (at::functionalization::impl::isFunctionalTensor(U)) {
        at::functionalization::impl::sync(U);
        U_ = at::functionalization::impl::from_functional_tensor(U);
      } else {
        U_ = U;
      }
      
      at::Tensor S_;
      if (at::functionalization::impl::isFunctionalTensor(S)) {
        at::functionalization::impl::sync(S);
        S_ = at::functionalization::impl::from_functional_tensor(S);
      } else {
        S_ = S;
      }
      
      at::Tensor Vh_;
      if (at::functionalization::impl::isFunctionalTensor(Vh)) {
        at::functionalization::impl::sync(Vh);
        Vh_ = at::functionalization::impl::from_functional_tensor(Vh);
      } else {
        Vh_ = Vh;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(U) && at::functionalization::impl::isFunctionalTensor(S) && at::functionalization::impl::isFunctionalTensor(Vh))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_linalg_svd_U::call(A_, full_matrices, compute_uv, driver, U_, S_, Vh_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(U, S, Vh);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_linalg_svd::call(A_, full_matrices, compute_uv, driver);
        }
          at::functionalization::impl::propagate_xla_data(U, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(U, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(U);
  at::functionalization::impl::sync(U);
  at::functionalization::impl::propagate_xla_data(S, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(S, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(S);
  at::functionalization::impl::sync(S);
  at::functionalization::impl::propagate_xla_data(Vh, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(Vh, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(Vh);
  at::functionalization::impl::sync(Vh);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(U, S, Vh);
      }
    }

    at::Tensor & linalg_svdvals_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, c10::optional<c10::string_view> driver, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_svdvals_out::call(A_meta, driver, out_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_svdvals_out::call(A_, driver, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_svdvals::call(A_, driver);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_pinv_out_atol_rtol_tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto atol_meta = to_meta(atol);
        auto rtol_meta = to_meta(rtol);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_pinv_atol_rtol_tensor_out::call(self_meta, atol_meta, rtol_meta, hermitian, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::optional<at::Tensor> atol_;
      if (at::functionalization::impl::isFunctionalTensor(atol)) {
        at::functionalization::impl::sync(atol);
        atol_ = at::functionalization::impl::from_functional_tensor(atol);
      } else {
        atol_ = atol;
      }
      
      c10::optional<at::Tensor> rtol_;
      if (at::functionalization::impl::isFunctionalTensor(rtol)) {
        at::functionalization::impl::sync(rtol);
        rtol_ = at::functionalization::impl::from_functional_tensor(rtol);
      } else {
        rtol_ = rtol;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(atol) || at::functionalization::impl::isFunctionalTensor(rtol))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_pinv_atol_rtol_tensor_out::call(self_, atol_, rtol_, hermitian, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_pinv_atol_rtol_tensor::call(self_, atol_, rtol_, hermitian);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_pinv_out_atol_rtol_float_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_pinv_atol_rtol_float_out::call(self_meta, atol, rtol, hermitian, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_pinv_atol_rtol_float_out::call(self_, atol, rtol, hermitian, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_pinv_atol_rtol_float::call(self_, atol, rtol, hermitian);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_pinv_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double rcond, bool hermitian, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_pinv_out::call(self_meta, rcond, hermitian, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_pinv_out::call(self_, rcond, hermitian, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_pinv::call(self_, rcond, hermitian);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_pinv_out_out_rcond_tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & rcond, bool hermitian, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto rcond_meta = to_meta(rcond);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_pinv_out_rcond_tensor::call(self_meta, rcond_meta, hermitian, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor rcond_;
      if (at::functionalization::impl::isFunctionalTensor(rcond)) {
        at::functionalization::impl::sync(rcond);
        rcond_ = at::functionalization::impl::from_functional_tensor(rcond);
      } else {
        rcond_ = rcond;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || rcond.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(rcond))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_pinv_out_rcond_tensor::call(self_, rcond_, hermitian, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_pinv_rcond_tensor::call(self_, rcond_, hermitian);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> linalg_qr_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, c10::string_view mode, at::Tensor & Q, at::Tensor & R) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto Q_meta = to_meta(Q);
        auto R_meta = to_meta(R);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_qr_out::call(A_meta, mode, Q_meta, R_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor Q_;
      if (at::functionalization::impl::isFunctionalTensor(Q)) {
        at::functionalization::impl::sync(Q);
        Q_ = at::functionalization::impl::from_functional_tensor(Q);
      } else {
        Q_ = Q;
      }
      
      at::Tensor R_;
      if (at::functionalization::impl::isFunctionalTensor(R)) {
        at::functionalization::impl::sync(R);
        R_ = at::functionalization::impl::from_functional_tensor(R);
      } else {
        R_ = R;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(Q) && at::functionalization::impl::isFunctionalTensor(R))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::linalg_qr_out::call(A_, mode, Q_, R_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(Q, R);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_qr::call(A_, mode);
        }
          at::functionalization::impl::propagate_xla_data(Q, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(Q, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(Q);
  at::functionalization::impl::sync(Q);
  at::functionalization::impl::propagate_xla_data(R, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(R, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(R);
  at::functionalization::impl::sync(R);
    return ::std::tuple<at::Tensor &,at::Tensor &>(Q, R);
      }
    }

    at::Tensor & _test_optional_intlist_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & values, at::OptionalIntArrayRef addends, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto values_meta = to_meta(values);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_test_optional_intlist_out::call(values_meta, addends, out_meta);
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || values.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(values))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_test_optional_intlist_out::call(values_, addends, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_test_optional_intlist::call(values_, addends);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _fw_primal_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t level, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fw_primal_copy_out::call(self_meta, level, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_fw_primal_copy_out::call(self_, level, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fw_primal_copy::call(self_, level);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & as_strided_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::as_strided_copy_out::call(self_meta, size, stride, storage_offset, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::as_strided_copy_out::call(self_, size, stride, storage_offset, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::as_strided_copy::call(self_, size, stride, storage_offset);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & expand_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, bool implicit, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::expand_copy_out::call(self_meta, size, implicit, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::expand_copy_out::call(self_, size, implicit, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::expand_copy::call(self_, size, implicit);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _reshape_alias_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_reshape_alias_copy_out::call(self_meta, size, stride, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_reshape_alias_copy_out::call(self_, size, stride, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_reshape_alias_copy::call(self_, size, stride);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & select_copy_out_int_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::SymInt index, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::select_copy_int_out::call(self_meta, dim, index, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::select_copy_int_out::call(self_, dim, index, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::select_copy_int::call(self_, dim, index);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    void split_with_sizes_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::split_with_sizes_copy_out::call(self_meta, split_sizes, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::split_with_sizes_copy_out::call(self_, split_sizes, dim, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::split_with_sizes_copy::call(self_, split_sizes, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    at::Tensor & squeeze_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::squeeze_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::squeeze_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::squeeze_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & squeeze_copy_out_dim_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::squeeze_copy_dim_out::call(self_meta, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::squeeze_copy_dim_out::call(self_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::squeeze_copy_dim::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & squeeze_copy_out_dims_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::squeeze_copy_dims_out::call(self_meta, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::squeeze_copy_dims_out::call(self_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::squeeze_copy_dims::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _indices_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_indices_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_indices_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_indices_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _values_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_values_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_values_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_values_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & crow_indices_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::crow_indices_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::crow_indices_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::crow_indices_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & col_indices_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::col_indices_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::col_indices_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::col_indices_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    void unbind_copy_out_int_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::unbind_copy_int_out::call(self_meta, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::unbind_copy_int_out::call(self_, dim, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::unbind_copy_int::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    at::Tensor & view_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::view_copy_out::call(self_meta, size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::view_copy_out::call(self_, size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::view_copy::call(self_, size);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & view_copy_out_dtype_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ScalarType dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::view_copy_dtype_out::call(self_meta, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::view_copy_dtype_out::call(self_, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::view_copy_dtype::call(self_, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & alias_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::alias_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::alias_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::alias_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_airy_ai_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_airy_ai_out::call(x_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_airy_ai_out::call(x_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_airy_ai::call(x_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_bessel_j0_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_bessel_j0_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_bessel_j0_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_bessel_j0::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_chebyshev_polynomial_v_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_chebyshev_polynomial_v_out::call(x_meta, n_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_chebyshev_polynomial_v_out::call(x_, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_chebyshev_polynomial_v::call(x_, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_chebyshev_polynomial_v_out_x_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_chebyshev_polynomial_v_x_scalar_out::call(x, n_meta, out_meta);
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_chebyshev_polynomial_v_x_scalar_out::call(x, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_chebyshev_polynomial_v_x_scalar::call(x, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_chebyshev_polynomial_v_out_n_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_chebyshev_polynomial_v_n_scalar_out::call(x_meta, n, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_chebyshev_polynomial_v_n_scalar_out::call(x_, n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_chebyshev_polynomial_v_n_scalar::call(x_, n);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_chebyshev_polynomial_w_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_chebyshev_polynomial_w_out::call(x_meta, n_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_chebyshev_polynomial_w_out::call(x_, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_chebyshev_polynomial_w::call(x_, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_chebyshev_polynomial_w_out_x_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_chebyshev_polynomial_w_x_scalar_out::call(x, n_meta, out_meta);
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_chebyshev_polynomial_w_x_scalar_out::call(x, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_chebyshev_polynomial_w_x_scalar::call(x, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_chebyshev_polynomial_w_out_n_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_chebyshev_polynomial_w_n_scalar_out::call(x_meta, n, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_chebyshev_polynomial_w_n_scalar_out::call(x_, n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_chebyshev_polynomial_w_n_scalar::call(x_, n);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_hermite_polynomial_he_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_hermite_polynomial_he_out::call(x_meta, n_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_hermite_polynomial_he_out::call(x_, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_hermite_polynomial_he::call(x_, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_hermite_polynomial_he_out_x_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_hermite_polynomial_he_x_scalar_out::call(x, n_meta, out_meta);
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_hermite_polynomial_he_x_scalar_out::call(x, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_hermite_polynomial_he_x_scalar::call(x, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_hermite_polynomial_he_out_n_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_hermite_polynomial_he_n_scalar_out::call(x_meta, n, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_hermite_polynomial_he_n_scalar_out::call(x_, n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_hermite_polynomial_he_n_scalar::call(x_, n);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_laguerre_polynomial_l_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_laguerre_polynomial_l_out::call(x_meta, n_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_laguerre_polynomial_l_out::call(x_, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_laguerre_polynomial_l::call(x_, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_laguerre_polynomial_l_out_x_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_laguerre_polynomial_l_x_scalar_out::call(x, n_meta, out_meta);
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_laguerre_polynomial_l_x_scalar_out::call(x, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_laguerre_polynomial_l_x_scalar::call(x, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_laguerre_polynomial_l_out_n_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_laguerre_polynomial_l_n_scalar_out::call(x_meta, n, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_laguerre_polynomial_l_n_scalar_out::call(x_, n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_laguerre_polynomial_l_n_scalar::call(x_, n);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_shifted_chebyshev_polynomial_v_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_shifted_chebyshev_polynomial_v_out::call(x_meta, n_meta, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_shifted_chebyshev_polynomial_v_out::call(x_, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_shifted_chebyshev_polynomial_v::call(x_, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_shifted_chebyshev_polynomial_v_out_x_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & x, const at::Tensor & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto n_meta = to_meta(n);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_shifted_chebyshev_polynomial_v_x_scalar_out::call(x, n_meta, out_meta);
      }
      
      at::Tensor n_;
      if (at::functionalization::impl::isFunctionalTensor(n)) {
        at::functionalization::impl::sync(n);
        n_ = at::functionalization::impl::from_functional_tensor(n);
      } else {
        n_ = n;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || n.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(n))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_shifted_chebyshev_polynomial_v_x_scalar_out::call(x, n_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_shifted_chebyshev_polynomial_v_x_scalar::call(x, n_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_shifted_chebyshev_polynomial_v_out_n_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Scalar & n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x_meta = to_meta(x);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_shifted_chebyshev_polynomial_v_n_scalar_out::call(x_meta, n, out_meta);
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_shifted_chebyshev_polynomial_v_n_scalar_out::call(x_, n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_shifted_chebyshev_polynomial_v_n_scalar::call(x_, n);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    void _fused_adamw_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grads_meta = to_meta(grads);
        auto exp_avgs_meta = to_meta(exp_avgs);
        auto exp_avg_sqs_meta = to_meta(exp_avg_sqs);
        auto max_exp_avg_sqs_meta = to_meta(max_exp_avg_sqs);
        auto state_steps_meta = to_meta(state_steps);
        auto grad_scale_meta = to_meta(grad_scale);
        auto found_inf_meta = to_meta(found_inf);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_adamw_out::call(self_meta, grads_meta, exp_avgs_meta, exp_avg_sqs_meta, max_exp_avg_sqs_meta, state_steps_meta, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_meta, found_inf_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> grads_;
      if (at::functionalization::impl::isFunctionalTensor(grads)) {
        at::functionalization::impl::sync(grads);
        grads_ = at::functionalization::impl::from_functional_tensor(grads);
      } else {
        grads_ = grads.vec();
      }
      
      ::std::vector<at::Tensor> exp_avgs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avgs)) {
        at::functionalization::impl::sync(exp_avgs);
        exp_avgs_ = at::functionalization::impl::from_functional_tensor(exp_avgs);
      } else {
        exp_avgs_ = exp_avgs.vec();
      }
      
      ::std::vector<at::Tensor> exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avg_sqs)) {
        at::functionalization::impl::sync(exp_avg_sqs);
        exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(exp_avg_sqs);
      } else {
        exp_avg_sqs_ = exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> max_exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs)) {
        at::functionalization::impl::sync(max_exp_avg_sqs);
        max_exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(max_exp_avg_sqs);
      } else {
        max_exp_avg_sqs_ = max_exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> state_steps_;
      if (at::functionalization::impl::isFunctionalTensor(state_steps)) {
        at::functionalization::impl::sync(state_steps);
        state_steps_ = at::functionalization::impl::from_functional_tensor(state_steps);
      } else {
        state_steps_ = state_steps.vec();
      }
      
      c10::optional<at::Tensor> grad_scale_;
      if (at::functionalization::impl::isFunctionalTensor(grad_scale)) {
        at::functionalization::impl::sync(grad_scale);
        grad_scale_ = at::functionalization::impl::from_functional_tensor(grad_scale);
      } else {
        grad_scale_ = grad_scale;
      }
      
      c10::optional<at::Tensor> found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grads) && at::functionalization::impl::isFunctionalTensor(exp_avgs) && at::functionalization::impl::isFunctionalTensor(exp_avg_sqs) && at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs) && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(state_steps) || at::functionalization::impl::isFunctionalTensor(grad_scale) || at::functionalization::impl::isFunctionalTensor(found_inf))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_fused_adamw_out::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_, out_);
         ;
        }
      } else {
        ::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_adamw::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_);
        }
          at::functionalization::impl::propagate_xla_data(grads, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(grads, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(grads);
  at::functionalization::impl::sync(grads);
  at::functionalization::impl::propagate_xla_data(exp_avgs, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(exp_avgs, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(exp_avgs);
  at::functionalization::impl::sync(exp_avgs);
  at::functionalization::impl::propagate_xla_data(exp_avg_sqs, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(exp_avg_sqs, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(exp_avg_sqs);
  at::functionalization::impl::sync(exp_avg_sqs);
  at::functionalization::impl::propagate_xla_data(max_exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(max_exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(max_exp_avg_sqs);
  at::functionalization::impl::sync(max_exp_avg_sqs);
  at::functionalization::impl::propagate_xla_data(out, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _fused_adamw_(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grads_meta = to_meta(grads);
        auto exp_avgs_meta = to_meta(exp_avgs);
        auto exp_avg_sqs_meta = to_meta(exp_avg_sqs);
        auto max_exp_avg_sqs_meta = to_meta(max_exp_avg_sqs);
        auto state_steps_meta = to_meta(state_steps);
        auto grad_scale_meta = to_meta(grad_scale);
        auto found_inf_meta = to_meta(found_inf);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_adamw_::call(self_meta, grads_meta, exp_avgs_meta, exp_avg_sqs_meta, max_exp_avg_sqs_meta, state_steps_meta, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_meta, found_inf_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> grads_;
      if (at::functionalization::impl::isFunctionalTensor(grads)) {
        at::functionalization::impl::sync(grads);
        grads_ = at::functionalization::impl::from_functional_tensor(grads);
      } else {
        grads_ = grads.vec();
      }
      
      ::std::vector<at::Tensor> exp_avgs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avgs)) {
        at::functionalization::impl::sync(exp_avgs);
        exp_avgs_ = at::functionalization::impl::from_functional_tensor(exp_avgs);
      } else {
        exp_avgs_ = exp_avgs.vec();
      }
      
      ::std::vector<at::Tensor> exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avg_sqs)) {
        at::functionalization::impl::sync(exp_avg_sqs);
        exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(exp_avg_sqs);
      } else {
        exp_avg_sqs_ = exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> max_exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs)) {
        at::functionalization::impl::sync(max_exp_avg_sqs);
        max_exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(max_exp_avg_sqs);
      } else {
        max_exp_avg_sqs_ = max_exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> state_steps_;
      if (at::functionalization::impl::isFunctionalTensor(state_steps)) {
        at::functionalization::impl::sync(state_steps);
        state_steps_ = at::functionalization::impl::from_functional_tensor(state_steps);
      } else {
        state_steps_ = state_steps.vec();
      }
      
      c10::optional<at::Tensor> grad_scale_;
      if (at::functionalization::impl::isFunctionalTensor(grad_scale)) {
        at::functionalization::impl::sync(grad_scale);
        grad_scale_ = at::functionalization::impl::from_functional_tensor(grad_scale);
      } else {
        grad_scale_ = grad_scale;
      }
      
      c10::optional<at::Tensor> found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self) && at::functionalization::impl::isFunctionalTensor(grads) && at::functionalization::impl::isFunctionalTensor(exp_avgs) && at::functionalization::impl::isFunctionalTensor(exp_avg_sqs) && at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(state_steps) || at::functionalization::impl::isFunctionalTensor(grad_scale) || at::functionalization::impl::isFunctionalTensor(found_inf))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_fused_adamw_::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_);
         ;
        }
      } else {
        ::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_adamw::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_);
        }
          at::functionalization::impl::propagate_xla_data(self, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(self, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
  at::functionalization::impl::propagate_xla_data(grads, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(grads, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(grads);
  at::functionalization::impl::sync(grads);
  at::functionalization::impl::propagate_xla_data(exp_avgs, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(exp_avgs, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(exp_avgs);
  at::functionalization::impl::sync(exp_avgs);
  at::functionalization::impl::propagate_xla_data(exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(exp_avg_sqs);
  at::functionalization::impl::sync(exp_avg_sqs);
  at::functionalization::impl::propagate_xla_data(max_exp_avg_sqs, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(max_exp_avg_sqs, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(max_exp_avg_sqs);
  at::functionalization::impl::sync(max_exp_avg_sqs);
    
      }
    }

    void _fused_adamw_out_tensor_lr_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, const at::Tensor & lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grads_meta = to_meta(grads);
        auto exp_avgs_meta = to_meta(exp_avgs);
        auto exp_avg_sqs_meta = to_meta(exp_avg_sqs);
        auto max_exp_avg_sqs_meta = to_meta(max_exp_avg_sqs);
        auto state_steps_meta = to_meta(state_steps);
        auto lr_meta = to_meta(lr);
        auto grad_scale_meta = to_meta(grad_scale);
        auto found_inf_meta = to_meta(found_inf);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_adamw_tensor_lr_out::call(self_meta, grads_meta, exp_avgs_meta, exp_avg_sqs_meta, max_exp_avg_sqs_meta, state_steps_meta, lr_meta, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_meta, found_inf_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> grads_;
      if (at::functionalization::impl::isFunctionalTensor(grads)) {
        at::functionalization::impl::sync(grads);
        grads_ = at::functionalization::impl::from_functional_tensor(grads);
      } else {
        grads_ = grads.vec();
      }
      
      ::std::vector<at::Tensor> exp_avgs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avgs)) {
        at::functionalization::impl::sync(exp_avgs);
        exp_avgs_ = at::functionalization::impl::from_functional_tensor(exp_avgs);
      } else {
        exp_avgs_ = exp_avgs.vec();
      }
      
      ::std::vector<at::Tensor> exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avg_sqs)) {
        at::functionalization::impl::sync(exp_avg_sqs);
        exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(exp_avg_sqs);
      } else {
        exp_avg_sqs_ = exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> max_exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs)) {
        at::functionalization::impl::sync(max_exp_avg_sqs);
        max_exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(max_exp_avg_sqs);
      } else {
        max_exp_avg_sqs_ = max_exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> state_steps_;
      if (at::functionalization::impl::isFunctionalTensor(state_steps)) {
        at::functionalization::impl::sync(state_steps);
        state_steps_ = at::functionalization::impl::from_functional_tensor(state_steps);
      } else {
        state_steps_ = state_steps.vec();
      }
      
      at::Tensor lr_;
      if (at::functionalization::impl::isFunctionalTensor(lr)) {
        at::functionalization::impl::sync(lr);
        lr_ = at::functionalization::impl::from_functional_tensor(lr);
      } else {
        lr_ = lr;
      }
      
      c10::optional<at::Tensor> grad_scale_;
      if (at::functionalization::impl::isFunctionalTensor(grad_scale)) {
        at::functionalization::impl::sync(grad_scale);
        grad_scale_ = at::functionalization::impl::from_functional_tensor(grad_scale);
      } else {
        grad_scale_ = grad_scale;
      }
      
      c10::optional<at::Tensor> found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grads) && at::functionalization::impl::isFunctionalTensor(exp_avgs) && at::functionalization::impl::isFunctionalTensor(exp_avg_sqs) && at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs) && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || lr.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(state_steps) || at::functionalization::impl::isFunctionalTensor(lr) || at::functionalization::impl::isFunctionalTensor(grad_scale) || at::functionalization::impl::isFunctionalTensor(found_inf))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_fused_adamw_tensor_lr_out::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr_, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_, out_);
         ;
        }
      } else {
        ::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_adamw_tensor_lr::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr_, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_);
        }
          at::functionalization::impl::propagate_xla_data(grads, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(grads, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(grads);
  at::functionalization::impl::sync(grads);
  at::functionalization::impl::propagate_xla_data(exp_avgs, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(exp_avgs, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(exp_avgs);
  at::functionalization::impl::sync(exp_avgs);
  at::functionalization::impl::propagate_xla_data(exp_avg_sqs, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(exp_avg_sqs, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(exp_avg_sqs);
  at::functionalization::impl::sync(exp_avg_sqs);
  at::functionalization::impl::propagate_xla_data(max_exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(max_exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(max_exp_avg_sqs);
  at::functionalization::impl::sync(max_exp_avg_sqs);
  at::functionalization::impl::propagate_xla_data(out, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _fused_adamw__tensor_lr(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList grads, at::TensorList exp_avgs, at::TensorList exp_avg_sqs, at::TensorList max_exp_avg_sqs, at::TensorList state_steps, const at::Tensor & lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, const c10::optional<at::Tensor> & grad_scale, const c10::optional<at::Tensor> & found_inf) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grads_meta = to_meta(grads);
        auto exp_avgs_meta = to_meta(exp_avgs);
        auto exp_avg_sqs_meta = to_meta(exp_avg_sqs);
        auto max_exp_avg_sqs_meta = to_meta(max_exp_avg_sqs);
        auto state_steps_meta = to_meta(state_steps);
        auto lr_meta = to_meta(lr);
        auto grad_scale_meta = to_meta(grad_scale);
        auto found_inf_meta = to_meta(found_inf);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fused_adamw__tensor_lr::call(self_meta, grads_meta, exp_avgs_meta, exp_avg_sqs_meta, max_exp_avg_sqs_meta, state_steps_meta, lr_meta, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_meta, found_inf_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> grads_;
      if (at::functionalization::impl::isFunctionalTensor(grads)) {
        at::functionalization::impl::sync(grads);
        grads_ = at::functionalization::impl::from_functional_tensor(grads);
      } else {
        grads_ = grads.vec();
      }
      
      ::std::vector<at::Tensor> exp_avgs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avgs)) {
        at::functionalization::impl::sync(exp_avgs);
        exp_avgs_ = at::functionalization::impl::from_functional_tensor(exp_avgs);
      } else {
        exp_avgs_ = exp_avgs.vec();
      }
      
      ::std::vector<at::Tensor> exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(exp_avg_sqs)) {
        at::functionalization::impl::sync(exp_avg_sqs);
        exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(exp_avg_sqs);
      } else {
        exp_avg_sqs_ = exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> max_exp_avg_sqs_;
      if (at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs)) {
        at::functionalization::impl::sync(max_exp_avg_sqs);
        max_exp_avg_sqs_ = at::functionalization::impl::from_functional_tensor(max_exp_avg_sqs);
      } else {
        max_exp_avg_sqs_ = max_exp_avg_sqs.vec();
      }
      
      ::std::vector<at::Tensor> state_steps_;
      if (at::functionalization::impl::isFunctionalTensor(state_steps)) {
        at::functionalization::impl::sync(state_steps);
        state_steps_ = at::functionalization::impl::from_functional_tensor(state_steps);
      } else {
        state_steps_ = state_steps.vec();
      }
      
      at::Tensor lr_;
      if (at::functionalization::impl::isFunctionalTensor(lr)) {
        at::functionalization::impl::sync(lr);
        lr_ = at::functionalization::impl::from_functional_tensor(lr);
      } else {
        lr_ = lr;
      }
      
      c10::optional<at::Tensor> grad_scale_;
      if (at::functionalization::impl::isFunctionalTensor(grad_scale)) {
        at::functionalization::impl::sync(grad_scale);
        grad_scale_ = at::functionalization::impl::from_functional_tensor(grad_scale);
      } else {
        grad_scale_ = grad_scale;
      }
      
      c10::optional<at::Tensor> found_inf_;
      if (at::functionalization::impl::isFunctionalTensor(found_inf)) {
        at::functionalization::impl::sync(found_inf);
        found_inf_ = at::functionalization::impl::from_functional_tensor(found_inf);
      } else {
        found_inf_ = found_inf;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self) && at::functionalization::impl::isFunctionalTensor(grads) && at::functionalization::impl::isFunctionalTensor(exp_avgs) && at::functionalization::impl::isFunctionalTensor(exp_avg_sqs) && at::functionalization::impl::isFunctionalTensor(max_exp_avg_sqs))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || lr.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(state_steps) || at::functionalization::impl::isFunctionalTensor(lr) || at::functionalization::impl::isFunctionalTensor(grad_scale) || at::functionalization::impl::isFunctionalTensor(found_inf))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_fused_adamw__tensor_lr::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr_, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_);
         ;
        }
      } else {
        ::std::tuple<::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>,::std::vector<at::Tensor>> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fused_adamw_tensor_lr::call(self_, grads_, exp_avgs_, exp_avg_sqs_, max_exp_avg_sqs_, state_steps_, lr_, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale_, found_inf_);
        }
          at::functionalization::impl::propagate_xla_data(self, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(self, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
  at::functionalization::impl::propagate_xla_data(grads, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(grads, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(grads);
  at::functionalization::impl::sync(grads);
  at::functionalization::impl::propagate_xla_data(exp_avgs, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(exp_avgs, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(exp_avgs);
  at::functionalization::impl::sync(exp_avgs);
  at::functionalization::impl::propagate_xla_data(exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(exp_avg_sqs, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(exp_avg_sqs);
  at::functionalization::impl::sync(exp_avg_sqs);
  at::functionalization::impl::propagate_xla_data(max_exp_avg_sqs, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(max_exp_avg_sqs, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(max_exp_avg_sqs);
  at::functionalization::impl::sync(max_exp_avg_sqs);
    
      }
    }

    at::Tensor _fw_primal(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t level) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::_fw_primal::call(self_, level);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::_fw_primal::call(self_meta, level);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::_fw_primal::call(self_, level);
        } else {
          tmp_output = at::_ops::_fw_primal_copy::call(self_, level);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, level = level](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::_fw_primal::call(base, level);
          } else {
            return at::_ops::_fw_primal_copy::call(base, level);
          }
        },
        [inverse_return_mode = inverse_return_mode, level = level](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::_fw_primal_inverse(base, mutated_view, inverse_return_mode, level);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor _make_dual(c10::DispatchKeySet dispatchKeySet, const at::Tensor & primal, const at::Tensor & tangent, int64_t level) {
      
      at::Tensor primal_;
      if (at::functionalization::impl::isFunctionalTensor(primal)) {
        
        primal_ = at::functionalization::impl::from_functional_tensor(primal);
      } else {
        primal_ = primal;
      }
      
      at::Tensor tangent_;
      if (at::functionalization::impl::isFunctionalTensor(tangent)) {
        
        tangent_ = at::functionalization::impl::from_functional_tensor(tangent);
      } else {
        tangent_ = tangent;
      }
      if (!at::functionalization::impl::isFunctionalTensor(primal)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::_make_dual::call(primal_, tangent_, level);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        primal.key_set().has_backend(c10::BackendComponent::XLABit) ||
        primal.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto primal_meta = to_meta(primal);
        auto tangent_meta = to_meta(tangent);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::_make_dual::call(primal_meta, tangent_meta, level);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::_make_dual::call(primal_, tangent_, level);
        } else {
          tmp_output = at::_ops::_make_dual_copy::call(primal_, tangent_, level);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, tangent = tangent, level = level](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::_make_dual::call(base, tangent, level);
          } else {
            return at::_ops::_make_dual_copy::call(base, tangent, level);
          }
        },
        [inverse_return_mode = inverse_return_mode, tangent = tangent, level = level](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::_make_dual_inverse(base, mutated_view, inverse_return_mode, tangent, level);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, primal, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor view_as_complex(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::view_as_complex::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::view_as_complex::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::view_as_complex::call(self_);
        } else {
          tmp_output = at::_ops::view_as_complex_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::view_as_complex::call(base);
          } else {
            return at::_ops::view_as_complex_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::view_as_complex_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor expand(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, bool implicit) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::expand::call(self_, size, implicit);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::expand::call(self_meta, size, implicit);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::expand::call(self_, size, implicit);
        } else {
          tmp_output = at::_ops::expand_copy::call(self_, size, implicit);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, size = size.vec(), implicit = implicit](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::expand::call(base, size, implicit);
          } else {
            return at::_ops::expand_copy::call(base, size, implicit);
          }
        },
        [inverse_return_mode = inverse_return_mode, size = size.vec(), implicit = implicit](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::expand_inverse(base, mutated_view, inverse_return_mode, size, implicit);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor transpose_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim0, int64_t dim1) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::transpose_int::call(self_, dim0, dim1);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::transpose_int::call(self_meta, dim0, dim1);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::transpose_int::call(self_, dim0, dim1);
        } else {
          tmp_output = at::_ops::transpose_copy_int::call(self_, dim0, dim1);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, dim0 = dim0, dim1 = dim1](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::transpose_int::call(base, dim0, dim1);
          } else {
            return at::_ops::transpose_copy_int::call(base, dim0, dim1);
          }
        },
        [inverse_return_mode = inverse_return_mode, dim0 = dim0, dim1 = dim1](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::transpose_int_inverse(base, mutated_view, inverse_return_mode, dim0, dim1);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor & transpose_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim0, int64_t dim1) {
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::transpose_::call(self_, dim0, dim1);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, dim0 = dim0, dim1 = dim1](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::transpose_int::call(base, dim0, dim1);
          } else {
            return at::_ops::transpose_copy_int::call(base, dim0, dim1);
          }
        },
        [inverse_return_mode = inverse_return_mode, dim0 = dim0, dim1 = dim1](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::transpose_int_inverse(base, mutated_view, inverse_return_mode, dim0, dim1);
        }
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::transpose_::call(self_meta, dim0, dim1);
      }
      // This function adds the above view meta to the current tensor and replays them off the base,
      // mutating the size/stride info of the current FunctionalTensorWrapper.
      // Because of this, we need to make sure to run the reference shape function above,
      // BEFORE doing this (otherwise we'll end up runnin the reference function using the wrong sizes/strides)
      at::functionalization::impl::mutate_view_meta(self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      // XLA/LTC don't implement the logic to propagate strides correctly, so we need to rely
      // on a reference implementation here (instead of relying on the output from the forward lambda
      // having the correct stride info)
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(self, reference_tensor_output);
      }
      return self;
    }

    at::Tensor _nested_view_from_buffer(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & nested_size, const at::Tensor & nested_strides, const at::Tensor & offsets) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor nested_size_;
      if (at::functionalization::impl::isFunctionalTensor(nested_size)) {
        
        nested_size_ = at::functionalization::impl::from_functional_tensor(nested_size);
      } else {
        nested_size_ = nested_size;
      }
      
      at::Tensor nested_strides_;
      if (at::functionalization::impl::isFunctionalTensor(nested_strides)) {
        
        nested_strides_ = at::functionalization::impl::from_functional_tensor(nested_strides);
      } else {
        nested_strides_ = nested_strides;
      }
      
      at::Tensor offsets_;
      if (at::functionalization::impl::isFunctionalTensor(offsets)) {
        
        offsets_ = at::functionalization::impl::from_functional_tensor(offsets);
      } else {
        offsets_ = offsets;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::_nested_view_from_buffer::call(self_, nested_size_, nested_strides_, offsets_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        auto nested_size_meta = to_meta(nested_size);
        auto nested_strides_meta = to_meta(nested_strides);
        auto offsets_meta = to_meta(offsets);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::_nested_view_from_buffer::call(self_meta, nested_size_meta, nested_strides_meta, offsets_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::_nested_view_from_buffer::call(self_, nested_size_, nested_strides_, offsets_);
        } else {
          tmp_output = at::_ops::_nested_view_from_buffer_copy::call(self_, nested_size_, nested_strides_, offsets_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, nested_size = nested_size, nested_strides = nested_strides, offsets = offsets](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::_nested_view_from_buffer::call(base, nested_size, nested_strides, offsets);
          } else {
            return at::_ops::_nested_view_from_buffer_copy::call(base, nested_size, nested_strides, offsets);
          }
        },
        [inverse_return_mode = inverse_return_mode, nested_size = nested_size, nested_strides = nested_strides, offsets = offsets](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::_nested_view_from_buffer_inverse(base, mutated_view, inverse_return_mode, nested_size, nested_strides, offsets);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor unsqueeze(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::unsqueeze::call(self_, dim);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::unsqueeze::call(self_meta, dim);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::unsqueeze::call(self_, dim);
        } else {
          tmp_output = at::_ops::unsqueeze_copy::call(self_, dim);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, dim = dim](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::unsqueeze::call(base, dim);
          } else {
            return at::_ops::unsqueeze_copy::call(base, dim);
          }
        },
        [inverse_return_mode = inverse_return_mode, dim = dim](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::unsqueeze_inverse(base, mutated_view, inverse_return_mode, dim);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor & unsqueeze_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim) {
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::unsqueeze_::call(self_, dim);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, dim = dim](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::unsqueeze::call(base, dim);
          } else {
            return at::_ops::unsqueeze_copy::call(base, dim);
          }
        },
        [inverse_return_mode = inverse_return_mode, dim = dim](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::unsqueeze_inverse(base, mutated_view, inverse_return_mode, dim);
        }
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::unsqueeze_::call(self_meta, dim);
      }
      // This function adds the above view meta to the current tensor and replays them off the base,
      // mutating the size/stride info of the current FunctionalTensorWrapper.
      // Because of this, we need to make sure to run the reference shape function above,
      // BEFORE doing this (otherwise we'll end up runnin the reference function using the wrong sizes/strides)
      at::functionalization::impl::mutate_view_meta(self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      // XLA/LTC don't implement the logic to propagate strides correctly, so we need to rely
      // on a reference implementation here (instead of relying on the output from the forward lambda
      // having the correct stride info)
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(self, reference_tensor_output);
      }
      return self;
    }

    at::Tensor _values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::_values::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::_values::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::_values::call(self_);
        } else {
          tmp_output = at::_ops::_values_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::_values::call(base);
          } else {
            return at::_ops::_values_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::_values_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor ccol_indices(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::ccol_indices::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::ccol_indices::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::ccol_indices::call(self_);
        } else {
          tmp_output = at::_ops::ccol_indices_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::ccol_indices::call(base);
          } else {
            return at::_ops::ccol_indices_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::ccol_indices_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor view(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::view::call(self_, size);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::view::call(self_meta, size);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::view::call(self_, size);
        } else {
          tmp_output = at::_ops::view_copy::call(self_, size);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, size = size.vec()](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::view::call(base, size);
          } else {
            return at::_ops::view_copy::call(base, size);
          }
        },
        [inverse_return_mode = inverse_return_mode, size = size.vec()](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::view_inverse(base, mutated_view, inverse_return_mode, size);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor view_dtype(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ScalarType dtype) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::view_dtype::call(self_, dtype);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::view_dtype::call(self_meta, dtype);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::view_dtype::call(self_, dtype);
        } else {
          tmp_output = at::_ops::view_copy_dtype::call(self_, dtype);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, dtype = dtype](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::view_dtype::call(base, dtype);
          } else {
            return at::_ops::view_copy_dtype::call(base, dtype);
          }
        },
        [inverse_return_mode = inverse_return_mode, dtype = dtype](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::view_dtype_inverse(base, mutated_view, inverse_return_mode, dtype);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

}  // namespace functionalization

namespace {

TORCH_LIBRARY_IMPL(aten, Functionalize, m) {
  m.impl("_masked_scale.out", TORCH_FN(functionalization::_masked_scale_out_out));
  m.impl("native_dropout.out", TORCH_FN(functionalization::native_dropout_out_out));
  m.impl("native_dropout_backward.out", TORCH_FN(functionalization::native_dropout_backward_out_out));
  m.impl("asinh.out", TORCH_FN(functionalization::asinh_out_out));
  m.impl("asinh_", TORCH_FN(functionalization::asinh_));
  m.impl("baddbmm.out", TORCH_FN(functionalization::baddbmm_out_out));
  m.impl("baddbmm_", TORCH_FN(functionalization::baddbmm_));
  m.impl("quantized_batch_norm.out", TORCH_FN(functionalization::quantized_batch_norm_out_out));
  m.impl("bernoulli.out", TORCH_FN(functionalization::bernoulli_out_out));
  m.impl("bernoulli.Tensor_out", TORCH_FN(functionalization::bernoulli_out_Tensor_out));
  m.impl("bernoulli_.Tensor", TORCH_FN(functionalization::bernoulli__Tensor));
  m.impl("bernoulli.float_out", TORCH_FN(functionalization::bernoulli_out_float_out));
  m.impl("bernoulli_.float", TORCH_FN(functionalization::bernoulli__float));
  m.impl("bmm.out", TORCH_FN(functionalization::bmm_out_out));
  m.impl("clamp_max.out", TORCH_FN(functionalization::clamp_max_out_out));
  m.impl("clamp_max_", TORCH_FN(functionalization::clamp_max_));
  m.impl("clamp_max.Tensor_out", TORCH_FN(functionalization::clamp_max_out_Tensor_out));
  m.impl("clamp_max_.Tensor", TORCH_FN(functionalization::clamp_max__Tensor));
  m.impl("clamp_min.out", TORCH_FN(functionalization::clamp_min_out_out));
  m.impl("clamp_min_", TORCH_FN(functionalization::clamp_min_));
  m.impl("clamp_min.Tensor_out", TORCH_FN(functionalization::clamp_min_out_Tensor_out));
  m.impl("clamp_min_.Tensor", TORCH_FN(functionalization::clamp_min__Tensor));
  m.impl("complex.out", TORCH_FN(functionalization::complex_out_out));
  m.impl("constant_pad_nd.out", TORCH_FN(functionalization::constant_pad_nd_out_out));
  m.impl("conv_tbc.out", TORCH_FN(functionalization::conv_tbc_out_out));
  m.impl("_copy_from_and_resize.out", TORCH_FN(functionalization::_copy_from_and_resize_out_out));
  m.impl("cos.out", TORCH_FN(functionalization::cos_out_out));
  m.impl("cos_", TORCH_FN(functionalization::cos_));
  m.impl("count_nonzero.dim_IntList_out", TORCH_FN(functionalization::count_nonzero_out_dim_IntList_out));
  m.impl("count_nonzero.out", TORCH_FN(functionalization::count_nonzero_out_out));
  m.impl("cudnn_affine_grid_generator_backward.out", TORCH_FN(functionalization::cudnn_affine_grid_generator_backward_out_out));
  m.impl("cudnn_batch_norm.out", TORCH_FN(functionalization::cudnn_batch_norm_out_out));
  m.impl("mps_convolution_transpose_backward.out", TORCH_FN(functionalization::mps_convolution_transpose_backward_out_out));
  m.impl("cudnn_grid_sampler_backward.out", TORCH_FN(functionalization::cudnn_grid_sampler_backward_out_out));
  m.impl("cummin.out", TORCH_FN(functionalization::cummin_out_out));
  m.impl("cumsum.out", TORCH_FN(functionalization::cumsum_out_out));
  m.impl("cumsum_", TORCH_FN(functionalization::cumsum_));
  m.impl("_ctc_loss.out", TORCH_FN(functionalization::_ctc_loss_out_out));
  m.impl("_ctc_loss.Tensor_out", TORCH_FN(functionalization::_ctc_loss_out_Tensor_out));
  m.impl("_ctc_loss_backward.out", TORCH_FN(functionalization::_ctc_loss_backward_out_out));
  m.impl("embedding.out", TORCH_FN(functionalization::embedding_out_out));
  m.impl("embedding_dense_backward.out", TORCH_FN(functionalization::embedding_dense_backward_out_out));
  m.impl("empty_permuted.out", TORCH_FN(functionalization::empty_permuted_out_out));
  m.impl("new_zeros.out", TORCH_FN(functionalization::new_zeros_out_out));
  m.impl("new_ones.out", TORCH_FN(functionalization::new_ones_out_out));
  m.impl("_empty_per_channel_affine_quantized.out", TORCH_FN(functionalization::_empty_per_channel_affine_quantized_out_out));
  m.impl("exp2.out", TORCH_FN(functionalization::exp2_out_out));
  m.impl("exp2_", TORCH_FN(functionalization::exp2_));
  m.impl("_grid_sampler_2d_cpu_fallback.out", TORCH_FN(functionalization::_grid_sampler_2d_cpu_fallback_out_out));
  m.impl("grid_sampler_3d_backward.out", TORCH_FN(functionalization::grid_sampler_3d_backward_out_out));
  m.impl("_fft_c2c.out", TORCH_FN(functionalization::_fft_c2c_out_out));
  m.impl("index_copy.out", TORCH_FN(functionalization::index_copy_out_out));
  m.impl("index_copy_", TORCH_FN(functionalization::index_copy_));
  m.impl("isin.Tensor_Tensor_out", TORCH_FN(functionalization::isin_out_Tensor_Tensor_out));
  m.impl("isin.Tensor_Scalar_out", TORCH_FN(functionalization::isin_out_Tensor_Scalar_out));
  m.impl("isin.Scalar_Tensor_out", TORCH_FN(functionalization::isin_out_Scalar_Tensor_out));
  m.impl("nan_to_num.out", TORCH_FN(functionalization::nan_to_num_out_out));
  m.impl("nan_to_num_", TORCH_FN(functionalization::nan_to_num_));
  m.impl("linear.out", TORCH_FN(functionalization::linear_out_out));
  m.impl("mkldnn_linear.out", TORCH_FN(functionalization::mkldnn_linear_out_out));
  m.impl("linspace.out", TORCH_FN(functionalization::linspace_out_out));
  m.impl("linspace.Tensor_Tensor_out", TORCH_FN(functionalization::linspace_out_Tensor_Tensor_out));
  m.impl("linspace.Tensor_Scalar_out", TORCH_FN(functionalization::linspace_out_Tensor_Scalar_out));
  m.impl("linspace.Scalar_Tensor_out", TORCH_FN(functionalization::linspace_out_Scalar_Tensor_out));
  m.impl("log.out", TORCH_FN(functionalization::log_out_out));
  m.impl("log_", TORCH_FN(functionalization::log_));
  m.impl("log_softmax.int_out", TORCH_FN(functionalization::log_softmax_out_int_out));
  m.impl("_log_softmax.out", TORCH_FN(functionalization::_log_softmax_out_out));
  m.impl("_log_softmax_backward_data.out", TORCH_FN(functionalization::_log_softmax_backward_data_out_out));
  m.impl("max.dim_max", TORCH_FN(functionalization::max_out_dim_max));
  m.impl("amax.out", TORCH_FN(functionalization::amax_out_out));
  m.impl("mkldnn_max_pool2d_backward.out", TORCH_FN(functionalization::mkldnn_max_pool2d_backward_out_out));
  m.impl("mkldnn_max_pool3d.out", TORCH_FN(functionalization::mkldnn_max_pool3d_out_out));
  m.impl("quantized_max_pool2d.out", TORCH_FN(functionalization::quantized_max_pool2d_out_out));
  m.impl("quantized_max_pool3d.out", TORCH_FN(functionalization::quantized_max_pool3d_out_out));
  m.impl("mean.out", TORCH_FN(functionalization::mean_out_out));
  m.impl("nanmedian.out", TORCH_FN(functionalization::nanmedian_out_out));
  m.impl("nanmedian.dim_values", TORCH_FN(functionalization::nanmedian_out_dim_values));
  m.impl("_int_mm.out", TORCH_FN(functionalization::_int_mm_out_out));
  m.impl("mode.values", TORCH_FN(functionalization::mode_out_values));
  m.impl("narrow_copy.out", TORCH_FN(functionalization::narrow_copy_out_out));
  m.impl("batch_norm_gather_stats.out", TORCH_FN(functionalization::batch_norm_gather_stats_out_out));
  m.impl("batch_norm_gather_stats_with_counts.out", TORCH_FN(functionalization::batch_norm_gather_stats_with_counts_out_out));
  m.impl("native_batch_norm_backward.out", TORCH_FN(functionalization::native_batch_norm_backward_out_out));
  m.impl("ones.names_out", TORCH_FN(functionalization::ones_out_names_out));
  m.impl("ones.out", TORCH_FN(functionalization::ones_out_out));
  m.impl("_pdist_forward.out", TORCH_FN(functionalization::_pdist_forward_out_out));
  m.impl("_pdist_backward.out", TORCH_FN(functionalization::_pdist_backward_out_out));
  m.impl("pixel_shuffle.out", TORCH_FN(functionalization::pixel_shuffle_out_out));
  m.impl("_pin_memory.out", TORCH_FN(functionalization::_pin_memory_out_out));
  m.impl("randn.names_out", TORCH_FN(functionalization::randn_out_names_out));
  m.impl("randn.generator_with_names_out", TORCH_FN(functionalization::randn_out_generator_with_names_out));
  m.impl("randn_like.out", TORCH_FN(functionalization::randn_like_out_out));
  m.impl("neg.out", TORCH_FN(functionalization::neg_out_out));
  m.impl("neg_", TORCH_FN(functionalization::neg_));
  m.impl("repeat_interleave.Tensor_out", TORCH_FN(functionalization::repeat_interleave_out_Tensor_out));
  m.impl("gelu.out", TORCH_FN(functionalization::gelu_out_out));
  m.impl("gelu_", TORCH_FN(functionalization::gelu_));
  m.impl("select_backward.out", TORCH_FN(functionalization::select_backward_out_out));
  m.impl("mish.out", TORCH_FN(functionalization::mish_out_out));
  m.impl("mish_", TORCH_FN(functionalization::mish_));
  m.impl("slice_scatter.out", TORCH_FN(functionalization::slice_scatter_out_out));
  m.impl("diagonal_scatter.out", TORCH_FN(functionalization::diagonal_scatter_out_out));
  m.impl("_softmax_backward_data.out", TORCH_FN(functionalization::_softmax_backward_data_out_out));
  m.impl("unsafe_split.Tensor_out", TORCH_FN(functionalization::unsafe_split_out_Tensor_out));
  m.impl("unsafe_split_with_sizes.out", TORCH_FN(functionalization::unsafe_split_with_sizes_out_out));
  m.impl("tanh.out", TORCH_FN(functionalization::tanh_out_out));
  m.impl("tanh_", TORCH_FN(functionalization::tanh_));
  m.impl("roll.out", TORCH_FN(functionalization::roll_out_out));
  m.impl("rot90.out", TORCH_FN(functionalization::rot90_out_out));
  m.impl("_nested_tensor_storage_offsets.out", TORCH_FN(functionalization::_nested_tensor_storage_offsets_out_out));
  m.impl("_trilinear.out", TORCH_FN(functionalization::_trilinear_out_out));
  m.impl("_unique.out", TORCH_FN(functionalization::_unique_out_out));
  m.impl("_unique2.out", TORCH_FN(functionalization::_unique2_out_out));
  m.impl("_weight_norm_interface.out", TORCH_FN(functionalization::_weight_norm_interface_out_out));
  m.impl("_efficientzerotensor.out", TORCH_FN(functionalization::_efficientzerotensor_out_out));
  m.impl("_standard_gamma.out", TORCH_FN(functionalization::_standard_gamma_out_out));
  m.impl("_dirichlet_grad.out", TORCH_FN(functionalization::_dirichlet_grad_out_out));
  m.impl("norm.ScalarOpt_dtype_out", TORCH_FN(functionalization::norm_out_ScalarOpt_dtype_out));
  m.impl("norm.Scalar_out", TORCH_FN(functionalization::norm_out_Scalar_out));
  m.impl("norm.dtype_out", TORCH_FN(functionalization::norm_out_dtype_out));
  m.impl("norm.out", TORCH_FN(functionalization::norm_out_out));
  m.impl("frexp.Tensor_out", TORCH_FN(functionalization::frexp_out_Tensor_out));
  m.impl("sparse_sampled_addmm.out", TORCH_FN(functionalization::sparse_sampled_addmm_out_out));
  m.impl("_addmm_activation.out", TORCH_FN(functionalization::_addmm_activation_out_out));
  m.impl("_to_dense.out", TORCH_FN(functionalization::_to_dense_out_out));
  m.impl("_coalesced.out", TORCH_FN(functionalization::_coalesced_out_out));
  m.impl("_coalesced_", TORCH_FN(functionalization::_coalesced_));
  m.impl("quantize_per_tensor_dynamic.out", TORCH_FN(functionalization::quantize_per_tensor_dynamic_out_out));
  m.impl("quantize_per_tensor.out", TORCH_FN(functionalization::quantize_per_tensor_out_out));
  m.impl("quantize_per_tensor.tensor_qparams_out", TORCH_FN(functionalization::quantize_per_tensor_out_tensor_qparams_out));
  m.impl("quantize_per_tensor.tensors_out", TORCH_FN(functionalization::quantize_per_tensor_out_tensors_out));
  m.impl("quantize_per_channel.out", TORCH_FN(functionalization::quantize_per_channel_out_out));
  m.impl("_make_per_channel_quantized_tensor.out", TORCH_FN(functionalization::_make_per_channel_quantized_tensor_out_out));
  m.impl("fake_quantize_per_tensor_affine_cachemask.out", TORCH_FN(functionalization::fake_quantize_per_tensor_affine_cachemask_out_out));
  m.impl("_fake_quantize_per_tensor_affine_cachemask_tensor_qparams.out", TORCH_FN(functionalization::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams_out_out));
  m.impl("_fake_quantize_learnable_per_tensor_affine.out", TORCH_FN(functionalization::_fake_quantize_learnable_per_tensor_affine_out_out));
  m.impl("fake_quantize_per_channel_affine_cachemask.out", TORCH_FN(functionalization::fake_quantize_per_channel_affine_cachemask_out_out));
  m.impl("lstm_mps_backward.out", TORCH_FN(functionalization::lstm_mps_backward_out_out));
  m.impl("lift_fresh_copy.out", TORCH_FN(functionalization::lift_fresh_copy_out_out));
  m.impl("_masked_softmax_backward.out", TORCH_FN(functionalization::_masked_softmax_backward_out_out));
  m.impl("put.out", TORCH_FN(functionalization::put_out_out));
  m.impl("put_", TORCH_FN(functionalization::put_));
  m.impl("index_add.out", TORCH_FN(functionalization::index_add_out_out));
  m.impl("index_add_", TORCH_FN(functionalization::index_add_));
  m.impl("index_fill.int_Scalar_out", TORCH_FN(functionalization::index_fill_out_int_Scalar_out));
  m.impl("index_fill_.int_Scalar", TORCH_FN(functionalization::index_fill__int_Scalar));
  m.impl("index_fill.int_Tensor_out", TORCH_FN(functionalization::index_fill_out_int_Tensor_out));
  m.impl("index_fill_.int_Tensor", TORCH_FN(functionalization::index_fill__int_Tensor));
  m.impl("scatter.src_out", TORCH_FN(functionalization::scatter_out_src_out));
  m.impl("scatter_.src", TORCH_FN(functionalization::scatter__src));
  m.impl("scatter.value_out", TORCH_FN(functionalization::scatter_out_value_out));
  m.impl("scatter_.value", TORCH_FN(functionalization::scatter__value));
  m.impl("scatter.reduce_out", TORCH_FN(functionalization::scatter_out_reduce_out));
  m.impl("scatter_.reduce", TORCH_FN(functionalization::scatter__reduce));
  m.impl("scatter.value_reduce_out", TORCH_FN(functionalization::scatter_out_value_reduce_out));
  m.impl("scatter_.value_reduce", TORCH_FN(functionalization::scatter__value_reduce));
  m.impl("scatter_add.out", TORCH_FN(functionalization::scatter_add_out_out));
  m.impl("scatter_add_", TORCH_FN(functionalization::scatter_add_));
  m.impl("__lshift__.Scalar_out", TORCH_FN(functionalization::__lshift___out_Scalar_out));
  m.impl("__ilshift__.Scalar", TORCH_FN(functionalization::__ilshift___Scalar));
  m.impl("__lshift__.Tensor_out", TORCH_FN(functionalization::__lshift___out_Tensor_out));
  m.impl("__ilshift__.Tensor", TORCH_FN(functionalization::__ilshift___Tensor));
  m.impl("lt.Scalar_out", TORCH_FN(functionalization::lt_out_Scalar_out));
  m.impl("lt_.Scalar", TORCH_FN(functionalization::lt__Scalar));
  m.impl("lt.Tensor_out", TORCH_FN(functionalization::lt_out_Tensor_out));
  m.impl("lt_.Tensor", TORCH_FN(functionalization::lt__Tensor));
  m.impl("take.out", TORCH_FN(functionalization::take_out_out));
  m.impl("nonzero_static.out", TORCH_FN(functionalization::nonzero_static_out_out));
  m.impl("gather.out", TORCH_FN(functionalization::gather_out_out));
  m.impl("cholesky.out", TORCH_FN(functionalization::cholesky_out_out));
  m.impl("_cholesky_solve_helper.out", TORCH_FN(functionalization::_cholesky_solve_helper_out_out));
  m.impl("polygamma.out", TORCH_FN(functionalization::polygamma_out_out));
  m.impl("igamma.out", TORCH_FN(functionalization::igamma_out_out));
  m.impl("igamma_", TORCH_FN(functionalization::igamma_));
  m.impl("fmin.out", TORCH_FN(functionalization::fmin_out_out));
  m.impl("max.unary_out", TORCH_FN(functionalization::max_out_unary_out));
  m.impl("fmax.out", TORCH_FN(functionalization::fmax_out_out));
  m.impl("maximum.out", TORCH_FN(functionalization::maximum_out_out));
  m.impl("_amp_foreach_non_finite_check_and_unscale.out", TORCH_FN(functionalization::_amp_foreach_non_finite_check_and_unscale_out_out));
  m.impl("_amp_foreach_non_finite_check_and_unscale_", TORCH_FN(functionalization::_amp_foreach_non_finite_check_and_unscale_));
  m.impl("_foreach_sub.Scalar_out", TORCH_FN(functionalization::_foreach_sub_out_Scalar_out));
  m.impl("_foreach_sub_.Scalar", TORCH_FN(functionalization::_foreach_sub__Scalar));
  m.impl("_foreach_sub.List_out", TORCH_FN(functionalization::_foreach_sub_out_List_out));
  m.impl("_foreach_sub_.List", TORCH_FN(functionalization::_foreach_sub__List));
  m.impl("_foreach_sub.ScalarList_out", TORCH_FN(functionalization::_foreach_sub_out_ScalarList_out));
  m.impl("_foreach_sub_.ScalarList", TORCH_FN(functionalization::_foreach_sub__ScalarList));
  m.impl("_foreach_maximum.Scalar_out", TORCH_FN(functionalization::_foreach_maximum_out_Scalar_out));
  m.impl("_foreach_maximum_.Scalar", TORCH_FN(functionalization::_foreach_maximum__Scalar));
  m.impl("_foreach_maximum.List_out", TORCH_FN(functionalization::_foreach_maximum_out_List_out));
  m.impl("_foreach_maximum_.List", TORCH_FN(functionalization::_foreach_maximum__List));
  m.impl("_foreach_maximum.ScalarList_out", TORCH_FN(functionalization::_foreach_maximum_out_ScalarList_out));
  m.impl("_foreach_maximum_.ScalarList", TORCH_FN(functionalization::_foreach_maximum__ScalarList));
  m.impl("_foreach_abs.out", TORCH_FN(functionalization::_foreach_abs_out_out));
  m.impl("_foreach_abs_", TORCH_FN(functionalization::_foreach_abs_));
  m.impl("_foreach_acos.out", TORCH_FN(functionalization::_foreach_acos_out_out));
  m.impl("_foreach_acos_", TORCH_FN(functionalization::_foreach_acos_));
  m.impl("_foreach_cos.out", TORCH_FN(functionalization::_foreach_cos_out_out));
  m.impl("_foreach_cos_", TORCH_FN(functionalization::_foreach_cos_));
  m.impl("_foreach_exp.out", TORCH_FN(functionalization::_foreach_exp_out_out));
  m.impl("_foreach_exp_", TORCH_FN(functionalization::_foreach_exp_));
  m.impl("_foreach_floor.out", TORCH_FN(functionalization::_foreach_floor_out_out));
  m.impl("_foreach_floor_", TORCH_FN(functionalization::_foreach_floor_));
  m.impl("_foreach_log10.out", TORCH_FN(functionalization::_foreach_log10_out_out));
  m.impl("_foreach_log10_", TORCH_FN(functionalization::_foreach_log10_));
  m.impl("_foreach_neg.out", TORCH_FN(functionalization::_foreach_neg_out_out));
  m.impl("_foreach_neg_", TORCH_FN(functionalization::_foreach_neg_));
  m.impl("_foreach_norm.Scalar_out", TORCH_FN(functionalization::_foreach_norm_out_Scalar_out));
  m.impl("_foreach_sigmoid.out", TORCH_FN(functionalization::_foreach_sigmoid_out_out));
  m.impl("_foreach_sigmoid_", TORCH_FN(functionalization::_foreach_sigmoid_));
  m.impl("_foreach_sign.out", TORCH_FN(functionalization::_foreach_sign_out_out));
  m.impl("_foreach_sign_", TORCH_FN(functionalization::_foreach_sign_));
  m.impl("_foreach_sqrt.out", TORCH_FN(functionalization::_foreach_sqrt_out_out));
  m.impl("_foreach_sqrt_", TORCH_FN(functionalization::_foreach_sqrt_));
  m.impl("_foreach_tan.out", TORCH_FN(functionalization::_foreach_tan_out_out));
  m.impl("_foreach_tan_", TORCH_FN(functionalization::_foreach_tan_));
  m.impl("searchsorted.Tensor_out", TORCH_FN(functionalization::searchsorted_out_Tensor_out));
  m.impl("searchsorted.Scalar_out", TORCH_FN(functionalization::searchsorted_out_Scalar_out));
  m.impl("mse_loss_backward.grad_input", TORCH_FN(functionalization::mse_loss_backward_out_grad_input));
  m.impl("smooth_l1_loss_backward.grad_input", TORCH_FN(functionalization::smooth_l1_loss_backward_out_grad_input));
  m.impl("huber_loss_backward.out", TORCH_FN(functionalization::huber_loss_backward_out_out));
  m.impl("elu_backward.grad_input", TORCH_FN(functionalization::elu_backward_out_grad_input));
  m.impl("glu_jvp.out", TORCH_FN(functionalization::glu_jvp_out_out));
  m.impl("hardsigmoid_backward.grad_input", TORCH_FN(functionalization::hardsigmoid_backward_out_grad_input));
  m.impl("log_sigmoid_forward.output", TORCH_FN(functionalization::log_sigmoid_forward_out_output));
  m.impl("rrelu_with_noise.out", TORCH_FN(functionalization::rrelu_with_noise_out_out));
  m.impl("rrelu_with_noise_", TORCH_FN(functionalization::rrelu_with_noise_));
  m.impl("rrelu_with_noise_backward.out", TORCH_FN(functionalization::rrelu_with_noise_backward_out_out));
  m.impl("softplus_backward.grad_input", TORCH_FN(functionalization::softplus_backward_out_grad_input));
  m.impl("mkldnn_adaptive_avg_pool2d.out", TORCH_FN(functionalization::mkldnn_adaptive_avg_pool2d_out_out));
  m.impl("adaptive_max_pool2d.out", TORCH_FN(functionalization::adaptive_max_pool2d_out_out));
  m.impl("adaptive_max_pool3d.out", TORCH_FN(functionalization::adaptive_max_pool3d_out_out));
  m.impl("avg_pool2d_backward.grad_input", TORCH_FN(functionalization::avg_pool2d_backward_out_grad_input));
  m.impl("max_pool2d_with_indices.out", TORCH_FN(functionalization::max_pool2d_with_indices_out_out));
  m.impl("upsample_linear1d.out", TORCH_FN(functionalization::upsample_linear1d_out_out));
  m.impl("upsample_linear1d_backward.grad_input", TORCH_FN(functionalization::upsample_linear1d_backward_out_grad_input));
  m.impl("upsample_bicubic2d.out", TORCH_FN(functionalization::upsample_bicubic2d_out_out));
  m.impl("upsample_bicubic2d_backward.grad_input", TORCH_FN(functionalization::upsample_bicubic2d_backward_out_grad_input));
  m.impl("_upsample_bicubic2d_aa_backward.grad_input", TORCH_FN(functionalization::_upsample_bicubic2d_aa_backward_out_grad_input));
  m.impl("upsample_nearest1d.out", TORCH_FN(functionalization::upsample_nearest1d_out_out));
  m.impl("_upsample_nearest_exact1d.out", TORCH_FN(functionalization::_upsample_nearest_exact1d_out_out));
  m.impl("upsample_nearest1d_backward.grad_input", TORCH_FN(functionalization::upsample_nearest1d_backward_out_grad_input));
  m.impl("upsample_nearest3d.out", TORCH_FN(functionalization::upsample_nearest3d_out_out));
  m.impl("slow_conv_transpose3d.out", TORCH_FN(functionalization::slow_conv_transpose3d_out_out));
  m.impl("isposinf.out", TORCH_FN(functionalization::isposinf_out_out));
  m.impl("special_entr.out", TORCH_FN(functionalization::special_entr_out_out));
  m.impl("special_zeta.out", TORCH_FN(functionalization::special_zeta_out_out));
  m.impl("special_zeta.self_scalar_out", TORCH_FN(functionalization::special_zeta_out_self_scalar_out));
  m.impl("special_zeta.other_scalar_out", TORCH_FN(functionalization::special_zeta_out_other_scalar_out));
  m.impl("special_i0e.out", TORCH_FN(functionalization::special_i0e_out_out));
  m.impl("fft_rfftfreq.out", TORCH_FN(functionalization::fft_rfftfreq_out_out));
  m.impl("linalg_lu.out", TORCH_FN(functionalization::linalg_lu_out_out));
  m.impl("_linalg_det.result", TORCH_FN(functionalization::_linalg_det_out_result));
  m.impl("linalg_eig.out", TORCH_FN(functionalization::linalg_eig_out_out));
  m.impl("_linalg_svd.U", TORCH_FN(functionalization::_linalg_svd_out_U));
  m.impl("linalg_pinv.atol_rtol_tensor_out", TORCH_FN(functionalization::linalg_pinv_out_atol_rtol_tensor_out));
  m.impl("linalg_qr.out", TORCH_FN(functionalization::linalg_qr_out_out));
  m.impl("_test_optional_intlist.out", TORCH_FN(functionalization::_test_optional_intlist_out_out));
  m.impl("_fw_primal_copy.out", TORCH_FN(functionalization::_fw_primal_copy_out_out));
  m.impl("as_strided_copy.out", TORCH_FN(functionalization::as_strided_copy_out_out));
  m.impl("expand_copy.out", TORCH_FN(functionalization::expand_copy_out_out));
  m.impl("_reshape_alias_copy.out", TORCH_FN(functionalization::_reshape_alias_copy_out_out));
  m.impl("select_copy.int_out", TORCH_FN(functionalization::select_copy_out_int_out));
  m.impl("split_with_sizes_copy.out", TORCH_FN(functionalization::split_with_sizes_copy_out_out));
  m.impl("squeeze_copy.out", TORCH_FN(functionalization::squeeze_copy_out_out));
  m.impl("squeeze_copy.dim_out", TORCH_FN(functionalization::squeeze_copy_out_dim_out));
  m.impl("squeeze_copy.dims_out", TORCH_FN(functionalization::squeeze_copy_out_dims_out));
  m.impl("_indices_copy.out", TORCH_FN(functionalization::_indices_copy_out_out));
  m.impl("_values_copy.out", TORCH_FN(functionalization::_values_copy_out_out));
  m.impl("crow_indices_copy.out", TORCH_FN(functionalization::crow_indices_copy_out_out));
  m.impl("col_indices_copy.out", TORCH_FN(functionalization::col_indices_copy_out_out));
  m.impl("unbind_copy.int_out", TORCH_FN(functionalization::unbind_copy_out_int_out));
  m.impl("view_copy.out", TORCH_FN(functionalization::view_copy_out_out));
  m.impl("view_copy.dtype_out", TORCH_FN(functionalization::view_copy_out_dtype_out));
  m.impl("alias_copy.out", TORCH_FN(functionalization::alias_copy_out_out));
  m.impl("special_airy_ai.out", TORCH_FN(functionalization::special_airy_ai_out_out));
  m.impl("special_bessel_j0.out", TORCH_FN(functionalization::special_bessel_j0_out_out));
  m.impl("special_chebyshev_polynomial_v.out", TORCH_FN(functionalization::special_chebyshev_polynomial_v_out_out));
  m.impl("special_chebyshev_polynomial_v.x_scalar_out", TORCH_FN(functionalization::special_chebyshev_polynomial_v_out_x_scalar_out));
  m.impl("special_chebyshev_polynomial_v.n_scalar_out", TORCH_FN(functionalization::special_chebyshev_polynomial_v_out_n_scalar_out));
  m.impl("special_chebyshev_polynomial_w.out", TORCH_FN(functionalization::special_chebyshev_polynomial_w_out_out));
  m.impl("special_chebyshev_polynomial_w.x_scalar_out", TORCH_FN(functionalization::special_chebyshev_polynomial_w_out_x_scalar_out));
  m.impl("special_chebyshev_polynomial_w.n_scalar_out", TORCH_FN(functionalization::special_chebyshev_polynomial_w_out_n_scalar_out));
  m.impl("special_hermite_polynomial_he.out", TORCH_FN(functionalization::special_hermite_polynomial_he_out_out));
  m.impl("special_hermite_polynomial_he.x_scalar_out", TORCH_FN(functionalization::special_hermite_polynomial_he_out_x_scalar_out));
  m.impl("special_hermite_polynomial_he.n_scalar_out", TORCH_FN(functionalization::special_hermite_polynomial_he_out_n_scalar_out));
  m.impl("special_laguerre_polynomial_l.out", TORCH_FN(functionalization::special_laguerre_polynomial_l_out_out));
  m.impl("special_laguerre_polynomial_l.x_scalar_out", TORCH_FN(functionalization::special_laguerre_polynomial_l_out_x_scalar_out));
  m.impl("special_laguerre_polynomial_l.n_scalar_out", TORCH_FN(functionalization::special_laguerre_polynomial_l_out_n_scalar_out));
  m.impl("special_shifted_chebyshev_polynomial_v.out", TORCH_FN(functionalization::special_shifted_chebyshev_polynomial_v_out_out));
  m.impl("special_shifted_chebyshev_polynomial_v.x_scalar_out", TORCH_FN(functionalization::special_shifted_chebyshev_polynomial_v_out_x_scalar_out));
  m.impl("special_shifted_chebyshev_polynomial_v.n_scalar_out", TORCH_FN(functionalization::special_shifted_chebyshev_polynomial_v_out_n_scalar_out));
  m.impl("_fused_adamw.out", TORCH_FN(functionalization::_fused_adamw_out_out));
  m.impl("_fused_adamw_", TORCH_FN(functionalization::_fused_adamw_));
  m.impl("_fused_adamw.tensor_lr_out", TORCH_FN(functionalization::_fused_adamw_out_tensor_lr_out));
  m.impl("_fused_adamw_.tensor_lr", TORCH_FN(functionalization::_fused_adamw__tensor_lr));
  m.impl("_fw_primal", TORCH_FN(functionalization::_fw_primal));
  m.impl("_make_dual", TORCH_FN(functionalization::_make_dual));
  m.impl("view_as_complex", TORCH_FN(functionalization::view_as_complex));
  m.impl("expand", TORCH_FN(functionalization::expand));
  m.impl("transpose.int", TORCH_FN(functionalization::transpose_int));
  m.impl("transpose_", TORCH_FN(functionalization::transpose_));
  m.impl("_nested_view_from_buffer", TORCH_FN(functionalization::_nested_view_from_buffer));
  m.impl("unsqueeze", TORCH_FN(functionalization::unsqueeze));
  m.impl("unsqueeze_", TORCH_FN(functionalization::unsqueeze_));
  m.impl("_values", TORCH_FN(functionalization::_values));
  m.impl("ccol_indices", TORCH_FN(functionalization::ccol_indices));
  m.impl("view", TORCH_FN(functionalization::view));
  m.impl("view.dtype", TORCH_FN(functionalization::view_dtype));;
}

}  // namespace

} // namespace at
