// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// an external backend might generate file within its code tree
// and check all the source files within the tree with clang-format.
// so, disable it since the backend might have a different config.
// clang-format off

// NOTE: This condition is true for all PyTorch internal libraries, it
//       just excludes external projects such as torch_xla which
//       re-use some of the PyTorch codegen machinery.
#if defined(CAFFE2_BUILD_MAIN_LIB)        || \
    defined(TORCH_CUDA_BUILD_MAIN_LIB)    || \
    defined(TORCH_HIP_BUILD_MAIN_LIB)     || \
    defined(TORCH_CUDA_CU_BUILD_MAIN_LIB) || \
    defined(TORCH_CUDA_CPP_BUILD_MAIN_LIB)
#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
#endif

// @generated by torchgen/gen.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/Tensor.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>


#include <ATen/NativeFunctions.h>
#include <ATen/Functions.h>
#include <ATen/NativeFunctions.h>

// See template file RegisterDispatchDefinitions.ini
namespace at {
// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {
void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      // TODO: avoid the redispatch here
      out.as_strided_(sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}
void check_inplace(const Tensor &self, IntArrayRef sizes, const TensorOptions &options) {
  // These checks are needed on those operators that:
  //   1) don't use 'TensorIterator' (e.g. 'addmm' and 'baddbmm')
  //   2) have particular typing rules (e.g. 'cumsum' and 'cumprod')
  // For other operators (e.g. 'add'), 'TensorIterator' already checks
  // these things separately.
  TORCH_CHECK(options.dtype() == self.dtype(),
      "Bad in-place call: ",
      "input tensor dtype ", self.dtype(), " and output tensor dtype ", options.dtype(), " should match");
  TORCH_CHECK(options.device() == self.device(),
      "Bad in-place call: ",
      "input tensor device ", self.device(), " and output tensor device ", options.device(), " should match");
  TORCH_CHECK(sizes == self.sizes(),
      "Bad in-place call: ",
      "input tensor size ", self.sizes(), " and output tensor size ", sizes, " should match");
}
namespace {
at::Tensor wrapper_MkldnnCPU_Tensor_add(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_add(self, other, alpha);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU_out_add_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_add_out(self, other, alpha, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU_Tensor_add_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_add_(self, other, alpha);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU__copy_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return at::native::copy_mkldnn_(self, src, non_blocking);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU_memory_format_empty(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::empty_mkldnn(C10_AS_INTARRAYREF_SLOW(size), dtype, layout, device, pin_memory, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__mkldnn_linear(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_linear(self, weight, bias);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__mkldnn_linear_backward_input(at::IntArrayRef input_size, const at::Tensor & grad_output, const at::Tensor & weight) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_linear_backward_input(input_size, grad_output, weight);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_MkldnnCPU__mkldnn_linear_backward_weights(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, bool bias_defined) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_linear_backward_weights(grad_output, input, weight, bias_defined);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_MkldnnCPU__mkldnn_linear_backward(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_linear_backward(self, grad_output, weight, output_mask);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__mkldnn_max_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_max_pool2d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__mkldnn_max_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_max_pool2d_backward(grad_output, output, input, kernel_size, stride, padding, dilation, ceil_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__mkldnn_max_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_max_pool3d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__mkldnn_max_pool3d_backward(const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_max_pool3d_backward(grad_output, output, input, kernel_size, stride, padding, dilation, ceil_mode);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrapper_MkldnnCPU__mkldnn_rnn_layer(const at::Tensor & input, const at::Tensor & weight0, const at::Tensor & weight1, const at::Tensor & weight2, const at::Tensor & weight3, const at::Tensor & hx_, const at::Tensor & cx_, bool reverse, at::IntArrayRef batch_sizes, int64_t mode, int64_t hidden_size, int64_t num_layers, bool has_biases, bool bidirectional, bool batch_first, bool train) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_rnn_layer(input, weight0, weight1, weight2, weight3, hx_, cx_, reverse, batch_sizes, mode, hidden_size, num_layers, has_biases, bidirectional, batch_first, train);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU_Tensor_mul(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_mul(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU_out_mul_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_mul_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU_Tensor_mul_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_mul_(self, other);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_MkldnnCPU__native_batch_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_MkldnnCPU___native_batch_norm_legit(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, at::Tensor & running_mean, at::Tensor & running_var, bool training, double momentum, double eps) {
    // No device check
  // DeviceGuard omitted
  return at::native::_mkldnn_batch_norm_legit(input, weight, bias, running_mean, running_var, training, momentum, eps);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_MkldnnCPU_no_stats__native_batch_norm_legit(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, bool training, double momentum, double eps) {
    // No device check
  // DeviceGuard omitted
  return at::native::_mkldnn_batch_norm_legit_no_stats(input, weight, bias, training, momentum, eps);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_MkldnnCPU__native_batch_norm_backward(const at::Tensor & grad_out, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_batch_norm_backward(grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU___mkldnn_reshape(const at::Tensor & self, at::IntArrayRef shape) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_reshape(self, shape);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__relu(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_relu(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU__relu_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_relu_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU___prelu_kernel(const at::Tensor & self, const at::Tensor & weight) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_prelu(self, weight);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_MkldnnCPU___prelu_kernel_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_prelu_backward(grad_output, self, weight);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__gelu(const at::Tensor & self, c10::string_view approximate) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_gelu(self, approximate);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__gelu_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::string_view approximate) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_gelu_backward(grad_output, self, approximate);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__sigmoid(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_sigmoid(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU__sigmoid_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_sigmoid_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU___softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_softmax(self, dim, half_to_float);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__tanh(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_tanh(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU__tanh_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_tanh_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__threshold_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_relu_backward(grad_output, self, threshold);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU___mkldnn_transpose(const at::Tensor & self, int64_t dim0, int64_t dim1) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_transpose(self, dim0, dim1);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU___mkldnn_transpose_(at::Tensor & self, int64_t dim0, int64_t dim1) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_transpose_(self, dim0, dim1);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_clone(self, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU__zero_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_zero_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU___to_dense(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<bool> masked_grad) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_to_dense(self, dtype, masked_grad);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__mkldnn_reorder_conv2d_weight(const at::Tensor & self, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::OptionalSymIntArrayRef input_size) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_reorder_conv2d_weight(self, C10_AS_INTARRAYREF_SLOW(padding), C10_AS_INTARRAYREF_SLOW(stride), C10_AS_INTARRAYREF_SLOW(dilation), groups.guard_int(__FILE__, __LINE__), input_size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*input_size)) : c10::nullopt);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__mkldnn_reorder_conv3d_weight(const at::Tensor & self, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_reorder_conv3d_weight(self, C10_AS_INTARRAYREF_SLOW(padding), C10_AS_INTARRAYREF_SLOW(stride), C10_AS_INTARRAYREF_SLOW(dilation), groups.guard_int(__FILE__, __LINE__));
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__view(const at::Tensor & self, c10::SymIntArrayRef size) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_view(self, C10_AS_INTARRAYREF_SLOW(size));
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU_out_adaptive_avg_pool2d_out(const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_adaptive_avg_pool2d_out_stub(self, C10_AS_INTARRAYREF_SLOW(output_size), out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__mkldnn_adaptive_avg_pool2d(const at::Tensor & self, at::IntArrayRef output_size) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_adaptive_avg_pool2d(self, output_size);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU_out_mkldnn_adaptive_avg_pool2d_out(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_adaptive_avg_pool2d_out(self, output_size, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__mkldnn_adaptive_avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_adaptive_avg_pool2d_backward(grad_output, self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__avg_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU_out_avg_pool2d_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_avg_pool2d_out(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_avg_pool2d_backward(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU_grad_input_avg_pool2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_avg_pool2d_backward_out(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__avg_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_avg_pool3d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU_out_avg_pool3d_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_avg_pool3d_out(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_MkldnnCPU__avg_pool3d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_avg_pool3d_backward(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_MkldnnCPU_grad_input_avg_pool3d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
    // No device check
  // DeviceGuard omitted
  return at::native::mkldnn_avg_pool3d_backward_out(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input);
}
} // anonymous namespace
TORCH_LIBRARY_IMPL(aten, MkldnnCPU, m) {
    m.impl("add.Tensor",
TORCH_FN(wrapper_MkldnnCPU_Tensor_add));
m.impl("add.out",
TORCH_FN(wrapper_MkldnnCPU_out_add_out));
m.impl("add_.Tensor",
TORCH_FN(wrapper_MkldnnCPU_Tensor_add_));
m.impl("copy_",
TORCH_FN(wrapper_MkldnnCPU__copy_));
m.impl("empty.memory_format",
TORCH_FN(wrapper_MkldnnCPU_memory_format_empty));
m.impl("mkldnn_linear",
TORCH_FN(wrapper_MkldnnCPU__mkldnn_linear));
m.impl("mkldnn_linear_backward_input",
TORCH_FN(wrapper_MkldnnCPU__mkldnn_linear_backward_input));
m.impl("mkldnn_linear_backward_weights",
TORCH_FN(wrapper_MkldnnCPU__mkldnn_linear_backward_weights));
m.impl("mkldnn_linear_backward",
TORCH_FN(wrapper_MkldnnCPU__mkldnn_linear_backward));
m.impl("mkldnn_max_pool2d",
TORCH_FN(wrapper_MkldnnCPU__mkldnn_max_pool2d));
m.impl("mkldnn_max_pool2d_backward",
TORCH_FN(wrapper_MkldnnCPU__mkldnn_max_pool2d_backward));
m.impl("mkldnn_max_pool3d",
TORCH_FN(wrapper_MkldnnCPU__mkldnn_max_pool3d));
m.impl("mkldnn_max_pool3d_backward",
TORCH_FN(wrapper_MkldnnCPU__mkldnn_max_pool3d_backward));
m.impl("mkldnn_rnn_layer",
TORCH_FN(wrapper_MkldnnCPU__mkldnn_rnn_layer));
m.impl("mul.Tensor",
TORCH_FN(wrapper_MkldnnCPU_Tensor_mul));
m.impl("mul.out",
TORCH_FN(wrapper_MkldnnCPU_out_mul_out));
m.impl("mul_.Tensor",
TORCH_FN(wrapper_MkldnnCPU_Tensor_mul_));
m.impl("native_batch_norm",
TORCH_FN(wrapper_MkldnnCPU__native_batch_norm));
m.impl("_native_batch_norm_legit",
TORCH_FN(wrapper_MkldnnCPU___native_batch_norm_legit));
m.impl("_native_batch_norm_legit.no_stats",
TORCH_FN(wrapper_MkldnnCPU_no_stats__native_batch_norm_legit));
m.impl("native_batch_norm_backward",
TORCH_FN(wrapper_MkldnnCPU__native_batch_norm_backward));
m.impl("_mkldnn_reshape",
TORCH_FN(wrapper_MkldnnCPU___mkldnn_reshape));
m.impl("relu",
TORCH_FN(wrapper_MkldnnCPU__relu));
m.impl("relu_",
TORCH_FN(wrapper_MkldnnCPU__relu_));
m.impl("_prelu_kernel",
TORCH_FN(wrapper_MkldnnCPU___prelu_kernel));
m.impl("_prelu_kernel_backward",
TORCH_FN(wrapper_MkldnnCPU___prelu_kernel_backward));
m.impl("gelu",
TORCH_FN(wrapper_MkldnnCPU__gelu));
m.impl("gelu_backward",
TORCH_FN(wrapper_MkldnnCPU__gelu_backward));
m.impl("sigmoid",
TORCH_FN(wrapper_MkldnnCPU__sigmoid));
m.impl("sigmoid_",
TORCH_FN(wrapper_MkldnnCPU__sigmoid_));
m.impl("_softmax",
TORCH_FN(wrapper_MkldnnCPU___softmax));
m.impl("tanh",
TORCH_FN(wrapper_MkldnnCPU__tanh));
m.impl("tanh_",
TORCH_FN(wrapper_MkldnnCPU__tanh_));
m.impl("threshold_backward",
TORCH_FN(wrapper_MkldnnCPU__threshold_backward));
m.impl("_mkldnn_transpose",
TORCH_FN(wrapper_MkldnnCPU___mkldnn_transpose));
m.impl("_mkldnn_transpose_",
TORCH_FN(wrapper_MkldnnCPU___mkldnn_transpose_));
m.impl("clone",
TORCH_FN(wrapper_MkldnnCPU__clone));
m.impl("zero_",
TORCH_FN(wrapper_MkldnnCPU__zero_));
m.impl("_to_dense",
TORCH_FN(wrapper_MkldnnCPU___to_dense));
m.impl("mkldnn_reorder_conv2d_weight",
TORCH_FN(wrapper_MkldnnCPU__mkldnn_reorder_conv2d_weight));
m.impl("mkldnn_reorder_conv3d_weight",
TORCH_FN(wrapper_MkldnnCPU__mkldnn_reorder_conv3d_weight));
m.impl("view",
TORCH_FN(wrapper_MkldnnCPU__view));
m.impl("adaptive_avg_pool2d.out",
TORCH_FN(wrapper_MkldnnCPU_out_adaptive_avg_pool2d_out));
m.impl("mkldnn_adaptive_avg_pool2d",
TORCH_FN(wrapper_MkldnnCPU__mkldnn_adaptive_avg_pool2d));
m.impl("mkldnn_adaptive_avg_pool2d.out",
TORCH_FN(wrapper_MkldnnCPU_out_mkldnn_adaptive_avg_pool2d_out));
m.impl("mkldnn_adaptive_avg_pool2d_backward",
TORCH_FN(wrapper_MkldnnCPU__mkldnn_adaptive_avg_pool2d_backward));
m.impl("avg_pool2d",
TORCH_FN(wrapper_MkldnnCPU__avg_pool2d));
m.impl("avg_pool2d.out",
TORCH_FN(wrapper_MkldnnCPU_out_avg_pool2d_out));
m.impl("avg_pool2d_backward",
TORCH_FN(wrapper_MkldnnCPU__avg_pool2d_backward));
m.impl("avg_pool2d_backward.grad_input",
TORCH_FN(wrapper_MkldnnCPU_grad_input_avg_pool2d_backward_out));
m.impl("avg_pool3d",
TORCH_FN(wrapper_MkldnnCPU__avg_pool3d));
m.impl("avg_pool3d.out",
TORCH_FN(wrapper_MkldnnCPU_out_avg_pool3d_out));
m.impl("avg_pool3d_backward",
TORCH_FN(wrapper_MkldnnCPU__avg_pool3d_backward));
m.impl("avg_pool3d_backward.grad_input",
TORCH_FN(wrapper_MkldnnCPU_grad_input_avg_pool3d_backward_out));
};
} // anonymous namespace
namespace mkldnncpu {
at::Tensor add(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_MkldnnCPU_Tensor_add(self, other, alpha);
}
at::Tensor & add_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_MkldnnCPU_out_add_out(self, other, alpha, out);
}
at::Tensor & add_outf(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
return wrapper_MkldnnCPU_out_add_out(self, other, alpha, out);
}
at::Tensor & add_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_MkldnnCPU_Tensor_add_(self, other, alpha);
}
at::Tensor & copy_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
return wrapper_MkldnnCPU__copy_(self, src, non_blocking);
}
at::Tensor empty(at::IntArrayRef size, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_MkldnnCPU_memory_format_empty(c10::fromIntArrayRefSlow(size), c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor empty(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_MkldnnCPU_memory_format_empty(c10::fromIntArrayRefSlow(size), dtype, layout, device, pin_memory, memory_format);
}
at::Tensor empty_symint(c10::SymIntArrayRef size, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_MkldnnCPU_memory_format_empty(size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor empty_symint(c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_MkldnnCPU_memory_format_empty(size, dtype, layout, device, pin_memory, memory_format);
}
at::Tensor mkldnn_linear(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
return wrapper_MkldnnCPU__mkldnn_linear(self, weight, bias);
}
at::Tensor mkldnn_linear_backward_input(at::IntArrayRef input_size, const at::Tensor & grad_output, const at::Tensor & weight) {
return wrapper_MkldnnCPU__mkldnn_linear_backward_input(input_size, grad_output, weight);
}
::std::tuple<at::Tensor,at::Tensor> mkldnn_linear_backward_weights(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, bool bias_defined) {
return wrapper_MkldnnCPU__mkldnn_linear_backward_weights(grad_output, input, weight, bias_defined);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> mkldnn_linear_backward(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask) {
return wrapper_MkldnnCPU__mkldnn_linear_backward(self, grad_output, weight, output_mask);
}
at::Tensor mkldnn_max_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper_MkldnnCPU__mkldnn_max_pool2d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
at::Tensor mkldnn_max_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper_MkldnnCPU__mkldnn_max_pool2d_backward(grad_output, output, input, kernel_size, stride, padding, dilation, ceil_mode);
}
at::Tensor mkldnn_max_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper_MkldnnCPU__mkldnn_max_pool3d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
at::Tensor mkldnn_max_pool3d_backward(const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper_MkldnnCPU__mkldnn_max_pool3d_backward(grad_output, output, input, kernel_size, stride, padding, dilation, ceil_mode);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> mkldnn_rnn_layer(const at::Tensor & input, const at::Tensor & weight0, const at::Tensor & weight1, const at::Tensor & weight2, const at::Tensor & weight3, const at::Tensor & hx_, const at::Tensor & cx_, bool reverse, at::IntArrayRef batch_sizes, int64_t mode, int64_t hidden_size, int64_t num_layers, bool has_biases, bool bidirectional, bool batch_first, bool train) {
return wrapper_MkldnnCPU__mkldnn_rnn_layer(input, weight0, weight1, weight2, weight3, hx_, cx_, reverse, batch_sizes, mode, hidden_size, num_layers, has_biases, bidirectional, batch_first, train);
}
at::Tensor mul(const at::Tensor & self, const at::Tensor & other) {
return wrapper_MkldnnCPU_Tensor_mul(self, other);
}
at::Tensor & mul_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_MkldnnCPU_out_mul_out(self, other, out);
}
at::Tensor & mul_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_MkldnnCPU_out_mul_out(self, other, out);
}
at::Tensor & mul_(at::Tensor & self, const at::Tensor & other) {
return wrapper_MkldnnCPU_Tensor_mul_(self, other);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_batch_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps) {
return wrapper_MkldnnCPU__native_batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _native_batch_norm_legit(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, at::Tensor & running_mean, at::Tensor & running_var, bool training, double momentum, double eps) {
return wrapper_MkldnnCPU___native_batch_norm_legit(input, weight, bias, running_mean, running_var, training, momentum, eps);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _native_batch_norm_legit(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, bool training, double momentum, double eps) {
return wrapper_MkldnnCPU_no_stats__native_batch_norm_legit(input, weight, bias, training, momentum, eps);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_batch_norm_backward(const at::Tensor & grad_out, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask) {
return wrapper_MkldnnCPU__native_batch_norm_backward(grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
}
at::Tensor _mkldnn_reshape(const at::Tensor & self, at::IntArrayRef shape) {
return wrapper_MkldnnCPU___mkldnn_reshape(self, shape);
}
at::Tensor relu(const at::Tensor & self) {
return wrapper_MkldnnCPU__relu(self);
}
at::Tensor & relu_(at::Tensor & self) {
return wrapper_MkldnnCPU__relu_(self);
}
at::Tensor _prelu_kernel(const at::Tensor & self, const at::Tensor & weight) {
return wrapper_MkldnnCPU___prelu_kernel(self, weight);
}
::std::tuple<at::Tensor,at::Tensor> _prelu_kernel_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight) {
return wrapper_MkldnnCPU___prelu_kernel_backward(grad_output, self, weight);
}
at::Tensor gelu(const at::Tensor & self, c10::string_view approximate) {
return wrapper_MkldnnCPU__gelu(self, approximate);
}
at::Tensor gelu_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::string_view approximate) {
return wrapper_MkldnnCPU__gelu_backward(grad_output, self, approximate);
}
at::Tensor sigmoid(const at::Tensor & self) {
return wrapper_MkldnnCPU__sigmoid(self);
}
at::Tensor & sigmoid_(at::Tensor & self) {
return wrapper_MkldnnCPU__sigmoid_(self);
}
at::Tensor _softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
return wrapper_MkldnnCPU___softmax(self, dim, half_to_float);
}
at::Tensor tanh(const at::Tensor & self) {
return wrapper_MkldnnCPU__tanh(self);
}
at::Tensor & tanh_(at::Tensor & self) {
return wrapper_MkldnnCPU__tanh_(self);
}
at::Tensor threshold_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
return wrapper_MkldnnCPU__threshold_backward(grad_output, self, threshold);
}
at::Tensor _mkldnn_transpose(const at::Tensor & self, int64_t dim0, int64_t dim1) {
return wrapper_MkldnnCPU___mkldnn_transpose(self, dim0, dim1);
}
at::Tensor & _mkldnn_transpose_(at::Tensor & self, int64_t dim0, int64_t dim1) {
return wrapper_MkldnnCPU___mkldnn_transpose_(self, dim0, dim1);
}
at::Tensor clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_MkldnnCPU__clone(self, memory_format);
}
at::Tensor & zero_(at::Tensor & self) {
return wrapper_MkldnnCPU__zero_(self);
}
at::Tensor _to_dense(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<bool> masked_grad) {
return wrapper_MkldnnCPU___to_dense(self, dtype, masked_grad);
}
at::Tensor mkldnn_reorder_conv2d_weight(const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, at::OptionalIntArrayRef input_size) {
return wrapper_MkldnnCPU__mkldnn_reorder_conv2d_weight(self, c10::fromIntArrayRefSlow(padding), c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(dilation), groups, input_size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*input_size)) : c10::nullopt);
}
at::Tensor mkldnn_reorder_conv2d_weight_symint(const at::Tensor & self, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::OptionalSymIntArrayRef input_size) {
return wrapper_MkldnnCPU__mkldnn_reorder_conv2d_weight(self, padding, stride, dilation, groups, input_size);
}
at::Tensor mkldnn_reorder_conv3d_weight(const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
return wrapper_MkldnnCPU__mkldnn_reorder_conv3d_weight(self, c10::fromIntArrayRefSlow(padding), c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(dilation), groups);
}
at::Tensor mkldnn_reorder_conv3d_weight_symint(const at::Tensor & self, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups) {
return wrapper_MkldnnCPU__mkldnn_reorder_conv3d_weight(self, padding, stride, dilation, groups);
}
at::Tensor view(const at::Tensor & self, at::IntArrayRef size) {
return wrapper_MkldnnCPU__view(self, c10::fromIntArrayRefSlow(size));
}
at::Tensor view_symint(const at::Tensor & self, c10::SymIntArrayRef size) {
return wrapper_MkldnnCPU__view(self, size);
}
at::Tensor & adaptive_avg_pool2d_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper_MkldnnCPU_out_adaptive_avg_pool2d_out(self, c10::fromIntArrayRefSlow(output_size), out);
}
at::Tensor & adaptive_avg_pool2d_outf(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out) {
return wrapper_MkldnnCPU_out_adaptive_avg_pool2d_out(self, c10::fromIntArrayRefSlow(output_size), out);
}
at::Tensor & adaptive_avg_pool2d_symint_out(at::Tensor & out, const at::Tensor & self, c10::SymIntArrayRef output_size) {
return wrapper_MkldnnCPU_out_adaptive_avg_pool2d_out(self, output_size, out);
}
at::Tensor & adaptive_avg_pool2d_symint_outf(const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
return wrapper_MkldnnCPU_out_adaptive_avg_pool2d_out(self, output_size, out);
}
at::Tensor mkldnn_adaptive_avg_pool2d(const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper_MkldnnCPU__mkldnn_adaptive_avg_pool2d(self, output_size);
}
at::Tensor & mkldnn_adaptive_avg_pool2d_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper_MkldnnCPU_out_mkldnn_adaptive_avg_pool2d_out(self, output_size, out);
}
at::Tensor & mkldnn_adaptive_avg_pool2d_outf(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out) {
return wrapper_MkldnnCPU_out_mkldnn_adaptive_avg_pool2d_out(self, output_size, out);
}
at::Tensor mkldnn_adaptive_avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self) {
return wrapper_MkldnnCPU__mkldnn_adaptive_avg_pool2d_backward(grad_output, self);
}
at::Tensor avg_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
return wrapper_MkldnnCPU__avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
at::Tensor & avg_pool2d_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
return wrapper_MkldnnCPU_out_avg_pool2d_out(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out);
}
at::Tensor & avg_pool2d_outf(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
return wrapper_MkldnnCPU_out_avg_pool2d_out(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out);
}
at::Tensor avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
return wrapper_MkldnnCPU__avg_pool2d_backward(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
at::Tensor & avg_pool2d_backward_out(at::Tensor & grad_input, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
return wrapper_MkldnnCPU_grad_input_avg_pool2d_backward_out(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input);
}
at::Tensor & avg_pool2d_backward_outf(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
return wrapper_MkldnnCPU_grad_input_avg_pool2d_backward_out(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input);
}
at::Tensor avg_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
return wrapper_MkldnnCPU__avg_pool3d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
at::Tensor & avg_pool3d_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
return wrapper_MkldnnCPU_out_avg_pool3d_out(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out);
}
at::Tensor & avg_pool3d_outf(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
return wrapper_MkldnnCPU_out_avg_pool3d_out(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out);
}
at::Tensor avg_pool3d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
return wrapper_MkldnnCPU__avg_pool3d_backward(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
at::Tensor & avg_pool3d_backward_out(at::Tensor & grad_input, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
return wrapper_MkldnnCPU_grad_input_avg_pool3d_backward_out(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input);
}
at::Tensor & avg_pool3d_backward_outf(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
return wrapper_MkldnnCPU_grad_input_avg_pool3d_backward_out(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input);
}
} // namespace mkldnncpu
} // namespace at
