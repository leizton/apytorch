// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// an external backend might generate file within its code tree
// and check all the source files within the tree with clang-format.
// so, disable it since the backend might have a different config.
// clang-format off

// NOTE: This condition is true for all PyTorch internal libraries, it
//       just excludes external projects such as torch_xla which
//       re-use some of the PyTorch codegen machinery.
#if defined(CAFFE2_BUILD_MAIN_LIB)        || \
    defined(TORCH_CUDA_BUILD_MAIN_LIB)    || \
    defined(TORCH_HIP_BUILD_MAIN_LIB)     || \
    defined(TORCH_CUDA_CU_BUILD_MAIN_LIB) || \
    defined(TORCH_CUDA_CPP_BUILD_MAIN_LIB)
#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
#endif

// @generated by torchgen/gen.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/Tensor.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>


#include <ATen/NativeFunctions.h>
#include <ATen/Functions.h>
#include <ATen/NativeFunctions.h>
#include <ATen/CompositeImplicitAutogradFunctions.h>

// See template file RegisterDispatchDefinitions.ini
namespace at {
// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {
void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      // TODO: avoid the redispatch here
      out.as_strided_(sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}
void check_inplace(const Tensor &self, IntArrayRef sizes, const TensorOptions &options) {
  // These checks are needed on those operators that:
  //   1) don't use 'TensorIterator' (e.g. 'addmm' and 'baddbmm')
  //   2) have particular typing rules (e.g. 'cumsum' and 'cumprod')
  // For other operators (e.g. 'add'), 'TensorIterator' already checks
  // these things separately.
  TORCH_CHECK(options.dtype() == self.dtype(),
      "Bad in-place call: ",
      "input tensor dtype ", self.dtype(), " and output tensor dtype ", options.dtype(), " should match");
  TORCH_CHECK(options.device() == self.device(),
      "Bad in-place call: ",
      "input tensor device ", self.device(), " and output tensor device ", options.device(), " should match");
  TORCH_CHECK(sizes == self.sizes(),
      "Bad in-place call: ",
      "input tensor size ", self.sizes(), " and output tensor size ", sizes, " should match");
}
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___cast_Byte(const at::Tensor & self, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return at::native::_cast_Byte(self, non_blocking);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___cast_Char(const at::Tensor & self, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return at::native::_cast_Char(self, non_blocking);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___cast_Double(const at::Tensor & self, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return at::native::_cast_Double(self, non_blocking);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___cast_Float(const at::Tensor & self, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return at::native::_cast_Float(self, non_blocking);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___cast_Int(const at::Tensor & self, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return at::native::_cast_Int(self, non_blocking);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___cast_Long(const at::Tensor & self, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return at::native::_cast_Long(self, non_blocking);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___cast_Short(const at::Tensor & self, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return at::native::_cast_Short(self, non_blocking);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___cast_Half(const at::Tensor & self, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return at::native::_cast_Half(self, non_blocking);
}
} // anonymous namespace
namespace {
void wrapper_CompositeImplicitAutograd___backward(const at::Tensor & self, at::TensorList inputs, const c10::optional<at::Tensor> & gradient, c10::optional<bool> retain_graph, bool create_graph) {
    // No device check
  // DeviceGuard omitted
  return at::native::_backward(self, inputs, gradient, retain_graph, create_graph);
}
} // anonymous namespace
namespace {
void wrapper_CompositeImplicitAutograd__set_data(at::Tensor & self, const at::Tensor & new_data) {
    // No device check
  // DeviceGuard omitted
  return at::native::set_data(self, new_data);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__data(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::data(self);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd__is_leaf(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::is_leaf(self);
}
} // anonymous namespace
namespace {
int64_t wrapper_CompositeImplicitAutograd__output_nr(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::output_nr(self);
}
} // anonymous namespace
namespace {
int64_t wrapper_CompositeImplicitAutograd___version(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::_version(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__requires_grad_(at::Tensor & self, bool requires_grad) {
    // No device check
  // DeviceGuard omitted
  return at::native::requires_grad_(self, requires_grad);
}
} // anonymous namespace
namespace {
void wrapper_CompositeImplicitAutograd__retain_grad(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::retain_grad(self);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd__retains_grad(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::retains_grad(self);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd___unpack_dual(const at::Tensor & dual, int64_t level) {
    // No device check
  // DeviceGuard omitted
  return at::native::_unpack_dual(dual, level);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__rename_(at::Tensor & self, c10::optional<at::DimnameList> names) {
    // No device check
  // DeviceGuard omitted
  return at::native::rename_(self, names);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__rename(const at::Tensor & self, c10::optional<at::DimnameList> names) {
    // No device check
  // DeviceGuard omitted
  return at::native::rename(self, names);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__align_to(const at::Tensor & self, at::DimnameList names) {
    // No device check
  // DeviceGuard omitted
  return at::native::align_to(self, names);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_ellipsis_idx_align_to(const at::Tensor & self, at::DimnameList order, int64_t ellipsis_idx) {
    // No device check
  // DeviceGuard omitted
  return at::native::align_to(self, order, ellipsis_idx);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__align_as(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::align_as(self, other);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd__align_tensors(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::align_tensors(tensors);
}
} // anonymous namespace
namespace {
void wrapper_CompositeImplicitAutograd___assert_tensor_metadata(const at::Tensor & a, at::OptionalSymIntArrayRef size, at::OptionalSymIntArrayRef stride, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::_assert_tensor_metadata(a, size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*size)) : c10::nullopt, stride.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*stride)) : c10::nullopt, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__refine_names(const at::Tensor & self, at::DimnameList names) {
    // No device check
  // DeviceGuard omitted
  return at::native::refine_names(self, names);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd___use_cudnn_rnn_flatten_weight() {
    // No device check
  // DeviceGuard omitted
  return at::native::_use_cudnn_rnn_flatten_weight();
}
} // anonymous namespace
namespace {
int64_t wrapper_CompositeImplicitAutograd___debug_has_internal_overlap(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::_debug_has_internal_overlap(self);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd___sobol_engine_draw(const at::Tensor & quasi, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sobol_engine_draw(quasi, n, sobolstate, dimension, num_generated, dtype);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd___sobol_engine_ff_(at::Tensor & self, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sobol_engine_ff_(self, n, sobolstate, dimension, num_generated);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd___sobol_engine_scramble_(at::Tensor & self, const at::Tensor & ltm, int64_t dimension) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sobol_engine_scramble_(self, ltm, dimension);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd___sobol_engine_initialize_state_(at::Tensor & self, int64_t dimension) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sobol_engine_initialize_state_(self, dimension);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___reshape_from_tensor(const at::Tensor & self, const at::Tensor & shape) {
    // No device check
  // DeviceGuard omitted
  return at::native::_reshape_from_tensor(self, shape);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___shape_as_tensor(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::_shape_as_tensor(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__dropout(const at::Tensor & input, double p, bool train) {
    // No device check
  // DeviceGuard omitted
  return at::native::dropout(input, p, train);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__dropout_(at::Tensor & self, double p, bool train) {
    // No device check
  // DeviceGuard omitted
  return at::native::dropout_(self, p, train);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__feature_dropout(const at::Tensor & input, double p, bool train) {
    // No device check
  // DeviceGuard omitted
  return at::native::feature_dropout(input, p, train);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__feature_dropout_(at::Tensor & self, double p, bool train) {
    // No device check
  // DeviceGuard omitted
  return at::native::feature_dropout_(self, p, train);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__alpha_dropout(const at::Tensor & input, double p, bool train) {
    // No device check
  // DeviceGuard omitted
  return at::native::alpha_dropout(input, p, train);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__alpha_dropout_(at::Tensor & self, double p, bool train) {
    // No device check
  // DeviceGuard omitted
  return at::native::alpha_dropout_(self, p, train);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__feature_alpha_dropout(const at::Tensor & input, double p, bool train) {
    // No device check
  // DeviceGuard omitted
  return at::native::feature_alpha_dropout(input, p, train);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__feature_alpha_dropout_(at::Tensor & self, double p, bool train) {
    // No device check
  // DeviceGuard omitted
  return at::native::feature_alpha_dropout_(self, p, train);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__absolute(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::absolute(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_absolute_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::absolute_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__absolute_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::absolute_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__chalf(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::chalf(self, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__real(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::real(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__imag(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::imag(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__conj(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::conj(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__conj_physical(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::conj_physical(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__resolve_conj(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::resolve_conj(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__resolve_neg(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::resolve_neg(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__arccos(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::arccos(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_arccos_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::arccos_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__arccos_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::arccos_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__avg_pool1d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad) {
    // No device check
  // DeviceGuard omitted
  return at::native::avg_pool1d(self, kernel_size, stride, padding, ceil_mode, count_include_pad);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__adaptive_avg_pool1d(const at::Tensor & self, at::IntArrayRef output_size) {
    // No device check
  // DeviceGuard omitted
  return at::native::adaptive_avg_pool1d(self, output_size);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__adaptive_max_pool1d(const at::Tensor & self, at::IntArrayRef output_size) {
    // No device check
  // DeviceGuard omitted
  return at::native::adaptive_max_pool1d(self, output_size);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__affine_grid_generator_backward(const at::Tensor & grad, c10::SymIntArrayRef size, bool align_corners) {
    // No device check
  // DeviceGuard omitted
  return at::native::affine_grid_generator_backward(grad, C10_AS_INTARRAYREF_SLOW(size), align_corners);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___test_check_tensor(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::_test_check_tensor(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dimname_all(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::all(self, dim, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_dimname_out_all_out(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::all_out(self, dim, keepdim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dimname_any(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::any(self, dim, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_dimname_out_any_out(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::any_out(self, dim, keepdim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___dim_arange(const at::Tensor & like, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::_dim_arange(like, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__arccosh(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::arccosh(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_arccosh_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::arccosh_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__arccosh_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::arccosh_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__arcsinh(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::arcsinh(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_arcsinh_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::arcsinh_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__arcsinh_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::arcsinh_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__arctanh(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::arctanh(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_arctanh_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::arctanh_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__arctanh_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::arctanh_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__arcsin(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::arcsin(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_arcsin_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::arcsin_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__arcsin_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::arcsin_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__arctan(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::arctan(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_arctan_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::arctan_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__arctan_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::arctan_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__atleast_1d(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::atleast_1d(self);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_Sequence_atleast_1d(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::atleast_1d(tensors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__atleast_2d(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::atleast_2d(self);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_Sequence_atleast_2d(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::atleast_2d(tensors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__atleast_3d(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::atleast_3d(self);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_Sequence_atleast_3d(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::atleast_3d(tensors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__batch_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
    // No device check
  // DeviceGuard omitted
  return at::native::batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,int64_t> wrapper_CompositeImplicitAutograd___batch_norm_impl_index(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
    // No device check
  // DeviceGuard omitted
  return at::native::_batch_norm_impl_index(input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd___batch_norm_impl_index_backward(int64_t impl_index, const at::Tensor & input, const at::Tensor & grad_output, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var_transform, bool train, double eps, ::std::array<bool,3> output_mask, const at::Tensor & reservedSpace) {
    // No device check
  // DeviceGuard omitted
  return at::native::_batch_norm_impl_index_backward(impl_index, input, grad_output, weight, running_mean, running_var, save_mean, save_var_transform, train, eps, output_mask, reservedSpace);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__bilinear(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
    // No device check
  // DeviceGuard omitted
  return at::native::bilinear(input1, input2, weight, bias);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd__broadcast_tensors(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::broadcast_tensors(tensors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__broadcast_to(const at::Tensor & self, c10::SymIntArrayRef size) {
    // No device check
  // DeviceGuard omitted
  return at::native::broadcast_to_symint(self, size);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_names_cat(at::TensorList tensors, at::Dimname dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::cat(tensors, dim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_names_out_cat_out(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::cat_out(tensors, dim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__concat(at::TensorList tensors, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::concat(tensors, dim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_concat_out(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::concat_out(tensors, dim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_names_concat(at::TensorList tensors, at::Dimname dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::concat(tensors, dim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_names_out_concat_out(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::concat_out(tensors, dim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__concatenate(at::TensorList tensors, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::concatenate(tensors, dim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_concatenate_out(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::concatenate_out(tensors, dim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_names_concatenate(at::TensorList tensors, at::Dimname dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::concatenate(tensors, dim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_names_out_concatenate_out(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::concatenate_out(tensors, dim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__chain_matmul(at::TensorList matrices) {
    // No device check
  // DeviceGuard omitted
  return at::native::chain_matmul(matrices);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_chain_matmul_out(at::TensorList matrices, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::chain_matmul_out(matrices, out);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd__unsafe_chunk(const at::Tensor & self, int64_t chunks, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::unsafe_chunk(self, chunks, dim);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd__chunk(const at::Tensor & self, int64_t chunks, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::chunk(self, chunks, dim);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_sections_tensor_split(const at::Tensor & self, c10::SymInt sections, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::tensor_split_sections_symint(self, sections, dim);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_indices_tensor_split(const at::Tensor & self, c10::SymIntArrayRef indices, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::tensor_split_indices_symint(self, indices, dim);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_tensor_indices_or_sections_tensor_split(const at::Tensor & self, const at::Tensor & tensor_indices_or_sections, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::tensor_split(self, tensor_indices_or_sections, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__clip(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
    // No device check
  // DeviceGuard omitted
  return at::native::clip(self, min, max);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_clip_out(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::clip_out(self, min, max, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__clip_(at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
    // No device check
  // DeviceGuard omitted
  return at::native::clip_(self, min, max);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_clip(const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
    // No device check
  // DeviceGuard omitted
  return at::native::clip(self, min, max);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_out_clip_out(const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::clip_out(self, min, max, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_clip_(at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
    // No device check
  // DeviceGuard omitted
  return at::native::clip_(self, min, max);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd__cudnn_is_acceptable(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::cudnn_is_acceptable(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__contiguous(const at::Tensor & self, at::MemoryFormat memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::contiguous(self, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_deprecated__convolution(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, c10::SymInt groups, bool benchmark, bool deterministic, bool cudnn_enabled) {
    // No device check
  // DeviceGuard omitted
  return at::native::_convolution(input, weight, bias, C10_AS_INTARRAYREF_SLOW(stride), C10_AS_INTARRAYREF_SLOW(padding), C10_AS_INTARRAYREF_SLOW(dilation), transposed, output_padding, groups.guard_int(__FILE__, __LINE__), benchmark, deterministic, cudnn_enabled);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___convolution_mode(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    // No device check
  // DeviceGuard omitted
  return at::native::_convolution_mode_symint(input, weight, bias, stride, padding, dilation, groups);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd___convolution_double_backward(const c10::optional<at::Tensor> & ggI, const c10::optional<at::Tensor> & ggW, const c10::optional<at::Tensor> & ggb, const at::Tensor & gO, const at::Tensor & weight, const at::Tensor & self, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array<bool,3> output_mask) {
    // No device check
  // DeviceGuard omitted
  return at::native::_convolution_double_backward(ggI, ggW, ggb, gO, weight, self, C10_AS_INTARRAYREF_SLOW(stride), C10_AS_INTARRAYREF_SLOW(padding), C10_AS_INTARRAYREF_SLOW(dilation), transposed, C10_AS_INTARRAYREF_SLOW(output_padding), groups.guard_int(__FILE__, __LINE__), output_mask);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__conv1d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    // No device check
  // DeviceGuard omitted
  return at::native::conv1d_symint(input, weight, bias, stride, padding, dilation, groups);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__conv2d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    // No device check
  // DeviceGuard omitted
  return at::native::conv2d_symint(input, weight, bias, stride, padding, dilation, groups);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__conv3d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    // No device check
  // DeviceGuard omitted
  return at::native::conv3d_symint(input, weight, bias, stride, padding, dilation, groups);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_padding_conv1d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    // No device check
  // DeviceGuard omitted
  return at::native::conv1d_padding_symint(input, weight, bias, stride, padding, dilation, groups);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_padding_conv2d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    // No device check
  // DeviceGuard omitted
  return at::native::conv2d_padding_symint(input, weight, bias, stride, padding, dilation, groups);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_padding_conv3d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
    // No device check
  // DeviceGuard omitted
  return at::native::conv3d_padding_symint(input, weight, bias, stride, padding, dilation, groups);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__conv_tbc_backward(const at::Tensor & self, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
    // No device check
  // DeviceGuard omitted
  return at::native::conv_tbc_backward(self, input, weight, bias, pad);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__conv_transpose1d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymInt groups, c10::SymIntArrayRef dilation) {
    // No device check
  // DeviceGuard omitted
  return at::native::conv_transpose1d_symint(input, weight, bias, stride, padding, output_padding, groups, dilation);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_input_conv_transpose2d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymInt groups, c10::SymIntArrayRef dilation) {
    // No device check
  // DeviceGuard omitted
  return at::native::conv_transpose2d_symint(input, weight, bias, stride, padding, output_padding, groups, dilation);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_input_conv_transpose3d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymInt groups, c10::SymIntArrayRef dilation) {
    // No device check
  // DeviceGuard omitted
  return at::native::conv_transpose3d_symint(input, weight, bias, stride, padding, output_padding, groups, dilation);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__cosine_embedding_loss(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
    // No device check
  // DeviceGuard omitted
  return at::native::cosine_embedding_loss(input1, input2, target, margin, reduction);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__cov(const at::Tensor & self, int64_t correction, const c10::optional<at::Tensor> & fweights, const c10::optional<at::Tensor> & aweights) {
    // No device check
  // DeviceGuard omitted
  return at::native::cov(self, correction, fweights, aweights);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__corrcoef(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::corrcoef(self);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_dimname_cummax(const at::Tensor & self, at::Dimname dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::cummax(self, dim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_dimname_out_cummax_out(const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
    // No device check
  // DeviceGuard omitted
  return at::native::cummax_out(self, dim, values, indices);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_dimname_cummin(const at::Tensor & self, at::Dimname dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::cummin(self, dim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_dimname_out_cummin_out(const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
    // No device check
  // DeviceGuard omitted
  return at::native::cummin_out(self, dim, values, indices);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__cummaxmin_backward(const at::Tensor & grad, const at::Tensor & input, const at::Tensor & indices, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::cummaxmin_backward(grad, input, indices, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dimname_cumprod(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::cumprod(self, dim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_dimname_out_cumprod_out(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::cumprod_out(self, dim, dtype, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_dimname_cumprod_(at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::cumprod_(self, dim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__cumprod_backward(const at::Tensor & grad, const at::Tensor & input, int64_t dim, const at::Tensor & output) {
    // No device check
  // DeviceGuard omitted
  return at::native::cumprod_backward(grad, input, dim, output);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dimname_cumsum(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::cumsum(self, dim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_dimname_out_cumsum_out(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::cumsum_out(self, dim, dtype, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_dimname_cumsum_(at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::cumsum_(self, dim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_x_cumulative_trapezoid(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::cumulative_trapezoid(y, x, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dx_cumulative_trapezoid(const at::Tensor & y, const at::Scalar & dx, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::cumulative_trapezoid(y, dx, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_IntList_ctc_loss(const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
    // No device check
  // DeviceGuard omitted
  return at::native::ctc_loss(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_ctc_loss(const at::Tensor & log_probs, const at::Tensor & targets, const at::Tensor & input_lengths, const at::Tensor & target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
    // No device check
  // DeviceGuard omitted
  return at::native::ctc_loss(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__diagflat(const at::Tensor & self, int64_t offset) {
    // No device check
  // DeviceGuard omitted
  return at::native::diagflat(self, offset);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_diagonal(const at::Tensor & A, int64_t offset, int64_t dim1, int64_t dim2) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_diagonal(A, offset, dim1, dim2);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Dimname_diagonal(const at::Tensor & self, at::Dimname outdim, at::Dimname dim1, at::Dimname dim2, int64_t offset) {
    // No device check
  // DeviceGuard omitted
  return at::native::diagonal(self, outdim, dim1, dim2, offset);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__fill_diagonal_(at::Tensor & self, const at::Scalar & fill_value, bool wrap) {
    // No device check
  // DeviceGuard omitted
  return at::native::fill_diagonal_(self, fill_value, wrap);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__diff(const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append) {
    // No device check
  // DeviceGuard omitted
  return at::native::diff(self, n, dim, prepend, append);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_diff_out(const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::diff_out(self, n, dim, prepend, append, out);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_scalarint_gradient(const at::Tensor & self, const c10::optional<at::Scalar> & spacing, c10::optional<int64_t> dim, int64_t edge_order) {
    // No device check
  // DeviceGuard omitted
  return at::native::gradient(self, spacing, dim, edge_order);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_scalararray_gradient(const at::Tensor & self, const at::Scalar & spacing, at::IntArrayRef dim, int64_t edge_order) {
    // No device check
  // DeviceGuard omitted
  return at::native::gradient(self, spacing, dim, edge_order);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_array_gradient(const at::Tensor & self, at::IntArrayRef dim, int64_t edge_order) {
    // No device check
  // DeviceGuard omitted
  return at::native::gradient(self, dim, edge_order);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_scalarrayint_gradient(const at::Tensor & self, at::ArrayRef<at::Scalar> spacing, c10::optional<int64_t> dim, int64_t edge_order) {
    // No device check
  // DeviceGuard omitted
  return at::native::gradient(self, spacing, dim, edge_order);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_scalarrayarray_gradient(const at::Tensor & self, at::ArrayRef<at::Scalar> spacing, at::IntArrayRef dim, int64_t edge_order) {
    // No device check
  // DeviceGuard omitted
  return at::native::gradient(self, spacing, dim, edge_order);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_tensorarrayint_gradient(const at::Tensor & self, at::TensorList spacing, c10::optional<int64_t> dim, int64_t edge_order) {
    // No device check
  // DeviceGuard omitted
  return at::native::gradient(self, spacing, dim, edge_order);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_tensorarray_gradient(const at::Tensor & self, at::TensorList spacing, at::IntArrayRef dim, int64_t edge_order) {
    // No device check
  // DeviceGuard omitted
  return at::native::gradient(self, spacing, dim, edge_order);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_divide(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::divide(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_divide_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::divide_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_divide_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::divide_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar_divide(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::divide(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_divide_(at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::divide_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_mode_divide(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::divide(self, other, rounding_mode);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_mode_divide_out(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::divide_out(self, other, rounding_mode, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_mode_divide_(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::divide_(self, other, rounding_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar_mode_divide(const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::divide(self, other, rounding_mode);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_mode_divide_(at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::divide_(self, other, rounding_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_true_divide(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::true_divide(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_true_divide_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::true_divide_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_true_divide_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::true_divide_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar_true_divide(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::true_divide(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_true_divide_(at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::true_divide_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__einsum(c10::string_view equation, at::TensorList tensors, at::OptionalIntArrayRef path) {
    // No device check
  // DeviceGuard omitted
  return at::native::einsum(equation, tensors, path);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__embedding_backward(const at::Tensor & grad, const at::Tensor & indices, c10::SymInt num_weights, c10::SymInt padding_idx, bool scale_grad_by_freq, bool sparse) {
    // No device check
  // DeviceGuard omitted
  return at::native::embedding_backward_symint(grad, indices, num_weights, padding_idx, scale_grad_by_freq, sparse);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__embedding_sparse_backward(const at::Tensor & grad, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
    // No device check
  // DeviceGuard omitted
  return at::native::embedding_sparse_backward(grad, indices, num_weights, padding_idx, scale_grad_by_freq);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd___rowwise_prune(const at::Tensor & weight, const at::Tensor & mask, at::ScalarType compressed_indices_dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::_rowwise_prune(weight, mask, compressed_indices_dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__row_stack(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::row_stack(tensors);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_row_stack_out(at::TensorList tensors, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::row_stack_out(tensors, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__embedding_bag(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset) {
    // No device check
  // DeviceGuard omitted
  return at::native::embedding_bag(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_padding_idx_embedding_bag(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, c10::optional<int64_t> padding_idx) {
    // No device check
  // DeviceGuard omitted
  return at::native::embedding_bag(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___embedding_bag_backward(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, c10::SymInt num_weights, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
    // No device check
  // DeviceGuard omitted
  return at::native::_embedding_bag_backward_symint(grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, sparse, per_sample_weights, padding_idx);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___embedding_bag_sparse_backward(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, c10::SymInt num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
    // No device check
  // DeviceGuard omitted
  return at::native::_embedding_bag_sparse_backward_symint(grad, indices, offsets, offset2bag, bag_size, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_empty_out(c10::SymIntArrayRef size, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::empty_out(C10_AS_INTARRAYREF_SLOW(size), memory_format, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__expand_as(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::expand_as(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_using_ints_flatten(const at::Tensor & self, int64_t start_dim, int64_t end_dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::flatten(self, start_dim, end_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_named_out_dim_flatten(const at::Tensor & self, int64_t start_dim, int64_t end_dim, at::Dimname out_dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::flatten(self, start_dim, end_dim, out_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_using_names_flatten(const at::Tensor & self, at::Dimname start_dim, at::Dimname end_dim, at::Dimname out_dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::flatten(self, start_dim, end_dim, out_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_DimnameList_flatten(const at::Tensor & self, at::DimnameList dims, at::Dimname out_dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::flatten(self, dims, out_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_int_unflatten(const at::Tensor & self, int64_t dim, c10::SymIntArrayRef sizes) {
    // No device check
  // DeviceGuard omitted
  return at::native::unflatten_symint(self, dim, sizes);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Dimname_unflatten(const at::Tensor & self, at::Dimname dim, c10::SymIntArrayRef sizes, at::DimnameList names) {
    // No device check
  // DeviceGuard omitted
  return at::native::unflatten_dimname_symint(self, dim, sizes, names);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__grid_sampler(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    // No device check
  // DeviceGuard omitted
  return at::native::grid_sampler(input, grid, interpolation_mode, padding_mode, align_corners);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd___grid_sampler_2d_cpu_fallback_backward(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    // No device check
  // DeviceGuard omitted
  return at::native::_grid_sampler_2d_cpu_fallback_backward(grad_output, input, grid, interpolation_mode, padding_mode, align_corners);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__hinge_embedding_loss(const at::Tensor & self, const at::Tensor & target, double margin, int64_t reduction) {
    // No device check
  // DeviceGuard omitted
  return at::native::hinge_embedding_loss(self, target, margin, reduction);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__group_norm(const at::Tensor & input, int64_t num_groups, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enabled) {
    // No device check
  // DeviceGuard omitted
  return at::native::group_norm(input, num_groups, weight, bias, eps, cudnn_enabled);
}
} // anonymous namespace
namespace {
int64_t wrapper_CompositeImplicitAutograd___cufft_get_plan_cache_size(at::DeviceIndex device_index) {
    // No device check
  // DeviceGuard omitted
  return at::native::_cufft_get_plan_cache_size(device_index);
}
} // anonymous namespace
namespace {
int64_t wrapper_CompositeImplicitAutograd___cufft_get_plan_cache_max_size(at::DeviceIndex device_index) {
    // No device check
  // DeviceGuard omitted
  return at::native::_cufft_get_plan_cache_max_size(device_index);
}
} // anonymous namespace
namespace {
void wrapper_CompositeImplicitAutograd___cufft_set_plan_cache_max_size(at::DeviceIndex device_index, int64_t max_size) {
    // No device check
  // DeviceGuard omitted
  return at::native::_cufft_set_plan_cache_max_size(device_index, max_size);
}
} // anonymous namespace
namespace {
void wrapper_CompositeImplicitAutograd___cufft_clear_plan_cache(at::DeviceIndex device_index) {
    // No device check
  // DeviceGuard omitted
  return at::native::_cufft_clear_plan_cache(device_index);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_dimname_index_copy_(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source) {
    // No device check
  // DeviceGuard omitted
  return at::native::index_copy_(self, dim, index, source);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dimname_index_copy(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source) {
    // No device check
  // DeviceGuard omitted
  return at::native::index_copy(self, dim, index, source);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__instance_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool use_input_stats, double momentum, double eps, bool cudnn_enabled) {
    // No device check
  // DeviceGuard omitted
  return at::native::instance_norm(input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, cudnn_enabled);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__isclose(const at::Tensor & self, const at::Tensor & other, double rtol, double atol, bool equal_nan) {
    // No device check
  // DeviceGuard omitted
  return at::native::isclose(self, other, rtol, atol, equal_nan);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd__is_distributed(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::is_distributed(self);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd__is_floating_point(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::is_floating_point(self);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd__is_complex(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::is_complex(self);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd__is_conj(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::is_conj(self);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd___is_zerotensor(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::_is_zerotensor(self);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd__is_neg(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::is_neg(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__isreal(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::isreal(self);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd__is_nonzero(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::is_nonzero(self);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd__is_signed(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::is_signed(self);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd__is_inference(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::is_inference(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__kl_div(const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
    // No device check
  // DeviceGuard omitted
  return at::native::kl_div(self, target, reduction, log_target);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__kron(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::kron(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_kron_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::kron_out(self, other, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_dimname_kthvalue(const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::kthvalue(self, k, dim, keepdim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_dimname_out_kthvalue_out(const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    // No device check
  // DeviceGuard omitted
  return at::native::kthvalue_out(self, k, dim, keepdim, values, indices);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__layer_norm(const at::Tensor & input, c10::SymIntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enable) {
    // No device check
  // DeviceGuard omitted
  return at::native::layer_norm_symint(input, normalized_shape, weight, bias, eps, cudnn_enable);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linear(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
    // No device check
  // DeviceGuard omitted
  return at::native::linear(input, weight, bias);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fbgemm_linear_int8_weight_fp32_activation(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
    // No device check
  // DeviceGuard omitted
  return at::native::fbgemm_linear_int8_weight_fp32_activation(input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fbgemm_linear_int8_weight(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
    // No device check
  // DeviceGuard omitted
  return at::native::fbgemm_linear_int8_weight(input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,double,int64_t> wrapper_CompositeImplicitAutograd__fbgemm_linear_quantize_weight(const at::Tensor & input) {
    // No device check
  // DeviceGuard omitted
  return at::native::fbgemm_linear_quantize_weight(input);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fbgemm_pack_gemm_matrix_fp16(const at::Tensor & input) {
    // No device check
  // DeviceGuard omitted
  return at::native::fbgemm_pack_gemm_matrix_fp16(input);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fbgemm_linear_fp16_weight_fp32_activation(const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
    // No device check
  // DeviceGuard omitted
  return at::native::fbgemm_linear_fp16_weight_fp32_activation(input, packed_weight, bias);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fbgemm_linear_fp16_weight(const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
    // No device check
  // DeviceGuard omitted
  return at::native::fbgemm_linear_fp16_weight(input, packed_weight, bias);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fbgemm_pack_quantized_matrix(const at::Tensor & input) {
    // No device check
  // DeviceGuard omitted
  return at::native::fbgemm_pack_quantized_matrix(input);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_KN_fbgemm_pack_quantized_matrix(const at::Tensor & input, int64_t K, int64_t N) {
    // No device check
  // DeviceGuard omitted
  return at::native::fbgemm_pack_quantized_matrix(input, K, N);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_ldexp(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::ldexp(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_ldexp_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::ldexp_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__ldexp_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::ldexp_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_int_log_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::log_softmax(self, dim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Dimname_log_softmax(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::log_softmax(self, dim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dimname_logcumsumexp(const at::Tensor & self, at::Dimname dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::logcumsumexp(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_dimname_out_logcumsumexp_out(const at::Tensor & self, at::Dimname dim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::logcumsumexp_out(self, dim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_names_logsumexp(const at::Tensor & self, at::DimnameList dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::logsumexp(self, dim, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_names_out_logsumexp_out(const at::Tensor & self, at::DimnameList dim, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::logsumexp_out(self, dim, keepdim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__margin_ranking_loss(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
    // No device check
  // DeviceGuard omitted
  return at::native::margin_ranking_loss(input1, input2, target, margin, reduction);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__matmul(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::matmul(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_matmul_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::matmul_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__matrix_power(const at::Tensor & self, int64_t n) {
    // No device check
  // DeviceGuard omitted
  return at::native::matrix_power(self, n);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_matrix_power_out(const at::Tensor & self, int64_t n, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::matrix_power_out(self, n, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__matrix_exp(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::matrix_exp(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__matrix_exp_backward(const at::Tensor & self, const at::Tensor & grad) {
    // No device check
  // DeviceGuard omitted
  return at::native::matrix_exp_backward(self, grad);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_names_dim_max(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::max(self, dim, keepdim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_names_dim_max_max_out(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
    // No device check
  // DeviceGuard omitted
  return at::native::max_out(self, dim, keepdim, max, max_values);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__value_selecting_reduction_backward(const at::Tensor & grad, int64_t dim, const at::Tensor & indices, c10::SymIntArrayRef sizes, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::value_selecting_reduction_backward_symint(grad, dim, indices, sizes, keepdim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__max_pool1d_with_indices(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::max_pool1d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__max_pool1d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::max_pool1d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__max_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::max_pool2d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__max_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::max_pool3d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_names_dim_mean(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::mean(self, dim, keepdim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_names_out_mean_out(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::mean_out(self, dim, keepdim, dtype, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__nanmean(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::nanmean(self, dim, keepdim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_nanmean_out(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::nanmean_out(self, dim, keepdim, dtype, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_names_dim_median(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::median(self, dim, keepdim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_names_dim_values_median_out(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    // No device check
  // DeviceGuard omitted
  return at::native::median_out(self, dim, keepdim, values, indices);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_names_dim_nanmedian(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::nanmedian(self, dim, keepdim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_names_dim_values_nanmedian_out(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    // No device check
  // DeviceGuard omitted
  return at::native::nanmedian_out(self, dim, keepdim, values, indices);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_names_dim_min(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::min(self, dim, keepdim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_names_dim_min_min_out(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
    // No device check
  // DeviceGuard omitted
  return at::native::min_out(self, dim, keepdim, min, min_indices);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___sparse_mm(const at::Tensor & sparse, const at::Tensor & dense) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_mm(sparse, dense);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_reduce__sparse_mm(const at::Tensor & sparse, const at::Tensor & dense, c10::string_view reduce) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_mm(sparse, dense, reduce);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_dimname_mode(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::mode(self, dim, keepdim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_dimname_out_mode_out(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    // No device check
  // DeviceGuard omitted
  return at::native::mode_out(self, dim, keepdim, values, indices);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_multiply(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::multiply(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_multiply_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::multiply_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_multiply_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::multiply_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar_multiply(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::multiply(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_multiply_(at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::multiply_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__narrow(const at::Tensor & self, int64_t dim, c10::SymInt start, c10::SymInt length) {
    // No device check
  // DeviceGuard omitted
  return at::native::narrow_symint(self, dim, start, length);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_narrow(const at::Tensor & self, int64_t dim, const at::Tensor & start, c10::SymInt length) {
    // No device check
  // DeviceGuard omitted
  return at::native::narrow_tensor_symint(self, dim, start, length);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd__is_vulkan_available() {
    // No device check
  // DeviceGuard omitted
  return at::native::is_vulkan_available();
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd___nnpack_available() {
    // No device check
  // DeviceGuard omitted
  return at::native::_nnpack_available();
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__pairwise_distance(const at::Tensor & x1, const at::Tensor & x2, double p, double eps, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::pairwise_distance(x1, x2, p, eps, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__cdist(const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode) {
    // No device check
  // DeviceGuard omitted
  return at::native::cdist(x1, x2, p, compute_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__pdist(const at::Tensor & self, double p) {
    // No device check
  // DeviceGuard omitted
  return at::native::pdist(self, p);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__cosine_similarity(const at::Tensor & x1, const at::Tensor & x2, int64_t dim, double eps) {
    // No device check
  // DeviceGuard omitted
  return at::native::cosine_similarity(x1, x2, dim, eps);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_intlist_movedim(const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
    // No device check
  // DeviceGuard omitted
  return at::native::movedim(self, source, destination);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_int_movedim(const at::Tensor & self, int64_t source, int64_t destination) {
    // No device check
  // DeviceGuard omitted
  return at::native::movedim(self, source, destination);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_intlist_moveaxis(const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
    // No device check
  // DeviceGuard omitted
  return at::native::moveaxis(self, source, destination);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_int_moveaxis(const at::Tensor & self, int64_t source, int64_t destination) {
    // No device check
  // DeviceGuard omitted
  return at::native::moveaxis(self, source, destination);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__numpy_T(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::numpy_T(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__matrix_H(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::matrix_H(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__mT(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::mT(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__mH(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::mH(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__adjoint(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::adjoint(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__native_channel_shuffle(const at::Tensor & self, c10::SymInt groups) {
    // No device check
  // DeviceGuard omitted
  return at::native::math_channel_shuffle(self, groups.guard_int(__FILE__, __LINE__));
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__pin_memory(const at::Tensor & self, c10::optional<at::Device> device) {
    // No device check
  // DeviceGuard omitted
  return at::native::pin_memory(self, device);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__pinverse(const at::Tensor & self, double rcond) {
    // No device check
  // DeviceGuard omitted
  return at::native::pinverse(self, rcond);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__poisson_nll_loss(const at::Tensor & input, const at::Tensor & target, bool log_input, bool full, double eps, int64_t reduction) {
    // No device check
  // DeviceGuard omitted
  return at::native::poisson_nll_loss(input, target, log_input, full, eps, reduction);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_generator_out_rand_out(c10::SymIntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::rand_out(C10_AS_INTARRAYREF_SLOW(size), generator, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_randn_out(c10::SymIntArrayRef size, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::randn_out(C10_AS_INTARRAYREF_SLOW(size), out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_generator_out_randn_out(c10::SymIntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::randn_out(C10_AS_INTARRAYREF_SLOW(size), generator, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__ravel(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::ravel(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__negative(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::negative(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_negative_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::negative_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__negative_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::negative_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_self_Tensor_repeat_interleave(const at::Tensor & self, const at::Tensor & repeats, c10::optional<int64_t> dim, c10::optional<c10::SymInt> output_size) {
    // No device check
  // DeviceGuard omitted
  return at::native::repeat_interleave_symint(self, repeats, dim, output_size);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_self_int_repeat_interleave(const at::Tensor & self, c10::SymInt repeats, c10::optional<int64_t> dim, c10::optional<c10::SymInt> output_size) {
    // No device check
  // DeviceGuard omitted
  return at::native::repeat_interleave_symint(self, repeats, dim, output_size);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__reshape(const at::Tensor & self, c10::SymIntArrayRef shape) {
    // No device check
  // DeviceGuard omitted
  return at::native::reshape_symint(self, shape);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__reshape_as(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::reshape_as(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__rrelu(const at::Tensor & self, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return at::native::rrelu(self, lower, upper, training, generator);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__rrelu_(at::Tensor & self, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return at::native::rrelu_(self, lower, upper, training, generator);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__relu6(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::relu6(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__relu6_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::relu6_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__prelu(const at::Tensor & self, const at::Tensor & weight) {
    // No device check
  // DeviceGuard omitted
  return at::native::prelu(self, weight);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__infinitely_differentiable_gelu_backward(const at::Tensor & grad, const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::infinitely_differentiable_gelu_backward(grad, self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Dimname_select(const at::Tensor & self, at::Dimname dim, int64_t index) {
    // No device check
  // DeviceGuard omitted
  return at::native::select(self, dim, index);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__selu(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::selu(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__selu_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::selu_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__silu_backward(const at::Tensor & grad_output, const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::math_silu_backward(grad_output, self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__mish_backward(const at::Tensor & grad_output, const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::math_mish_backward(grad_output, self);
}
} // anonymous namespace
namespace {
int64_t wrapper_CompositeImplicitAutograd_int_size(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::size(self, dim);
}
} // anonymous namespace
namespace {
int64_t wrapper_CompositeImplicitAutograd_Dimname_size(const at::Tensor & self, at::Dimname dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::size(self, dim);
}
} // anonymous namespace
namespace {
c10::SymInt wrapper_CompositeImplicitAutograd_int_sym_size(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::sym_size(self, dim);
}
} // anonymous namespace
namespace {
c10::SymInt wrapper_CompositeImplicitAutograd__sym_numel(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::sym_numel(self);
}
} // anonymous namespace
namespace {
c10::SymInt wrapper_CompositeImplicitAutograd__sym_storage_offset(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::sym_storage_offset(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__smm(const at::Tensor & self, const at::Tensor & mat2) {
    // No device check
  // DeviceGuard omitted
  return at::native::smm(self, mat2);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_int_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::softmax(self, dim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Dimname_softmax(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::softmax(self, dim, dtype);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_sizes_split(const at::Tensor & self, c10::SymIntArrayRef split_size, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::split_symint(self, split_size, dim);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_int_hsplit(const at::Tensor & self, int64_t sections) {
    // No device check
  // DeviceGuard omitted
  return at::native::hsplit(self, sections);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_array_hsplit(const at::Tensor & self, at::IntArrayRef indices) {
    // No device check
  // DeviceGuard omitted
  return at::native::hsplit(self, indices);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_int_vsplit(const at::Tensor & self, int64_t sections) {
    // No device check
  // DeviceGuard omitted
  return at::native::vsplit(self, sections);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_array_vsplit(const at::Tensor & self, at::IntArrayRef indices) {
    // No device check
  // DeviceGuard omitted
  return at::native::vsplit(self, indices);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_int_dsplit(const at::Tensor & self, int64_t sections) {
    // No device check
  // DeviceGuard omitted
  return at::native::dsplit(self, sections);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_array_dsplit(const at::Tensor & self, at::IntArrayRef indices) {
    // No device check
  // DeviceGuard omitted
  return at::native::dsplit(self, indices);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dimname_squeeze(const at::Tensor & self, at::Dimname dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::squeeze(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_dimname_squeeze_(at::Tensor & self, at::Dimname dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::squeeze_(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__sspaddmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return at::native::sspaddmm(self, mat1, mat2, beta, alpha);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__hstack(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::hstack(tensors);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_hstack_out(at::TensorList tensors, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::hstack_out(tensors, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__vstack(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::vstack(tensors);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_vstack_out(at::TensorList tensors, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::vstack_out(tensors, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__dstack(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::dstack(tensors);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_dstack_out(at::TensorList tensors, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::dstack_out(tensors, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__stft(const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex) {
    // No device check
  // DeviceGuard omitted
  return at::native::stft(self, n_fft, hop_length, win_length, window, normalized, onesided, return_complex);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_center_stft(const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool center, c10::string_view pad_mode, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex) {
    // No device check
  // DeviceGuard omitted
  return at::native::stft(self, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__istft(const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool center, bool normalized, c10::optional<bool> onesided, c10::optional<int64_t> length, bool return_complex) {
    // No device check
  // DeviceGuard omitted
  return at::native::istft(self, n_fft, hop_length, win_length, window, center, normalized, onesided, length, return_complex);
}
} // anonymous namespace
namespace {
int64_t wrapper_CompositeImplicitAutograd_int_stride(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::stride(self, dim);
}
} // anonymous namespace
namespace {
int64_t wrapper_CompositeImplicitAutograd_Dimname_stride(const at::Tensor & self, at::Dimname dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::stride(self, dim);
}
} // anonymous namespace
namespace {
c10::SymInt wrapper_CompositeImplicitAutograd_int_sym_stride(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::sym_stride(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dim_DimnameList_sum(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::sum(self, dim, keepdim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_DimnameList_out_sum_out(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::sum_out(self, dim, keepdim, dtype, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__sum_to_size(const at::Tensor & self, c10::SymIntArrayRef size) {
    // No device check
  // DeviceGuard omitted
  return at::native::sum_to_size_symint(self, size);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__square(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::square(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_square_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::square_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__square_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::square_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__std(const at::Tensor & self, bool unbiased) {
    // No device check
  // DeviceGuard omitted
  return at::native::std(self, unbiased);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dim_std(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::std(self, dim, unbiased, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_std_out(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::std_out(self, dim, unbiased, keepdim, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__std_mean(const at::Tensor & self, bool unbiased) {
    // No device check
  // DeviceGuard omitted
  return at::native::std_mean(self, unbiased);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_dim_std_mean(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::std_mean(self, dim, unbiased, keepdim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_names_dim_std_mean(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::std_mean(self, dim, unbiased, keepdim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_correction_names_std_mean(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::std_mean(self, dim, correction, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_names_dim_std(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::std(self, dim, unbiased, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_names_out_std_out(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::std_out(self, dim, unbiased, keepdim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_correction_names_std(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::std(self, dim, correction, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_correction_names_out_std_out(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::std_out(self, dim, correction, keepdim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dim_Dimname_prod(const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::prod(self, dim, keepdim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Dimname_out_prod_out(const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::prod_out(self, dim, keepdim, dtype, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__tensordot(const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other) {
    // No device check
  // DeviceGuard omitted
  return at::native::tensordot(self, other, dims_self, dims_other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_tensordot_out(const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::tensordot_out(self, other, dims_self, dims_other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__tile(const at::Tensor & self, c10::SymIntArrayRef dims) {
    // No device check
  // DeviceGuard omitted
  return at::native::tile_symint(self, dims);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Dimname_transpose(const at::Tensor & self, at::Dimname dim0, at::Dimname dim1) {
    // No device check
  // DeviceGuard omitted
  return at::native::transpose(self, dim0, dim1);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__one_hot(const at::Tensor & self, int64_t num_classes) {
    // No device check
  // DeviceGuard omitted
  return at::native::one_hot(self, num_classes);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fliplr(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::fliplr(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__flipud(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::flipud(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_x_trapezoid(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::trapezoid(y, x, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dx_trapezoid(const at::Tensor & y, const at::Scalar & dx, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::trapezoid(y, dx, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_x_trapz(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::trapz(y, x, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dx_trapz(const at::Tensor & y, double dx, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::trapz(y, dx, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__triplet_margin_loss(const at::Tensor & anchor, const at::Tensor & positive, const at::Tensor & negative, double margin, double p, double eps, bool swap, int64_t reduction) {
    // No device check
  // DeviceGuard omitted
  return at::native::triplet_margin_loss(anchor, positive, negative, margin, p, eps, swap, reduction);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fix(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::fix(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fix_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fix_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__fix_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::fix_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__type_as(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::type_as(self, other);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd___has_compatible_shallow_copy_type(const at::Tensor & self, const at::Tensor & from) {
    // No device check
  // DeviceGuard omitted
  return at::native::_has_compatible_shallow_copy_type(self, from);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__vander(const at::Tensor & x, c10::optional<int64_t> N, bool increasing) {
    // No device check
  // DeviceGuard omitted
  return at::native::vander(x, N, increasing);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__var(const at::Tensor & self, bool unbiased) {
    // No device check
  // DeviceGuard omitted
  return at::native::var(self, unbiased);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dim_var(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::var(self, dim, unbiased, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_var_out(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::var_out(self, dim, unbiased, keepdim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_names_dim_var(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::var(self, dim, unbiased, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_names_out_var_out(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::var_out(self, dim, unbiased, keepdim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_correction_names_var(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::var(self, dim, correction, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_correction_names_out_var_out(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::var_out(self, dim, correction, keepdim, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__var_mean(const at::Tensor & self, bool unbiased) {
    // No device check
  // DeviceGuard omitted
  return at::native::var_mean(self, unbiased);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_dim_var_mean(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::var_mean(self, dim, unbiased, keepdim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_names_dim_var_mean(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::var_mean(self, dim, unbiased, keepdim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_correction_names_var_mean(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::var_mean(self, dim, correction, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__view_as(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::view_as(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_ScalarSelf_where(const at::Tensor & condition, const at::Scalar & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::where(condition, self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_ScalarOther_where(const at::Tensor & condition, const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::where(condition, self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar_where(const at::Tensor & condition, const at::Scalar & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::where(condition, self, other);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd__where(const at::Tensor & condition) {
    // No device check
  // DeviceGuard omitted
  return at::native::where(condition);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__norm_except_dim(const at::Tensor & v, int64_t pow, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::norm_except_dim(v, pow, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___weight_norm(const at::Tensor & v, const at::Tensor & g, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::_weight_norm(v, g, dim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd___weight_norm_differentiable_backward(const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::_weight_norm_differentiable_backward(grad_w, saved_v, saved_g, saved_norms, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___sparse_sum(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_sum(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dtype__sparse_sum(const at::Tensor & self, at::ScalarType dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_sum(self, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dim_dtype__sparse_sum(const at::Tensor & self, at::IntArrayRef dim, at::ScalarType dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_sum(self, dim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_int__sparse_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_softmax(self, dim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Dimname__sparse_softmax(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_softmax(self, dim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_int__sparse_log_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_log_softmax(self, dim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Dimname__sparse_log_softmax(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_log_softmax(self, dim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_names_ScalarOpt_dim_dtype_norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::norm(self, p, dim, keepdim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_names_dtype_out_norm_out(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::norm_out(self, p, dim, keepdim, dtype, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_names_ScalarOpt_dim_norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::norm(self, p, dim, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_names_out_norm_out(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::norm_out(self, p, dim, keepdim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dim_frobenius_norm(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::frobenius_norm(self, dim, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_frobenius_norm_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::frobenius_norm_out(self, dim, keepdim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__nuclear_norm(const at::Tensor & self, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::nuclear_norm(self, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_nuclear_norm_out(const at::Tensor & self, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::nuclear_norm_out(self, keepdim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dim_nuclear_norm(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::nuclear_norm(self, dim, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_dim_out_nuclear_norm_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::nuclear_norm_out(self, dim, keepdim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__positive(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::positive(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_subtract(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return at::native::subtract(self, other, alpha);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_subtract_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::subtract_out(self, other, alpha, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_subtract_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return at::native::subtract_(self, other, alpha);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar_subtract(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return at::native::subtract(self, other, alpha);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_subtract_(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return at::native::subtract_(self, other, alpha);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_crow_col_value_size_sparse_csr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return at::native::sparse_csr_tensor(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_ccol_row_value_size_sparse_csc_tensor(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return at::native::sparse_csc_tensor(ccol_indices, row_indices, values, size, dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_crow_col_value_size_sparse_bsr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return at::native::sparse_bsr_tensor(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_ccol_row_value_size_sparse_bsc_tensor(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return at::native::sparse_bsc_tensor(ccol_indices, row_indices, values, size, dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_crow_col_value_sparse_csr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return at::native::sparse_csr_tensor(crow_indices, col_indices, values, dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_ccol_row_value_sparse_csc_tensor(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return at::native::sparse_csc_tensor(ccol_indices, row_indices, values, dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_crow_col_value_sparse_bsr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return at::native::sparse_bsr_tensor(crow_indices, col_indices, values, dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_ccol_row_value_sparse_bsc_tensor(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return at::native::sparse_bsc_tensor(ccol_indices, row_indices, values, dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___sparse_compressed_tensor_unsafe(const at::Tensor & compressed_indices, const at::Tensor & plain_indices, const at::Tensor & values, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_compressed_tensor_unsafe_symint(compressed_indices, plain_indices, values, size, dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___sparse_csr_tensor_unsafe(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_csr_tensor_unsafe(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___sparse_csc_tensor_unsafe(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_csc_tensor_unsafe(ccol_indices, row_indices, values, size, dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___sparse_bsr_tensor_unsafe(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_bsr_tensor_unsafe(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___sparse_bsc_tensor_unsafe(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_bsc_tensor_unsafe(ccol_indices, row_indices, values, size, dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_indices_sparse_coo_tensor(const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<bool> is_coalesced) {
    // No device check
  // DeviceGuard omitted
  return at::native::sparse_coo_tensor(indices, values, dtype, layout, device, pin_memory, is_coalesced);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_indices_size_sparse_coo_tensor(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<bool> is_coalesced) {
    // No device check
  // DeviceGuard omitted
  return at::native::sparse_coo_tensor(indices, values, size, dtype, layout, device, pin_memory, is_coalesced);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___sparse_coo_tensor_unsafe(const at::Tensor & indices, const at::Tensor & values, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<bool> is_coalesced) {
    // No device check
  // DeviceGuard omitted
  return at::native::_sparse_coo_tensor_unsafe_symint(indices, values, size, dtype, layout, device, pin_memory, is_coalesced);
}
} // anonymous namespace
namespace {
void wrapper_CompositeImplicitAutograd___validate_sparse_coo_tensor_args(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<bool> is_coalesced) {
    // No device check
  // DeviceGuard omitted
  return at::native::_validate_sparse_coo_tensor_args(indices, values, size, is_coalesced);
}
} // anonymous namespace
namespace {
void wrapper_CompositeImplicitAutograd___validate_sparse_compressed_tensor_args(const at::Tensor & compressed_indices, const at::Tensor & plain_indices, const at::Tensor & values, at::IntArrayRef size, at::Layout layout) {
    // No device check
  // DeviceGuard omitted
  return at::native::_validate_sparse_compressed_tensor_args(compressed_indices, plain_indices, values, size, layout);
}
} // anonymous namespace
namespace {
void wrapper_CompositeImplicitAutograd___validate_sparse_csr_tensor_args(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size) {
    // No device check
  // DeviceGuard omitted
  return at::native::_validate_sparse_csr_tensor_args(crow_indices, col_indices, values, size);
}
} // anonymous namespace
namespace {
void wrapper_CompositeImplicitAutograd___validate_sparse_csc_tensor_args(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size) {
    // No device check
  // DeviceGuard omitted
  return at::native::_validate_sparse_csc_tensor_args(ccol_indices, row_indices, values, size);
}
} // anonymous namespace
namespace {
void wrapper_CompositeImplicitAutograd___validate_sparse_bsr_tensor_args(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size) {
    // No device check
  // DeviceGuard omitted
  return at::native::_validate_sparse_bsr_tensor_args(crow_indices, col_indices, values, size);
}
} // anonymous namespace
namespace {
void wrapper_CompositeImplicitAutograd___validate_sparse_bsc_tensor_args(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size) {
    // No device check
  // DeviceGuard omitted
  return at::native::_validate_sparse_bsc_tensor_args(ccol_indices, row_indices, values, size);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd___to_cpu(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::_to_cpu(tensors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__to_dense(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<bool> masked_grad) {
    // No device check
  // DeviceGuard omitted
  return at::native::to_dense(self, dtype, masked_grad);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__to_dense_backward(const at::Tensor & grad, const at::Tensor & input, c10::optional<bool> masked_grad) {
    // No device check
  // DeviceGuard omitted
  return at::native::to_dense_backward(grad, input, masked_grad);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__coalesce(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::coalesce(self);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_Dimname_unbind(const at::Tensor & self, at::Dimname dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::unbind(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_sparse_dim_to_sparse(const at::Tensor & self, int64_t sparse_dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::to_sparse(self, sparse_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__to_sparse(const at::Tensor & self, c10::optional<at::Layout> layout, at::OptionalIntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::to_sparse(self, layout, blocksize, dense_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__to_sparse_csr(const at::Tensor & self, c10::optional<int64_t> dense_dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::to_sparse_csr(self, dense_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__to_sparse_csc(const at::Tensor & self, c10::optional<int64_t> dense_dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::to_sparse_csc(self, dense_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__to_sparse_bsr(const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::to_sparse_bsr(self, blocksize, dense_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__to_sparse_bsc(const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::to_sparse_bsc(self, blocksize, dense_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__to_mkldnn_backward(const at::Tensor & grad, const at::Tensor & input) {
    // No device check
  // DeviceGuard omitted
  return at::native::to_mkldnn_backward(grad, input);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fake_quantize_per_tensor_affine(const at::Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
    // No device check
  // DeviceGuard omitted
  return at::native::fake_quantize_per_tensor_affine(self, scale, zero_point, quant_min, quant_max);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_tensor_qparams_fake_quantize_per_tensor_affine(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max) {
    // No device check
  // DeviceGuard omitted
  return at::native::fake_quantize_per_tensor_affine(self, scale, zero_point, quant_min, quant_max);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fake_quantize_per_tensor_affine_cachemask_backward(const at::Tensor & grad, const at::Tensor & mask) {
    // No device check
  // DeviceGuard omitted
  return at::native::fake_quantize_per_tensor_affine_cachemask_backward(grad, mask);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fake_quantize_per_channel_affine(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
    // No device check
  // DeviceGuard omitted
  return at::native::fake_quantize_per_channel_affine(self, scale, zero_point, axis, quant_min, quant_max);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fake_quantize_per_channel_affine_cachemask_backward(const at::Tensor & grad, const at::Tensor & mask) {
    // No device check
  // DeviceGuard omitted
  return at::native::fake_quantize_per_channel_affine_cachemask_backward(grad, mask);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fused_moving_avg_obs_fake_quant(const at::Tensor & self, const at::Tensor & observer_on, const at::Tensor & fake_quant_on, at::Tensor & running_min, at::Tensor & running_max, at::Tensor & scale, at::Tensor & zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, bool per_row_fake_quant, bool symmetric_quant) {
    // No device check
  // DeviceGuard omitted
  return at::native::fused_moving_avg_obs_fake_quant(self, observer_on, fake_quant_on, running_min, running_max, scale, zero_point, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
}
} // anonymous namespace
namespace {
::std::tuple<double,int64_t> wrapper_CompositeImplicitAutograd___choose_qparams_per_tensor(const at::Tensor & self, bool reduce_range) {
    // No device check
  // DeviceGuard omitted
  return at::native::_choose_qparams_per_tensor(self, reduce_range);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___saturate_weight_to_fp16(const at::Tensor & weight) {
    // No device check
  // DeviceGuard omitted
  return at::native::_saturate_weight_to_fp16(weight);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__choose_qparams_optimized(const at::Tensor & input, int64_t numel, int64_t n_bins, double ratio, int64_t bit_width) {
    // No device check
  // DeviceGuard omitted
  return at::native::choose_qparams_optimized(input, numel, n_bins, ratio, bit_width);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___autocast_to_reduced_precision(const at::Tensor & self, bool cuda_enabled, bool cpu_enabled, at::ScalarType cuda_dtype, at::ScalarType cpu_dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::_autocast_to_reduced_precision(self, cuda_enabled, cpu_enabled, cuda_dtype, cpu_dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___autocast_to_full_precision(const at::Tensor & self, bool cuda_enabled, bool cpu_enabled) {
    // No device check
  // DeviceGuard omitted
  return at::native::_autocast_to_full_precision(self, cuda_enabled, cpu_enabled);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dtype_layout_to(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::to(self, dtype, layout, device, pin_memory, non_blocking, copy, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_device_to(const at::Tensor & self, at::Device device, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::to(self, device, dtype, non_blocking, copy, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dtype_to(const at::Tensor & self, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::to(self, dtype, non_blocking, copy, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_other_to(const at::Tensor & self, const at::Tensor & other, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::to(self, other, non_blocking, copy, memory_format);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd__meshgrid(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::meshgrid(tensors);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd_indexing_meshgrid(at::TensorList tensors, c10::string_view indexing) {
    // No device check
  // DeviceGuard omitted
  return at::native::meshgrid(tensors, indexing);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__cartesian_prod(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::cartesian_prod(tensors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__combinations(const at::Tensor & self, int64_t r, bool with_replacement) {
    // No device check
  // DeviceGuard omitted
  return at::native::combinations(self, r, with_replacement);
}
} // anonymous namespace
namespace {
at::Scalar wrapper_CompositeImplicitAutograd__item(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::item(self);
}
} // anonymous namespace
namespace {
at::ScalarType wrapper_CompositeImplicitAutograd_Tensor_result_type(const at::Tensor & tensor, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::result_type(tensor, other);
}
} // anonymous namespace
namespace {
at::ScalarType wrapper_CompositeImplicitAutograd_Scalar_result_type(const at::Tensor & tensor, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::result_type(tensor, other);
}
} // anonymous namespace
namespace {
at::ScalarType wrapper_CompositeImplicitAutograd_Scalar_Tensor_result_type(const at::Scalar & scalar, const at::Tensor & tensor) {
    // No device check
  // DeviceGuard omitted
  return at::native::result_type(scalar, tensor);
}
} // anonymous namespace
namespace {
at::ScalarType wrapper_CompositeImplicitAutograd_Scalar_Scalar_result_type(const at::Scalar & scalar1, const at::Scalar & scalar2) {
    // No device check
  // DeviceGuard omitted
  return at::native::result_type(scalar1, scalar2);
}
} // anonymous namespace
namespace {
bool wrapper_CompositeImplicitAutograd__can_cast(at::ScalarType from, at::ScalarType to) {
    // No device check
  // DeviceGuard omitted
  return at::native::can_cast(from, to);
}
} // anonymous namespace
namespace {
at::ScalarType wrapper_CompositeImplicitAutograd__promote_types(at::ScalarType type1, at::ScalarType type2) {
    // No device check
  // DeviceGuard omitted
  return at::native::promote_types(type1, type2);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd___thnn_fused_lstm_cell_backward(const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & cx, const at::Tensor & cy, const at::Tensor & workspace, bool has_bias) {
    // No device check
  // DeviceGuard omitted
  return at::native::_thnn_fused_lstm_cell_backward(grad_hy, grad_cy, cx, cy, workspace, has_bias);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd___thnn_differentiable_lstm_cell_backward(const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias, const at::Tensor & cx, const at::Tensor & cy) {
    // No device check
  // DeviceGuard omitted
  return at::native::_thnn_differentiable_lstm_cell_backward(grad_hy, grad_cy, input_gates, hidden_gates, input_bias, hidden_bias, cx, cy);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd___thnn_differentiable_gru_cell_backward(const at::Tensor & grad_hy, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
    // No device check
  // DeviceGuard omitted
  return at::native::_thnn_differentiable_gru_cell_backward(grad_hy, input_gates, hidden_gates, hx, input_bias, hidden_bias);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_input_lstm(const at::Tensor & input, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    // No device check
  // DeviceGuard omitted
  return at::native::lstm(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_data_lstm(const at::Tensor & data, const at::Tensor & batch_sizes, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    // No device check
  // DeviceGuard omitted
  return at::native::lstm(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_input_gru(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    // No device check
  // DeviceGuard omitted
  return at::native::gru(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_data_gru(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    // No device check
  // DeviceGuard omitted
  return at::native::gru(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_input_rnn_tanh(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    // No device check
  // DeviceGuard omitted
  return at::native::rnn_tanh(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_data_rnn_tanh(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    // No device check
  // DeviceGuard omitted
  return at::native::rnn_tanh(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_input_rnn_relu(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    // No device check
  // DeviceGuard omitted
  return at::native::rnn_relu(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_data_rnn_relu(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    // No device check
  // DeviceGuard omitted
  return at::native::rnn_relu(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__lstm_cell(const at::Tensor & input, at::TensorList hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    // No device check
  // DeviceGuard omitted
  return at::native::lstm_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__gru_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    // No device check
  // DeviceGuard omitted
  return at::native::gru_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__rnn_tanh_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    // No device check
  // DeviceGuard omitted
  return at::native::rnn_tanh_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__rnn_relu_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    // No device check
  // DeviceGuard omitted
  return at::native::rnn_relu_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__quantized_lstm_cell(const at::Tensor & input, at::TensorList hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantized_lstm_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__quantized_gru_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantized_gru_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__quantized_rnn_relu_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantized_rnn_relu_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__quantized_rnn_tanh_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantized_rnn_tanh_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___pack_padded_sequence_backward(const at::Tensor & grad, c10::SymIntArrayRef input_size, const at::Tensor & batch_sizes, bool batch_first) {
    // No device check
  // DeviceGuard omitted
  return at::native::_pack_padded_sequence_backward_symint(grad, input_size, batch_sizes, batch_first);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd___pad_packed_sequence(const at::Tensor & data, const at::Tensor & batch_sizes, bool batch_first, const at::Scalar & padding_value, int64_t total_length) {
    // No device check
  // DeviceGuard omitted
  return at::native::_pad_packed_sequence(data, batch_sizes, batch_first, padding_value, total_length);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_source_Tensor_storage_offset_set_(at::Tensor & self, const at::Tensor & source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
    // No device check
  // DeviceGuard omitted
  return at::native::set__symint(self, source, storage_offset, size, stride);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dimname_index_add(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return at::native::index_add(self, dim, index, source, alpha);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Dimname_Scalar_index_fill_(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return at::native::index_fill_(self, dim, index, value);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Dimname_Scalar_index_fill(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return at::native::index_fill(self, dim, index, value);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Dimname_Tensor_index_fill_(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
    // No device check
  // DeviceGuard omitted
  return at::native::index_fill_(self, dim, index, value);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Dimname_Tensor_index_fill(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
    // No device check
  // DeviceGuard omitted
  return at::native::index_fill(self, dim, index, value);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dimname_src_scatter(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
    // No device check
  // DeviceGuard omitted
  return at::native::scatter(self, dim, index, src);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dimname_value_scatter(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return at::native::scatter(self, dim, index, value);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dimname_scatter_add(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
    // No device check
  // DeviceGuard omitted
  return at::native::scatter_add(self, dim, index, src);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar___and__(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::__and__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar___iand__(at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::__iand__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor___and__(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::__and__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor___iand__(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::__iand__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar___or__(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::__or__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar___ior__(at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::__ior__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor___or__(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::__or__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor___ior__(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::__ior__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar___xor__(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::__xor__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar___ixor__(at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::__ixor__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor___xor__(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::__xor__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor___ixor__(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::__ixor__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__diag(const at::Tensor & self, int64_t diagonal) {
    // No device check
  // DeviceGuard omitted
  return at::native::diag(self, diagonal);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_diag_out(const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::diag_out(self, diagonal, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__cross(const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::cross(self, other, dim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_cross_out(const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::cross_out(self, other, dim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__trace_backward(const at::Tensor & grad, c10::SymIntArrayRef sizes) {
    // No device check
  // DeviceGuard omitted
  return at::native::trace_backward_symint(grad, sizes);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar_not_equal(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::not_equal(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_out_not_equal_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::not_equal_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_not_equal_(at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::not_equal_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_not_equal(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::not_equal(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_out_not_equal_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::not_equal_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_not_equal_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::not_equal_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar_greater_equal(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::greater_equal(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_out_greater_equal_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::greater_equal_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_greater_equal_(at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::greater_equal_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_greater_equal(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::greater_equal(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_out_greater_equal_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::greater_equal_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_greater_equal_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::greater_equal_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar_less_equal(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::less_equal(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_out_less_equal_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::less_equal_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_less_equal_(at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::less_equal_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_less_equal(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::less_equal(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_out_less_equal_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::less_equal_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_less_equal_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::less_equal_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar_greater(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::greater(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_out_greater_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::greater_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_greater_(at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::greater_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_greater(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::greater(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_out_greater_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::greater_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_greater_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::greater_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar_less(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::less(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_out_less_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::less_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_less_(at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::less_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_less(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::less(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_out_less_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::less_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_less_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::less_(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__take_along_dim(const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::take_along_dim(self, indices, dim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_take_along_dim_out(const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::take_along_dim_out(self, indices, dim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dimname_index_select(const at::Tensor & self, at::Dimname dim, const at::Tensor & index) {
    // No device check
  // DeviceGuard omitted
  return at::native::index_select(self, dim, index);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_dimname_out_index_select_out(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::index_select_out(self, dim, index, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__index_select_backward(const at::Tensor & grad, c10::SymIntArrayRef self_sizes, int64_t dim, const at::Tensor & index) {
    // No device check
  // DeviceGuard omitted
  return at::native::index_select_backward_symint(grad, self_sizes, dim, index);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__masked_select_backward(const at::Tensor & grad, const at::Tensor & input, const at::Tensor & mask) {
    // No device check
  // DeviceGuard omitted
  return at::native::masked_select_backward(grad, input, mask);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd__nonzero_numpy(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::nonzero_numpy(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__argwhere(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::argwhere(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__gather_backward(const at::Tensor & grad, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
    // No device check
  // DeviceGuard omitted
  return at::native::gather_backward(grad, self, dim, index, sparse_grad);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dimname_gather(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad) {
    // No device check
  // DeviceGuard omitted
  return at::native::gather(self, dim, index, sparse_grad);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_dimname_out_gather_out(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::gather_out(self, dim, index, sparse_grad, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___gather_sparse_backward(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & grad) {
    // No device check
  // DeviceGuard omitted
  return at::native::_gather_sparse_backward(self, dim, index, grad);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__cross_entropy_loss(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, double label_smoothing) {
    // No device check
  // DeviceGuard omitted
  return at::native::cross_entropy_loss_symint(self, target, weight, reduction, ignore_index, label_smoothing);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_vander(const at::Tensor & x, c10::optional<c10::SymInt> N) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_vander_symint(x, N);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__svd(const at::Tensor & self, bool some, bool compute_uv) {
    // No device check
  // DeviceGuard omitted
  return at::native::svd(self, some, compute_uv);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_U_svd_out(const at::Tensor & self, bool some, bool compute_uv, at::Tensor & U, at::Tensor & S, at::Tensor & V) {
    // No device check
  // DeviceGuard omitted
  return at::native::svd_out(self, some, compute_uv, U, S, V);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__swapaxes(const at::Tensor & self, int64_t axis0, int64_t axis1) {
    // No device check
  // DeviceGuard omitted
  return at::native::swapaxes(self, axis0, axis1);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__swapaxes_(at::Tensor & self, int64_t axis0, int64_t axis1) {
    // No device check
  // DeviceGuard omitted
  return at::native::swapaxes_(self, axis0, axis1);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__swapdims(const at::Tensor & self, int64_t dim0, int64_t dim1) {
    // No device check
  // DeviceGuard omitted
  return at::native::swapdims(self, dim0, dim1);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__swapdims_(at::Tensor & self, int64_t dim0, int64_t dim1) {
    // No device check
  // DeviceGuard omitted
  return at::native::swapdims_(self, dim0, dim1);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__qr(const at::Tensor & self, bool some) {
    // No device check
  // DeviceGuard omitted
  return at::native::qr(self, some);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_Q_qr_out(const at::Tensor & self, bool some, at::Tensor & Q, at::Tensor & R) {
    // No device check
  // DeviceGuard omitted
  return at::native::qr_out(self, some, Q, R);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__orgqr(const at::Tensor & self, const at::Tensor & input2) {
    // No device check
  // DeviceGuard omitted
  return at::native::orgqr(self, input2);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_orgqr_out(const at::Tensor & self, const at::Tensor & input2, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::orgqr_out(self, input2, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd___lu_with_info(const at::Tensor & self, bool pivot, bool check_errors) {
    // No device check
  // DeviceGuard omitted
  return at::native::_lu_with_info(self, pivot, check_errors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__lu_solve(const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots) {
    // No device check
  // DeviceGuard omitted
  return at::native::lu_solve(self, LU_data, LU_pivots);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_lu_solve_out(const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::lu_solve_out(self, LU_data, LU_pivots, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__arctan2(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::arctan2(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_arctan2_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::arctan2_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd__arctan2_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::arctan2_(self, other);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,::std::vector<at::Tensor>> wrapper_CompositeImplicitAutograd__histogramdd(const at::Tensor & self, at::IntArrayRef bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density) {
    // No device check
  // DeviceGuard omitted
  return at::native::histogramdd(self, bins, range, weight, density);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,::std::vector<at::Tensor>> wrapper_CompositeImplicitAutograd_int_bins_histogramdd(const at::Tensor & self, int64_t bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density) {
    // No device check
  // DeviceGuard omitted
  return at::native::histogramdd(self, bins, range, weight, density);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,::std::vector<at::Tensor>> wrapper_CompositeImplicitAutograd_TensorList_bins_histogramdd(const at::Tensor & self, at::TensorList bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density) {
    // No device check
  // DeviceGuard omitted
  return at::native::histogramdd(self, bins, range, weight, density);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_other_max(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::max(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_max_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::max_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_other_min(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::min(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_min_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::min_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__quantile(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantile(self, q, dim, keepdim, interpolation);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_quantile_out(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantile_out(self, q, dim, keepdim, interpolation, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_scalar_quantile(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantile(self, q, dim, keepdim, interpolation);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_scalar_out_quantile_out(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::quantile_out(self, q, dim, keepdim, interpolation, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__nanquantile(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    // No device check
  // DeviceGuard omitted
  return at::native::nanquantile(self, q, dim, keepdim, interpolation);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_nanquantile_out(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::nanquantile_out(self, q, dim, keepdim, interpolation, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_scalar_nanquantile(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    // No device check
  // DeviceGuard omitted
  return at::native::nanquantile(self, q, dim, keepdim, interpolation);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_scalar_out_nanquantile_out(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::nanquantile_out(self, q, dim, keepdim, interpolation, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_dimname_sort(const at::Tensor & self, at::Dimname dim, bool descending) {
    // No device check
  // DeviceGuard omitted
  return at::native::sort(self, dim, descending);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_dimname_values_sort_out(const at::Tensor & self, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
    // No device check
  // DeviceGuard omitted
  return at::native::sort_out(self, dim, descending, values, indices);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd_dimname_stable_sort(const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending) {
    // No device check
  // DeviceGuard omitted
  return at::native::sort(self, stable, dim, descending);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_dimname_values_stable_sort_out(const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
    // No device check
  // DeviceGuard omitted
  return at::native::sort_out(self, stable, dim, descending, values, indices);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__msort(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::msort(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_msort_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::msort_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__argsort(const at::Tensor & self, int64_t dim, bool descending) {
    // No device check
  // DeviceGuard omitted
  return at::native::argsort(self, dim, descending);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_dimname_argsort(const at::Tensor & self, at::Dimname dim, bool descending) {
    // No device check
  // DeviceGuard omitted
  return at::native::argsort(self, dim, descending);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_Tensor_float_power(const at::Tensor & self, const at::Tensor & exponent) {
    // No device check
  // DeviceGuard omitted
  return at::native::float_power(self, exponent);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_Tensor_out_float_power_out(const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::float_power_out(self, exponent, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_float_power_(at::Tensor & self, const at::Tensor & exponent) {
    // No device check
  // DeviceGuard omitted
  return at::native::float_power_(self, exponent);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Scalar_float_power(const at::Scalar & self, const at::Tensor & exponent) {
    // No device check
  // DeviceGuard omitted
  return at::native::float_power(self, exponent);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_out_float_power_out(const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::float_power_out(self, exponent, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_Tensor_Scalar_float_power(const at::Tensor & self, const at::Scalar & exponent) {
    // No device check
  // DeviceGuard omitted
  return at::native::float_power(self, exponent);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Tensor_Scalar_out_float_power_out(const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::float_power_out(self, exponent, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_Scalar_float_power_(at::Tensor & self, const at::Scalar & exponent) {
    // No device check
  // DeviceGuard omitted
  return at::native::float_power_(self, exponent);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__l1_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    // No device check
  // DeviceGuard omitted
  return at::native::l1_loss(self, target, reduction);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__multilabel_margin_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    // No device check
  // DeviceGuard omitted
  return at::native::multilabel_margin_loss(self, target, reduction);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_multilabel_margin_loss_out(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::multilabel_margin_loss_out(self, target, reduction, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__nll_loss(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
    // No device check
  // DeviceGuard omitted
  return at::native::nll_loss_symint(self, target, weight, reduction, ignore_index);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_nll_loss_out(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::nll_loss_out(self, target, weight, reduction, ignore_index.guard_int(__FILE__, __LINE__), out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__nll_loss_nd(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
    // No device check
  // DeviceGuard omitted
  return at::native::nll_loss_nd_symint(self, target, weight, reduction, ignore_index);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__nll_loss2d(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
    // No device check
  // DeviceGuard omitted
  return at::native::nll_loss2d_symint(self, target, weight, reduction, ignore_index);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_nll_loss2d_out(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::nll_loss2d_out(self, target, weight, reduction, ignore_index.guard_int(__FILE__, __LINE__), out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__log_sigmoid(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::log_sigmoid(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_log_sigmoid_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::log_sigmoid_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__adaptive_avg_pool2d(const at::Tensor & self, c10::SymIntArrayRef output_size) {
    // No device check
  // DeviceGuard omitted
  return at::native::adaptive_avg_pool2d_symint(self, output_size);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__adaptive_avg_pool3d(const at::Tensor & self, c10::SymIntArrayRef output_size) {
    // No device check
  // DeviceGuard omitted
  return at::native::adaptive_avg_pool3d_symint(self, output_size);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___pad_circular(const at::Tensor & self, c10::SymIntArrayRef pad) {
    // No device check
  // DeviceGuard omitted
  return at::native::_pad_circular_symint(self, pad);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___pad_enum(const at::Tensor & self, c10::SymIntArrayRef pad, int64_t mode, c10::optional<double> value) {
    // No device check
  // DeviceGuard omitted
  return at::native::_pad_enum_symint(self, pad, mode, value);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__pad(const at::Tensor & self, c10::SymIntArrayRef pad, c10::string_view mode, c10::optional<double> value) {
    // No device check
  // DeviceGuard omitted
  return at::native::pad_symint(self, pad, mode, value);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_vec_upsample_linear1d(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check
  // DeviceGuard omitted
  return at::native::upsample_linear1d(input, output_size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*output_size)) : c10::nullopt, align_corners, scale_factors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_vec_upsample_bilinear2d(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check
  // DeviceGuard omitted
  return at::native::upsample_bilinear2d(input, output_size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*output_size)) : c10::nullopt, align_corners, scale_factors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_vec__upsample_bilinear2d_aa(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check
  // DeviceGuard omitted
  return at::native::_upsample_bilinear2d_aa(input, output_size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*output_size)) : c10::nullopt, align_corners, scale_factors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_vec_upsample_trilinear3d(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check
  // DeviceGuard omitted
  return at::native::upsample_trilinear3d(input, output_size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*output_size)) : c10::nullopt, align_corners, scale_factors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_vec_upsample_bicubic2d(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check
  // DeviceGuard omitted
  return at::native::upsample_bicubic2d(input, output_size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*output_size)) : c10::nullopt, align_corners, scale_factors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_vec__upsample_bicubic2d_aa(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check
  // DeviceGuard omitted
  return at::native::_upsample_bicubic2d_aa(input, output_size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*output_size)) : c10::nullopt, align_corners, scale_factors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_vec_upsample_nearest1d(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check
  // DeviceGuard omitted
  return at::native::upsample_nearest1d(input, output_size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*output_size)) : c10::nullopt, scale_factors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_vec__upsample_nearest_exact1d(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check
  // DeviceGuard omitted
  return at::native::_upsample_nearest_exact1d(input, output_size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*output_size)) : c10::nullopt, scale_factors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_vec_upsample_nearest2d(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check
  // DeviceGuard omitted
  return at::native::upsample_nearest2d(input, output_size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*output_size)) : c10::nullopt, scale_factors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_vec__upsample_nearest_exact2d(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check
  // DeviceGuard omitted
  return at::native::_upsample_nearest_exact2d(input, output_size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*output_size)) : c10::nullopt, scale_factors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_vec_upsample_nearest3d(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check
  // DeviceGuard omitted
  return at::native::upsample_nearest3d(input, output_size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*output_size)) : c10::nullopt, scale_factors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_vec__upsample_nearest_exact3d(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check
  // DeviceGuard omitted
  return at::native::_upsample_nearest_exact3d(input, output_size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*output_size)) : c10::nullopt, scale_factors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__thnn_conv2d(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return at::native::thnn_conv2d(self, weight, C10_AS_INTARRAYREF_SLOW(kernel_size), bias, C10_AS_INTARRAYREF_SLOW(stride), C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_thnn_conv2d_out(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::thnn_conv2d_out(self, weight, C10_AS_INTARRAYREF_SLOW(kernel_size), bias, C10_AS_INTARRAYREF_SLOW(stride), C10_AS_INTARRAYREF_SLOW(padding), out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__slow_conv3d(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return at::native::slow_conv3d(self, weight, C10_AS_INTARRAYREF_SLOW(kernel_size), bias, C10_AS_INTARRAYREF_SLOW(stride), C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_slow_conv3d_out(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::slow_conv3d_out(self, weight, C10_AS_INTARRAYREF_SLOW(kernel_size), bias, C10_AS_INTARRAYREF_SLOW(stride), C10_AS_INTARRAYREF_SLOW(padding), out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__column_stack(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::column_stack(tensors);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_column_stack_out(at::TensorList tensors, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::column_stack_out(tensors, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__isfinite(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::isfinite(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___add_batch_dim(const at::Tensor & self, int64_t batch_dim, int64_t level) {
    // No device check
  // DeviceGuard omitted
  return at::native::_add_batch_dim(self, batch_dim, level);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___remove_batch_dim(const at::Tensor & self, int64_t level, int64_t batch_size, int64_t out_dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::_remove_batch_dim(self, level, batch_size, out_dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_expm1(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_expm1(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_expm1_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_expm1_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_exp2(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_exp2(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_exp2_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_exp2_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_psi(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_psi(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_psi_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_psi_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_digamma(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_digamma(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_digamma_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_digamma_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_gammaln(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_gammaln(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_gammaln_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_gammaln_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_erf(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_erf(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_erf_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_erf_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_erfc(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_erfc(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_erfc_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_erfc_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_erfinv(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_erfinv(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_erfinv_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_erfinv_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_ndtr(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_ndtr(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_ndtr_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_ndtr_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_xlogy(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_xlogy(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_xlogy_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_xlogy_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_self_scalar_special_xlogy(const at::Scalar & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_xlogy(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_self_scalar_out_special_xlogy_out(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_xlogy_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_other_scalar_special_xlogy(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_xlogy(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_other_scalar_out_special_xlogy_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_xlogy_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_i0(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_i0(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_i0_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_i0_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_logit(const at::Tensor & self, c10::optional<double> eps) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_logit(self, eps);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_logit_out(const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_logit_out(self, eps, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_polygamma(int64_t n, const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_polygamma(n, self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_polygamma_out(int64_t n, const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_polygamma_out(n, self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_logsumexp(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_logsumexp(self, dim, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_logsumexp_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_logsumexp_out(self, dim, keepdim, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_expit(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_expit(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_expit_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_expit_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_sinc(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_sinc(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_sinc_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_sinc_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_round(const at::Tensor & self, int64_t decimals) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_round(self, decimals);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_round_out(const at::Tensor & self, int64_t decimals, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_round_out(self, decimals, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_log1p(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_log1p(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_log1p_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_log1p_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_log_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_log_softmax(self, dim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_gammainc(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_gammainc(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_gammainc_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_gammainc_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_gammaincc(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_gammaincc(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_gammaincc_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_gammaincc_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_multigammaln(const at::Tensor & self, int64_t p) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_multigammaln(self, p);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_special_multigammaln_out(const at::Tensor & self, int64_t p, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_multigammaln_out(self, p, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__special_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::special_softmax(self, dim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_fft(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_fft_symint(self, n, dim, norm);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_fft_out(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_fft_symint_out(self, n, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_ifft(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_ifft_symint(self, n, dim, norm);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_ifft_out(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_ifft_symint_out(self, n, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_rfft(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_rfft_symint(self, n, dim, norm);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_rfft_out(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_rfft_symint_out(self, n, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_irfft(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_irfft_symint(self, n, dim, norm);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_irfft_out(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_irfft_symint_out(self, n, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_hfft(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_hfft_symint(self, n, dim, norm);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_hfft_out(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_hfft_symint_out(self, n, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_ihfft(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_ihfft_symint(self, n, dim, norm);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_ihfft_out(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_ihfft_symint_out(self, n, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_fft2(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_fft2_symint(self, s, dim, norm);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_fft2_out(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_fft2_symint_out(self, s, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_ifft2(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_ifft2_symint(self, s, dim, norm);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_ifft2_out(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_ifft2_symint_out(self, s, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_rfft2(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_rfft2_symint(self, s, dim, norm);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_rfft2_out(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_rfft2_symint_out(self, s, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_irfft2(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_irfft2_symint(self, s, dim, norm);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_irfft2_out(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_irfft2_symint_out(self, s, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_hfft2(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_hfft2_symint(self, s, dim, norm);
}
} // anonymous namespace
namespace {
const at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_hfft2_out(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_hfft2_symint_out(self, s, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_ihfft2(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_ihfft2_symint(self, s, dim, norm);
}
} // anonymous namespace
namespace {
const at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_ihfft2_out(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_ihfft2_symint_out(self, s, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_fftn(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_fftn_symint(self, s, dim, norm);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_fftn_out(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_fftn_symint_out(self, s, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_ifftn(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_ifftn_symint(self, s, dim, norm);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_ifftn_out(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_ifftn_symint_out(self, s, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_rfftn(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_rfftn_symint(self, s, dim, norm);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_rfftn_out(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_rfftn_symint_out(self, s, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_irfftn(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_irfftn_symint(self, s, dim, norm);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_irfftn_out(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_irfftn_symint_out(self, s, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_hfftn(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_hfftn_symint(self, s, dim, norm);
}
} // anonymous namespace
namespace {
const at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_hfftn_out(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_hfftn_symint_out(self, s, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_ihfftn(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_ihfftn_symint(self, s, dim, norm);
}
} // anonymous namespace
namespace {
const at::Tensor & wrapper_CompositeImplicitAutograd_out_fft_ihfftn_out(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_ihfftn_symint_out(self, s, dim, norm, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_fftshift(const at::Tensor & self, at::OptionalIntArrayRef dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_fftshift(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__fft_ifftshift(const at::Tensor & self, at::OptionalIntArrayRef dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::fft_ifftshift(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_cholesky(const at::Tensor & self, bool upper) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_cholesky(self, upper);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_cholesky_out(const at::Tensor & self, bool upper, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_cholesky_out(self, upper, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__linalg_lu_factor(const at::Tensor & A, bool pivot) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_lu_factor(A, pivot);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_out_linalg_lu_factor_out(const at::Tensor & A, bool pivot, at::Tensor & LU, at::Tensor & pivots) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_lu_factor_out(A, pivot, LU, pivots);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_det(const at::Tensor & A) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_det(A);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_det_out(const at::Tensor & A, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_det_out(A, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__det(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::det(self);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__linalg_ldl_factor(const at::Tensor & self, bool hermitian) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_ldl_factor(self, hermitian);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_out_linalg_ldl_factor_out(const at::Tensor & self, bool hermitian, at::Tensor & LD, at::Tensor & pivots) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_ldl_factor_out(self, hermitian, LD, pivots);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_matmul(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matmul(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_matmul_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matmul_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_vecdot(const at::Tensor & x, const at::Tensor & y, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_vecdot(x, y, dim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_vecdot_out(const at::Tensor & x, const at::Tensor & y, int64_t dim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_vecdot_out(x, y, dim, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__linalg_slogdet(const at::Tensor & A) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_slogdet(A);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_out_linalg_slogdet_out(const at::Tensor & A, at::Tensor & sign, at::Tensor & logabsdet) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_slogdet_out(A, sign, logabsdet);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__slogdet(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::slogdet(self);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_out_slogdet_out(const at::Tensor & self, at::Tensor & sign, at::Tensor & logabsdet) {
    // No device check
  // DeviceGuard omitted
  return at::native::slogdet_out(self, sign, logabsdet);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__logdet(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::logdet(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_eigvals(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_eigvals(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_eigvals_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_eigvals_out(self, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__linalg_eigh(const at::Tensor & self, c10::string_view UPLO) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_eigh(self, UPLO);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_eigvals_linalg_eigh_out(const at::Tensor & self, c10::string_view UPLO, at::Tensor & eigvals, at::Tensor & eigvecs) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_eigh_out(self, UPLO, eigvals, eigvecs);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_eigvalsh(const at::Tensor & self, c10::string_view UPLO) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_eigvalsh(self, UPLO);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_eigvalsh_out(const at::Tensor & self, c10::string_view UPLO, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_eigvalsh_out(self, UPLO, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_inv(const at::Tensor & A) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_inv(A);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_inv_out(const at::Tensor & A, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_inv_out(A, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__inverse(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::inverse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_inverse_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::inverse_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__inner(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::inner(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_inner_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::inner_out(self, other, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__outer(const at::Tensor & self, const at::Tensor & vec2) {
    // No device check
  // DeviceGuard omitted
  return at::native::outer(self, vec2);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_outer_out(const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::outer_out(self, vec2, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__ger(const at::Tensor & self, const at::Tensor & vec2) {
    // No device check
  // DeviceGuard omitted
  return at::native::ger(self, vec2);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_ger_out(const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::ger_out(self, vec2, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_norm(const at::Tensor & self, const c10::optional<at::Scalar> & ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_norm(self, ord, dim, keepdim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_norm_out(const at::Tensor & self, const c10::optional<at::Scalar> & ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_norm_out(self, ord, dim, keepdim, dtype, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_ord_str_linalg_norm(const at::Tensor & self, c10::string_view ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_norm(self, ord, dim, keepdim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_ord_str_out_linalg_norm_out(const at::Tensor & self, c10::string_view ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_norm_out(self, ord, dim, keepdim, dtype, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_matrix_norm(const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matrix_norm(self, ord, dim, keepdim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_matrix_norm_out(const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matrix_norm_out(self, ord, dim, keepdim, dtype, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_str_ord_linalg_matrix_norm(const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matrix_norm(self, ord, dim, keepdim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_str_ord_out_linalg_matrix_norm_out(const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matrix_norm_out(self, ord, dim, keepdim, dtype, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__linalg_svd(const at::Tensor & A, bool full_matrices, c10::optional<c10::string_view> driver) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_svd(A, full_matrices, driver);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_U_linalg_svd_out(const at::Tensor & A, bool full_matrices, c10::optional<c10::string_view> driver, at::Tensor & U, at::Tensor & S, at::Tensor & Vh) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_svd_out(A, full_matrices, driver, U, S, Vh);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_svdvals(const at::Tensor & A, c10::optional<c10::string_view> driver) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_svdvals(A, driver);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_svdvals_out(const at::Tensor & A, c10::optional<c10::string_view> driver, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_svdvals_out(A, driver, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_cond(const at::Tensor & self, const c10::optional<at::Scalar> & p) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_cond(self, p);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_cond_out(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_cond_out(self, p, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_p_str_linalg_cond(const at::Tensor & self, c10::string_view p) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_cond(self, p);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_p_str_out_linalg_cond_out(const at::Tensor & self, c10::string_view p, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_cond_out(self, p, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_atol_rtol_float_linalg_pinv(const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_pinv(self, atol, rtol, hermitian);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_atol_rtol_float_out_linalg_pinv_out(const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_pinv_out(self, atol, rtol, hermitian, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_pinv(const at::Tensor & self, double rcond, bool hermitian) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_pinv(self, rcond, hermitian);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_pinv_out(const at::Tensor & self, double rcond, bool hermitian, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_pinv_out(self, rcond, hermitian, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_rcond_tensor_linalg_pinv(const at::Tensor & self, const at::Tensor & rcond, bool hermitian) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_pinv(self, rcond, hermitian);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_rcond_tensor_linalg_pinv_out(const at::Tensor & self, const at::Tensor & rcond, bool hermitian, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_pinv_out(self, rcond, hermitian, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd__linalg_solve_ex(const at::Tensor & A, const at::Tensor & B, bool left, bool check_errors) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_solve_ex(A, B, left, check_errors);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_CompositeImplicitAutograd_out_linalg_solve_ex_out(const at::Tensor & A, const at::Tensor & B, bool left, bool check_errors, at::Tensor & result, at::Tensor & info) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_solve_ex_out(A, B, left, check_errors, result, info);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_solve(const at::Tensor & A, const at::Tensor & B, bool left) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_solve(A, B, left);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_solve_out(const at::Tensor & A, const at::Tensor & B, bool left, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_solve_out(A, B, left, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_tensorinv(const at::Tensor & self, int64_t ind) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_tensorinv(self, ind);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_tensorinv_out(const at::Tensor & self, int64_t ind, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_tensorinv_out(self, ind, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_tensorsolve(const at::Tensor & self, const at::Tensor & other, at::OptionalIntArrayRef dims) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_tensorsolve(self, other, dims);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_tensorsolve_out(const at::Tensor & self, const at::Tensor & other, at::OptionalIntArrayRef dims, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_tensorsolve_out(self, other, dims, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_matrix_power(const at::Tensor & self, int64_t n) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matrix_power(self, n);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_matrix_power_out(const at::Tensor & self, int64_t n, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matrix_power_out(self, n, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_atol_rtol_tensor_linalg_matrix_rank(const at::Tensor & input, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matrix_rank(input, atol, rtol, hermitian);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_atol_rtol_tensor_out_linalg_matrix_rank_out(const at::Tensor & input, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matrix_rank_out(input, atol, rtol, hermitian, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_atol_rtol_float_linalg_matrix_rank(const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matrix_rank(self, atol, rtol, hermitian);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_atol_rtol_float_out_linalg_matrix_rank_out(const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matrix_rank_out(self, atol, rtol, hermitian, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_matrix_rank(const at::Tensor & self, double tol, bool hermitian) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matrix_rank(self, tol, hermitian);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_matrix_rank_out(const at::Tensor & self, double tol, bool hermitian, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matrix_rank_out(self, tol, hermitian, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_tol_tensor_linalg_matrix_rank(const at::Tensor & input, const at::Tensor & tol, bool hermitian) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matrix_rank(input, tol, hermitian);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_tol_tensor_linalg_matrix_rank_out(const at::Tensor & input, const at::Tensor & tol, bool hermitian, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_matrix_rank_out(input, tol, hermitian, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__linalg_multi_dot(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_multi_dot(tensors);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_CompositeImplicitAutograd_out_linalg_multi_dot_out(at::TensorList tensors, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::linalg_multi_dot_out(tensors, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__nested_to_padded_tensor(const at::Tensor & self, double padding, at::OptionalIntArrayRef output_size) {
    // No device check
  // DeviceGuard omitted
  return at::native::nested_to_padded_tensor(self, padding, output_size);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___test_serialization_subcmul(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return at::native::_test_serialization_subcmul(self, other, alpha);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd___test_string_default(const at::Tensor & dummy, c10::string_view a, c10::string_view b) {
    // No device check
  // DeviceGuard omitted
  return at::native::_test_string_default(dummy, a, b);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_a__test_ambiguous_defaults(const at::Tensor & dummy, int64_t a, int64_t b) {
    // No device check
  // DeviceGuard omitted
  return at::native::_test_ambiguous_defaults(dummy, a, b);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_b__test_ambiguous_defaults(const at::Tensor & dummy, int64_t a, c10::string_view b) {
    // No device check
  // DeviceGuard omitted
  return at::native::_test_ambiguous_defaults(dummy, a, b);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd_ntonly__test_autograd_multiple_dispatch(const at::Tensor & self, bool b) {
    // No device check
  // DeviceGuard omitted
  return at::native::_test_autograd_multiple_dispatch_ntonly(self, b);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__pad_sequence(at::TensorList sequences, bool batch_first, double padding_value) {
    // No device check
  // DeviceGuard omitted
  return at::native::pad_sequence(sequences, batch_first, padding_value);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__flatten_dense_tensors(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::flatten_dense_tensors(tensors);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_CompositeImplicitAutograd__unflatten_dense_tensors(const at::Tensor & flat, at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return at::native::unflatten_dense_tensors(flat, tensors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_CompositeImplicitAutograd__scaled_dot_product_attention(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & attn_mask, double dropout_p, bool is_causal, c10::optional<double> scale) {
    // No device check
  // DeviceGuard omitted
  return at::native::scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal, scale);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_CompositeImplicitAutograd___scaled_dot_product_attention_math(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & attn_mask, double dropout_p, bool is_causal, const c10::optional<at::Tensor> & dropout_mask, c10::optional<double> scale) {
    // No device check
  // DeviceGuard omitted
  return at::native::_scaled_dot_product_attention_math(query, key, value, attn_mask, dropout_p, is_causal, dropout_mask, scale);
}
} // anonymous namespace
namespace {
void wrapper_CompositeImplicitAutograd___propagate_xla_data(const at::Tensor & input, const at::Tensor & output) {
    // No device check
  // DeviceGuard omitted
  return at::native::_propagate_xla_data(input, output);
}
} // anonymous namespace
TORCH_LIBRARY_IMPL(aten, CompositeImplicitAutograd, m) {
    m.impl("_cast_Byte",
TORCH_FN(wrapper_CompositeImplicitAutograd___cast_Byte));
m.impl("_cast_Char",
TORCH_FN(wrapper_CompositeImplicitAutograd___cast_Char));
m.impl("_cast_Double",
TORCH_FN(wrapper_CompositeImplicitAutograd___cast_Double));
m.impl("_cast_Float",
TORCH_FN(wrapper_CompositeImplicitAutograd___cast_Float));
m.impl("_cast_Int",
TORCH_FN(wrapper_CompositeImplicitAutograd___cast_Int));
m.impl("_cast_Long",
TORCH_FN(wrapper_CompositeImplicitAutograd___cast_Long));
m.impl("_cast_Short",
TORCH_FN(wrapper_CompositeImplicitAutograd___cast_Short));
m.impl("_cast_Half",
TORCH_FN(wrapper_CompositeImplicitAutograd___cast_Half));
m.impl("_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd___backward));
m.impl("set_data",
TORCH_FN(wrapper_CompositeImplicitAutograd__set_data));
m.impl("data",
TORCH_FN(wrapper_CompositeImplicitAutograd__data));
m.impl("is_leaf",
TORCH_FN(wrapper_CompositeImplicitAutograd__is_leaf));
m.impl("output_nr",
TORCH_FN(wrapper_CompositeImplicitAutograd__output_nr));
m.impl("_version",
TORCH_FN(wrapper_CompositeImplicitAutograd___version));
m.impl("requires_grad_",
TORCH_FN(wrapper_CompositeImplicitAutograd__requires_grad_));
m.impl("retain_grad",
TORCH_FN(wrapper_CompositeImplicitAutograd__retain_grad));
m.impl("retains_grad",
TORCH_FN(wrapper_CompositeImplicitAutograd__retains_grad));
m.impl("_unpack_dual",
TORCH_FN(wrapper_CompositeImplicitAutograd___unpack_dual));
m.impl("rename_",
TORCH_FN(wrapper_CompositeImplicitAutograd__rename_));
m.impl("rename",
TORCH_FN(wrapper_CompositeImplicitAutograd__rename));
m.impl("align_to",
TORCH_FN(wrapper_CompositeImplicitAutograd__align_to));
m.impl("align_to.ellipsis_idx",
TORCH_FN(wrapper_CompositeImplicitAutograd_ellipsis_idx_align_to));
m.impl("align_as",
TORCH_FN(wrapper_CompositeImplicitAutograd__align_as));
m.impl("align_tensors",
TORCH_FN(wrapper_CompositeImplicitAutograd__align_tensors));
m.impl("_assert_tensor_metadata",
TORCH_FN(wrapper_CompositeImplicitAutograd___assert_tensor_metadata));
m.impl("refine_names",
TORCH_FN(wrapper_CompositeImplicitAutograd__refine_names));
m.impl("_use_cudnn_rnn_flatten_weight",
TORCH_FN(wrapper_CompositeImplicitAutograd___use_cudnn_rnn_flatten_weight));
m.impl("_debug_has_internal_overlap",
TORCH_FN(wrapper_CompositeImplicitAutograd___debug_has_internal_overlap));
m.impl("_sobol_engine_draw",
TORCH_FN(wrapper_CompositeImplicitAutograd___sobol_engine_draw));
m.impl("_sobol_engine_ff_",
TORCH_FN(wrapper_CompositeImplicitAutograd___sobol_engine_ff_));
m.impl("_sobol_engine_scramble_",
TORCH_FN(wrapper_CompositeImplicitAutograd___sobol_engine_scramble_));
m.impl("_sobol_engine_initialize_state_",
TORCH_FN(wrapper_CompositeImplicitAutograd___sobol_engine_initialize_state_));
m.impl("_reshape_from_tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd___reshape_from_tensor));
m.impl("_shape_as_tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd___shape_as_tensor));
m.impl("dropout",
TORCH_FN(wrapper_CompositeImplicitAutograd__dropout));
m.impl("dropout_",
TORCH_FN(wrapper_CompositeImplicitAutograd__dropout_));
m.impl("feature_dropout",
TORCH_FN(wrapper_CompositeImplicitAutograd__feature_dropout));
m.impl("feature_dropout_",
TORCH_FN(wrapper_CompositeImplicitAutograd__feature_dropout_));
m.impl("alpha_dropout",
TORCH_FN(wrapper_CompositeImplicitAutograd__alpha_dropout));
m.impl("alpha_dropout_",
TORCH_FN(wrapper_CompositeImplicitAutograd__alpha_dropout_));
m.impl("feature_alpha_dropout",
TORCH_FN(wrapper_CompositeImplicitAutograd__feature_alpha_dropout));
m.impl("feature_alpha_dropout_",
TORCH_FN(wrapper_CompositeImplicitAutograd__feature_alpha_dropout_));
m.impl("absolute",
TORCH_FN(wrapper_CompositeImplicitAutograd__absolute));
m.impl("absolute.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_absolute_out));
m.impl("absolute_",
TORCH_FN(wrapper_CompositeImplicitAutograd__absolute_));
m.impl("chalf",
TORCH_FN(wrapper_CompositeImplicitAutograd__chalf));
m.impl("real",
TORCH_FN(wrapper_CompositeImplicitAutograd__real));
m.impl("imag",
TORCH_FN(wrapper_CompositeImplicitAutograd__imag));
m.impl("conj",
TORCH_FN(wrapper_CompositeImplicitAutograd__conj));
m.impl("conj_physical",
TORCH_FN(wrapper_CompositeImplicitAutograd__conj_physical));
m.impl("resolve_conj",
TORCH_FN(wrapper_CompositeImplicitAutograd__resolve_conj));
m.impl("resolve_neg",
TORCH_FN(wrapper_CompositeImplicitAutograd__resolve_neg));
m.impl("arccos",
TORCH_FN(wrapper_CompositeImplicitAutograd__arccos));
m.impl("arccos.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_arccos_out));
m.impl("arccos_",
TORCH_FN(wrapper_CompositeImplicitAutograd__arccos_));
m.impl("avg_pool1d",
TORCH_FN(wrapper_CompositeImplicitAutograd__avg_pool1d));
m.impl("adaptive_avg_pool1d",
TORCH_FN(wrapper_CompositeImplicitAutograd__adaptive_avg_pool1d));
m.impl("adaptive_max_pool1d",
TORCH_FN(wrapper_CompositeImplicitAutograd__adaptive_max_pool1d));
m.impl("affine_grid_generator_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__affine_grid_generator_backward));
m.impl("_test_check_tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd___test_check_tensor));
m.impl("all.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_all));
m.impl("all.dimname_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_out_all_out));
m.impl("any.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_any));
m.impl("any.dimname_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_out_any_out));
m.impl("_dim_arange",
TORCH_FN(wrapper_CompositeImplicitAutograd___dim_arange));
m.impl("arccosh",
TORCH_FN(wrapper_CompositeImplicitAutograd__arccosh));
m.impl("arccosh.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_arccosh_out));
m.impl("arccosh_",
TORCH_FN(wrapper_CompositeImplicitAutograd__arccosh_));
m.impl("arcsinh",
TORCH_FN(wrapper_CompositeImplicitAutograd__arcsinh));
m.impl("arcsinh.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_arcsinh_out));
m.impl("arcsinh_",
TORCH_FN(wrapper_CompositeImplicitAutograd__arcsinh_));
m.impl("arctanh",
TORCH_FN(wrapper_CompositeImplicitAutograd__arctanh));
m.impl("arctanh.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_arctanh_out));
m.impl("arctanh_",
TORCH_FN(wrapper_CompositeImplicitAutograd__arctanh_));
m.impl("arcsin",
TORCH_FN(wrapper_CompositeImplicitAutograd__arcsin));
m.impl("arcsin.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_arcsin_out));
m.impl("arcsin_",
TORCH_FN(wrapper_CompositeImplicitAutograd__arcsin_));
m.impl("arctan",
TORCH_FN(wrapper_CompositeImplicitAutograd__arctan));
m.impl("arctan.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_arctan_out));
m.impl("arctan_",
TORCH_FN(wrapper_CompositeImplicitAutograd__arctan_));
m.impl("atleast_1d",
TORCH_FN(wrapper_CompositeImplicitAutograd__atleast_1d));
m.impl("atleast_1d.Sequence",
TORCH_FN(wrapper_CompositeImplicitAutograd_Sequence_atleast_1d));
m.impl("atleast_2d",
TORCH_FN(wrapper_CompositeImplicitAutograd__atleast_2d));
m.impl("atleast_2d.Sequence",
TORCH_FN(wrapper_CompositeImplicitAutograd_Sequence_atleast_2d));
m.impl("atleast_3d",
TORCH_FN(wrapper_CompositeImplicitAutograd__atleast_3d));
m.impl("atleast_3d.Sequence",
TORCH_FN(wrapper_CompositeImplicitAutograd_Sequence_atleast_3d));
m.impl("batch_norm",
TORCH_FN(wrapper_CompositeImplicitAutograd__batch_norm));
m.impl("_batch_norm_impl_index",
TORCH_FN(wrapper_CompositeImplicitAutograd___batch_norm_impl_index));
m.impl("_batch_norm_impl_index_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd___batch_norm_impl_index_backward));
m.impl("bilinear",
TORCH_FN(wrapper_CompositeImplicitAutograd__bilinear));
m.impl("broadcast_tensors",
TORCH_FN(wrapper_CompositeImplicitAutograd__broadcast_tensors));
m.impl("broadcast_to",
TORCH_FN(wrapper_CompositeImplicitAutograd__broadcast_to));
m.impl("cat.names",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_cat));
m.impl("cat.names_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_out_cat_out));
m.impl("concat",
TORCH_FN(wrapper_CompositeImplicitAutograd__concat));
m.impl("concat.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_concat_out));
m.impl("concat.names",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_concat));
m.impl("concat.names_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_out_concat_out));
m.impl("concatenate",
TORCH_FN(wrapper_CompositeImplicitAutograd__concatenate));
m.impl("concatenate.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_concatenate_out));
m.impl("concatenate.names",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_concatenate));
m.impl("concatenate.names_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_out_concatenate_out));
m.impl("chain_matmul",
TORCH_FN(wrapper_CompositeImplicitAutograd__chain_matmul));
m.impl("chain_matmul.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_chain_matmul_out));
m.impl("unsafe_chunk",
TORCH_FN(wrapper_CompositeImplicitAutograd__unsafe_chunk));
m.impl("chunk",
TORCH_FN(wrapper_CompositeImplicitAutograd__chunk));
m.impl("tensor_split.sections",
TORCH_FN(wrapper_CompositeImplicitAutograd_sections_tensor_split));
m.impl("tensor_split.indices",
TORCH_FN(wrapper_CompositeImplicitAutograd_indices_tensor_split));
m.impl("tensor_split.tensor_indices_or_sections",
TORCH_FN(wrapper_CompositeImplicitAutograd_tensor_indices_or_sections_tensor_split));
m.impl("clip",
TORCH_FN(wrapper_CompositeImplicitAutograd__clip));
m.impl("clip.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_clip_out));
m.impl("clip_",
TORCH_FN(wrapper_CompositeImplicitAutograd__clip_));
m.impl("clip.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_clip));
m.impl("clip.Tensor_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_out_clip_out));
m.impl("clip_.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_clip_));
m.impl("cudnn_is_acceptable",
TORCH_FN(wrapper_CompositeImplicitAutograd__cudnn_is_acceptable));
m.impl("contiguous",
TORCH_FN(wrapper_CompositeImplicitAutograd__contiguous));
m.impl("_convolution.deprecated",
TORCH_FN(wrapper_CompositeImplicitAutograd_deprecated__convolution));
m.impl("_convolution_mode",
TORCH_FN(wrapper_CompositeImplicitAutograd___convolution_mode));
m.impl("_convolution_double_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd___convolution_double_backward));
m.impl("conv1d",
TORCH_FN(wrapper_CompositeImplicitAutograd__conv1d));
m.impl("conv2d",
TORCH_FN(wrapper_CompositeImplicitAutograd__conv2d));
m.impl("conv3d",
TORCH_FN(wrapper_CompositeImplicitAutograd__conv3d));
m.impl("conv1d.padding",
TORCH_FN(wrapper_CompositeImplicitAutograd_padding_conv1d));
m.impl("conv2d.padding",
TORCH_FN(wrapper_CompositeImplicitAutograd_padding_conv2d));
m.impl("conv3d.padding",
TORCH_FN(wrapper_CompositeImplicitAutograd_padding_conv3d));
m.impl("conv_tbc_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__conv_tbc_backward));
m.impl("conv_transpose1d",
TORCH_FN(wrapper_CompositeImplicitAutograd__conv_transpose1d));
m.impl("conv_transpose2d.input",
TORCH_FN(wrapper_CompositeImplicitAutograd_input_conv_transpose2d));
m.impl("conv_transpose3d.input",
TORCH_FN(wrapper_CompositeImplicitAutograd_input_conv_transpose3d));
m.impl("cosine_embedding_loss",
TORCH_FN(wrapper_CompositeImplicitAutograd__cosine_embedding_loss));
m.impl("cov",
TORCH_FN(wrapper_CompositeImplicitAutograd__cov));
m.impl("corrcoef",
TORCH_FN(wrapper_CompositeImplicitAutograd__corrcoef));
m.impl("cummax.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_cummax));
m.impl("cummax.dimname_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_out_cummax_out));
m.impl("cummin.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_cummin));
m.impl("cummin.dimname_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_out_cummin_out));
m.impl("cummaxmin_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__cummaxmin_backward));
m.impl("cumprod.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_cumprod));
m.impl("cumprod.dimname_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_out_cumprod_out));
m.impl("cumprod_.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_cumprod_));
m.impl("cumprod_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__cumprod_backward));
m.impl("cumsum.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_cumsum));
m.impl("cumsum.dimname_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_out_cumsum_out));
m.impl("cumsum_.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_cumsum_));
m.impl("cumulative_trapezoid.x",
TORCH_FN(wrapper_CompositeImplicitAutograd_x_cumulative_trapezoid));
m.impl("cumulative_trapezoid.dx",
TORCH_FN(wrapper_CompositeImplicitAutograd_dx_cumulative_trapezoid));
m.impl("ctc_loss.IntList",
TORCH_FN(wrapper_CompositeImplicitAutograd_IntList_ctc_loss));
m.impl("ctc_loss.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_ctc_loss));
m.impl("diagflat",
TORCH_FN(wrapper_CompositeImplicitAutograd__diagflat));
m.impl("linalg_diagonal",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_diagonal));
m.impl("diagonal.Dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname_diagonal));
m.impl("fill_diagonal_",
TORCH_FN(wrapper_CompositeImplicitAutograd__fill_diagonal_));
m.impl("diff",
TORCH_FN(wrapper_CompositeImplicitAutograd__diff));
m.impl("diff.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_diff_out));
m.impl("gradient.scalarint",
TORCH_FN(wrapper_CompositeImplicitAutograd_scalarint_gradient));
m.impl("gradient.scalararray",
TORCH_FN(wrapper_CompositeImplicitAutograd_scalararray_gradient));
m.impl("gradient.array",
TORCH_FN(wrapper_CompositeImplicitAutograd_array_gradient));
m.impl("gradient.scalarrayint",
TORCH_FN(wrapper_CompositeImplicitAutograd_scalarrayint_gradient));
m.impl("gradient.scalarrayarray",
TORCH_FN(wrapper_CompositeImplicitAutograd_scalarrayarray_gradient));
m.impl("gradient.tensorarrayint",
TORCH_FN(wrapper_CompositeImplicitAutograd_tensorarrayint_gradient));
m.impl("gradient.tensorarray",
TORCH_FN(wrapper_CompositeImplicitAutograd_tensorarray_gradient));
m.impl("divide.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_divide));
m.impl("divide.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_divide_out));
m.impl("divide_.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_divide_));
m.impl("divide.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_divide));
m.impl("divide_.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_divide_));
m.impl("divide.Tensor_mode",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_mode_divide));
m.impl("divide.out_mode",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_mode_divide_out));
m.impl("divide_.Tensor_mode",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_mode_divide_));
m.impl("divide.Scalar_mode",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_mode_divide));
m.impl("divide_.Scalar_mode",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_mode_divide_));
m.impl("true_divide.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_true_divide));
m.impl("true_divide.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_true_divide_out));
m.impl("true_divide_.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_true_divide_));
m.impl("true_divide.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_true_divide));
m.impl("true_divide_.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_true_divide_));
m.impl("einsum",
TORCH_FN(wrapper_CompositeImplicitAutograd__einsum));
m.impl("embedding_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__embedding_backward));
m.impl("embedding_sparse_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__embedding_sparse_backward));
m.impl("_rowwise_prune",
TORCH_FN(wrapper_CompositeImplicitAutograd___rowwise_prune));
m.impl("row_stack",
TORCH_FN(wrapper_CompositeImplicitAutograd__row_stack));
m.impl("row_stack.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_row_stack_out));
m.impl("embedding_bag",
TORCH_FN(wrapper_CompositeImplicitAutograd__embedding_bag));
m.impl("embedding_bag.padding_idx",
TORCH_FN(wrapper_CompositeImplicitAutograd_padding_idx_embedding_bag));
m.impl("_embedding_bag_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd___embedding_bag_backward));
m.impl("_embedding_bag_sparse_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd___embedding_bag_sparse_backward));
m.impl("empty.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_empty_out));
m.impl("expand_as",
TORCH_FN(wrapper_CompositeImplicitAutograd__expand_as));
m.impl("flatten.using_ints",
TORCH_FN(wrapper_CompositeImplicitAutograd_using_ints_flatten));
m.impl("flatten.named_out_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_named_out_dim_flatten));
m.impl("flatten.using_names",
TORCH_FN(wrapper_CompositeImplicitAutograd_using_names_flatten));
m.impl("flatten.DimnameList",
TORCH_FN(wrapper_CompositeImplicitAutograd_DimnameList_flatten));
m.impl("unflatten.int",
TORCH_FN(wrapper_CompositeImplicitAutograd_int_unflatten));
m.impl("unflatten.Dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname_unflatten));
m.impl("grid_sampler",
TORCH_FN(wrapper_CompositeImplicitAutograd__grid_sampler));
m.impl("_grid_sampler_2d_cpu_fallback_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd___grid_sampler_2d_cpu_fallback_backward));
m.impl("hinge_embedding_loss",
TORCH_FN(wrapper_CompositeImplicitAutograd__hinge_embedding_loss));
m.impl("group_norm",
TORCH_FN(wrapper_CompositeImplicitAutograd__group_norm));
m.impl("_cufft_get_plan_cache_size",
TORCH_FN(wrapper_CompositeImplicitAutograd___cufft_get_plan_cache_size));
m.impl("_cufft_get_plan_cache_max_size",
TORCH_FN(wrapper_CompositeImplicitAutograd___cufft_get_plan_cache_max_size));
m.impl("_cufft_set_plan_cache_max_size",
TORCH_FN(wrapper_CompositeImplicitAutograd___cufft_set_plan_cache_max_size));
m.impl("_cufft_clear_plan_cache",
TORCH_FN(wrapper_CompositeImplicitAutograd___cufft_clear_plan_cache));
m.impl("index_copy_.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_index_copy_));
m.impl("index_copy.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_index_copy));
m.impl("instance_norm",
TORCH_FN(wrapper_CompositeImplicitAutograd__instance_norm));
m.impl("isclose",
TORCH_FN(wrapper_CompositeImplicitAutograd__isclose));
m.impl("is_distributed",
TORCH_FN(wrapper_CompositeImplicitAutograd__is_distributed));
m.impl("is_floating_point",
TORCH_FN(wrapper_CompositeImplicitAutograd__is_floating_point));
m.impl("is_complex",
TORCH_FN(wrapper_CompositeImplicitAutograd__is_complex));
m.impl("is_conj",
TORCH_FN(wrapper_CompositeImplicitAutograd__is_conj));
m.impl("_is_zerotensor",
TORCH_FN(wrapper_CompositeImplicitAutograd___is_zerotensor));
m.impl("is_neg",
TORCH_FN(wrapper_CompositeImplicitAutograd__is_neg));
m.impl("isreal",
TORCH_FN(wrapper_CompositeImplicitAutograd__isreal));
m.impl("is_nonzero",
TORCH_FN(wrapper_CompositeImplicitAutograd__is_nonzero));
m.impl("is_signed",
TORCH_FN(wrapper_CompositeImplicitAutograd__is_signed));
m.impl("is_inference",
TORCH_FN(wrapper_CompositeImplicitAutograd__is_inference));
m.impl("kl_div",
TORCH_FN(wrapper_CompositeImplicitAutograd__kl_div));
m.impl("kron",
TORCH_FN(wrapper_CompositeImplicitAutograd__kron));
m.impl("kron.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_kron_out));
m.impl("kthvalue.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_kthvalue));
m.impl("kthvalue.dimname_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_out_kthvalue_out));
m.impl("layer_norm",
TORCH_FN(wrapper_CompositeImplicitAutograd__layer_norm));
m.impl("linear",
TORCH_FN(wrapper_CompositeImplicitAutograd__linear));
m.impl("fbgemm_linear_int8_weight_fp32_activation",
TORCH_FN(wrapper_CompositeImplicitAutograd__fbgemm_linear_int8_weight_fp32_activation));
m.impl("fbgemm_linear_int8_weight",
TORCH_FN(wrapper_CompositeImplicitAutograd__fbgemm_linear_int8_weight));
m.impl("fbgemm_linear_quantize_weight",
TORCH_FN(wrapper_CompositeImplicitAutograd__fbgemm_linear_quantize_weight));
m.impl("fbgemm_pack_gemm_matrix_fp16",
TORCH_FN(wrapper_CompositeImplicitAutograd__fbgemm_pack_gemm_matrix_fp16));
m.impl("fbgemm_linear_fp16_weight_fp32_activation",
TORCH_FN(wrapper_CompositeImplicitAutograd__fbgemm_linear_fp16_weight_fp32_activation));
m.impl("fbgemm_linear_fp16_weight",
TORCH_FN(wrapper_CompositeImplicitAutograd__fbgemm_linear_fp16_weight));
m.impl("fbgemm_pack_quantized_matrix",
TORCH_FN(wrapper_CompositeImplicitAutograd__fbgemm_pack_quantized_matrix));
m.impl("fbgemm_pack_quantized_matrix.KN",
TORCH_FN(wrapper_CompositeImplicitAutograd_KN_fbgemm_pack_quantized_matrix));
m.impl("ldexp.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_ldexp));
m.impl("ldexp.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_ldexp_out));
m.impl("ldexp_",
TORCH_FN(wrapper_CompositeImplicitAutograd__ldexp_));
m.impl("log_softmax.int",
TORCH_FN(wrapper_CompositeImplicitAutograd_int_log_softmax));
m.impl("log_softmax.Dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname_log_softmax));
m.impl("logcumsumexp.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_logcumsumexp));
m.impl("logcumsumexp.dimname_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_out_logcumsumexp_out));
m.impl("logsumexp.names",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_logsumexp));
m.impl("logsumexp.names_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_out_logsumexp_out));
m.impl("margin_ranking_loss",
TORCH_FN(wrapper_CompositeImplicitAutograd__margin_ranking_loss));
m.impl("matmul",
TORCH_FN(wrapper_CompositeImplicitAutograd__matmul));
m.impl("matmul.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_matmul_out));
m.impl("matrix_power",
TORCH_FN(wrapper_CompositeImplicitAutograd__matrix_power));
m.impl("matrix_power.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_matrix_power_out));
m.impl("matrix_exp",
TORCH_FN(wrapper_CompositeImplicitAutograd__matrix_exp));
m.impl("matrix_exp_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__matrix_exp_backward));
m.impl("max.names_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_dim_max));
m.impl("max.names_dim_max",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_dim_max_max_out));
m.impl("value_selecting_reduction_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__value_selecting_reduction_backward));
m.impl("max_pool1d_with_indices",
TORCH_FN(wrapper_CompositeImplicitAutograd__max_pool1d_with_indices));
m.impl("max_pool1d",
TORCH_FN(wrapper_CompositeImplicitAutograd__max_pool1d));
m.impl("max_pool2d",
TORCH_FN(wrapper_CompositeImplicitAutograd__max_pool2d));
m.impl("max_pool3d",
TORCH_FN(wrapper_CompositeImplicitAutograd__max_pool3d));
m.impl("mean.names_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_dim_mean));
m.impl("mean.names_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_out_mean_out));
m.impl("nanmean",
TORCH_FN(wrapper_CompositeImplicitAutograd__nanmean));
m.impl("nanmean.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_nanmean_out));
m.impl("median.names_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_dim_median));
m.impl("median.names_dim_values",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_dim_values_median_out));
m.impl("nanmedian.names_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_dim_nanmedian));
m.impl("nanmedian.names_dim_values",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_dim_values_nanmedian_out));
m.impl("min.names_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_dim_min));
m.impl("min.names_dim_min",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_dim_min_min_out));
m.impl("_sparse_mm",
TORCH_FN(wrapper_CompositeImplicitAutograd___sparse_mm));
m.impl("_sparse_mm.reduce",
TORCH_FN(wrapper_CompositeImplicitAutograd_reduce__sparse_mm));
m.impl("mode.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_mode));
m.impl("mode.dimname_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_out_mode_out));
m.impl("multiply.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_multiply));
m.impl("multiply.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_multiply_out));
m.impl("multiply_.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_multiply_));
m.impl("multiply.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_multiply));
m.impl("multiply_.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_multiply_));
m.impl("narrow",
TORCH_FN(wrapper_CompositeImplicitAutograd__narrow));
m.impl("narrow.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_narrow));
m.impl("is_vulkan_available",
TORCH_FN(wrapper_CompositeImplicitAutograd__is_vulkan_available));
m.impl("_nnpack_available",
TORCH_FN(wrapper_CompositeImplicitAutograd___nnpack_available));
m.impl("pairwise_distance",
TORCH_FN(wrapper_CompositeImplicitAutograd__pairwise_distance));
m.impl("cdist",
TORCH_FN(wrapper_CompositeImplicitAutograd__cdist));
m.impl("pdist",
TORCH_FN(wrapper_CompositeImplicitAutograd__pdist));
m.impl("cosine_similarity",
TORCH_FN(wrapper_CompositeImplicitAutograd__cosine_similarity));
m.impl("movedim.intlist",
TORCH_FN(wrapper_CompositeImplicitAutograd_intlist_movedim));
m.impl("movedim.int",
TORCH_FN(wrapper_CompositeImplicitAutograd_int_movedim));
m.impl("moveaxis.intlist",
TORCH_FN(wrapper_CompositeImplicitAutograd_intlist_moveaxis));
m.impl("moveaxis.int",
TORCH_FN(wrapper_CompositeImplicitAutograd_int_moveaxis));
m.impl("numpy_T",
TORCH_FN(wrapper_CompositeImplicitAutograd__numpy_T));
m.impl("matrix_H",
TORCH_FN(wrapper_CompositeImplicitAutograd__matrix_H));
m.impl("mT",
TORCH_FN(wrapper_CompositeImplicitAutograd__mT));
m.impl("mH",
TORCH_FN(wrapper_CompositeImplicitAutograd__mH));
m.impl("adjoint",
TORCH_FN(wrapper_CompositeImplicitAutograd__adjoint));
m.impl("native_channel_shuffle",
TORCH_FN(wrapper_CompositeImplicitAutograd__native_channel_shuffle));
m.impl("pin_memory",
TORCH_FN(wrapper_CompositeImplicitAutograd__pin_memory));
m.impl("pinverse",
TORCH_FN(wrapper_CompositeImplicitAutograd__pinverse));
m.impl("poisson_nll_loss",
TORCH_FN(wrapper_CompositeImplicitAutograd__poisson_nll_loss));
m.impl("rand.generator_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_generator_out_rand_out));
m.impl("randn.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_randn_out));
m.impl("randn.generator_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_generator_out_randn_out));
m.impl("ravel",
TORCH_FN(wrapper_CompositeImplicitAutograd__ravel));
m.impl("negative",
TORCH_FN(wrapper_CompositeImplicitAutograd__negative));
m.impl("negative.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_negative_out));
m.impl("negative_",
TORCH_FN(wrapper_CompositeImplicitAutograd__negative_));
m.impl("repeat_interleave.self_Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_self_Tensor_repeat_interleave));
m.impl("repeat_interleave.self_int",
TORCH_FN(wrapper_CompositeImplicitAutograd_self_int_repeat_interleave));
m.impl("reshape",
TORCH_FN(wrapper_CompositeImplicitAutograd__reshape));
m.impl("reshape_as",
TORCH_FN(wrapper_CompositeImplicitAutograd__reshape_as));
m.impl("rrelu",
TORCH_FN(wrapper_CompositeImplicitAutograd__rrelu));
m.impl("rrelu_",
TORCH_FN(wrapper_CompositeImplicitAutograd__rrelu_));
m.impl("relu6",
TORCH_FN(wrapper_CompositeImplicitAutograd__relu6));
m.impl("relu6_",
TORCH_FN(wrapper_CompositeImplicitAutograd__relu6_));
m.impl("prelu",
TORCH_FN(wrapper_CompositeImplicitAutograd__prelu));
m.impl("infinitely_differentiable_gelu_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__infinitely_differentiable_gelu_backward));
m.impl("select.Dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname_select));
m.impl("selu",
TORCH_FN(wrapper_CompositeImplicitAutograd__selu));
m.impl("selu_",
TORCH_FN(wrapper_CompositeImplicitAutograd__selu_));
m.impl("silu_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__silu_backward));
m.impl("mish_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__mish_backward));
m.impl("size.int",
TORCH_FN(wrapper_CompositeImplicitAutograd_int_size));
m.impl("size.Dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname_size));
m.impl("sym_size.int",
TORCH_FN(wrapper_CompositeImplicitAutograd_int_sym_size));
m.impl("sym_numel",
TORCH_FN(wrapper_CompositeImplicitAutograd__sym_numel));
m.impl("sym_storage_offset",
TORCH_FN(wrapper_CompositeImplicitAutograd__sym_storage_offset));
m.impl("smm",
TORCH_FN(wrapper_CompositeImplicitAutograd__smm));
m.impl("softmax.int",
TORCH_FN(wrapper_CompositeImplicitAutograd_int_softmax));
m.impl("softmax.Dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname_softmax));
m.impl("split.sizes",
TORCH_FN(wrapper_CompositeImplicitAutograd_sizes_split));
m.impl("hsplit.int",
TORCH_FN(wrapper_CompositeImplicitAutograd_int_hsplit));
m.impl("hsplit.array",
TORCH_FN(wrapper_CompositeImplicitAutograd_array_hsplit));
m.impl("vsplit.int",
TORCH_FN(wrapper_CompositeImplicitAutograd_int_vsplit));
m.impl("vsplit.array",
TORCH_FN(wrapper_CompositeImplicitAutograd_array_vsplit));
m.impl("dsplit.int",
TORCH_FN(wrapper_CompositeImplicitAutograd_int_dsplit));
m.impl("dsplit.array",
TORCH_FN(wrapper_CompositeImplicitAutograd_array_dsplit));
m.impl("squeeze.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_squeeze));
m.impl("squeeze_.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_squeeze_));
m.impl("sspaddmm",
TORCH_FN(wrapper_CompositeImplicitAutograd__sspaddmm));
m.impl("hstack",
TORCH_FN(wrapper_CompositeImplicitAutograd__hstack));
m.impl("hstack.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_hstack_out));
m.impl("vstack",
TORCH_FN(wrapper_CompositeImplicitAutograd__vstack));
m.impl("vstack.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_vstack_out));
m.impl("dstack",
TORCH_FN(wrapper_CompositeImplicitAutograd__dstack));
m.impl("dstack.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_dstack_out));
m.impl("stft",
TORCH_FN(wrapper_CompositeImplicitAutograd__stft));
m.impl("stft.center",
TORCH_FN(wrapper_CompositeImplicitAutograd_center_stft));
m.impl("istft",
TORCH_FN(wrapper_CompositeImplicitAutograd__istft));
m.impl("stride.int",
TORCH_FN(wrapper_CompositeImplicitAutograd_int_stride));
m.impl("stride.Dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname_stride));
m.impl("sym_stride.int",
TORCH_FN(wrapper_CompositeImplicitAutograd_int_sym_stride));
m.impl("sum.dim_DimnameList",
TORCH_FN(wrapper_CompositeImplicitAutograd_dim_DimnameList_sum));
m.impl("sum.DimnameList_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_DimnameList_out_sum_out));
m.impl("sum_to_size",
TORCH_FN(wrapper_CompositeImplicitAutograd__sum_to_size));
m.impl("square",
TORCH_FN(wrapper_CompositeImplicitAutograd__square));
m.impl("square.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_square_out));
m.impl("square_",
TORCH_FN(wrapper_CompositeImplicitAutograd__square_));
m.impl("std",
TORCH_FN(wrapper_CompositeImplicitAutograd__std));
m.impl("std.dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_dim_std));
m.impl("std.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_std_out));
m.impl("std_mean",
TORCH_FN(wrapper_CompositeImplicitAutograd__std_mean));
m.impl("std_mean.dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_dim_std_mean));
m.impl("std_mean.names_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_dim_std_mean));
m.impl("std_mean.correction_names",
TORCH_FN(wrapper_CompositeImplicitAutograd_correction_names_std_mean));
m.impl("std.names_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_dim_std));
m.impl("std.names_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_out_std_out));
m.impl("std.correction_names",
TORCH_FN(wrapper_CompositeImplicitAutograd_correction_names_std));
m.impl("std.correction_names_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_correction_names_out_std_out));
m.impl("prod.dim_Dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dim_Dimname_prod));
m.impl("prod.Dimname_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname_out_prod_out));
m.impl("tensordot",
TORCH_FN(wrapper_CompositeImplicitAutograd__tensordot));
m.impl("tensordot.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_tensordot_out));
m.impl("tile",
TORCH_FN(wrapper_CompositeImplicitAutograd__tile));
m.impl("transpose.Dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname_transpose));
m.impl("one_hot",
TORCH_FN(wrapper_CompositeImplicitAutograd__one_hot));
m.impl("fliplr",
TORCH_FN(wrapper_CompositeImplicitAutograd__fliplr));
m.impl("flipud",
TORCH_FN(wrapper_CompositeImplicitAutograd__flipud));
m.impl("trapezoid.x",
TORCH_FN(wrapper_CompositeImplicitAutograd_x_trapezoid));
m.impl("trapezoid.dx",
TORCH_FN(wrapper_CompositeImplicitAutograd_dx_trapezoid));
m.impl("trapz.x",
TORCH_FN(wrapper_CompositeImplicitAutograd_x_trapz));
m.impl("trapz.dx",
TORCH_FN(wrapper_CompositeImplicitAutograd_dx_trapz));
m.impl("triplet_margin_loss",
TORCH_FN(wrapper_CompositeImplicitAutograd__triplet_margin_loss));
m.impl("fix",
TORCH_FN(wrapper_CompositeImplicitAutograd__fix));
m.impl("fix.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fix_out));
m.impl("fix_",
TORCH_FN(wrapper_CompositeImplicitAutograd__fix_));
m.impl("type_as",
TORCH_FN(wrapper_CompositeImplicitAutograd__type_as));
m.impl("_has_compatible_shallow_copy_type",
TORCH_FN(wrapper_CompositeImplicitAutograd___has_compatible_shallow_copy_type));
m.impl("vander",
TORCH_FN(wrapper_CompositeImplicitAutograd__vander));
m.impl("var",
TORCH_FN(wrapper_CompositeImplicitAutograd__var));
m.impl("var.dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_dim_var));
m.impl("var.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_var_out));
m.impl("var.names_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_dim_var));
m.impl("var.names_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_out_var_out));
m.impl("var.correction_names",
TORCH_FN(wrapper_CompositeImplicitAutograd_correction_names_var));
m.impl("var.correction_names_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_correction_names_out_var_out));
m.impl("var_mean",
TORCH_FN(wrapper_CompositeImplicitAutograd__var_mean));
m.impl("var_mean.dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_dim_var_mean));
m.impl("var_mean.names_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_dim_var_mean));
m.impl("var_mean.correction_names",
TORCH_FN(wrapper_CompositeImplicitAutograd_correction_names_var_mean));
m.impl("view_as",
TORCH_FN(wrapper_CompositeImplicitAutograd__view_as));
m.impl("where.ScalarSelf",
TORCH_FN(wrapper_CompositeImplicitAutograd_ScalarSelf_where));
m.impl("where.ScalarOther",
TORCH_FN(wrapper_CompositeImplicitAutograd_ScalarOther_where));
m.impl("where.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_where));
m.impl("where",
TORCH_FN(wrapper_CompositeImplicitAutograd__where));
m.impl("norm_except_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd__norm_except_dim));
m.impl("_weight_norm",
TORCH_FN(wrapper_CompositeImplicitAutograd___weight_norm));
m.impl("_weight_norm_differentiable_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd___weight_norm_differentiable_backward));
m.impl("_sparse_sum",
TORCH_FN(wrapper_CompositeImplicitAutograd___sparse_sum));
m.impl("_sparse_sum.dtype",
TORCH_FN(wrapper_CompositeImplicitAutograd_dtype__sparse_sum));
m.impl("_sparse_sum.dim_dtype",
TORCH_FN(wrapper_CompositeImplicitAutograd_dim_dtype__sparse_sum));
m.impl("_sparse_softmax.int",
TORCH_FN(wrapper_CompositeImplicitAutograd_int__sparse_softmax));
m.impl("_sparse_softmax.Dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname__sparse_softmax));
m.impl("_sparse_log_softmax.int",
TORCH_FN(wrapper_CompositeImplicitAutograd_int__sparse_log_softmax));
m.impl("_sparse_log_softmax.Dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname__sparse_log_softmax));
m.impl("norm.names_ScalarOpt_dim_dtype",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_ScalarOpt_dim_dtype_norm));
m.impl("norm.names_dtype_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_dtype_out_norm_out));
m.impl("norm.names_ScalarOpt_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_ScalarOpt_dim_norm));
m.impl("norm.names_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_names_out_norm_out));
m.impl("frobenius_norm.dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_dim_frobenius_norm));
m.impl("frobenius_norm.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_frobenius_norm_out));
m.impl("nuclear_norm",
TORCH_FN(wrapper_CompositeImplicitAutograd__nuclear_norm));
m.impl("nuclear_norm.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_nuclear_norm_out));
m.impl("nuclear_norm.dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_dim_nuclear_norm));
m.impl("nuclear_norm.dim_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_dim_out_nuclear_norm_out));
m.impl("positive",
TORCH_FN(wrapper_CompositeImplicitAutograd__positive));
m.impl("subtract.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_subtract));
m.impl("subtract.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_subtract_out));
m.impl("subtract_.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_subtract_));
m.impl("subtract.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_subtract));
m.impl("subtract_.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_subtract_));
m.impl("sparse_csr_tensor.crow_col_value_size",
TORCH_FN(wrapper_CompositeImplicitAutograd_crow_col_value_size_sparse_csr_tensor));
m.impl("sparse_csc_tensor.ccol_row_value_size",
TORCH_FN(wrapper_CompositeImplicitAutograd_ccol_row_value_size_sparse_csc_tensor));
m.impl("sparse_bsr_tensor.crow_col_value_size",
TORCH_FN(wrapper_CompositeImplicitAutograd_crow_col_value_size_sparse_bsr_tensor));
m.impl("sparse_bsc_tensor.ccol_row_value_size",
TORCH_FN(wrapper_CompositeImplicitAutograd_ccol_row_value_size_sparse_bsc_tensor));
m.impl("sparse_csr_tensor.crow_col_value",
TORCH_FN(wrapper_CompositeImplicitAutograd_crow_col_value_sparse_csr_tensor));
m.impl("sparse_csc_tensor.ccol_row_value",
TORCH_FN(wrapper_CompositeImplicitAutograd_ccol_row_value_sparse_csc_tensor));
m.impl("sparse_bsr_tensor.crow_col_value",
TORCH_FN(wrapper_CompositeImplicitAutograd_crow_col_value_sparse_bsr_tensor));
m.impl("sparse_bsc_tensor.ccol_row_value",
TORCH_FN(wrapper_CompositeImplicitAutograd_ccol_row_value_sparse_bsc_tensor));
m.impl("_sparse_compressed_tensor_unsafe",
TORCH_FN(wrapper_CompositeImplicitAutograd___sparse_compressed_tensor_unsafe));
m.impl("_sparse_csr_tensor_unsafe",
TORCH_FN(wrapper_CompositeImplicitAutograd___sparse_csr_tensor_unsafe));
m.impl("_sparse_csc_tensor_unsafe",
TORCH_FN(wrapper_CompositeImplicitAutograd___sparse_csc_tensor_unsafe));
m.impl("_sparse_bsr_tensor_unsafe",
TORCH_FN(wrapper_CompositeImplicitAutograd___sparse_bsr_tensor_unsafe));
m.impl("_sparse_bsc_tensor_unsafe",
TORCH_FN(wrapper_CompositeImplicitAutograd___sparse_bsc_tensor_unsafe));
m.impl("sparse_coo_tensor.indices",
TORCH_FN(wrapper_CompositeImplicitAutograd_indices_sparse_coo_tensor));
m.impl("sparse_coo_tensor.indices_size",
TORCH_FN(wrapper_CompositeImplicitAutograd_indices_size_sparse_coo_tensor));
m.impl("_sparse_coo_tensor_unsafe",
TORCH_FN(wrapper_CompositeImplicitAutograd___sparse_coo_tensor_unsafe));
m.impl("_validate_sparse_coo_tensor_args",
TORCH_FN(wrapper_CompositeImplicitAutograd___validate_sparse_coo_tensor_args));
m.impl("_validate_sparse_compressed_tensor_args",
TORCH_FN(wrapper_CompositeImplicitAutograd___validate_sparse_compressed_tensor_args));
m.impl("_validate_sparse_csr_tensor_args",
TORCH_FN(wrapper_CompositeImplicitAutograd___validate_sparse_csr_tensor_args));
m.impl("_validate_sparse_csc_tensor_args",
TORCH_FN(wrapper_CompositeImplicitAutograd___validate_sparse_csc_tensor_args));
m.impl("_validate_sparse_bsr_tensor_args",
TORCH_FN(wrapper_CompositeImplicitAutograd___validate_sparse_bsr_tensor_args));
m.impl("_validate_sparse_bsc_tensor_args",
TORCH_FN(wrapper_CompositeImplicitAutograd___validate_sparse_bsc_tensor_args));
m.impl("_to_cpu",
TORCH_FN(wrapper_CompositeImplicitAutograd___to_cpu));
m.impl("to_dense",
TORCH_FN(wrapper_CompositeImplicitAutograd__to_dense));
m.impl("to_dense_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__to_dense_backward));
m.impl("coalesce",
TORCH_FN(wrapper_CompositeImplicitAutograd__coalesce));
m.impl("unbind.Dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname_unbind));
m.impl("to_sparse.sparse_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd_sparse_dim_to_sparse));
m.impl("to_sparse",
TORCH_FN(wrapper_CompositeImplicitAutograd__to_sparse));
m.impl("to_sparse_csr",
TORCH_FN(wrapper_CompositeImplicitAutograd__to_sparse_csr));
m.impl("to_sparse_csc",
TORCH_FN(wrapper_CompositeImplicitAutograd__to_sparse_csc));
m.impl("to_sparse_bsr",
TORCH_FN(wrapper_CompositeImplicitAutograd__to_sparse_bsr));
m.impl("to_sparse_bsc",
TORCH_FN(wrapper_CompositeImplicitAutograd__to_sparse_bsc));
m.impl("to_mkldnn_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__to_mkldnn_backward));
m.impl("fake_quantize_per_tensor_affine",
TORCH_FN(wrapper_CompositeImplicitAutograd__fake_quantize_per_tensor_affine));
m.impl("fake_quantize_per_tensor_affine.tensor_qparams",
TORCH_FN(wrapper_CompositeImplicitAutograd_tensor_qparams_fake_quantize_per_tensor_affine));
m.impl("fake_quantize_per_tensor_affine_cachemask_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__fake_quantize_per_tensor_affine_cachemask_backward));
m.impl("fake_quantize_per_channel_affine",
TORCH_FN(wrapper_CompositeImplicitAutograd__fake_quantize_per_channel_affine));
m.impl("fake_quantize_per_channel_affine_cachemask_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__fake_quantize_per_channel_affine_cachemask_backward));
m.impl("fused_moving_avg_obs_fake_quant",
TORCH_FN(wrapper_CompositeImplicitAutograd__fused_moving_avg_obs_fake_quant));
m.impl("_choose_qparams_per_tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd___choose_qparams_per_tensor));
m.impl("_saturate_weight_to_fp16",
TORCH_FN(wrapper_CompositeImplicitAutograd___saturate_weight_to_fp16));
m.impl("choose_qparams_optimized",
TORCH_FN(wrapper_CompositeImplicitAutograd__choose_qparams_optimized));
m.impl("_autocast_to_reduced_precision",
TORCH_FN(wrapper_CompositeImplicitAutograd___autocast_to_reduced_precision));
m.impl("_autocast_to_full_precision",
TORCH_FN(wrapper_CompositeImplicitAutograd___autocast_to_full_precision));
m.impl("to.dtype_layout",
TORCH_FN(wrapper_CompositeImplicitAutograd_dtype_layout_to));
m.impl("to.device",
TORCH_FN(wrapper_CompositeImplicitAutograd_device_to));
m.impl("to.dtype",
TORCH_FN(wrapper_CompositeImplicitAutograd_dtype_to));
m.impl("to.other",
TORCH_FN(wrapper_CompositeImplicitAutograd_other_to));
m.impl("meshgrid",
TORCH_FN(wrapper_CompositeImplicitAutograd__meshgrid));
m.impl("meshgrid.indexing",
TORCH_FN(wrapper_CompositeImplicitAutograd_indexing_meshgrid));
m.impl("cartesian_prod",
TORCH_FN(wrapper_CompositeImplicitAutograd__cartesian_prod));
m.impl("combinations",
TORCH_FN(wrapper_CompositeImplicitAutograd__combinations));
m.impl("item",
TORCH_FN(wrapper_CompositeImplicitAutograd__item));
m.impl("result_type.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_result_type));
m.impl("result_type.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_result_type));
m.impl("result_type.Scalar_Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_Tensor_result_type));
m.impl("result_type.Scalar_Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_Scalar_result_type));
m.impl("can_cast",
TORCH_FN(wrapper_CompositeImplicitAutograd__can_cast));
m.impl("promote_types",
TORCH_FN(wrapper_CompositeImplicitAutograd__promote_types));
m.impl("_thnn_fused_lstm_cell_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd___thnn_fused_lstm_cell_backward));
m.impl("_thnn_differentiable_lstm_cell_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd___thnn_differentiable_lstm_cell_backward));
m.impl("_thnn_differentiable_gru_cell_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd___thnn_differentiable_gru_cell_backward));
m.impl("lstm.input",
TORCH_FN(wrapper_CompositeImplicitAutograd_input_lstm));
m.impl("lstm.data",
TORCH_FN(wrapper_CompositeImplicitAutograd_data_lstm));
m.impl("gru.input",
TORCH_FN(wrapper_CompositeImplicitAutograd_input_gru));
m.impl("gru.data",
TORCH_FN(wrapper_CompositeImplicitAutograd_data_gru));
m.impl("rnn_tanh.input",
TORCH_FN(wrapper_CompositeImplicitAutograd_input_rnn_tanh));
m.impl("rnn_tanh.data",
TORCH_FN(wrapper_CompositeImplicitAutograd_data_rnn_tanh));
m.impl("rnn_relu.input",
TORCH_FN(wrapper_CompositeImplicitAutograd_input_rnn_relu));
m.impl("rnn_relu.data",
TORCH_FN(wrapper_CompositeImplicitAutograd_data_rnn_relu));
m.impl("lstm_cell",
TORCH_FN(wrapper_CompositeImplicitAutograd__lstm_cell));
m.impl("gru_cell",
TORCH_FN(wrapper_CompositeImplicitAutograd__gru_cell));
m.impl("rnn_tanh_cell",
TORCH_FN(wrapper_CompositeImplicitAutograd__rnn_tanh_cell));
m.impl("rnn_relu_cell",
TORCH_FN(wrapper_CompositeImplicitAutograd__rnn_relu_cell));
m.impl("quantized_lstm_cell",
TORCH_FN(wrapper_CompositeImplicitAutograd__quantized_lstm_cell));
m.impl("quantized_gru_cell",
TORCH_FN(wrapper_CompositeImplicitAutograd__quantized_gru_cell));
m.impl("quantized_rnn_relu_cell",
TORCH_FN(wrapper_CompositeImplicitAutograd__quantized_rnn_relu_cell));
m.impl("quantized_rnn_tanh_cell",
TORCH_FN(wrapper_CompositeImplicitAutograd__quantized_rnn_tanh_cell));
m.impl("_pack_padded_sequence_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd___pack_padded_sequence_backward));
m.impl("_pad_packed_sequence",
TORCH_FN(wrapper_CompositeImplicitAutograd___pad_packed_sequence));
m.impl("set_.source_Tensor_storage_offset",
TORCH_FN(wrapper_CompositeImplicitAutograd_source_Tensor_storage_offset_set_));
m.impl("index_add.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_index_add));
m.impl("index_fill_.Dimname_Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname_Scalar_index_fill_));
m.impl("index_fill.Dimname_Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname_Scalar_index_fill));
m.impl("index_fill_.Dimname_Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname_Tensor_index_fill_));
m.impl("index_fill.Dimname_Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Dimname_Tensor_index_fill));
m.impl("scatter.dimname_src",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_src_scatter));
m.impl("scatter.dimname_value",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_value_scatter));
m.impl("scatter_add.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_scatter_add));
m.impl("__and__.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar___and__));
m.impl("__iand__.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar___iand__));
m.impl("__and__.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor___and__));
m.impl("__iand__.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor___iand__));
m.impl("__or__.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar___or__));
m.impl("__ior__.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar___ior__));
m.impl("__or__.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor___or__));
m.impl("__ior__.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor___ior__));
m.impl("__xor__.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar___xor__));
m.impl("__ixor__.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar___ixor__));
m.impl("__xor__.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor___xor__));
m.impl("__ixor__.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor___ixor__));
m.impl("diag",
TORCH_FN(wrapper_CompositeImplicitAutograd__diag));
m.impl("diag.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_diag_out));
m.impl("cross",
TORCH_FN(wrapper_CompositeImplicitAutograd__cross));
m.impl("cross.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_cross_out));
m.impl("trace_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__trace_backward));
m.impl("not_equal.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_not_equal));
m.impl("not_equal.Scalar_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_out_not_equal_out));
m.impl("not_equal_.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_not_equal_));
m.impl("not_equal.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_not_equal));
m.impl("not_equal.Tensor_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_out_not_equal_out));
m.impl("not_equal_.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_not_equal_));
m.impl("greater_equal.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_greater_equal));
m.impl("greater_equal.Scalar_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_out_greater_equal_out));
m.impl("greater_equal_.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_greater_equal_));
m.impl("greater_equal.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_greater_equal));
m.impl("greater_equal.Tensor_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_out_greater_equal_out));
m.impl("greater_equal_.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_greater_equal_));
m.impl("less_equal.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_less_equal));
m.impl("less_equal.Scalar_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_out_less_equal_out));
m.impl("less_equal_.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_less_equal_));
m.impl("less_equal.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_less_equal));
m.impl("less_equal.Tensor_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_out_less_equal_out));
m.impl("less_equal_.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_less_equal_));
m.impl("greater.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_greater));
m.impl("greater.Scalar_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_out_greater_out));
m.impl("greater_.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_greater_));
m.impl("greater.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_greater));
m.impl("greater.Tensor_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_out_greater_out));
m.impl("greater_.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_greater_));
m.impl("less.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_less));
m.impl("less.Scalar_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_out_less_out));
m.impl("less_.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_less_));
m.impl("less.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_less));
m.impl("less.Tensor_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_out_less_out));
m.impl("less_.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_less_));
m.impl("take_along_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd__take_along_dim));
m.impl("take_along_dim.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_take_along_dim_out));
m.impl("index_select.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_index_select));
m.impl("index_select.dimname_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_out_index_select_out));
m.impl("index_select_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__index_select_backward));
m.impl("masked_select_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__masked_select_backward));
m.impl("nonzero_numpy",
TORCH_FN(wrapper_CompositeImplicitAutograd__nonzero_numpy));
m.impl("argwhere",
TORCH_FN(wrapper_CompositeImplicitAutograd__argwhere));
m.impl("gather_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd__gather_backward));
m.impl("gather.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_gather));
m.impl("gather.dimname_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_out_gather_out));
m.impl("_gather_sparse_backward",
TORCH_FN(wrapper_CompositeImplicitAutograd___gather_sparse_backward));
m.impl("cross_entropy_loss",
TORCH_FN(wrapper_CompositeImplicitAutograd__cross_entropy_loss));
m.impl("linalg_vander",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_vander));
m.impl("svd",
TORCH_FN(wrapper_CompositeImplicitAutograd__svd));
m.impl("svd.U",
TORCH_FN(wrapper_CompositeImplicitAutograd_U_svd_out));
m.impl("swapaxes",
TORCH_FN(wrapper_CompositeImplicitAutograd__swapaxes));
m.impl("swapaxes_",
TORCH_FN(wrapper_CompositeImplicitAutograd__swapaxes_));
m.impl("swapdims",
TORCH_FN(wrapper_CompositeImplicitAutograd__swapdims));
m.impl("swapdims_",
TORCH_FN(wrapper_CompositeImplicitAutograd__swapdims_));
m.impl("qr",
TORCH_FN(wrapper_CompositeImplicitAutograd__qr));
m.impl("qr.Q",
TORCH_FN(wrapper_CompositeImplicitAutograd_Q_qr_out));
m.impl("orgqr",
TORCH_FN(wrapper_CompositeImplicitAutograd__orgqr));
m.impl("orgqr.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_orgqr_out));
m.impl("_lu_with_info",
TORCH_FN(wrapper_CompositeImplicitAutograd___lu_with_info));
m.impl("lu_solve",
TORCH_FN(wrapper_CompositeImplicitAutograd__lu_solve));
m.impl("lu_solve.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_lu_solve_out));
m.impl("arctan2",
TORCH_FN(wrapper_CompositeImplicitAutograd__arctan2));
m.impl("arctan2.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_arctan2_out));
m.impl("arctan2_",
TORCH_FN(wrapper_CompositeImplicitAutograd__arctan2_));
m.impl("histogramdd",
TORCH_FN(wrapper_CompositeImplicitAutograd__histogramdd));
m.impl("histogramdd.int_bins",
TORCH_FN(wrapper_CompositeImplicitAutograd_int_bins_histogramdd));
m.impl("histogramdd.TensorList_bins",
TORCH_FN(wrapper_CompositeImplicitAutograd_TensorList_bins_histogramdd));
m.impl("max.other",
TORCH_FN(wrapper_CompositeImplicitAutograd_other_max));
m.impl("max.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_max_out));
m.impl("min.other",
TORCH_FN(wrapper_CompositeImplicitAutograd_other_min));
m.impl("min.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_min_out));
m.impl("quantile",
TORCH_FN(wrapper_CompositeImplicitAutograd__quantile));
m.impl("quantile.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_quantile_out));
m.impl("quantile.scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_scalar_quantile));
m.impl("quantile.scalar_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_scalar_out_quantile_out));
m.impl("nanquantile",
TORCH_FN(wrapper_CompositeImplicitAutograd__nanquantile));
m.impl("nanquantile.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_nanquantile_out));
m.impl("nanquantile.scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_scalar_nanquantile));
m.impl("nanquantile.scalar_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_scalar_out_nanquantile_out));
m.impl("sort.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_sort));
m.impl("sort.dimname_values",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_values_sort_out));
m.impl("sort.dimname_stable",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_stable_sort));
m.impl("sort.dimname_values_stable",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_values_stable_sort_out));
m.impl("msort",
TORCH_FN(wrapper_CompositeImplicitAutograd__msort));
m.impl("msort.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_msort_out));
m.impl("argsort",
TORCH_FN(wrapper_CompositeImplicitAutograd__argsort));
m.impl("argsort.dimname",
TORCH_FN(wrapper_CompositeImplicitAutograd_dimname_argsort));
m.impl("float_power.Tensor_Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_Tensor_float_power));
m.impl("float_power.Tensor_Tensor_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_Tensor_out_float_power_out));
m.impl("float_power_.Tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_float_power_));
m.impl("float_power.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_float_power));
m.impl("float_power.Scalar_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_out_float_power_out));
m.impl("float_power.Tensor_Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_Scalar_float_power));
m.impl("float_power.Tensor_Scalar_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_Tensor_Scalar_out_float_power_out));
m.impl("float_power_.Scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_Scalar_float_power_));
m.impl("l1_loss",
TORCH_FN(wrapper_CompositeImplicitAutograd__l1_loss));
m.impl("multilabel_margin_loss",
TORCH_FN(wrapper_CompositeImplicitAutograd__multilabel_margin_loss));
m.impl("multilabel_margin_loss.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_multilabel_margin_loss_out));
m.impl("nll_loss",
TORCH_FN(wrapper_CompositeImplicitAutograd__nll_loss));
m.impl("nll_loss.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_nll_loss_out));
m.impl("nll_loss_nd",
TORCH_FN(wrapper_CompositeImplicitAutograd__nll_loss_nd));
m.impl("nll_loss2d",
TORCH_FN(wrapper_CompositeImplicitAutograd__nll_loss2d));
m.impl("nll_loss2d.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_nll_loss2d_out));
m.impl("log_sigmoid",
TORCH_FN(wrapper_CompositeImplicitAutograd__log_sigmoid));
m.impl("log_sigmoid.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_log_sigmoid_out));
m.impl("adaptive_avg_pool2d",
TORCH_FN(wrapper_CompositeImplicitAutograd__adaptive_avg_pool2d));
m.impl("adaptive_avg_pool3d",
TORCH_FN(wrapper_CompositeImplicitAutograd__adaptive_avg_pool3d));
m.impl("_pad_circular",
TORCH_FN(wrapper_CompositeImplicitAutograd___pad_circular));
m.impl("_pad_enum",
TORCH_FN(wrapper_CompositeImplicitAutograd___pad_enum));
m.impl("pad",
TORCH_FN(wrapper_CompositeImplicitAutograd__pad));
m.impl("upsample_linear1d.vec",
TORCH_FN(wrapper_CompositeImplicitAutograd_vec_upsample_linear1d));
m.impl("upsample_bilinear2d.vec",
TORCH_FN(wrapper_CompositeImplicitAutograd_vec_upsample_bilinear2d));
m.impl("_upsample_bilinear2d_aa.vec",
TORCH_FN(wrapper_CompositeImplicitAutograd_vec__upsample_bilinear2d_aa));
m.impl("upsample_trilinear3d.vec",
TORCH_FN(wrapper_CompositeImplicitAutograd_vec_upsample_trilinear3d));
m.impl("upsample_bicubic2d.vec",
TORCH_FN(wrapper_CompositeImplicitAutograd_vec_upsample_bicubic2d));
m.impl("_upsample_bicubic2d_aa.vec",
TORCH_FN(wrapper_CompositeImplicitAutograd_vec__upsample_bicubic2d_aa));
m.impl("upsample_nearest1d.vec",
TORCH_FN(wrapper_CompositeImplicitAutograd_vec_upsample_nearest1d));
m.impl("_upsample_nearest_exact1d.vec",
TORCH_FN(wrapper_CompositeImplicitAutograd_vec__upsample_nearest_exact1d));
m.impl("upsample_nearest2d.vec",
TORCH_FN(wrapper_CompositeImplicitAutograd_vec_upsample_nearest2d));
m.impl("_upsample_nearest_exact2d.vec",
TORCH_FN(wrapper_CompositeImplicitAutograd_vec__upsample_nearest_exact2d));
m.impl("upsample_nearest3d.vec",
TORCH_FN(wrapper_CompositeImplicitAutograd_vec_upsample_nearest3d));
m.impl("_upsample_nearest_exact3d.vec",
TORCH_FN(wrapper_CompositeImplicitAutograd_vec__upsample_nearest_exact3d));
m.impl("thnn_conv2d",
TORCH_FN(wrapper_CompositeImplicitAutograd__thnn_conv2d));
m.impl("thnn_conv2d.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_thnn_conv2d_out));
m.impl("slow_conv3d",
TORCH_FN(wrapper_CompositeImplicitAutograd__slow_conv3d));
m.impl("slow_conv3d.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_slow_conv3d_out));
m.impl("column_stack",
TORCH_FN(wrapper_CompositeImplicitAutograd__column_stack));
m.impl("column_stack.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_column_stack_out));
m.impl("isfinite",
TORCH_FN(wrapper_CompositeImplicitAutograd__isfinite));
m.impl("_add_batch_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd___add_batch_dim));
m.impl("_remove_batch_dim",
TORCH_FN(wrapper_CompositeImplicitAutograd___remove_batch_dim));
m.impl("special_expm1",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_expm1));
m.impl("special_expm1.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_expm1_out));
m.impl("special_exp2",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_exp2));
m.impl("special_exp2.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_exp2_out));
m.impl("special_psi",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_psi));
m.impl("special_psi.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_psi_out));
m.impl("special_digamma",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_digamma));
m.impl("special_digamma.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_digamma_out));
m.impl("special_gammaln",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_gammaln));
m.impl("special_gammaln.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_gammaln_out));
m.impl("special_erf",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_erf));
m.impl("special_erf.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_erf_out));
m.impl("special_erfc",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_erfc));
m.impl("special_erfc.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_erfc_out));
m.impl("special_erfinv",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_erfinv));
m.impl("special_erfinv.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_erfinv_out));
m.impl("special_ndtr",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_ndtr));
m.impl("special_ndtr.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_ndtr_out));
m.impl("special_xlogy",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_xlogy));
m.impl("special_xlogy.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_xlogy_out));
m.impl("special_xlogy.self_scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_self_scalar_special_xlogy));
m.impl("special_xlogy.self_scalar_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_self_scalar_out_special_xlogy_out));
m.impl("special_xlogy.other_scalar",
TORCH_FN(wrapper_CompositeImplicitAutograd_other_scalar_special_xlogy));
m.impl("special_xlogy.other_scalar_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_other_scalar_out_special_xlogy_out));
m.impl("special_i0",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_i0));
m.impl("special_i0.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_i0_out));
m.impl("special_logit",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_logit));
m.impl("special_logit.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_logit_out));
m.impl("special_polygamma",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_polygamma));
m.impl("special_polygamma.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_polygamma_out));
m.impl("special_logsumexp",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_logsumexp));
m.impl("special_logsumexp.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_logsumexp_out));
m.impl("special_expit",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_expit));
m.impl("special_expit.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_expit_out));
m.impl("special_sinc",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_sinc));
m.impl("special_sinc.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_sinc_out));
m.impl("special_round",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_round));
m.impl("special_round.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_round_out));
m.impl("special_log1p",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_log1p));
m.impl("special_log1p.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_log1p_out));
m.impl("special_log_softmax",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_log_softmax));
m.impl("special_gammainc",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_gammainc));
m.impl("special_gammainc.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_gammainc_out));
m.impl("special_gammaincc",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_gammaincc));
m.impl("special_gammaincc.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_gammaincc_out));
m.impl("special_multigammaln",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_multigammaln));
m.impl("special_multigammaln.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_special_multigammaln_out));
m.impl("special_softmax",
TORCH_FN(wrapper_CompositeImplicitAutograd__special_softmax));
m.impl("fft_fft",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_fft));
m.impl("fft_fft.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_fft_out));
m.impl("fft_ifft",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_ifft));
m.impl("fft_ifft.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_ifft_out));
m.impl("fft_rfft",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_rfft));
m.impl("fft_rfft.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_rfft_out));
m.impl("fft_irfft",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_irfft));
m.impl("fft_irfft.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_irfft_out));
m.impl("fft_hfft",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_hfft));
m.impl("fft_hfft.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_hfft_out));
m.impl("fft_ihfft",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_ihfft));
m.impl("fft_ihfft.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_ihfft_out));
m.impl("fft_fft2",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_fft2));
m.impl("fft_fft2.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_fft2_out));
m.impl("fft_ifft2",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_ifft2));
m.impl("fft_ifft2.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_ifft2_out));
m.impl("fft_rfft2",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_rfft2));
m.impl("fft_rfft2.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_rfft2_out));
m.impl("fft_irfft2",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_irfft2));
m.impl("fft_irfft2.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_irfft2_out));
m.impl("fft_hfft2",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_hfft2));
m.impl("fft_hfft2.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_hfft2_out));
m.impl("fft_ihfft2",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_ihfft2));
m.impl("fft_ihfft2.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_ihfft2_out));
m.impl("fft_fftn",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_fftn));
m.impl("fft_fftn.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_fftn_out));
m.impl("fft_ifftn",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_ifftn));
m.impl("fft_ifftn.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_ifftn_out));
m.impl("fft_rfftn",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_rfftn));
m.impl("fft_rfftn.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_rfftn_out));
m.impl("fft_irfftn",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_irfftn));
m.impl("fft_irfftn.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_irfftn_out));
m.impl("fft_hfftn",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_hfftn));
m.impl("fft_hfftn.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_hfftn_out));
m.impl("fft_ihfftn",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_ihfftn));
m.impl("fft_ihfftn.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_fft_ihfftn_out));
m.impl("fft_fftshift",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_fftshift));
m.impl("fft_ifftshift",
TORCH_FN(wrapper_CompositeImplicitAutograd__fft_ifftshift));
m.impl("linalg_cholesky",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_cholesky));
m.impl("linalg_cholesky.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_cholesky_out));
m.impl("linalg_lu_factor",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_lu_factor));
m.impl("linalg_lu_factor.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_lu_factor_out));
m.impl("linalg_det",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_det));
m.impl("linalg_det.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_det_out));
m.impl("det",
TORCH_FN(wrapper_CompositeImplicitAutograd__det));
m.impl("linalg_ldl_factor",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_ldl_factor));
m.impl("linalg_ldl_factor.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_ldl_factor_out));
m.impl("linalg_matmul",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_matmul));
m.impl("linalg_matmul.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_matmul_out));
m.impl("linalg_vecdot",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_vecdot));
m.impl("linalg_vecdot.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_vecdot_out));
m.impl("linalg_slogdet",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_slogdet));
m.impl("linalg_slogdet.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_slogdet_out));
m.impl("slogdet",
TORCH_FN(wrapper_CompositeImplicitAutograd__slogdet));
m.impl("slogdet.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_slogdet_out));
m.impl("logdet",
TORCH_FN(wrapper_CompositeImplicitAutograd__logdet));
m.impl("linalg_eigvals",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_eigvals));
m.impl("linalg_eigvals.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_eigvals_out));
m.impl("linalg_eigh",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_eigh));
m.impl("linalg_eigh.eigvals",
TORCH_FN(wrapper_CompositeImplicitAutograd_eigvals_linalg_eigh_out));
m.impl("linalg_eigvalsh",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_eigvalsh));
m.impl("linalg_eigvalsh.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_eigvalsh_out));
m.impl("linalg_inv",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_inv));
m.impl("linalg_inv.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_inv_out));
m.impl("inverse",
TORCH_FN(wrapper_CompositeImplicitAutograd__inverse));
m.impl("inverse.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_inverse_out));
m.impl("inner",
TORCH_FN(wrapper_CompositeImplicitAutograd__inner));
m.impl("inner.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_inner_out));
m.impl("outer",
TORCH_FN(wrapper_CompositeImplicitAutograd__outer));
m.impl("outer.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_outer_out));
m.impl("ger",
TORCH_FN(wrapper_CompositeImplicitAutograd__ger));
m.impl("ger.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_ger_out));
m.impl("linalg_norm",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_norm));
m.impl("linalg_norm.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_norm_out));
m.impl("linalg_norm.ord_str",
TORCH_FN(wrapper_CompositeImplicitAutograd_ord_str_linalg_norm));
m.impl("linalg_norm.ord_str_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_ord_str_out_linalg_norm_out));
m.impl("linalg_matrix_norm",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_matrix_norm));
m.impl("linalg_matrix_norm.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_matrix_norm_out));
m.impl("linalg_matrix_norm.str_ord",
TORCH_FN(wrapper_CompositeImplicitAutograd_str_ord_linalg_matrix_norm));
m.impl("linalg_matrix_norm.str_ord_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_str_ord_out_linalg_matrix_norm_out));
m.impl("linalg_svd",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_svd));
m.impl("linalg_svd.U",
TORCH_FN(wrapper_CompositeImplicitAutograd_U_linalg_svd_out));
m.impl("linalg_svdvals",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_svdvals));
m.impl("linalg_svdvals.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_svdvals_out));
m.impl("linalg_cond",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_cond));
m.impl("linalg_cond.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_cond_out));
m.impl("linalg_cond.p_str",
TORCH_FN(wrapper_CompositeImplicitAutograd_p_str_linalg_cond));
m.impl("linalg_cond.p_str_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_p_str_out_linalg_cond_out));
m.impl("linalg_pinv.atol_rtol_float",
TORCH_FN(wrapper_CompositeImplicitAutograd_atol_rtol_float_linalg_pinv));
m.impl("linalg_pinv.atol_rtol_float_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_atol_rtol_float_out_linalg_pinv_out));
m.impl("linalg_pinv",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_pinv));
m.impl("linalg_pinv.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_pinv_out));
m.impl("linalg_pinv.rcond_tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_rcond_tensor_linalg_pinv));
m.impl("linalg_pinv.out_rcond_tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_rcond_tensor_linalg_pinv_out));
m.impl("linalg_solve_ex",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_solve_ex));
m.impl("linalg_solve_ex.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_solve_ex_out));
m.impl("linalg_solve",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_solve));
m.impl("linalg_solve.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_solve_out));
m.impl("linalg_tensorinv",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_tensorinv));
m.impl("linalg_tensorinv.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_tensorinv_out));
m.impl("linalg_tensorsolve",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_tensorsolve));
m.impl("linalg_tensorsolve.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_tensorsolve_out));
m.impl("linalg_matrix_power",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_matrix_power));
m.impl("linalg_matrix_power.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_matrix_power_out));
m.impl("linalg_matrix_rank.atol_rtol_tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_atol_rtol_tensor_linalg_matrix_rank));
m.impl("linalg_matrix_rank.atol_rtol_tensor_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_atol_rtol_tensor_out_linalg_matrix_rank_out));
m.impl("linalg_matrix_rank.atol_rtol_float",
TORCH_FN(wrapper_CompositeImplicitAutograd_atol_rtol_float_linalg_matrix_rank));
m.impl("linalg_matrix_rank.atol_rtol_float_out",
TORCH_FN(wrapper_CompositeImplicitAutograd_atol_rtol_float_out_linalg_matrix_rank_out));
m.impl("linalg_matrix_rank",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_matrix_rank));
m.impl("linalg_matrix_rank.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_matrix_rank_out));
m.impl("linalg_matrix_rank.tol_tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_tol_tensor_linalg_matrix_rank));
m.impl("linalg_matrix_rank.out_tol_tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_tol_tensor_linalg_matrix_rank_out));
m.impl("linalg_multi_dot",
TORCH_FN(wrapper_CompositeImplicitAutograd__linalg_multi_dot));
m.impl("linalg_multi_dot.out",
TORCH_FN(wrapper_CompositeImplicitAutograd_out_linalg_multi_dot_out));
m.impl("nested_to_padded_tensor",
TORCH_FN(wrapper_CompositeImplicitAutograd__nested_to_padded_tensor));
m.impl("_test_serialization_subcmul",
TORCH_FN(wrapper_CompositeImplicitAutograd___test_serialization_subcmul));
m.impl("_test_string_default",
TORCH_FN(wrapper_CompositeImplicitAutograd___test_string_default));
m.impl("_test_ambiguous_defaults.a",
TORCH_FN(wrapper_CompositeImplicitAutograd_a__test_ambiguous_defaults));
m.impl("_test_ambiguous_defaults.b",
TORCH_FN(wrapper_CompositeImplicitAutograd_b__test_ambiguous_defaults));
m.impl("_test_autograd_multiple_dispatch.ntonly",
TORCH_FN(wrapper_CompositeImplicitAutograd_ntonly__test_autograd_multiple_dispatch));
m.impl("pad_sequence",
TORCH_FN(wrapper_CompositeImplicitAutograd__pad_sequence));
m.impl("flatten_dense_tensors",
TORCH_FN(wrapper_CompositeImplicitAutograd__flatten_dense_tensors));
m.impl("unflatten_dense_tensors",
TORCH_FN(wrapper_CompositeImplicitAutograd__unflatten_dense_tensors));
m.impl("scaled_dot_product_attention",
TORCH_FN(wrapper_CompositeImplicitAutograd__scaled_dot_product_attention));
m.impl("_scaled_dot_product_attention_math",
TORCH_FN(wrapper_CompositeImplicitAutograd___scaled_dot_product_attention_math));
m.impl("_propagate_xla_data",
TORCH_FN(wrapper_CompositeImplicitAutograd___propagate_xla_data));
};
} // anonymous namespace
namespace compositeimplicitautograd {
at::Tensor _cast_Byte(const at::Tensor & self, bool non_blocking) {
return wrapper_CompositeImplicitAutograd___cast_Byte(self, non_blocking);
}
at::Tensor _cast_Char(const at::Tensor & self, bool non_blocking) {
return wrapper_CompositeImplicitAutograd___cast_Char(self, non_blocking);
}
at::Tensor _cast_Double(const at::Tensor & self, bool non_blocking) {
return wrapper_CompositeImplicitAutograd___cast_Double(self, non_blocking);
}
at::Tensor _cast_Float(const at::Tensor & self, bool non_blocking) {
return wrapper_CompositeImplicitAutograd___cast_Float(self, non_blocking);
}
at::Tensor _cast_Int(const at::Tensor & self, bool non_blocking) {
return wrapper_CompositeImplicitAutograd___cast_Int(self, non_blocking);
}
at::Tensor _cast_Long(const at::Tensor & self, bool non_blocking) {
return wrapper_CompositeImplicitAutograd___cast_Long(self, non_blocking);
}
at::Tensor _cast_Short(const at::Tensor & self, bool non_blocking) {
return wrapper_CompositeImplicitAutograd___cast_Short(self, non_blocking);
}
at::Tensor _cast_Half(const at::Tensor & self, bool non_blocking) {
return wrapper_CompositeImplicitAutograd___cast_Half(self, non_blocking);
}
void _backward(const at::Tensor & self, at::TensorList inputs, const c10::optional<at::Tensor> & gradient, c10::optional<bool> retain_graph, bool create_graph) {
return wrapper_CompositeImplicitAutograd___backward(self, inputs, gradient, retain_graph, create_graph);
}
void set_data(at::Tensor & self, const at::Tensor & new_data) {
return wrapper_CompositeImplicitAutograd__set_data(self, new_data);
}
at::Tensor data(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__data(self);
}
bool is_leaf(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__is_leaf(self);
}
int64_t output_nr(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__output_nr(self);
}
int64_t _version(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd___version(self);
}
at::Tensor & requires_grad_(at::Tensor & self, bool requires_grad) {
return wrapper_CompositeImplicitAutograd__requires_grad_(self, requires_grad);
}
void retain_grad(at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__retain_grad(self);
}
bool retains_grad(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__retains_grad(self);
}
::std::tuple<at::Tensor,at::Tensor> _unpack_dual(const at::Tensor & dual, int64_t level) {
return wrapper_CompositeImplicitAutograd___unpack_dual(dual, level);
}
at::Tensor & rename_(at::Tensor & self, c10::optional<at::DimnameList> names) {
return wrapper_CompositeImplicitAutograd__rename_(self, names);
}
at::Tensor rename(const at::Tensor & self, c10::optional<at::DimnameList> names) {
return wrapper_CompositeImplicitAutograd__rename(self, names);
}
at::Tensor align_to(const at::Tensor & self, at::DimnameList names) {
return wrapper_CompositeImplicitAutograd__align_to(self, names);
}
at::Tensor align_to(const at::Tensor & self, at::DimnameList order, int64_t ellipsis_idx) {
return wrapper_CompositeImplicitAutograd_ellipsis_idx_align_to(self, order, ellipsis_idx);
}
at::Tensor align_as(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__align_as(self, other);
}
::std::vector<at::Tensor> align_tensors(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd__align_tensors(tensors);
}
void _assert_tensor_metadata(const at::Tensor & a, at::OptionalIntArrayRef size, at::OptionalIntArrayRef stride, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd___assert_tensor_metadata(a, size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*size)) : c10::nullopt, stride.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*stride)) : c10::nullopt, dtype);
}
void _assert_tensor_metadata_symint(const at::Tensor & a, at::OptionalSymIntArrayRef size, at::OptionalSymIntArrayRef stride, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd___assert_tensor_metadata(a, size, stride, dtype);
}
at::Tensor refine_names(const at::Tensor & self, at::DimnameList names) {
return wrapper_CompositeImplicitAutograd__refine_names(self, names);
}
bool _use_cudnn_rnn_flatten_weight() {
return wrapper_CompositeImplicitAutograd___use_cudnn_rnn_flatten_weight();
}
int64_t _debug_has_internal_overlap(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd___debug_has_internal_overlap(self);
}
::std::tuple<at::Tensor,at::Tensor> _sobol_engine_draw(const at::Tensor & quasi, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd___sobol_engine_draw(quasi, n, sobolstate, dimension, num_generated, dtype);
}
at::Tensor & _sobol_engine_ff_(at::Tensor & self, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated) {
return wrapper_CompositeImplicitAutograd___sobol_engine_ff_(self, n, sobolstate, dimension, num_generated);
}
at::Tensor & _sobol_engine_scramble_(at::Tensor & self, const at::Tensor & ltm, int64_t dimension) {
return wrapper_CompositeImplicitAutograd___sobol_engine_scramble_(self, ltm, dimension);
}
at::Tensor & _sobol_engine_initialize_state_(at::Tensor & self, int64_t dimension) {
return wrapper_CompositeImplicitAutograd___sobol_engine_initialize_state_(self, dimension);
}
at::Tensor _reshape_from_tensor(const at::Tensor & self, const at::Tensor & shape) {
return wrapper_CompositeImplicitAutograd___reshape_from_tensor(self, shape);
}
at::Tensor _shape_as_tensor(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd___shape_as_tensor(self);
}
at::Tensor dropout(const at::Tensor & input, double p, bool train) {
return wrapper_CompositeImplicitAutograd__dropout(input, p, train);
}
at::Tensor & dropout_(at::Tensor & self, double p, bool train) {
return wrapper_CompositeImplicitAutograd__dropout_(self, p, train);
}
at::Tensor feature_dropout(const at::Tensor & input, double p, bool train) {
return wrapper_CompositeImplicitAutograd__feature_dropout(input, p, train);
}
at::Tensor & feature_dropout_(at::Tensor & self, double p, bool train) {
return wrapper_CompositeImplicitAutograd__feature_dropout_(self, p, train);
}
at::Tensor alpha_dropout(const at::Tensor & input, double p, bool train) {
return wrapper_CompositeImplicitAutograd__alpha_dropout(input, p, train);
}
at::Tensor & alpha_dropout_(at::Tensor & self, double p, bool train) {
return wrapper_CompositeImplicitAutograd__alpha_dropout_(self, p, train);
}
at::Tensor feature_alpha_dropout(const at::Tensor & input, double p, bool train) {
return wrapper_CompositeImplicitAutograd__feature_alpha_dropout(input, p, train);
}
at::Tensor & feature_alpha_dropout_(at::Tensor & self, double p, bool train) {
return wrapper_CompositeImplicitAutograd__feature_alpha_dropout_(self, p, train);
}
at::Tensor absolute(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__absolute(self);
}
at::Tensor & absolute_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_absolute_out(self, out);
}
at::Tensor & absolute_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_absolute_out(self, out);
}
at::Tensor & absolute_(at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__absolute_(self);
}
at::Tensor chalf(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_CompositeImplicitAutograd__chalf(self, memory_format);
}
at::Tensor real(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__real(self);
}
at::Tensor imag(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__imag(self);
}
at::Tensor conj(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__conj(self);
}
at::Tensor conj_physical(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__conj_physical(self);
}
at::Tensor resolve_conj(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__resolve_conj(self);
}
at::Tensor resolve_neg(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__resolve_neg(self);
}
at::Tensor arccos(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__arccos(self);
}
at::Tensor & arccos_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_arccos_out(self, out);
}
at::Tensor & arccos_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_arccos_out(self, out);
}
at::Tensor & arccos_(at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__arccos_(self);
}
at::Tensor avg_pool1d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad) {
return wrapper_CompositeImplicitAutograd__avg_pool1d(self, kernel_size, stride, padding, ceil_mode, count_include_pad);
}
at::Tensor adaptive_avg_pool1d(const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper_CompositeImplicitAutograd__adaptive_avg_pool1d(self, output_size);
}
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool1d(const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper_CompositeImplicitAutograd__adaptive_max_pool1d(self, output_size);
}
at::Tensor affine_grid_generator_backward(const at::Tensor & grad, at::IntArrayRef size, bool align_corners) {
return wrapper_CompositeImplicitAutograd__affine_grid_generator_backward(grad, c10::fromIntArrayRefSlow(size), align_corners);
}
at::Tensor affine_grid_generator_backward_symint(const at::Tensor & grad, c10::SymIntArrayRef size, bool align_corners) {
return wrapper_CompositeImplicitAutograd__affine_grid_generator_backward(grad, size, align_corners);
}
at::Tensor _test_check_tensor(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd___test_check_tensor(self);
}
at::Tensor all(const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dimname_all(self, dim, keepdim);
}
at::Tensor & all_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dimname_out_all_out(self, dim, keepdim, out);
}
at::Tensor & all_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_dimname_out_all_out(self, dim, keepdim, out);
}
at::Tensor any(const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dimname_any(self, dim, keepdim);
}
at::Tensor & any_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dimname_out_any_out(self, dim, keepdim, out);
}
at::Tensor & any_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_dimname_out_any_out(self, dim, keepdim, out);
}
at::Tensor _dim_arange(const at::Tensor & like, int64_t dim) {
return wrapper_CompositeImplicitAutograd___dim_arange(like, dim);
}
at::Tensor arccosh(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__arccosh(self);
}
at::Tensor & arccosh_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_arccosh_out(self, out);
}
at::Tensor & arccosh_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_arccosh_out(self, out);
}
at::Tensor & arccosh_(at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__arccosh_(self);
}
at::Tensor arcsinh(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__arcsinh(self);
}
at::Tensor & arcsinh_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_arcsinh_out(self, out);
}
at::Tensor & arcsinh_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_arcsinh_out(self, out);
}
at::Tensor & arcsinh_(at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__arcsinh_(self);
}
at::Tensor arctanh(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__arctanh(self);
}
at::Tensor & arctanh_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_arctanh_out(self, out);
}
at::Tensor & arctanh_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_arctanh_out(self, out);
}
at::Tensor & arctanh_(at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__arctanh_(self);
}
at::Tensor arcsin(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__arcsin(self);
}
at::Tensor & arcsin_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_arcsin_out(self, out);
}
at::Tensor & arcsin_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_arcsin_out(self, out);
}
at::Tensor & arcsin_(at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__arcsin_(self);
}
at::Tensor arctan(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__arctan(self);
}
at::Tensor & arctan_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_arctan_out(self, out);
}
at::Tensor & arctan_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_arctan_out(self, out);
}
at::Tensor & arctan_(at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__arctan_(self);
}
at::Tensor atleast_1d(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__atleast_1d(self);
}
::std::vector<at::Tensor> atleast_1d(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd_Sequence_atleast_1d(tensors);
}
at::Tensor atleast_2d(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__atleast_2d(self);
}
::std::vector<at::Tensor> atleast_2d(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd_Sequence_atleast_2d(tensors);
}
at::Tensor atleast_3d(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__atleast_3d(self);
}
::std::vector<at::Tensor> atleast_3d(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd_Sequence_atleast_3d(tensors);
}
at::Tensor batch_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
return wrapper_CompositeImplicitAutograd__batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,int64_t> _batch_norm_impl_index(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
return wrapper_CompositeImplicitAutograd___batch_norm_impl_index(input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _batch_norm_impl_index_backward(int64_t impl_index, const at::Tensor & input, const at::Tensor & grad_output, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var_transform, bool train, double eps, ::std::array<bool,3> output_mask, const at::Tensor & reservedSpace) {
return wrapper_CompositeImplicitAutograd___batch_norm_impl_index_backward(impl_index, input, grad_output, weight, running_mean, running_var, save_mean, save_var_transform, train, eps, output_mask, reservedSpace);
}
at::Tensor bilinear(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
return wrapper_CompositeImplicitAutograd__bilinear(input1, input2, weight, bias);
}
::std::vector<at::Tensor> broadcast_tensors(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd__broadcast_tensors(tensors);
}
at::Tensor broadcast_to(const at::Tensor & self, at::IntArrayRef size) {
return wrapper_CompositeImplicitAutograd__broadcast_to(self, c10::fromIntArrayRefSlow(size));
}
at::Tensor broadcast_to_symint(const at::Tensor & self, c10::SymIntArrayRef size) {
return wrapper_CompositeImplicitAutograd__broadcast_to(self, size);
}
at::Tensor cat(at::TensorList tensors, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_names_cat(tensors, dim);
}
at::Tensor & cat_out(at::Tensor & out, at::TensorList tensors, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_names_out_cat_out(tensors, dim, out);
}
at::Tensor & cat_outf(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_names_out_cat_out(tensors, dim, out);
}
at::Tensor concat(at::TensorList tensors, int64_t dim) {
return wrapper_CompositeImplicitAutograd__concat(tensors, dim);
}
at::Tensor & concat_out(at::Tensor & out, at::TensorList tensors, int64_t dim) {
return wrapper_CompositeImplicitAutograd_out_concat_out(tensors, dim, out);
}
at::Tensor & concat_outf(at::TensorList tensors, int64_t dim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_concat_out(tensors, dim, out);
}
at::Tensor concat(at::TensorList tensors, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_names_concat(tensors, dim);
}
at::Tensor & concat_out(at::Tensor & out, at::TensorList tensors, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_names_out_concat_out(tensors, dim, out);
}
at::Tensor & concat_outf(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_names_out_concat_out(tensors, dim, out);
}
at::Tensor concatenate(at::TensorList tensors, int64_t dim) {
return wrapper_CompositeImplicitAutograd__concatenate(tensors, dim);
}
at::Tensor & concatenate_out(at::Tensor & out, at::TensorList tensors, int64_t dim) {
return wrapper_CompositeImplicitAutograd_out_concatenate_out(tensors, dim, out);
}
at::Tensor & concatenate_outf(at::TensorList tensors, int64_t dim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_concatenate_out(tensors, dim, out);
}
at::Tensor concatenate(at::TensorList tensors, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_names_concatenate(tensors, dim);
}
at::Tensor & concatenate_out(at::Tensor & out, at::TensorList tensors, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_names_out_concatenate_out(tensors, dim, out);
}
at::Tensor & concatenate_outf(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_names_out_concatenate_out(tensors, dim, out);
}
at::Tensor chain_matmul(at::TensorList matrices) {
return wrapper_CompositeImplicitAutograd__chain_matmul(matrices);
}
at::Tensor & chain_matmul_out(at::Tensor & out, at::TensorList matrices) {
return wrapper_CompositeImplicitAutograd_out_chain_matmul_out(matrices, out);
}
at::Tensor & chain_matmul_outf(at::TensorList matrices, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_chain_matmul_out(matrices, out);
}
::std::vector<at::Tensor> unsafe_chunk(const at::Tensor & self, int64_t chunks, int64_t dim) {
return wrapper_CompositeImplicitAutograd__unsafe_chunk(self, chunks, dim);
}
::std::vector<at::Tensor> chunk(const at::Tensor & self, int64_t chunks, int64_t dim) {
return wrapper_CompositeImplicitAutograd__chunk(self, chunks, dim);
}
::std::vector<at::Tensor> tensor_split(const at::Tensor & self, int64_t sections, int64_t dim) {
return wrapper_CompositeImplicitAutograd_sections_tensor_split(self, sections, dim);
}
::std::vector<at::Tensor> tensor_split_symint(const at::Tensor & self, c10::SymInt sections, int64_t dim) {
return wrapper_CompositeImplicitAutograd_sections_tensor_split(self, sections, dim);
}
::std::vector<at::Tensor> tensor_split(const at::Tensor & self, at::IntArrayRef indices, int64_t dim) {
return wrapper_CompositeImplicitAutograd_indices_tensor_split(self, c10::fromIntArrayRefSlow(indices), dim);
}
::std::vector<at::Tensor> tensor_split_symint(const at::Tensor & self, c10::SymIntArrayRef indices, int64_t dim) {
return wrapper_CompositeImplicitAutograd_indices_tensor_split(self, indices, dim);
}
::std::vector<at::Tensor> tensor_split(const at::Tensor & self, const at::Tensor & tensor_indices_or_sections, int64_t dim) {
return wrapper_CompositeImplicitAutograd_tensor_indices_or_sections_tensor_split(self, tensor_indices_or_sections, dim);
}
at::Tensor clip(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
return wrapper_CompositeImplicitAutograd__clip(self, min, max);
}
at::Tensor & clip_out(at::Tensor & out, const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
return wrapper_CompositeImplicitAutograd_out_clip_out(self, min, max, out);
}
at::Tensor & clip_outf(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_clip_out(self, min, max, out);
}
at::Tensor & clip_(at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
return wrapper_CompositeImplicitAutograd__clip_(self, min, max);
}
at::Tensor clip(const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
return wrapper_CompositeImplicitAutograd_Tensor_clip(self, min, max);
}
at::Tensor & clip_out(at::Tensor & out, const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
return wrapper_CompositeImplicitAutograd_Tensor_out_clip_out(self, min, max, out);
}
at::Tensor & clip_outf(const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Tensor_out_clip_out(self, min, max, out);
}
at::Tensor & clip_(at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
return wrapper_CompositeImplicitAutograd_Tensor_clip_(self, min, max);
}
bool cudnn_is_acceptable(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__cudnn_is_acceptable(self);
}
at::Tensor contiguous(const at::Tensor & self, at::MemoryFormat memory_format) {
return wrapper_CompositeImplicitAutograd__contiguous(self, memory_format);
}
at::Tensor _convolution(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled) {
return wrapper_CompositeImplicitAutograd_deprecated__convolution(input, weight, bias, c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(padding), c10::fromIntArrayRefSlow(dilation), transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled);
}
at::Tensor _convolution_symint(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, c10::SymInt groups, bool benchmark, bool deterministic, bool cudnn_enabled) {
return wrapper_CompositeImplicitAutograd_deprecated__convolution(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled);
}
at::Tensor _convolution_mode(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
return wrapper_CompositeImplicitAutograd___convolution_mode(input, weight, bias, c10::fromIntArrayRefSlow(stride), padding, c10::fromIntArrayRefSlow(dilation), groups);
}
at::Tensor _convolution_mode_symint(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
return wrapper_CompositeImplicitAutograd___convolution_mode(input, weight, bias, stride, padding, dilation, groups);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _convolution_double_backward(const c10::optional<at::Tensor> & ggI, const c10::optional<at::Tensor> & ggW, const c10::optional<at::Tensor> & ggb, const at::Tensor & gO, const at::Tensor & weight, const at::Tensor & self, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, ::std::array<bool,3> output_mask) {
return wrapper_CompositeImplicitAutograd___convolution_double_backward(ggI, ggW, ggb, gO, weight, self, c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(padding), c10::fromIntArrayRefSlow(dilation), transposed, c10::fromIntArrayRefSlow(output_padding), groups, output_mask);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _convolution_double_backward_symint(const c10::optional<at::Tensor> & ggI, const c10::optional<at::Tensor> & ggW, const c10::optional<at::Tensor> & ggb, const at::Tensor & gO, const at::Tensor & weight, const at::Tensor & self, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array<bool,3> output_mask) {
return wrapper_CompositeImplicitAutograd___convolution_double_backward(ggI, ggW, ggb, gO, weight, self, stride, padding, dilation, transposed, output_padding, groups, output_mask);
}
at::Tensor conv1d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
return wrapper_CompositeImplicitAutograd__conv1d(input, weight, bias, c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(padding), c10::fromIntArrayRefSlow(dilation), groups);
}
at::Tensor conv1d_symint(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
return wrapper_CompositeImplicitAutograd__conv1d(input, weight, bias, stride, padding, dilation, groups);
}
at::Tensor conv2d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
return wrapper_CompositeImplicitAutograd__conv2d(input, weight, bias, c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(padding), c10::fromIntArrayRefSlow(dilation), groups);
}
at::Tensor conv2d_symint(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
return wrapper_CompositeImplicitAutograd__conv2d(input, weight, bias, stride, padding, dilation, groups);
}
at::Tensor conv3d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
return wrapper_CompositeImplicitAutograd__conv3d(input, weight, bias, c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(padding), c10::fromIntArrayRefSlow(dilation), groups);
}
at::Tensor conv3d_symint(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
return wrapper_CompositeImplicitAutograd__conv3d(input, weight, bias, stride, padding, dilation, groups);
}
at::Tensor conv1d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
return wrapper_CompositeImplicitAutograd_padding_conv1d(input, weight, bias, c10::fromIntArrayRefSlow(stride), padding, c10::fromIntArrayRefSlow(dilation), groups);
}
at::Tensor conv1d_symint(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
return wrapper_CompositeImplicitAutograd_padding_conv1d(input, weight, bias, stride, padding, dilation, groups);
}
at::Tensor conv2d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
return wrapper_CompositeImplicitAutograd_padding_conv2d(input, weight, bias, c10::fromIntArrayRefSlow(stride), padding, c10::fromIntArrayRefSlow(dilation), groups);
}
at::Tensor conv2d_symint(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
return wrapper_CompositeImplicitAutograd_padding_conv2d(input, weight, bias, stride, padding, dilation, groups);
}
at::Tensor conv3d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
return wrapper_CompositeImplicitAutograd_padding_conv3d(input, weight, bias, c10::fromIntArrayRefSlow(stride), padding, c10::fromIntArrayRefSlow(dilation), groups);
}
at::Tensor conv3d_symint(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::string_view padding, c10::SymIntArrayRef dilation, c10::SymInt groups) {
return wrapper_CompositeImplicitAutograd_padding_conv3d(input, weight, bias, stride, padding, dilation, groups);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> conv_tbc_backward(const at::Tensor & self, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
return wrapper_CompositeImplicitAutograd__conv_tbc_backward(self, input, weight, bias, pad);
}
at::Tensor conv_transpose1d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
return wrapper_CompositeImplicitAutograd__conv_transpose1d(input, weight, bias, c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(padding), c10::fromIntArrayRefSlow(output_padding), groups, c10::fromIntArrayRefSlow(dilation));
}
at::Tensor conv_transpose1d_symint(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymInt groups, c10::SymIntArrayRef dilation) {
return wrapper_CompositeImplicitAutograd__conv_transpose1d(input, weight, bias, stride, padding, output_padding, groups, dilation);
}
at::Tensor conv_transpose2d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
return wrapper_CompositeImplicitAutograd_input_conv_transpose2d(input, weight, bias, c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(padding), c10::fromIntArrayRefSlow(output_padding), groups, c10::fromIntArrayRefSlow(dilation));
}
at::Tensor conv_transpose2d_symint(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymInt groups, c10::SymIntArrayRef dilation) {
return wrapper_CompositeImplicitAutograd_input_conv_transpose2d(input, weight, bias, stride, padding, output_padding, groups, dilation);
}
at::Tensor conv_transpose3d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
return wrapper_CompositeImplicitAutograd_input_conv_transpose3d(input, weight, bias, c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(padding), c10::fromIntArrayRefSlow(output_padding), groups, c10::fromIntArrayRefSlow(dilation));
}
at::Tensor conv_transpose3d_symint(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymInt groups, c10::SymIntArrayRef dilation) {
return wrapper_CompositeImplicitAutograd_input_conv_transpose3d(input, weight, bias, stride, padding, output_padding, groups, dilation);
}
at::Tensor cosine_embedding_loss(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
return wrapper_CompositeImplicitAutograd__cosine_embedding_loss(input1, input2, target, margin, reduction);
}
at::Tensor cov(const at::Tensor & self, int64_t correction, const c10::optional<at::Tensor> & fweights, const c10::optional<at::Tensor> & aweights) {
return wrapper_CompositeImplicitAutograd__cov(self, correction, fweights, aweights);
}
at::Tensor corrcoef(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__corrcoef(self);
}
::std::tuple<at::Tensor,at::Tensor> cummax(const at::Tensor & self, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_dimname_cummax(self, dim);
}
::std::tuple<at::Tensor &,at::Tensor &> cummax_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_dimname_out_cummax_out(self, dim, values, indices);
}
::std::tuple<at::Tensor &,at::Tensor &> cummax_outf(const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
return wrapper_CompositeImplicitAutograd_dimname_out_cummax_out(self, dim, values, indices);
}
::std::tuple<at::Tensor,at::Tensor> cummin(const at::Tensor & self, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_dimname_cummin(self, dim);
}
::std::tuple<at::Tensor &,at::Tensor &> cummin_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_dimname_out_cummin_out(self, dim, values, indices);
}
::std::tuple<at::Tensor &,at::Tensor &> cummin_outf(const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
return wrapper_CompositeImplicitAutograd_dimname_out_cummin_out(self, dim, values, indices);
}
at::Tensor cummaxmin_backward(const at::Tensor & grad, const at::Tensor & input, const at::Tensor & indices, int64_t dim) {
return wrapper_CompositeImplicitAutograd__cummaxmin_backward(grad, input, indices, dim);
}
at::Tensor cumprod(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_dimname_cumprod(self, dim, dtype);
}
at::Tensor & cumprod_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_dimname_out_cumprod_out(self, dim, dtype, out);
}
at::Tensor & cumprod_outf(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_dimname_out_cumprod_out(self, dim, dtype, out);
}
at::Tensor & cumprod_(at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_dimname_cumprod_(self, dim, dtype);
}
at::Tensor cumprod_backward(const at::Tensor & grad, const at::Tensor & input, int64_t dim, const at::Tensor & output) {
return wrapper_CompositeImplicitAutograd__cumprod_backward(grad, input, dim, output);
}
at::Tensor cumsum(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_dimname_cumsum(self, dim, dtype);
}
at::Tensor & cumsum_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_dimname_out_cumsum_out(self, dim, dtype, out);
}
at::Tensor & cumsum_outf(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_dimname_out_cumsum_out(self, dim, dtype, out);
}
at::Tensor & cumsum_(at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_dimname_cumsum_(self, dim, dtype);
}
at::Tensor cumulative_trapezoid(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
return wrapper_CompositeImplicitAutograd_x_cumulative_trapezoid(y, x, dim);
}
at::Tensor cumulative_trapezoid(const at::Tensor & y, const at::Scalar & dx, int64_t dim) {
return wrapper_CompositeImplicitAutograd_dx_cumulative_trapezoid(y, dx, dim);
}
at::Tensor ctc_loss(const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
return wrapper_CompositeImplicitAutograd_IntList_ctc_loss(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}
at::Tensor ctc_loss(const at::Tensor & log_probs, const at::Tensor & targets, const at::Tensor & input_lengths, const at::Tensor & target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
return wrapper_CompositeImplicitAutograd_Tensor_ctc_loss(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}
at::Tensor diagflat(const at::Tensor & self, int64_t offset) {
return wrapper_CompositeImplicitAutograd__diagflat(self, offset);
}
at::Tensor linalg_diagonal(const at::Tensor & A, int64_t offset, int64_t dim1, int64_t dim2) {
return wrapper_CompositeImplicitAutograd__linalg_diagonal(A, offset, dim1, dim2);
}
at::Tensor diagonal(const at::Tensor & self, at::Dimname outdim, at::Dimname dim1, at::Dimname dim2, int64_t offset) {
return wrapper_CompositeImplicitAutograd_Dimname_diagonal(self, outdim, dim1, dim2, offset);
}
at::Tensor & fill_diagonal_(at::Tensor & self, const at::Scalar & fill_value, bool wrap) {
return wrapper_CompositeImplicitAutograd__fill_diagonal_(self, fill_value, wrap);
}
at::Tensor diff(const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append) {
return wrapper_CompositeImplicitAutograd__diff(self, n, dim, prepend, append);
}
at::Tensor & diff_out(at::Tensor & out, const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append) {
return wrapper_CompositeImplicitAutograd_out_diff_out(self, n, dim, prepend, append, out);
}
at::Tensor & diff_outf(const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_diff_out(self, n, dim, prepend, append, out);
}
::std::vector<at::Tensor> gradient(const at::Tensor & self, const c10::optional<at::Scalar> & spacing, c10::optional<int64_t> dim, int64_t edge_order) {
return wrapper_CompositeImplicitAutograd_scalarint_gradient(self, spacing, dim, edge_order);
}
::std::vector<at::Tensor> gradient(const at::Tensor & self, const at::Scalar & spacing, at::IntArrayRef dim, int64_t edge_order) {
return wrapper_CompositeImplicitAutograd_scalararray_gradient(self, spacing, dim, edge_order);
}
::std::vector<at::Tensor> gradient(const at::Tensor & self, at::IntArrayRef dim, int64_t edge_order) {
return wrapper_CompositeImplicitAutograd_array_gradient(self, dim, edge_order);
}
::std::vector<at::Tensor> gradient(const at::Tensor & self, at::ArrayRef<at::Scalar> spacing, c10::optional<int64_t> dim, int64_t edge_order) {
return wrapper_CompositeImplicitAutograd_scalarrayint_gradient(self, spacing, dim, edge_order);
}
::std::vector<at::Tensor> gradient(const at::Tensor & self, at::ArrayRef<at::Scalar> spacing, at::IntArrayRef dim, int64_t edge_order) {
return wrapper_CompositeImplicitAutograd_scalarrayarray_gradient(self, spacing, dim, edge_order);
}
::std::vector<at::Tensor> gradient(const at::Tensor & self, at::TensorList spacing, c10::optional<int64_t> dim, int64_t edge_order) {
return wrapper_CompositeImplicitAutograd_tensorarrayint_gradient(self, spacing, dim, edge_order);
}
::std::vector<at::Tensor> gradient(const at::Tensor & self, at::TensorList spacing, at::IntArrayRef dim, int64_t edge_order) {
return wrapper_CompositeImplicitAutograd_tensorarray_gradient(self, spacing, dim, edge_order);
}
at::Tensor divide(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_divide(self, other);
}
at::Tensor & divide_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_out_divide_out(self, other, out);
}
at::Tensor & divide_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_divide_out(self, other, out);
}
at::Tensor & divide_(at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_divide_(self, other);
}
at::Tensor divide(const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_divide(self, other);
}
at::Tensor & divide_(at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_divide_(self, other);
}
at::Tensor divide(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_CompositeImplicitAutograd_Tensor_mode_divide(self, other, rounding_mode);
}
at::Tensor & divide_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_CompositeImplicitAutograd_out_mode_divide_out(self, other, rounding_mode, out);
}
at::Tensor & divide_outf(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_mode_divide_out(self, other, rounding_mode, out);
}
at::Tensor & divide_(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_CompositeImplicitAutograd_Tensor_mode_divide_(self, other, rounding_mode);
}
at::Tensor divide(const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_CompositeImplicitAutograd_Scalar_mode_divide(self, other, rounding_mode);
}
at::Tensor & divide_(at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_CompositeImplicitAutograd_Scalar_mode_divide_(self, other, rounding_mode);
}
at::Tensor true_divide(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_true_divide(self, other);
}
at::Tensor & true_divide_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_out_true_divide_out(self, other, out);
}
at::Tensor & true_divide_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_true_divide_out(self, other, out);
}
at::Tensor & true_divide_(at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_true_divide_(self, other);
}
at::Tensor true_divide(const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_true_divide(self, other);
}
at::Tensor & true_divide_(at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_true_divide_(self, other);
}
at::Tensor einsum(c10::string_view equation, at::TensorList tensors, at::OptionalIntArrayRef path) {
return wrapper_CompositeImplicitAutograd__einsum(equation, tensors, path);
}
at::Tensor embedding_backward(const at::Tensor & grad, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
return wrapper_CompositeImplicitAutograd__embedding_backward(grad, indices, num_weights, padding_idx, scale_grad_by_freq, sparse);
}
at::Tensor embedding_backward_symint(const at::Tensor & grad, const at::Tensor & indices, c10::SymInt num_weights, c10::SymInt padding_idx, bool scale_grad_by_freq, bool sparse) {
return wrapper_CompositeImplicitAutograd__embedding_backward(grad, indices, num_weights, padding_idx, scale_grad_by_freq, sparse);
}
at::Tensor embedding_sparse_backward(const at::Tensor & grad, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
return wrapper_CompositeImplicitAutograd__embedding_sparse_backward(grad, indices, num_weights, padding_idx, scale_grad_by_freq);
}
::std::tuple<at::Tensor,at::Tensor> _rowwise_prune(const at::Tensor & weight, const at::Tensor & mask, at::ScalarType compressed_indices_dtype) {
return wrapper_CompositeImplicitAutograd___rowwise_prune(weight, mask, compressed_indices_dtype);
}
at::Tensor row_stack(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd__row_stack(tensors);
}
at::Tensor & row_stack_out(at::Tensor & out, at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd_out_row_stack_out(tensors, out);
}
at::Tensor & row_stack_outf(at::TensorList tensors, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_row_stack_out(tensors, out);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> embedding_bag(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset) {
return wrapper_CompositeImplicitAutograd__embedding_bag(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> embedding_bag(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, c10::optional<int64_t> padding_idx) {
return wrapper_CompositeImplicitAutograd_padding_idx_embedding_bag(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}
at::Tensor _embedding_bag_backward(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
return wrapper_CompositeImplicitAutograd___embedding_bag_backward(grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, sparse, per_sample_weights, padding_idx);
}
at::Tensor _embedding_bag_backward_symint(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, c10::SymInt num_weights, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
return wrapper_CompositeImplicitAutograd___embedding_bag_backward(grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, sparse, per_sample_weights, padding_idx);
}
at::Tensor _embedding_bag_sparse_backward(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
return wrapper_CompositeImplicitAutograd___embedding_bag_sparse_backward(grad, indices, offsets, offset2bag, bag_size, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx);
}
at::Tensor _embedding_bag_sparse_backward_symint(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, c10::SymInt num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
return wrapper_CompositeImplicitAutograd___embedding_bag_sparse_backward(grad, indices, offsets, offset2bag, bag_size, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx);
}
at::Tensor & empty_out(at::Tensor & out, at::IntArrayRef size, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_CompositeImplicitAutograd_out_empty_out(c10::fromIntArrayRefSlow(size), memory_format, out);
}
at::Tensor & empty_outf(at::IntArrayRef size, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_empty_out(c10::fromIntArrayRefSlow(size), memory_format, out);
}
at::Tensor & empty_symint_out(at::Tensor & out, c10::SymIntArrayRef size, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_CompositeImplicitAutograd_out_empty_out(size, memory_format, out);
}
at::Tensor & empty_symint_outf(c10::SymIntArrayRef size, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_empty_out(size, memory_format, out);
}
at::Tensor expand_as(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__expand_as(self, other);
}
at::Tensor flatten(const at::Tensor & self, int64_t start_dim, int64_t end_dim) {
return wrapper_CompositeImplicitAutograd_using_ints_flatten(self, start_dim, end_dim);
}
at::Tensor flatten(const at::Tensor & self, int64_t start_dim, int64_t end_dim, at::Dimname out_dim) {
return wrapper_CompositeImplicitAutograd_named_out_dim_flatten(self, start_dim, end_dim, out_dim);
}
at::Tensor flatten(const at::Tensor & self, at::Dimname start_dim, at::Dimname end_dim, at::Dimname out_dim) {
return wrapper_CompositeImplicitAutograd_using_names_flatten(self, start_dim, end_dim, out_dim);
}
at::Tensor flatten(const at::Tensor & self, at::DimnameList dims, at::Dimname out_dim) {
return wrapper_CompositeImplicitAutograd_DimnameList_flatten(self, dims, out_dim);
}
at::Tensor unflatten(const at::Tensor & self, int64_t dim, at::IntArrayRef sizes) {
return wrapper_CompositeImplicitAutograd_int_unflatten(self, dim, c10::fromIntArrayRefSlow(sizes));
}
at::Tensor unflatten_symint(const at::Tensor & self, int64_t dim, c10::SymIntArrayRef sizes) {
return wrapper_CompositeImplicitAutograd_int_unflatten(self, dim, sizes);
}
at::Tensor unflatten(const at::Tensor & self, at::Dimname dim, at::IntArrayRef sizes, at::DimnameList names) {
return wrapper_CompositeImplicitAutograd_Dimname_unflatten(self, dim, c10::fromIntArrayRefSlow(sizes), names);
}
at::Tensor unflatten_symint(const at::Tensor & self, at::Dimname dim, c10::SymIntArrayRef sizes, at::DimnameList names) {
return wrapper_CompositeImplicitAutograd_Dimname_unflatten(self, dim, sizes, names);
}
at::Tensor grid_sampler(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
return wrapper_CompositeImplicitAutograd__grid_sampler(input, grid, interpolation_mode, padding_mode, align_corners);
}
::std::tuple<at::Tensor,at::Tensor> _grid_sampler_2d_cpu_fallback_backward(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
return wrapper_CompositeImplicitAutograd___grid_sampler_2d_cpu_fallback_backward(grad_output, input, grid, interpolation_mode, padding_mode, align_corners);
}
at::Tensor hinge_embedding_loss(const at::Tensor & self, const at::Tensor & target, double margin, int64_t reduction) {
return wrapper_CompositeImplicitAutograd__hinge_embedding_loss(self, target, margin, reduction);
}
at::Tensor group_norm(const at::Tensor & input, int64_t num_groups, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enabled) {
return wrapper_CompositeImplicitAutograd__group_norm(input, num_groups, weight, bias, eps, cudnn_enabled);
}
int64_t _cufft_get_plan_cache_size(at::DeviceIndex device_index) {
return wrapper_CompositeImplicitAutograd___cufft_get_plan_cache_size(device_index);
}
int64_t _cufft_get_plan_cache_max_size(at::DeviceIndex device_index) {
return wrapper_CompositeImplicitAutograd___cufft_get_plan_cache_max_size(device_index);
}
void _cufft_set_plan_cache_max_size(at::DeviceIndex device_index, int64_t max_size) {
return wrapper_CompositeImplicitAutograd___cufft_set_plan_cache_max_size(device_index, max_size);
}
void _cufft_clear_plan_cache(at::DeviceIndex device_index) {
return wrapper_CompositeImplicitAutograd___cufft_clear_plan_cache(device_index);
}
at::Tensor & index_copy_(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source) {
return wrapper_CompositeImplicitAutograd_dimname_index_copy_(self, dim, index, source);
}
at::Tensor index_copy(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source) {
return wrapper_CompositeImplicitAutograd_dimname_index_copy(self, dim, index, source);
}
at::Tensor instance_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool use_input_stats, double momentum, double eps, bool cudnn_enabled) {
return wrapper_CompositeImplicitAutograd__instance_norm(input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, cudnn_enabled);
}
at::Tensor isclose(const at::Tensor & self, const at::Tensor & other, double rtol, double atol, bool equal_nan) {
return wrapper_CompositeImplicitAutograd__isclose(self, other, rtol, atol, equal_nan);
}
bool is_distributed(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__is_distributed(self);
}
bool is_floating_point(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__is_floating_point(self);
}
bool is_complex(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__is_complex(self);
}
bool is_conj(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__is_conj(self);
}
bool _is_zerotensor(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd___is_zerotensor(self);
}
bool is_neg(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__is_neg(self);
}
at::Tensor isreal(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__isreal(self);
}
bool is_nonzero(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__is_nonzero(self);
}
bool is_signed(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__is_signed(self);
}
bool is_inference(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__is_inference(self);
}
at::Tensor kl_div(const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
return wrapper_CompositeImplicitAutograd__kl_div(self, target, reduction, log_target);
}
at::Tensor kron(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__kron(self, other);
}
at::Tensor & kron_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_out_kron_out(self, other, out);
}
at::Tensor & kron_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_kron_out(self, other, out);
}
::std::tuple<at::Tensor,at::Tensor> kthvalue(const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dimname_kthvalue(self, k, dim, keepdim);
}
::std::tuple<at::Tensor &,at::Tensor &> kthvalue_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dimname_out_kthvalue_out(self, k, dim, keepdim, values, indices);
}
::std::tuple<at::Tensor &,at::Tensor &> kthvalue_outf(const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
return wrapper_CompositeImplicitAutograd_dimname_out_kthvalue_out(self, k, dim, keepdim, values, indices);
}
at::Tensor layer_norm(const at::Tensor & input, at::IntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enable) {
return wrapper_CompositeImplicitAutograd__layer_norm(input, c10::fromIntArrayRefSlow(normalized_shape), weight, bias, eps, cudnn_enable);
}
at::Tensor layer_norm_symint(const at::Tensor & input, c10::SymIntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enable) {
return wrapper_CompositeImplicitAutograd__layer_norm(input, normalized_shape, weight, bias, eps, cudnn_enable);
}
at::Tensor linear(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
return wrapper_CompositeImplicitAutograd__linear(input, weight, bias);
}
at::Tensor fbgemm_linear_int8_weight_fp32_activation(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
return wrapper_CompositeImplicitAutograd__fbgemm_linear_int8_weight_fp32_activation(input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}
at::Tensor fbgemm_linear_int8_weight(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
return wrapper_CompositeImplicitAutograd__fbgemm_linear_int8_weight(input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}
::std::tuple<at::Tensor,at::Tensor,double,int64_t> fbgemm_linear_quantize_weight(const at::Tensor & input) {
return wrapper_CompositeImplicitAutograd__fbgemm_linear_quantize_weight(input);
}
at::Tensor fbgemm_pack_gemm_matrix_fp16(const at::Tensor & input) {
return wrapper_CompositeImplicitAutograd__fbgemm_pack_gemm_matrix_fp16(input);
}
at::Tensor fbgemm_linear_fp16_weight_fp32_activation(const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
return wrapper_CompositeImplicitAutograd__fbgemm_linear_fp16_weight_fp32_activation(input, packed_weight, bias);
}
at::Tensor fbgemm_linear_fp16_weight(const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
return wrapper_CompositeImplicitAutograd__fbgemm_linear_fp16_weight(input, packed_weight, bias);
}
at::Tensor fbgemm_pack_quantized_matrix(const at::Tensor & input) {
return wrapper_CompositeImplicitAutograd__fbgemm_pack_quantized_matrix(input);
}
at::Tensor fbgemm_pack_quantized_matrix(const at::Tensor & input, int64_t K, int64_t N) {
return wrapper_CompositeImplicitAutograd_KN_fbgemm_pack_quantized_matrix(input, K, N);
}
at::Tensor ldexp(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_ldexp(self, other);
}
at::Tensor & ldexp_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_out_ldexp_out(self, other, out);
}
at::Tensor & ldexp_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_ldexp_out(self, other, out);
}
at::Tensor & ldexp_(at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__ldexp_(self, other);
}
at::Tensor log_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_int_log_softmax(self, dim, dtype);
}
at::Tensor log_softmax(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_Dimname_log_softmax(self, dim, dtype);
}
at::Tensor logcumsumexp(const at::Tensor & self, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_dimname_logcumsumexp(self, dim);
}
at::Tensor & logcumsumexp_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_dimname_out_logcumsumexp_out(self, dim, out);
}
at::Tensor & logcumsumexp_outf(const at::Tensor & self, at::Dimname dim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_dimname_out_logcumsumexp_out(self, dim, out);
}
at::Tensor logsumexp(const at::Tensor & self, at::DimnameList dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_logsumexp(self, dim, keepdim);
}
at::Tensor & logsumexp_out(at::Tensor & out, const at::Tensor & self, at::DimnameList dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_out_logsumexp_out(self, dim, keepdim, out);
}
at::Tensor & logsumexp_outf(const at::Tensor & self, at::DimnameList dim, bool keepdim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_names_out_logsumexp_out(self, dim, keepdim, out);
}
at::Tensor margin_ranking_loss(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
return wrapper_CompositeImplicitAutograd__margin_ranking_loss(input1, input2, target, margin, reduction);
}
at::Tensor matmul(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__matmul(self, other);
}
at::Tensor & matmul_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_out_matmul_out(self, other, out);
}
at::Tensor & matmul_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_matmul_out(self, other, out);
}
at::Tensor matrix_power(const at::Tensor & self, int64_t n) {
return wrapper_CompositeImplicitAutograd__matrix_power(self, n);
}
at::Tensor & matrix_power_out(at::Tensor & out, const at::Tensor & self, int64_t n) {
return wrapper_CompositeImplicitAutograd_out_matrix_power_out(self, n, out);
}
at::Tensor & matrix_power_outf(const at::Tensor & self, int64_t n, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_matrix_power_out(self, n, out);
}
at::Tensor matrix_exp(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__matrix_exp(self);
}
at::Tensor matrix_exp_backward(const at::Tensor & self, const at::Tensor & grad) {
return wrapper_CompositeImplicitAutograd__matrix_exp_backward(self, grad);
}
::std::tuple<at::Tensor,at::Tensor> max(const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_dim_max(self, dim, keepdim);
}
::std::tuple<at::Tensor &,at::Tensor &> max_out(at::Tensor & max, at::Tensor & max_values, const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_dim_max_max_out(self, dim, keepdim, max, max_values);
}
::std::tuple<at::Tensor &,at::Tensor &> max_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
return wrapper_CompositeImplicitAutograd_names_dim_max_max_out(self, dim, keepdim, max, max_values);
}
at::Tensor value_selecting_reduction_backward(const at::Tensor & grad, int64_t dim, const at::Tensor & indices, at::IntArrayRef sizes, bool keepdim) {
return wrapper_CompositeImplicitAutograd__value_selecting_reduction_backward(grad, dim, indices, c10::fromIntArrayRefSlow(sizes), keepdim);
}
at::Tensor value_selecting_reduction_backward_symint(const at::Tensor & grad, int64_t dim, const at::Tensor & indices, c10::SymIntArrayRef sizes, bool keepdim) {
return wrapper_CompositeImplicitAutograd__value_selecting_reduction_backward(grad, dim, indices, sizes, keepdim);
}
::std::tuple<at::Tensor,at::Tensor> max_pool1d_with_indices(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper_CompositeImplicitAutograd__max_pool1d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
}
at::Tensor max_pool1d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper_CompositeImplicitAutograd__max_pool1d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
at::Tensor max_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper_CompositeImplicitAutograd__max_pool2d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
at::Tensor max_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper_CompositeImplicitAutograd__max_pool3d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
at::Tensor mean(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_names_dim_mean(self, dim, keepdim, dtype);
}
at::Tensor & mean_out(at::Tensor & out, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_names_out_mean_out(self, dim, keepdim, dtype, out);
}
at::Tensor & mean_outf(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_names_out_mean_out(self, dim, keepdim, dtype, out);
}
at::Tensor nanmean(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd__nanmean(self, dim, keepdim, dtype);
}
at::Tensor & nanmean_out(at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_out_nanmean_out(self, dim, keepdim, dtype, out);
}
at::Tensor & nanmean_outf(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_nanmean_out(self, dim, keepdim, dtype, out);
}
::std::tuple<at::Tensor,at::Tensor> median(const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_dim_median(self, dim, keepdim);
}
::std::tuple<at::Tensor &,at::Tensor &> median_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_dim_values_median_out(self, dim, keepdim, values, indices);
}
::std::tuple<at::Tensor &,at::Tensor &> median_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
return wrapper_CompositeImplicitAutograd_names_dim_values_median_out(self, dim, keepdim, values, indices);
}
::std::tuple<at::Tensor,at::Tensor> nanmedian(const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_dim_nanmedian(self, dim, keepdim);
}
::std::tuple<at::Tensor &,at::Tensor &> nanmedian_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_dim_values_nanmedian_out(self, dim, keepdim, values, indices);
}
::std::tuple<at::Tensor &,at::Tensor &> nanmedian_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
return wrapper_CompositeImplicitAutograd_names_dim_values_nanmedian_out(self, dim, keepdim, values, indices);
}
::std::tuple<at::Tensor,at::Tensor> min(const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_dim_min(self, dim, keepdim);
}
::std::tuple<at::Tensor &,at::Tensor &> min_out(at::Tensor & min, at::Tensor & min_indices, const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_dim_min_min_out(self, dim, keepdim, min, min_indices);
}
::std::tuple<at::Tensor &,at::Tensor &> min_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
return wrapper_CompositeImplicitAutograd_names_dim_min_min_out(self, dim, keepdim, min, min_indices);
}
at::Tensor _sparse_mm(const at::Tensor & sparse, const at::Tensor & dense) {
return wrapper_CompositeImplicitAutograd___sparse_mm(sparse, dense);
}
at::Tensor _sparse_mm(const at::Tensor & sparse, const at::Tensor & dense, c10::string_view reduce) {
return wrapper_CompositeImplicitAutograd_reduce__sparse_mm(sparse, dense, reduce);
}
::std::tuple<at::Tensor,at::Tensor> mode(const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dimname_mode(self, dim, keepdim);
}
::std::tuple<at::Tensor &,at::Tensor &> mode_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dimname_out_mode_out(self, dim, keepdim, values, indices);
}
::std::tuple<at::Tensor &,at::Tensor &> mode_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
return wrapper_CompositeImplicitAutograd_dimname_out_mode_out(self, dim, keepdim, values, indices);
}
at::Tensor multiply(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_multiply(self, other);
}
at::Tensor & multiply_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_out_multiply_out(self, other, out);
}
at::Tensor & multiply_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_multiply_out(self, other, out);
}
at::Tensor & multiply_(at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_multiply_(self, other);
}
at::Tensor multiply(const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_multiply(self, other);
}
at::Tensor & multiply_(at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_multiply_(self, other);
}
at::Tensor narrow(const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
return wrapper_CompositeImplicitAutograd__narrow(self, dim, start, length);
}
at::Tensor narrow_symint(const at::Tensor & self, int64_t dim, c10::SymInt start, c10::SymInt length) {
return wrapper_CompositeImplicitAutograd__narrow(self, dim, start, length);
}
at::Tensor narrow(const at::Tensor & self, int64_t dim, const at::Tensor & start, int64_t length) {
return wrapper_CompositeImplicitAutograd_Tensor_narrow(self, dim, start, length);
}
at::Tensor narrow_symint(const at::Tensor & self, int64_t dim, const at::Tensor & start, c10::SymInt length) {
return wrapper_CompositeImplicitAutograd_Tensor_narrow(self, dim, start, length);
}
bool is_vulkan_available() {
return wrapper_CompositeImplicitAutograd__is_vulkan_available();
}
bool _nnpack_available() {
return wrapper_CompositeImplicitAutograd___nnpack_available();
}
at::Tensor pairwise_distance(const at::Tensor & x1, const at::Tensor & x2, double p, double eps, bool keepdim) {
return wrapper_CompositeImplicitAutograd__pairwise_distance(x1, x2, p, eps, keepdim);
}
at::Tensor cdist(const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode) {
return wrapper_CompositeImplicitAutograd__cdist(x1, x2, p, compute_mode);
}
at::Tensor pdist(const at::Tensor & self, double p) {
return wrapper_CompositeImplicitAutograd__pdist(self, p);
}
at::Tensor cosine_similarity(const at::Tensor & x1, const at::Tensor & x2, int64_t dim, double eps) {
return wrapper_CompositeImplicitAutograd__cosine_similarity(x1, x2, dim, eps);
}
at::Tensor movedim(const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
return wrapper_CompositeImplicitAutograd_intlist_movedim(self, source, destination);
}
at::Tensor movedim(const at::Tensor & self, int64_t source, int64_t destination) {
return wrapper_CompositeImplicitAutograd_int_movedim(self, source, destination);
}
at::Tensor moveaxis(const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
return wrapper_CompositeImplicitAutograd_intlist_moveaxis(self, source, destination);
}
at::Tensor moveaxis(const at::Tensor & self, int64_t source, int64_t destination) {
return wrapper_CompositeImplicitAutograd_int_moveaxis(self, source, destination);
}
at::Tensor numpy_T(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__numpy_T(self);
}
at::Tensor matrix_H(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__matrix_H(self);
}
at::Tensor mT(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__mT(self);
}
at::Tensor mH(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__mH(self);
}
at::Tensor adjoint(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__adjoint(self);
}
at::Tensor native_channel_shuffle(const at::Tensor & self, int64_t groups) {
return wrapper_CompositeImplicitAutograd__native_channel_shuffle(self, groups);
}
at::Tensor native_channel_shuffle_symint(const at::Tensor & self, c10::SymInt groups) {
return wrapper_CompositeImplicitAutograd__native_channel_shuffle(self, groups);
}
at::Tensor pin_memory(const at::Tensor & self, c10::optional<at::Device> device) {
return wrapper_CompositeImplicitAutograd__pin_memory(self, device);
}
at::Tensor pinverse(const at::Tensor & self, double rcond) {
return wrapper_CompositeImplicitAutograd__pinverse(self, rcond);
}
at::Tensor poisson_nll_loss(const at::Tensor & input, const at::Tensor & target, bool log_input, bool full, double eps, int64_t reduction) {
return wrapper_CompositeImplicitAutograd__poisson_nll_loss(input, target, log_input, full, eps, reduction);
}
at::Tensor & rand_out(at::Tensor & out, at::IntArrayRef size, c10::optional<at::Generator> generator) {
return wrapper_CompositeImplicitAutograd_generator_out_rand_out(c10::fromIntArrayRefSlow(size), generator, out);
}
at::Tensor & rand_outf(at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_generator_out_rand_out(c10::fromIntArrayRefSlow(size), generator, out);
}
at::Tensor & rand_symint_out(at::Tensor & out, c10::SymIntArrayRef size, c10::optional<at::Generator> generator) {
return wrapper_CompositeImplicitAutograd_generator_out_rand_out(size, generator, out);
}
at::Tensor & rand_symint_outf(c10::SymIntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_generator_out_rand_out(size, generator, out);
}
at::Tensor & randn_out(at::Tensor & out, at::IntArrayRef size) {
return wrapper_CompositeImplicitAutograd_out_randn_out(c10::fromIntArrayRefSlow(size), out);
}
at::Tensor & randn_outf(at::IntArrayRef size, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_randn_out(c10::fromIntArrayRefSlow(size), out);
}
at::Tensor & randn_symint_out(at::Tensor & out, c10::SymIntArrayRef size) {
return wrapper_CompositeImplicitAutograd_out_randn_out(size, out);
}
at::Tensor & randn_symint_outf(c10::SymIntArrayRef size, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_randn_out(size, out);
}
at::Tensor & randn_out(at::Tensor & out, at::IntArrayRef size, c10::optional<at::Generator> generator) {
return wrapper_CompositeImplicitAutograd_generator_out_randn_out(c10::fromIntArrayRefSlow(size), generator, out);
}
at::Tensor & randn_outf(at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_generator_out_randn_out(c10::fromIntArrayRefSlow(size), generator, out);
}
at::Tensor & randn_symint_out(at::Tensor & out, c10::SymIntArrayRef size, c10::optional<at::Generator> generator) {
return wrapper_CompositeImplicitAutograd_generator_out_randn_out(size, generator, out);
}
at::Tensor & randn_symint_outf(c10::SymIntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_generator_out_randn_out(size, generator, out);
}
at::Tensor ravel(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__ravel(self);
}
at::Tensor negative(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__negative(self);
}
at::Tensor & negative_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_negative_out(self, out);
}
at::Tensor & negative_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_negative_out(self, out);
}
at::Tensor & negative_(at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__negative_(self);
}
at::Tensor repeat_interleave(const at::Tensor & self, const at::Tensor & repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size) {
return wrapper_CompositeImplicitAutograd_self_Tensor_repeat_interleave(self, repeats, dim, output_size.has_value() ? c10::make_optional(c10::SymInt(*output_size)) : c10::nullopt);
}
at::Tensor repeat_interleave_symint(const at::Tensor & self, const at::Tensor & repeats, c10::optional<int64_t> dim, c10::optional<c10::SymInt> output_size) {
return wrapper_CompositeImplicitAutograd_self_Tensor_repeat_interleave(self, repeats, dim, output_size);
}
at::Tensor repeat_interleave(const at::Tensor & self, int64_t repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size) {
return wrapper_CompositeImplicitAutograd_self_int_repeat_interleave(self, repeats, dim, output_size.has_value() ? c10::make_optional(c10::SymInt(*output_size)) : c10::nullopt);
}
at::Tensor repeat_interleave_symint(const at::Tensor & self, c10::SymInt repeats, c10::optional<int64_t> dim, c10::optional<c10::SymInt> output_size) {
return wrapper_CompositeImplicitAutograd_self_int_repeat_interleave(self, repeats, dim, output_size);
}
at::Tensor reshape(const at::Tensor & self, at::IntArrayRef shape) {
return wrapper_CompositeImplicitAutograd__reshape(self, c10::fromIntArrayRefSlow(shape));
}
at::Tensor reshape_symint(const at::Tensor & self, c10::SymIntArrayRef shape) {
return wrapper_CompositeImplicitAutograd__reshape(self, shape);
}
at::Tensor reshape_as(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__reshape_as(self, other);
}
at::Tensor rrelu(const at::Tensor & self, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
return wrapper_CompositeImplicitAutograd__rrelu(self, lower, upper, training, generator);
}
at::Tensor & rrelu_(at::Tensor & self, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
return wrapper_CompositeImplicitAutograd__rrelu_(self, lower, upper, training, generator);
}
at::Tensor relu6(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__relu6(self);
}
at::Tensor & relu6_(at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__relu6_(self);
}
at::Tensor prelu(const at::Tensor & self, const at::Tensor & weight) {
return wrapper_CompositeImplicitAutograd__prelu(self, weight);
}
at::Tensor infinitely_differentiable_gelu_backward(const at::Tensor & grad, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__infinitely_differentiable_gelu_backward(grad, self);
}
at::Tensor select(const at::Tensor & self, at::Dimname dim, int64_t index) {
return wrapper_CompositeImplicitAutograd_Dimname_select(self, dim, index);
}
at::Tensor selu(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__selu(self);
}
at::Tensor & selu_(at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__selu_(self);
}
at::Tensor silu_backward(const at::Tensor & grad_output, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__silu_backward(grad_output, self);
}
at::Tensor mish_backward(const at::Tensor & grad_output, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__mish_backward(grad_output, self);
}
int64_t size(const at::Tensor & self, int64_t dim) {
return wrapper_CompositeImplicitAutograd_int_size(self, dim);
}
int64_t size(const at::Tensor & self, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_Dimname_size(self, dim);
}
c10::SymInt sym_size(const at::Tensor & self, int64_t dim) {
return wrapper_CompositeImplicitAutograd_int_sym_size(self, dim);
}
c10::SymInt sym_numel(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__sym_numel(self);
}
c10::SymInt sym_storage_offset(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__sym_storage_offset(self);
}
at::Tensor smm(const at::Tensor & self, const at::Tensor & mat2) {
return wrapper_CompositeImplicitAutograd__smm(self, mat2);
}
at::Tensor softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_int_softmax(self, dim, dtype);
}
at::Tensor softmax(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_Dimname_softmax(self, dim, dtype);
}
::std::vector<at::Tensor> split(const at::Tensor & self, at::IntArrayRef split_size, int64_t dim) {
return wrapper_CompositeImplicitAutograd_sizes_split(self, c10::fromIntArrayRefSlow(split_size), dim);
}
::std::vector<at::Tensor> split_symint(const at::Tensor & self, c10::SymIntArrayRef split_size, int64_t dim) {
return wrapper_CompositeImplicitAutograd_sizes_split(self, split_size, dim);
}
::std::vector<at::Tensor> hsplit(const at::Tensor & self, int64_t sections) {
return wrapper_CompositeImplicitAutograd_int_hsplit(self, sections);
}
::std::vector<at::Tensor> hsplit(const at::Tensor & self, at::IntArrayRef indices) {
return wrapper_CompositeImplicitAutograd_array_hsplit(self, indices);
}
::std::vector<at::Tensor> vsplit(const at::Tensor & self, int64_t sections) {
return wrapper_CompositeImplicitAutograd_int_vsplit(self, sections);
}
::std::vector<at::Tensor> vsplit(const at::Tensor & self, at::IntArrayRef indices) {
return wrapper_CompositeImplicitAutograd_array_vsplit(self, indices);
}
::std::vector<at::Tensor> dsplit(const at::Tensor & self, int64_t sections) {
return wrapper_CompositeImplicitAutograd_int_dsplit(self, sections);
}
::std::vector<at::Tensor> dsplit(const at::Tensor & self, at::IntArrayRef indices) {
return wrapper_CompositeImplicitAutograd_array_dsplit(self, indices);
}
at::Tensor squeeze(const at::Tensor & self, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_dimname_squeeze(self, dim);
}
at::Tensor & squeeze_(at::Tensor & self, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_dimname_squeeze_(self, dim);
}
at::Tensor sspaddmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper_CompositeImplicitAutograd__sspaddmm(self, mat1, mat2, beta, alpha);
}
at::Tensor hstack(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd__hstack(tensors);
}
at::Tensor & hstack_out(at::Tensor & out, at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd_out_hstack_out(tensors, out);
}
at::Tensor & hstack_outf(at::TensorList tensors, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_hstack_out(tensors, out);
}
at::Tensor vstack(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd__vstack(tensors);
}
at::Tensor & vstack_out(at::Tensor & out, at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd_out_vstack_out(tensors, out);
}
at::Tensor & vstack_outf(at::TensorList tensors, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_vstack_out(tensors, out);
}
at::Tensor dstack(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd__dstack(tensors);
}
at::Tensor & dstack_out(at::Tensor & out, at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd_out_dstack_out(tensors, out);
}
at::Tensor & dstack_outf(at::TensorList tensors, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_dstack_out(tensors, out);
}
at::Tensor stft(const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex) {
return wrapper_CompositeImplicitAutograd__stft(self, n_fft, hop_length, win_length, window, normalized, onesided, return_complex);
}
at::Tensor stft(const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool center, c10::string_view pad_mode, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex) {
return wrapper_CompositeImplicitAutograd_center_stft(self, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex);
}
at::Tensor istft(const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool center, bool normalized, c10::optional<bool> onesided, c10::optional<int64_t> length, bool return_complex) {
return wrapper_CompositeImplicitAutograd__istft(self, n_fft, hop_length, win_length, window, center, normalized, onesided, length, return_complex);
}
int64_t stride(const at::Tensor & self, int64_t dim) {
return wrapper_CompositeImplicitAutograd_int_stride(self, dim);
}
int64_t stride(const at::Tensor & self, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_Dimname_stride(self, dim);
}
c10::SymInt sym_stride(const at::Tensor & self, int64_t dim) {
return wrapper_CompositeImplicitAutograd_int_sym_stride(self, dim);
}
at::Tensor sum(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_dim_DimnameList_sum(self, dim, keepdim, dtype);
}
at::Tensor & sum_out(at::Tensor & out, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_DimnameList_out_sum_out(self, dim, keepdim, dtype, out);
}
at::Tensor & sum_outf(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_DimnameList_out_sum_out(self, dim, keepdim, dtype, out);
}
at::Tensor sum_to_size(const at::Tensor & self, at::IntArrayRef size) {
return wrapper_CompositeImplicitAutograd__sum_to_size(self, c10::fromIntArrayRefSlow(size));
}
at::Tensor sum_to_size_symint(const at::Tensor & self, c10::SymIntArrayRef size) {
return wrapper_CompositeImplicitAutograd__sum_to_size(self, size);
}
at::Tensor square(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__square(self);
}
at::Tensor & square_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_square_out(self, out);
}
at::Tensor & square_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_square_out(self, out);
}
at::Tensor & square_(at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__square_(self);
}
at::Tensor std(const at::Tensor & self, bool unbiased) {
return wrapper_CompositeImplicitAutograd__std(self, unbiased);
}
at::Tensor std(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dim_std(self, dim, unbiased, keepdim);
}
at::Tensor & std_out(at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
return wrapper_CompositeImplicitAutograd_out_std_out(self, dim, unbiased, keepdim, out);
}
at::Tensor & std_outf(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_std_out(self, dim, unbiased, keepdim, out);
}
::std::tuple<at::Tensor,at::Tensor> std_mean(const at::Tensor & self, bool unbiased) {
return wrapper_CompositeImplicitAutograd__std_mean(self, unbiased);
}
::std::tuple<at::Tensor,at::Tensor> std_mean(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dim_std_mean(self, dim, unbiased, keepdim);
}
::std::tuple<at::Tensor,at::Tensor> std_mean(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_dim_std_mean(self, dim, unbiased, keepdim);
}
::std::tuple<at::Tensor,at::Tensor> std_mean(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
return wrapper_CompositeImplicitAutograd_correction_names_std_mean(self, dim, correction, keepdim);
}
at::Tensor std(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_dim_std(self, dim, unbiased, keepdim);
}
at::Tensor & std_out(at::Tensor & out, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_out_std_out(self, dim, unbiased, keepdim, out);
}
at::Tensor & std_outf(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_names_out_std_out(self, dim, unbiased, keepdim, out);
}
at::Tensor std(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
return wrapper_CompositeImplicitAutograd_correction_names_std(self, dim, correction, keepdim);
}
at::Tensor & std_out(at::Tensor & out, const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
return wrapper_CompositeImplicitAutograd_correction_names_out_std_out(self, dim, correction, keepdim, out);
}
at::Tensor & std_outf(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_correction_names_out_std_out(self, dim, correction, keepdim, out);
}
at::Tensor prod(const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_dim_Dimname_prod(self, dim, keepdim, dtype);
}
at::Tensor & prod_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_Dimname_out_prod_out(self, dim, keepdim, dtype, out);
}
at::Tensor & prod_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Dimname_out_prod_out(self, dim, keepdim, dtype, out);
}
at::Tensor tensordot(const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other) {
return wrapper_CompositeImplicitAutograd__tensordot(self, other, dims_self, dims_other);
}
at::Tensor & tensordot_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other) {
return wrapper_CompositeImplicitAutograd_out_tensordot_out(self, other, dims_self, dims_other, out);
}
at::Tensor & tensordot_outf(const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_tensordot_out(self, other, dims_self, dims_other, out);
}
at::Tensor tile(const at::Tensor & self, at::IntArrayRef dims) {
return wrapper_CompositeImplicitAutograd__tile(self, c10::fromIntArrayRefSlow(dims));
}
at::Tensor tile_symint(const at::Tensor & self, c10::SymIntArrayRef dims) {
return wrapper_CompositeImplicitAutograd__tile(self, dims);
}
at::Tensor transpose(const at::Tensor & self, at::Dimname dim0, at::Dimname dim1) {
return wrapper_CompositeImplicitAutograd_Dimname_transpose(self, dim0, dim1);
}
at::Tensor one_hot(const at::Tensor & self, int64_t num_classes) {
return wrapper_CompositeImplicitAutograd__one_hot(self, num_classes);
}
at::Tensor fliplr(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__fliplr(self);
}
at::Tensor flipud(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__flipud(self);
}
at::Tensor trapezoid(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
return wrapper_CompositeImplicitAutograd_x_trapezoid(y, x, dim);
}
at::Tensor trapezoid(const at::Tensor & y, const at::Scalar & dx, int64_t dim) {
return wrapper_CompositeImplicitAutograd_dx_trapezoid(y, dx, dim);
}
at::Tensor trapz(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
return wrapper_CompositeImplicitAutograd_x_trapz(y, x, dim);
}
at::Tensor trapz(const at::Tensor & y, double dx, int64_t dim) {
return wrapper_CompositeImplicitAutograd_dx_trapz(y, dx, dim);
}
at::Tensor triplet_margin_loss(const at::Tensor & anchor, const at::Tensor & positive, const at::Tensor & negative, double margin, double p, double eps, bool swap, int64_t reduction) {
return wrapper_CompositeImplicitAutograd__triplet_margin_loss(anchor, positive, negative, margin, p, eps, swap, reduction);
}
at::Tensor fix(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__fix(self);
}
at::Tensor & fix_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_fix_out(self, out);
}
at::Tensor & fix_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fix_out(self, out);
}
at::Tensor & fix_(at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__fix_(self);
}
at::Tensor type_as(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__type_as(self, other);
}
bool _has_compatible_shallow_copy_type(const at::Tensor & self, const at::Tensor & from) {
return wrapper_CompositeImplicitAutograd___has_compatible_shallow_copy_type(self, from);
}
at::Tensor vander(const at::Tensor & x, c10::optional<int64_t> N, bool increasing) {
return wrapper_CompositeImplicitAutograd__vander(x, N, increasing);
}
at::Tensor var(const at::Tensor & self, bool unbiased) {
return wrapper_CompositeImplicitAutograd__var(self, unbiased);
}
at::Tensor var(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dim_var(self, dim, unbiased, keepdim);
}
at::Tensor & var_out(at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
return wrapper_CompositeImplicitAutograd_out_var_out(self, dim, unbiased, keepdim, out);
}
at::Tensor & var_outf(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_var_out(self, dim, unbiased, keepdim, out);
}
at::Tensor var(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_dim_var(self, dim, unbiased, keepdim);
}
at::Tensor & var_out(at::Tensor & out, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_out_var_out(self, dim, unbiased, keepdim, out);
}
at::Tensor & var_outf(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_names_out_var_out(self, dim, unbiased, keepdim, out);
}
at::Tensor var(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
return wrapper_CompositeImplicitAutograd_correction_names_var(self, dim, correction, keepdim);
}
at::Tensor & var_out(at::Tensor & out, const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
return wrapper_CompositeImplicitAutograd_correction_names_out_var_out(self, dim, correction, keepdim, out);
}
at::Tensor & var_outf(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_correction_names_out_var_out(self, dim, correction, keepdim, out);
}
::std::tuple<at::Tensor,at::Tensor> var_mean(const at::Tensor & self, bool unbiased) {
return wrapper_CompositeImplicitAutograd__var_mean(self, unbiased);
}
::std::tuple<at::Tensor,at::Tensor> var_mean(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dim_var_mean(self, dim, unbiased, keepdim);
}
::std::tuple<at::Tensor,at::Tensor> var_mean(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_dim_var_mean(self, dim, unbiased, keepdim);
}
::std::tuple<at::Tensor,at::Tensor> var_mean(const at::Tensor & self, at::DimnameList dim, const c10::optional<at::Scalar> & correction, bool keepdim) {
return wrapper_CompositeImplicitAutograd_correction_names_var_mean(self, dim, correction, keepdim);
}
at::Tensor view_as(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__view_as(self, other);
}
at::Tensor where(const at::Tensor & condition, const at::Scalar & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_ScalarSelf_where(condition, self, other);
}
at::Tensor where(const at::Tensor & condition, const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_ScalarOther_where(condition, self, other);
}
at::Tensor where(const at::Tensor & condition, const at::Scalar & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_where(condition, self, other);
}
::std::vector<at::Tensor> where(const at::Tensor & condition) {
return wrapper_CompositeImplicitAutograd__where(condition);
}
at::Tensor norm_except_dim(const at::Tensor & v, int64_t pow, int64_t dim) {
return wrapper_CompositeImplicitAutograd__norm_except_dim(v, pow, dim);
}
at::Tensor _weight_norm(const at::Tensor & v, const at::Tensor & g, int64_t dim) {
return wrapper_CompositeImplicitAutograd___weight_norm(v, g, dim);
}
::std::tuple<at::Tensor,at::Tensor> _weight_norm_differentiable_backward(const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim) {
return wrapper_CompositeImplicitAutograd___weight_norm_differentiable_backward(grad_w, saved_v, saved_g, saved_norms, dim);
}
at::Tensor _sparse_sum(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd___sparse_sum(self);
}
at::Tensor _sparse_sum(const at::Tensor & self, at::ScalarType dtype) {
return wrapper_CompositeImplicitAutograd_dtype__sparse_sum(self, dtype);
}
at::Tensor _sparse_sum(const at::Tensor & self, at::IntArrayRef dim, at::ScalarType dtype) {
return wrapper_CompositeImplicitAutograd_dim_dtype__sparse_sum(self, dim, dtype);
}
at::Tensor _sparse_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_int__sparse_softmax(self, dim, dtype);
}
at::Tensor _sparse_softmax(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_Dimname__sparse_softmax(self, dim, dtype);
}
at::Tensor _sparse_log_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_int__sparse_log_softmax(self, dim, dtype);
}
at::Tensor _sparse_log_softmax(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_Dimname__sparse_log_softmax(self, dim, dtype);
}
at::Tensor norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype) {
return wrapper_CompositeImplicitAutograd_names_ScalarOpt_dim_dtype_norm(self, p, dim, keepdim, dtype);
}
at::Tensor & norm_out(at::Tensor & out, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype) {
return wrapper_CompositeImplicitAutograd_names_dtype_out_norm_out(self, p, dim, keepdim, dtype, out);
}
at::Tensor & norm_outf(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_names_dtype_out_norm_out(self, p, dim, keepdim, dtype, out);
}
at::Tensor norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_ScalarOpt_dim_norm(self, p, dim, keepdim);
}
at::Tensor & norm_out(at::Tensor & out, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_names_out_norm_out(self, p, dim, keepdim, out);
}
at::Tensor & norm_outf(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_names_out_norm_out(self, p, dim, keepdim, out);
}
at::Tensor frobenius_norm(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dim_frobenius_norm(self, dim, keepdim);
}
at::Tensor & frobenius_norm_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_out_frobenius_norm_out(self, dim, keepdim, out);
}
at::Tensor & frobenius_norm_outf(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_frobenius_norm_out(self, dim, keepdim, out);
}
at::Tensor nuclear_norm(const at::Tensor & self, bool keepdim) {
return wrapper_CompositeImplicitAutograd__nuclear_norm(self, keepdim);
}
at::Tensor & nuclear_norm_out(at::Tensor & out, const at::Tensor & self, bool keepdim) {
return wrapper_CompositeImplicitAutograd_out_nuclear_norm_out(self, keepdim, out);
}
at::Tensor & nuclear_norm_outf(const at::Tensor & self, bool keepdim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_nuclear_norm_out(self, keepdim, out);
}
at::Tensor nuclear_norm(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dim_nuclear_norm(self, dim, keepdim);
}
at::Tensor & nuclear_norm_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_dim_out_nuclear_norm_out(self, dim, keepdim, out);
}
at::Tensor & nuclear_norm_outf(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_dim_out_nuclear_norm_out(self, dim, keepdim, out);
}
at::Tensor positive(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__positive(self);
}
at::Tensor subtract(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_CompositeImplicitAutograd_Tensor_subtract(self, other, alpha);
}
at::Tensor & subtract_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_CompositeImplicitAutograd_out_subtract_out(self, other, alpha, out);
}
at::Tensor & subtract_outf(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_subtract_out(self, other, alpha, out);
}
at::Tensor & subtract_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_CompositeImplicitAutograd_Tensor_subtract_(self, other, alpha);
}
at::Tensor subtract(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
return wrapper_CompositeImplicitAutograd_Scalar_subtract(self, other, alpha);
}
at::Tensor & subtract_(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
return wrapper_CompositeImplicitAutograd_Scalar_subtract_(self, other, alpha);
}
at::Tensor sparse_csr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options) {
return wrapper_CompositeImplicitAutograd_crow_col_value_size_sparse_csr_tensor(crow_indices, col_indices, values, size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor sparse_csr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_CompositeImplicitAutograd_crow_col_value_size_sparse_csr_tensor(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}
at::Tensor sparse_csc_tensor(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options) {
return wrapper_CompositeImplicitAutograd_ccol_row_value_size_sparse_csc_tensor(ccol_indices, row_indices, values, size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor sparse_csc_tensor(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_CompositeImplicitAutograd_ccol_row_value_size_sparse_csc_tensor(ccol_indices, row_indices, values, size, dtype, layout, device, pin_memory);
}
at::Tensor sparse_bsr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options) {
return wrapper_CompositeImplicitAutograd_crow_col_value_size_sparse_bsr_tensor(crow_indices, col_indices, values, size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor sparse_bsr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_CompositeImplicitAutograd_crow_col_value_size_sparse_bsr_tensor(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}
at::Tensor sparse_bsc_tensor(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options) {
return wrapper_CompositeImplicitAutograd_ccol_row_value_size_sparse_bsc_tensor(ccol_indices, row_indices, values, size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor sparse_bsc_tensor(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_CompositeImplicitAutograd_ccol_row_value_size_sparse_bsc_tensor(ccol_indices, row_indices, values, size, dtype, layout, device, pin_memory);
}
at::Tensor sparse_csr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::TensorOptions options) {
return wrapper_CompositeImplicitAutograd_crow_col_value_sparse_csr_tensor(crow_indices, col_indices, values, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor sparse_csr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_CompositeImplicitAutograd_crow_col_value_sparse_csr_tensor(crow_indices, col_indices, values, dtype, layout, device, pin_memory);
}
at::Tensor sparse_csc_tensor(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::TensorOptions options) {
return wrapper_CompositeImplicitAutograd_ccol_row_value_sparse_csc_tensor(ccol_indices, row_indices, values, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor sparse_csc_tensor(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_CompositeImplicitAutograd_ccol_row_value_sparse_csc_tensor(ccol_indices, row_indices, values, dtype, layout, device, pin_memory);
}
at::Tensor sparse_bsr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::TensorOptions options) {
return wrapper_CompositeImplicitAutograd_crow_col_value_sparse_bsr_tensor(crow_indices, col_indices, values, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor sparse_bsr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_CompositeImplicitAutograd_crow_col_value_sparse_bsr_tensor(crow_indices, col_indices, values, dtype, layout, device, pin_memory);
}
at::Tensor sparse_bsc_tensor(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::TensorOptions options) {
return wrapper_CompositeImplicitAutograd_ccol_row_value_sparse_bsc_tensor(ccol_indices, row_indices, values, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor sparse_bsc_tensor(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_CompositeImplicitAutograd_ccol_row_value_sparse_bsc_tensor(ccol_indices, row_indices, values, dtype, layout, device, pin_memory);
}
at::Tensor _sparse_compressed_tensor_unsafe(const at::Tensor & compressed_indices, const at::Tensor & plain_indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options) {
return wrapper_CompositeImplicitAutograd___sparse_compressed_tensor_unsafe(compressed_indices, plain_indices, values, c10::fromIntArrayRefSlow(size), c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor _sparse_compressed_tensor_unsafe(const at::Tensor & compressed_indices, const at::Tensor & plain_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_CompositeImplicitAutograd___sparse_compressed_tensor_unsafe(compressed_indices, plain_indices, values, c10::fromIntArrayRefSlow(size), dtype, layout, device, pin_memory);
}
at::Tensor _sparse_compressed_tensor_unsafe_symint(const at::Tensor & compressed_indices, const at::Tensor & plain_indices, const at::Tensor & values, c10::SymIntArrayRef size, at::TensorOptions options) {
return wrapper_CompositeImplicitAutograd___sparse_compressed_tensor_unsafe(compressed_indices, plain_indices, values, size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor _sparse_compressed_tensor_unsafe_symint(const at::Tensor & compressed_indices, const at::Tensor & plain_indices, const at::Tensor & values, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_CompositeImplicitAutograd___sparse_compressed_tensor_unsafe(compressed_indices, plain_indices, values, size, dtype, layout, device, pin_memory);
}
at::Tensor _sparse_csr_tensor_unsafe(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options) {
return wrapper_CompositeImplicitAutograd___sparse_csr_tensor_unsafe(crow_indices, col_indices, values, size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor _sparse_csr_tensor_unsafe(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_CompositeImplicitAutograd___sparse_csr_tensor_unsafe(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}
at::Tensor _sparse_csc_tensor_unsafe(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options) {
return wrapper_CompositeImplicitAutograd___sparse_csc_tensor_unsafe(ccol_indices, row_indices, values, size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor _sparse_csc_tensor_unsafe(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_CompositeImplicitAutograd___sparse_csc_tensor_unsafe(ccol_indices, row_indices, values, size, dtype, layout, device, pin_memory);
}
at::Tensor _sparse_bsr_tensor_unsafe(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options) {
return wrapper_CompositeImplicitAutograd___sparse_bsr_tensor_unsafe(crow_indices, col_indices, values, size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor _sparse_bsr_tensor_unsafe(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_CompositeImplicitAutograd___sparse_bsr_tensor_unsafe(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}
at::Tensor _sparse_bsc_tensor_unsafe(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options) {
return wrapper_CompositeImplicitAutograd___sparse_bsc_tensor_unsafe(ccol_indices, row_indices, values, size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
at::Tensor _sparse_bsc_tensor_unsafe(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_CompositeImplicitAutograd___sparse_bsc_tensor_unsafe(ccol_indices, row_indices, values, size, dtype, layout, device, pin_memory);
}
at::Tensor sparse_coo_tensor(const at::Tensor & indices, const at::Tensor & values, at::TensorOptions options, c10::optional<bool> is_coalesced) {
return wrapper_CompositeImplicitAutograd_indices_sparse_coo_tensor(indices, values, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), is_coalesced);
}
at::Tensor sparse_coo_tensor(const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<bool> is_coalesced) {
return wrapper_CompositeImplicitAutograd_indices_sparse_coo_tensor(indices, values, dtype, layout, device, pin_memory, is_coalesced);
}
at::Tensor sparse_coo_tensor(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options, c10::optional<bool> is_coalesced) {
return wrapper_CompositeImplicitAutograd_indices_size_sparse_coo_tensor(indices, values, size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), is_coalesced);
}
at::Tensor sparse_coo_tensor(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<bool> is_coalesced) {
return wrapper_CompositeImplicitAutograd_indices_size_sparse_coo_tensor(indices, values, size, dtype, layout, device, pin_memory, is_coalesced);
}
at::Tensor _sparse_coo_tensor_unsafe(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options, c10::optional<bool> is_coalesced) {
return wrapper_CompositeImplicitAutograd___sparse_coo_tensor_unsafe(indices, values, c10::fromIntArrayRefSlow(size), c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), is_coalesced);
}
at::Tensor _sparse_coo_tensor_unsafe(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<bool> is_coalesced) {
return wrapper_CompositeImplicitAutograd___sparse_coo_tensor_unsafe(indices, values, c10::fromIntArrayRefSlow(size), dtype, layout, device, pin_memory, is_coalesced);
}
at::Tensor _sparse_coo_tensor_unsafe_symint(const at::Tensor & indices, const at::Tensor & values, c10::SymIntArrayRef size, at::TensorOptions options, c10::optional<bool> is_coalesced) {
return wrapper_CompositeImplicitAutograd___sparse_coo_tensor_unsafe(indices, values, size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), is_coalesced);
}
at::Tensor _sparse_coo_tensor_unsafe_symint(const at::Tensor & indices, const at::Tensor & values, c10::SymIntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<bool> is_coalesced) {
return wrapper_CompositeImplicitAutograd___sparse_coo_tensor_unsafe(indices, values, size, dtype, layout, device, pin_memory, is_coalesced);
}
void _validate_sparse_coo_tensor_args(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<bool> is_coalesced) {
return wrapper_CompositeImplicitAutograd___validate_sparse_coo_tensor_args(indices, values, size, is_coalesced);
}
void _validate_sparse_compressed_tensor_args(const at::Tensor & compressed_indices, const at::Tensor & plain_indices, const at::Tensor & values, at::IntArrayRef size, at::Layout layout) {
return wrapper_CompositeImplicitAutograd___validate_sparse_compressed_tensor_args(compressed_indices, plain_indices, values, size, layout);
}
void _validate_sparse_csr_tensor_args(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size) {
return wrapper_CompositeImplicitAutograd___validate_sparse_csr_tensor_args(crow_indices, col_indices, values, size);
}
void _validate_sparse_csc_tensor_args(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size) {
return wrapper_CompositeImplicitAutograd___validate_sparse_csc_tensor_args(ccol_indices, row_indices, values, size);
}
void _validate_sparse_bsr_tensor_args(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size) {
return wrapper_CompositeImplicitAutograd___validate_sparse_bsr_tensor_args(crow_indices, col_indices, values, size);
}
void _validate_sparse_bsc_tensor_args(const at::Tensor & ccol_indices, const at::Tensor & row_indices, const at::Tensor & values, at::IntArrayRef size) {
return wrapper_CompositeImplicitAutograd___validate_sparse_bsc_tensor_args(ccol_indices, row_indices, values, size);
}
::std::vector<at::Tensor> _to_cpu(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd___to_cpu(tensors);
}
at::Tensor to_dense(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<bool> masked_grad) {
return wrapper_CompositeImplicitAutograd__to_dense(self, dtype, masked_grad);
}
at::Tensor to_dense_backward(const at::Tensor & grad, const at::Tensor & input, c10::optional<bool> masked_grad) {
return wrapper_CompositeImplicitAutograd__to_dense_backward(grad, input, masked_grad);
}
at::Tensor coalesce(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__coalesce(self);
}
::std::vector<at::Tensor> unbind(const at::Tensor & self, at::Dimname dim) {
return wrapper_CompositeImplicitAutograd_Dimname_unbind(self, dim);
}
at::Tensor to_sparse(const at::Tensor & self, int64_t sparse_dim) {
return wrapper_CompositeImplicitAutograd_sparse_dim_to_sparse(self, sparse_dim);
}
at::Tensor to_sparse(const at::Tensor & self, c10::optional<at::Layout> layout, at::OptionalIntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
return wrapper_CompositeImplicitAutograd__to_sparse(self, layout, blocksize, dense_dim);
}
at::Tensor to_sparse_csr(const at::Tensor & self, c10::optional<int64_t> dense_dim) {
return wrapper_CompositeImplicitAutograd__to_sparse_csr(self, dense_dim);
}
at::Tensor to_sparse_csc(const at::Tensor & self, c10::optional<int64_t> dense_dim) {
return wrapper_CompositeImplicitAutograd__to_sparse_csc(self, dense_dim);
}
at::Tensor to_sparse_bsr(const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
return wrapper_CompositeImplicitAutograd__to_sparse_bsr(self, blocksize, dense_dim);
}
at::Tensor to_sparse_bsc(const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim) {
return wrapper_CompositeImplicitAutograd__to_sparse_bsc(self, blocksize, dense_dim);
}
at::Tensor to_mkldnn_backward(const at::Tensor & grad, const at::Tensor & input) {
return wrapper_CompositeImplicitAutograd__to_mkldnn_backward(grad, input);
}
at::Tensor fake_quantize_per_tensor_affine(const at::Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
return wrapper_CompositeImplicitAutograd__fake_quantize_per_tensor_affine(self, scale, zero_point, quant_min, quant_max);
}
at::Tensor fake_quantize_per_tensor_affine(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max) {
return wrapper_CompositeImplicitAutograd_tensor_qparams_fake_quantize_per_tensor_affine(self, scale, zero_point, quant_min, quant_max);
}
at::Tensor fake_quantize_per_tensor_affine_cachemask_backward(const at::Tensor & grad, const at::Tensor & mask) {
return wrapper_CompositeImplicitAutograd__fake_quantize_per_tensor_affine_cachemask_backward(grad, mask);
}
at::Tensor fake_quantize_per_channel_affine(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
return wrapper_CompositeImplicitAutograd__fake_quantize_per_channel_affine(self, scale, zero_point, axis, quant_min, quant_max);
}
at::Tensor fake_quantize_per_channel_affine_cachemask_backward(const at::Tensor & grad, const at::Tensor & mask) {
return wrapper_CompositeImplicitAutograd__fake_quantize_per_channel_affine_cachemask_backward(grad, mask);
}
at::Tensor fused_moving_avg_obs_fake_quant(const at::Tensor & self, const at::Tensor & observer_on, const at::Tensor & fake_quant_on, at::Tensor & running_min, at::Tensor & running_max, at::Tensor & scale, at::Tensor & zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, bool per_row_fake_quant, bool symmetric_quant) {
return wrapper_CompositeImplicitAutograd__fused_moving_avg_obs_fake_quant(self, observer_on, fake_quant_on, running_min, running_max, scale, zero_point, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
}
::std::tuple<double,int64_t> _choose_qparams_per_tensor(const at::Tensor & self, bool reduce_range) {
return wrapper_CompositeImplicitAutograd___choose_qparams_per_tensor(self, reduce_range);
}
at::Tensor _saturate_weight_to_fp16(const at::Tensor & weight) {
return wrapper_CompositeImplicitAutograd___saturate_weight_to_fp16(weight);
}
::std::tuple<at::Tensor,at::Tensor> choose_qparams_optimized(const at::Tensor & input, int64_t numel, int64_t n_bins, double ratio, int64_t bit_width) {
return wrapper_CompositeImplicitAutograd__choose_qparams_optimized(input, numel, n_bins, ratio, bit_width);
}
at::Tensor _autocast_to_reduced_precision(const at::Tensor & self, bool cuda_enabled, bool cpu_enabled, at::ScalarType cuda_dtype, at::ScalarType cpu_dtype) {
return wrapper_CompositeImplicitAutograd___autocast_to_reduced_precision(self, cuda_enabled, cpu_enabled, cuda_dtype, cpu_dtype);
}
at::Tensor _autocast_to_full_precision(const at::Tensor & self, bool cuda_enabled, bool cpu_enabled) {
return wrapper_CompositeImplicitAutograd___autocast_to_full_precision(self, cuda_enabled, cpu_enabled);
}
at::Tensor to(const at::Tensor & self, at::TensorOptions options, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_CompositeImplicitAutograd_dtype_layout_to(self, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), non_blocking, copy, c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor to(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_CompositeImplicitAutograd_dtype_layout_to(self, dtype, layout, device, pin_memory, non_blocking, copy, memory_format);
}
at::Tensor to(const at::Tensor & self, at::Device device, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_CompositeImplicitAutograd_device_to(self, device, dtype, non_blocking, copy, memory_format);
}
at::Tensor to(const at::Tensor & self, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_CompositeImplicitAutograd_dtype_to(self, dtype, non_blocking, copy, memory_format);
}
at::Tensor to(const at::Tensor & self, const at::Tensor & other, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_CompositeImplicitAutograd_other_to(self, other, non_blocking, copy, memory_format);
}
::std::vector<at::Tensor> meshgrid(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd__meshgrid(tensors);
}
::std::vector<at::Tensor> meshgrid(at::TensorList tensors, c10::string_view indexing) {
return wrapper_CompositeImplicitAutograd_indexing_meshgrid(tensors, indexing);
}
at::Tensor cartesian_prod(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd__cartesian_prod(tensors);
}
at::Tensor combinations(const at::Tensor & self, int64_t r, bool with_replacement) {
return wrapper_CompositeImplicitAutograd__combinations(self, r, with_replacement);
}
at::Scalar item(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__item(self);
}
at::ScalarType result_type(const at::Tensor & tensor, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_result_type(tensor, other);
}
at::ScalarType result_type(const at::Tensor & tensor, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_result_type(tensor, other);
}
at::ScalarType result_type(const at::Scalar & scalar, const at::Tensor & tensor) {
return wrapper_CompositeImplicitAutograd_Scalar_Tensor_result_type(scalar, tensor);
}
at::ScalarType result_type(const at::Scalar & scalar1, const at::Scalar & scalar2) {
return wrapper_CompositeImplicitAutograd_Scalar_Scalar_result_type(scalar1, scalar2);
}
bool can_cast(at::ScalarType from, at::ScalarType to) {
return wrapper_CompositeImplicitAutograd__can_cast(from, to);
}
at::ScalarType promote_types(at::ScalarType type1, at::ScalarType type2) {
return wrapper_CompositeImplicitAutograd__promote_types(type1, type2);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _thnn_fused_lstm_cell_backward(const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & cx, const at::Tensor & cy, const at::Tensor & workspace, bool has_bias) {
return wrapper_CompositeImplicitAutograd___thnn_fused_lstm_cell_backward(grad_hy, grad_cy, cx, cy, workspace, has_bias);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _thnn_differentiable_lstm_cell_backward(const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias, const at::Tensor & cx, const at::Tensor & cy) {
return wrapper_CompositeImplicitAutograd___thnn_differentiable_lstm_cell_backward(grad_hy, grad_cy, input_gates, hidden_gates, input_bias, hidden_bias, cx, cy);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _thnn_differentiable_gru_cell_backward(const at::Tensor & grad_hy, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
return wrapper_CompositeImplicitAutograd___thnn_differentiable_gru_cell_backward(grad_hy, input_gates, hidden_gates, hx, input_bias, hidden_bias);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> lstm(const at::Tensor & input, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
return wrapper_CompositeImplicitAutograd_input_lstm(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> lstm(const at::Tensor & data, const at::Tensor & batch_sizes, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
return wrapper_CompositeImplicitAutograd_data_lstm(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}
::std::tuple<at::Tensor,at::Tensor> gru(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
return wrapper_CompositeImplicitAutograd_input_gru(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}
::std::tuple<at::Tensor,at::Tensor> gru(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
return wrapper_CompositeImplicitAutograd_data_gru(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}
::std::tuple<at::Tensor,at::Tensor> rnn_tanh(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
return wrapper_CompositeImplicitAutograd_input_rnn_tanh(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}
::std::tuple<at::Tensor,at::Tensor> rnn_tanh(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
return wrapper_CompositeImplicitAutograd_data_rnn_tanh(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}
::std::tuple<at::Tensor,at::Tensor> rnn_relu(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
return wrapper_CompositeImplicitAutograd_input_rnn_relu(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}
::std::tuple<at::Tensor,at::Tensor> rnn_relu(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
return wrapper_CompositeImplicitAutograd_data_rnn_relu(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}
::std::tuple<at::Tensor,at::Tensor> lstm_cell(const at::Tensor & input, at::TensorList hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
return wrapper_CompositeImplicitAutograd__lstm_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}
at::Tensor gru_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
return wrapper_CompositeImplicitAutograd__gru_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}
at::Tensor rnn_tanh_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
return wrapper_CompositeImplicitAutograd__rnn_tanh_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}
at::Tensor rnn_relu_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
return wrapper_CompositeImplicitAutograd__rnn_relu_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}
::std::tuple<at::Tensor,at::Tensor> quantized_lstm_cell(const at::Tensor & input, at::TensorList hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
return wrapper_CompositeImplicitAutograd__quantized_lstm_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}
at::Tensor quantized_gru_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
return wrapper_CompositeImplicitAutograd__quantized_gru_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}
at::Tensor quantized_rnn_relu_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
return wrapper_CompositeImplicitAutograd__quantized_rnn_relu_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}
at::Tensor quantized_rnn_tanh_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
return wrapper_CompositeImplicitAutograd__quantized_rnn_tanh_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}
at::Tensor _pack_padded_sequence_backward(const at::Tensor & grad, at::IntArrayRef input_size, const at::Tensor & batch_sizes, bool batch_first) {
return wrapper_CompositeImplicitAutograd___pack_padded_sequence_backward(grad, c10::fromIntArrayRefSlow(input_size), batch_sizes, batch_first);
}
at::Tensor _pack_padded_sequence_backward_symint(const at::Tensor & grad, c10::SymIntArrayRef input_size, const at::Tensor & batch_sizes, bool batch_first) {
return wrapper_CompositeImplicitAutograd___pack_padded_sequence_backward(grad, input_size, batch_sizes, batch_first);
}
::std::tuple<at::Tensor,at::Tensor> _pad_packed_sequence(const at::Tensor & data, const at::Tensor & batch_sizes, bool batch_first, const at::Scalar & padding_value, int64_t total_length) {
return wrapper_CompositeImplicitAutograd___pad_packed_sequence(data, batch_sizes, batch_first, padding_value, total_length);
}
at::Tensor & set_(at::Tensor & self, const at::Tensor & source, int64_t storage_offset, at::IntArrayRef size, at::IntArrayRef stride) {
return wrapper_CompositeImplicitAutograd_source_Tensor_storage_offset_set_(self, source, storage_offset, c10::fromIntArrayRefSlow(size), c10::fromIntArrayRefSlow(stride));
}
at::Tensor & set__symint(at::Tensor & self, const at::Tensor & source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
return wrapper_CompositeImplicitAutograd_source_Tensor_storage_offset_set_(self, source, storage_offset, size, stride);
}
at::Tensor index_add(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
return wrapper_CompositeImplicitAutograd_dimname_index_add(self, dim, index, source, alpha);
}
at::Tensor & index_fill_(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
return wrapper_CompositeImplicitAutograd_Dimname_Scalar_index_fill_(self, dim, index, value);
}
at::Tensor index_fill(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
return wrapper_CompositeImplicitAutograd_Dimname_Scalar_index_fill(self, dim, index, value);
}
at::Tensor & index_fill_(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
return wrapper_CompositeImplicitAutograd_Dimname_Tensor_index_fill_(self, dim, index, value);
}
at::Tensor index_fill(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
return wrapper_CompositeImplicitAutograd_Dimname_Tensor_index_fill(self, dim, index, value);
}
at::Tensor scatter(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
return wrapper_CompositeImplicitAutograd_dimname_src_scatter(self, dim, index, src);
}
at::Tensor scatter(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
return wrapper_CompositeImplicitAutograd_dimname_value_scatter(self, dim, index, value);
}
at::Tensor scatter_add(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
return wrapper_CompositeImplicitAutograd_dimname_scatter_add(self, dim, index, src);
}
at::Tensor __and__(const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar___and__(self, other);
}
at::Tensor & __iand__(at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar___iand__(self, other);
}
at::Tensor __and__(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor___and__(self, other);
}
at::Tensor & __iand__(at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor___iand__(self, other);
}
at::Tensor __or__(const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar___or__(self, other);
}
at::Tensor & __ior__(at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar___ior__(self, other);
}
at::Tensor __or__(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor___or__(self, other);
}
at::Tensor & __ior__(at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor___ior__(self, other);
}
at::Tensor __xor__(const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar___xor__(self, other);
}
at::Tensor & __ixor__(at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar___ixor__(self, other);
}
at::Tensor __xor__(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor___xor__(self, other);
}
at::Tensor & __ixor__(at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor___ixor__(self, other);
}
at::Tensor diag(const at::Tensor & self, int64_t diagonal) {
return wrapper_CompositeImplicitAutograd__diag(self, diagonal);
}
at::Tensor & diag_out(at::Tensor & out, const at::Tensor & self, int64_t diagonal) {
return wrapper_CompositeImplicitAutograd_out_diag_out(self, diagonal, out);
}
at::Tensor & diag_outf(const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_diag_out(self, diagonal, out);
}
at::Tensor cross(const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim) {
return wrapper_CompositeImplicitAutograd__cross(self, other, dim);
}
at::Tensor & cross_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim) {
return wrapper_CompositeImplicitAutograd_out_cross_out(self, other, dim, out);
}
at::Tensor & cross_outf(const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_cross_out(self, other, dim, out);
}
at::Tensor trace_backward(const at::Tensor & grad, at::IntArrayRef sizes) {
return wrapper_CompositeImplicitAutograd__trace_backward(grad, c10::fromIntArrayRefSlow(sizes));
}
at::Tensor trace_backward_symint(const at::Tensor & grad, c10::SymIntArrayRef sizes) {
return wrapper_CompositeImplicitAutograd__trace_backward(grad, sizes);
}
at::Tensor not_equal(const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_not_equal(self, other);
}
at::Tensor & not_equal_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_out_not_equal_out(self, other, out);
}
at::Tensor & not_equal_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Scalar_out_not_equal_out(self, other, out);
}
at::Tensor & not_equal_(at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_not_equal_(self, other);
}
at::Tensor not_equal(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_not_equal(self, other);
}
at::Tensor & not_equal_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_out_not_equal_out(self, other, out);
}
at::Tensor & not_equal_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Tensor_out_not_equal_out(self, other, out);
}
at::Tensor & not_equal_(at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_not_equal_(self, other);
}
at::Tensor greater_equal(const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_greater_equal(self, other);
}
at::Tensor & greater_equal_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_out_greater_equal_out(self, other, out);
}
at::Tensor & greater_equal_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Scalar_out_greater_equal_out(self, other, out);
}
at::Tensor & greater_equal_(at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_greater_equal_(self, other);
}
at::Tensor greater_equal(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_greater_equal(self, other);
}
at::Tensor & greater_equal_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_out_greater_equal_out(self, other, out);
}
at::Tensor & greater_equal_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Tensor_out_greater_equal_out(self, other, out);
}
at::Tensor & greater_equal_(at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_greater_equal_(self, other);
}
at::Tensor less_equal(const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_less_equal(self, other);
}
at::Tensor & less_equal_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_out_less_equal_out(self, other, out);
}
at::Tensor & less_equal_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Scalar_out_less_equal_out(self, other, out);
}
at::Tensor & less_equal_(at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_less_equal_(self, other);
}
at::Tensor less_equal(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_less_equal(self, other);
}
at::Tensor & less_equal_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_out_less_equal_out(self, other, out);
}
at::Tensor & less_equal_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Tensor_out_less_equal_out(self, other, out);
}
at::Tensor & less_equal_(at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_less_equal_(self, other);
}
at::Tensor greater(const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_greater(self, other);
}
at::Tensor & greater_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_out_greater_out(self, other, out);
}
at::Tensor & greater_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Scalar_out_greater_out(self, other, out);
}
at::Tensor & greater_(at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_greater_(self, other);
}
at::Tensor greater(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_greater(self, other);
}
at::Tensor & greater_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_out_greater_out(self, other, out);
}
at::Tensor & greater_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Tensor_out_greater_out(self, other, out);
}
at::Tensor & greater_(at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_greater_(self, other);
}
at::Tensor less(const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_less(self, other);
}
at::Tensor & less_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_out_less_out(self, other, out);
}
at::Tensor & less_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Scalar_out_less_out(self, other, out);
}
at::Tensor & less_(at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_Scalar_less_(self, other);
}
at::Tensor less(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_less(self, other);
}
at::Tensor & less_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_out_less_out(self, other, out);
}
at::Tensor & less_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Tensor_out_less_out(self, other, out);
}
at::Tensor & less_(at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_Tensor_less_(self, other);
}
at::Tensor take_along_dim(const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim) {
return wrapper_CompositeImplicitAutograd__take_along_dim(self, indices, dim);
}
at::Tensor & take_along_dim_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim) {
return wrapper_CompositeImplicitAutograd_out_take_along_dim_out(self, indices, dim, out);
}
at::Tensor & take_along_dim_outf(const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_take_along_dim_out(self, indices, dim, out);
}
at::Tensor index_select(const at::Tensor & self, at::Dimname dim, const at::Tensor & index) {
return wrapper_CompositeImplicitAutograd_dimname_index_select(self, dim, index);
}
at::Tensor & index_select_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim, const at::Tensor & index) {
return wrapper_CompositeImplicitAutograd_dimname_out_index_select_out(self, dim, index, out);
}
at::Tensor & index_select_outf(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_dimname_out_index_select_out(self, dim, index, out);
}
at::Tensor index_select_backward(const at::Tensor & grad, at::IntArrayRef self_sizes, int64_t dim, const at::Tensor & index) {
return wrapper_CompositeImplicitAutograd__index_select_backward(grad, c10::fromIntArrayRefSlow(self_sizes), dim, index);
}
at::Tensor index_select_backward_symint(const at::Tensor & grad, c10::SymIntArrayRef self_sizes, int64_t dim, const at::Tensor & index) {
return wrapper_CompositeImplicitAutograd__index_select_backward(grad, self_sizes, dim, index);
}
at::Tensor masked_select_backward(const at::Tensor & grad, const at::Tensor & input, const at::Tensor & mask) {
return wrapper_CompositeImplicitAutograd__masked_select_backward(grad, input, mask);
}
::std::vector<at::Tensor> nonzero_numpy(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__nonzero_numpy(self);
}
at::Tensor argwhere(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__argwhere(self);
}
at::Tensor gather_backward(const at::Tensor & grad, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
return wrapper_CompositeImplicitAutograd__gather_backward(grad, self, dim, index, sparse_grad);
}
at::Tensor gather(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad) {
return wrapper_CompositeImplicitAutograd_dimname_gather(self, dim, index, sparse_grad);
}
at::Tensor & gather_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad) {
return wrapper_CompositeImplicitAutograd_dimname_out_gather_out(self, dim, index, sparse_grad, out);
}
at::Tensor & gather_outf(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_dimname_out_gather_out(self, dim, index, sparse_grad, out);
}
at::Tensor _gather_sparse_backward(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & grad) {
return wrapper_CompositeImplicitAutograd___gather_sparse_backward(self, dim, index, grad);
}
at::Tensor cross_entropy_loss(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, double label_smoothing) {
return wrapper_CompositeImplicitAutograd__cross_entropy_loss(self, target, weight, reduction, ignore_index, label_smoothing);
}
at::Tensor cross_entropy_loss_symint(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, double label_smoothing) {
return wrapper_CompositeImplicitAutograd__cross_entropy_loss(self, target, weight, reduction, ignore_index, label_smoothing);
}
at::Tensor linalg_vander(const at::Tensor & x, c10::optional<int64_t> N) {
return wrapper_CompositeImplicitAutograd__linalg_vander(x, N.has_value() ? c10::make_optional(c10::SymInt(*N)) : c10::nullopt);
}
at::Tensor linalg_vander_symint(const at::Tensor & x, c10::optional<c10::SymInt> N) {
return wrapper_CompositeImplicitAutograd__linalg_vander(x, N);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> svd(const at::Tensor & self, bool some, bool compute_uv) {
return wrapper_CompositeImplicitAutograd__svd(self, some, compute_uv);
}
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> svd_out(at::Tensor & U, at::Tensor & S, at::Tensor & V, const at::Tensor & self, bool some, bool compute_uv) {
return wrapper_CompositeImplicitAutograd_U_svd_out(self, some, compute_uv, U, S, V);
}
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> svd_outf(const at::Tensor & self, bool some, bool compute_uv, at::Tensor & U, at::Tensor & S, at::Tensor & V) {
return wrapper_CompositeImplicitAutograd_U_svd_out(self, some, compute_uv, U, S, V);
}
at::Tensor swapaxes(const at::Tensor & self, int64_t axis0, int64_t axis1) {
return wrapper_CompositeImplicitAutograd__swapaxes(self, axis0, axis1);
}
at::Tensor & swapaxes_(at::Tensor & self, int64_t axis0, int64_t axis1) {
return wrapper_CompositeImplicitAutograd__swapaxes_(self, axis0, axis1);
}
at::Tensor swapdims(const at::Tensor & self, int64_t dim0, int64_t dim1) {
return wrapper_CompositeImplicitAutograd__swapdims(self, dim0, dim1);
}
at::Tensor & swapdims_(at::Tensor & self, int64_t dim0, int64_t dim1) {
return wrapper_CompositeImplicitAutograd__swapdims_(self, dim0, dim1);
}
::std::tuple<at::Tensor,at::Tensor> qr(const at::Tensor & self, bool some) {
return wrapper_CompositeImplicitAutograd__qr(self, some);
}
::std::tuple<at::Tensor &,at::Tensor &> qr_out(at::Tensor & Q, at::Tensor & R, const at::Tensor & self, bool some) {
return wrapper_CompositeImplicitAutograd_Q_qr_out(self, some, Q, R);
}
::std::tuple<at::Tensor &,at::Tensor &> qr_outf(const at::Tensor & self, bool some, at::Tensor & Q, at::Tensor & R) {
return wrapper_CompositeImplicitAutograd_Q_qr_out(self, some, Q, R);
}
at::Tensor orgqr(const at::Tensor & self, const at::Tensor & input2) {
return wrapper_CompositeImplicitAutograd__orgqr(self, input2);
}
at::Tensor & orgqr_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & input2) {
return wrapper_CompositeImplicitAutograd_out_orgqr_out(self, input2, out);
}
at::Tensor & orgqr_outf(const at::Tensor & self, const at::Tensor & input2, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_orgqr_out(self, input2, out);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _lu_with_info(const at::Tensor & self, bool pivot, bool check_errors) {
return wrapper_CompositeImplicitAutograd___lu_with_info(self, pivot, check_errors);
}
at::Tensor lu_solve(const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots) {
return wrapper_CompositeImplicitAutograd__lu_solve(self, LU_data, LU_pivots);
}
at::Tensor & lu_solve_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots) {
return wrapper_CompositeImplicitAutograd_out_lu_solve_out(self, LU_data, LU_pivots, out);
}
at::Tensor & lu_solve_outf(const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_lu_solve_out(self, LU_data, LU_pivots, out);
}
at::Tensor arctan2(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__arctan2(self, other);
}
at::Tensor & arctan2_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_out_arctan2_out(self, other, out);
}
at::Tensor & arctan2_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_arctan2_out(self, other, out);
}
at::Tensor & arctan2_(at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__arctan2_(self, other);
}
::std::tuple<at::Tensor,::std::vector<at::Tensor>> histogramdd(const at::Tensor & self, at::IntArrayRef bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density) {
return wrapper_CompositeImplicitAutograd__histogramdd(self, bins, range, weight, density);
}
::std::tuple<at::Tensor,::std::vector<at::Tensor>> histogramdd(const at::Tensor & self, int64_t bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density) {
return wrapper_CompositeImplicitAutograd_int_bins_histogramdd(self, bins, range, weight, density);
}
::std::tuple<at::Tensor,::std::vector<at::Tensor>> histogramdd(const at::Tensor & self, at::TensorList bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density) {
return wrapper_CompositeImplicitAutograd_TensorList_bins_histogramdd(self, bins, range, weight, density);
}
at::Tensor max(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_other_max(self, other);
}
at::Tensor & max_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_out_max_out(self, other, out);
}
at::Tensor & max_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_max_out(self, other, out);
}
at::Tensor min(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_other_min(self, other);
}
at::Tensor & min_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_out_min_out(self, other, out);
}
at::Tensor & min_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_min_out(self, other, out);
}
at::Tensor quantile(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_CompositeImplicitAutograd__quantile(self, q, dim, keepdim, interpolation);
}
at::Tensor & quantile_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_CompositeImplicitAutograd_out_quantile_out(self, q, dim, keepdim, interpolation, out);
}
at::Tensor & quantile_outf(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_quantile_out(self, q, dim, keepdim, interpolation, out);
}
at::Tensor quantile(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_CompositeImplicitAutograd_scalar_quantile(self, q, dim, keepdim, interpolation);
}
at::Tensor & quantile_out(at::Tensor & out, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_CompositeImplicitAutograd_scalar_out_quantile_out(self, q, dim, keepdim, interpolation, out);
}
at::Tensor & quantile_outf(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_scalar_out_quantile_out(self, q, dim, keepdim, interpolation, out);
}
at::Tensor nanquantile(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_CompositeImplicitAutograd__nanquantile(self, q, dim, keepdim, interpolation);
}
at::Tensor & nanquantile_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_CompositeImplicitAutograd_out_nanquantile_out(self, q, dim, keepdim, interpolation, out);
}
at::Tensor & nanquantile_outf(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_nanquantile_out(self, q, dim, keepdim, interpolation, out);
}
at::Tensor nanquantile(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_CompositeImplicitAutograd_scalar_nanquantile(self, q, dim, keepdim, interpolation);
}
at::Tensor & nanquantile_out(at::Tensor & out, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_CompositeImplicitAutograd_scalar_out_nanquantile_out(self, q, dim, keepdim, interpolation, out);
}
at::Tensor & nanquantile_outf(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_scalar_out_nanquantile_out(self, q, dim, keepdim, interpolation, out);
}
::std::tuple<at::Tensor,at::Tensor> sort(const at::Tensor & self, at::Dimname dim, bool descending) {
return wrapper_CompositeImplicitAutograd_dimname_sort(self, dim, descending);
}
::std::tuple<at::Tensor &,at::Tensor &> sort_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, at::Dimname dim, bool descending) {
return wrapper_CompositeImplicitAutograd_dimname_values_sort_out(self, dim, descending, values, indices);
}
::std::tuple<at::Tensor &,at::Tensor &> sort_outf(const at::Tensor & self, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
return wrapper_CompositeImplicitAutograd_dimname_values_sort_out(self, dim, descending, values, indices);
}
::std::tuple<at::Tensor,at::Tensor> sort(const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending) {
return wrapper_CompositeImplicitAutograd_dimname_stable_sort(self, stable, dim, descending);
}
::std::tuple<at::Tensor &,at::Tensor &> sort_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending) {
return wrapper_CompositeImplicitAutograd_dimname_values_stable_sort_out(self, stable, dim, descending, values, indices);
}
::std::tuple<at::Tensor &,at::Tensor &> sort_outf(const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
return wrapper_CompositeImplicitAutograd_dimname_values_stable_sort_out(self, stable, dim, descending, values, indices);
}
at::Tensor msort(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__msort(self);
}
at::Tensor & msort_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_msort_out(self, out);
}
at::Tensor & msort_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_msort_out(self, out);
}
at::Tensor argsort(const at::Tensor & self, int64_t dim, bool descending) {
return wrapper_CompositeImplicitAutograd__argsort(self, dim, descending);
}
at::Tensor argsort(const at::Tensor & self, at::Dimname dim, bool descending) {
return wrapper_CompositeImplicitAutograd_dimname_argsort(self, dim, descending);
}
at::Tensor float_power(const at::Tensor & self, const at::Tensor & exponent) {
return wrapper_CompositeImplicitAutograd_Tensor_Tensor_float_power(self, exponent);
}
at::Tensor & float_power_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & exponent) {
return wrapper_CompositeImplicitAutograd_Tensor_Tensor_out_float_power_out(self, exponent, out);
}
at::Tensor & float_power_outf(const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Tensor_Tensor_out_float_power_out(self, exponent, out);
}
at::Tensor & float_power_(at::Tensor & self, const at::Tensor & exponent) {
return wrapper_CompositeImplicitAutograd_Tensor_float_power_(self, exponent);
}
at::Tensor float_power(const at::Scalar & self, const at::Tensor & exponent) {
return wrapper_CompositeImplicitAutograd_Scalar_float_power(self, exponent);
}
at::Tensor & float_power_out(at::Tensor & out, const at::Scalar & self, const at::Tensor & exponent) {
return wrapper_CompositeImplicitAutograd_Scalar_out_float_power_out(self, exponent, out);
}
at::Tensor & float_power_outf(const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Scalar_out_float_power_out(self, exponent, out);
}
at::Tensor float_power(const at::Tensor & self, const at::Scalar & exponent) {
return wrapper_CompositeImplicitAutograd_Tensor_Scalar_float_power(self, exponent);
}
at::Tensor & float_power_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & exponent) {
return wrapper_CompositeImplicitAutograd_Tensor_Scalar_out_float_power_out(self, exponent, out);
}
at::Tensor & float_power_outf(const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_Tensor_Scalar_out_float_power_out(self, exponent, out);
}
at::Tensor & float_power_(at::Tensor & self, const at::Scalar & exponent) {
return wrapper_CompositeImplicitAutograd_Scalar_float_power_(self, exponent);
}
at::Tensor l1_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
return wrapper_CompositeImplicitAutograd__l1_loss(self, target, reduction);
}
at::Tensor multilabel_margin_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
return wrapper_CompositeImplicitAutograd__multilabel_margin_loss(self, target, reduction);
}
at::Tensor & multilabel_margin_loss_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
return wrapper_CompositeImplicitAutograd_out_multilabel_margin_loss_out(self, target, reduction, out);
}
at::Tensor & multilabel_margin_loss_outf(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_multilabel_margin_loss_out(self, target, reduction, out);
}
at::Tensor nll_loss(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
return wrapper_CompositeImplicitAutograd__nll_loss(self, target, weight, reduction, ignore_index);
}
at::Tensor nll_loss_symint(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
return wrapper_CompositeImplicitAutograd__nll_loss(self, target, weight, reduction, ignore_index);
}
at::Tensor & nll_loss_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
return wrapper_CompositeImplicitAutograd_out_nll_loss_out(self, target, weight, reduction, ignore_index, out);
}
at::Tensor & nll_loss_outf(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_nll_loss_out(self, target, weight, reduction, ignore_index, out);
}
at::Tensor & nll_loss_symint_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
return wrapper_CompositeImplicitAutograd_out_nll_loss_out(self, target, weight, reduction, ignore_index, out);
}
at::Tensor & nll_loss_symint_outf(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_nll_loss_out(self, target, weight, reduction, ignore_index, out);
}
at::Tensor nll_loss_nd(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
return wrapper_CompositeImplicitAutograd__nll_loss_nd(self, target, weight, reduction, ignore_index);
}
at::Tensor nll_loss_nd_symint(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
return wrapper_CompositeImplicitAutograd__nll_loss_nd(self, target, weight, reduction, ignore_index);
}
at::Tensor nll_loss2d(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
return wrapper_CompositeImplicitAutograd__nll_loss2d(self, target, weight, reduction, ignore_index);
}
at::Tensor nll_loss2d_symint(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
return wrapper_CompositeImplicitAutograd__nll_loss2d(self, target, weight, reduction, ignore_index);
}
at::Tensor & nll_loss2d_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
return wrapper_CompositeImplicitAutograd_out_nll_loss2d_out(self, target, weight, reduction, ignore_index, out);
}
at::Tensor & nll_loss2d_outf(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_nll_loss2d_out(self, target, weight, reduction, ignore_index, out);
}
at::Tensor & nll_loss2d_symint_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
return wrapper_CompositeImplicitAutograd_out_nll_loss2d_out(self, target, weight, reduction, ignore_index, out);
}
at::Tensor & nll_loss2d_symint_outf(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_nll_loss2d_out(self, target, weight, reduction, ignore_index, out);
}
at::Tensor log_sigmoid(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__log_sigmoid(self);
}
at::Tensor & log_sigmoid_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_log_sigmoid_out(self, out);
}
at::Tensor & log_sigmoid_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_log_sigmoid_out(self, out);
}
at::Tensor adaptive_avg_pool2d(const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper_CompositeImplicitAutograd__adaptive_avg_pool2d(self, c10::fromIntArrayRefSlow(output_size));
}
at::Tensor adaptive_avg_pool2d_symint(const at::Tensor & self, c10::SymIntArrayRef output_size) {
return wrapper_CompositeImplicitAutograd__adaptive_avg_pool2d(self, output_size);
}
at::Tensor adaptive_avg_pool3d(const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper_CompositeImplicitAutograd__adaptive_avg_pool3d(self, c10::fromIntArrayRefSlow(output_size));
}
at::Tensor adaptive_avg_pool3d_symint(const at::Tensor & self, c10::SymIntArrayRef output_size) {
return wrapper_CompositeImplicitAutograd__adaptive_avg_pool3d(self, output_size);
}
at::Tensor _pad_circular(const at::Tensor & self, at::IntArrayRef pad) {
return wrapper_CompositeImplicitAutograd___pad_circular(self, c10::fromIntArrayRefSlow(pad));
}
at::Tensor _pad_circular_symint(const at::Tensor & self, c10::SymIntArrayRef pad) {
return wrapper_CompositeImplicitAutograd___pad_circular(self, pad);
}
at::Tensor _pad_enum(const at::Tensor & self, at::IntArrayRef pad, int64_t mode, c10::optional<double> value) {
return wrapper_CompositeImplicitAutograd___pad_enum(self, c10::fromIntArrayRefSlow(pad), mode, value);
}
at::Tensor _pad_enum_symint(const at::Tensor & self, c10::SymIntArrayRef pad, int64_t mode, c10::optional<double> value) {
return wrapper_CompositeImplicitAutograd___pad_enum(self, pad, mode, value);
}
at::Tensor pad(const at::Tensor & self, at::IntArrayRef pad, c10::string_view mode, c10::optional<double> value) {
return wrapper_CompositeImplicitAutograd__pad(self, c10::fromIntArrayRefSlow(pad), mode, value);
}
at::Tensor pad_symint(const at::Tensor & self, c10::SymIntArrayRef pad, c10::string_view mode, c10::optional<double> value) {
return wrapper_CompositeImplicitAutograd__pad(self, pad, mode, value);
}
at::Tensor upsample_linear1d(const at::Tensor & input, at::OptionalIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec_upsample_linear1d(input, output_size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*output_size)) : c10::nullopt, align_corners, scale_factors);
}
at::Tensor upsample_linear1d_symint(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec_upsample_linear1d(input, output_size, align_corners, scale_factors);
}
at::Tensor upsample_bilinear2d(const at::Tensor & input, at::OptionalIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec_upsample_bilinear2d(input, output_size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*output_size)) : c10::nullopt, align_corners, scale_factors);
}
at::Tensor upsample_bilinear2d_symint(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec_upsample_bilinear2d(input, output_size, align_corners, scale_factors);
}
at::Tensor _upsample_bilinear2d_aa(const at::Tensor & input, at::OptionalIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec__upsample_bilinear2d_aa(input, output_size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*output_size)) : c10::nullopt, align_corners, scale_factors);
}
at::Tensor _upsample_bilinear2d_aa_symint(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec__upsample_bilinear2d_aa(input, output_size, align_corners, scale_factors);
}
at::Tensor upsample_trilinear3d(const at::Tensor & input, at::OptionalIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec_upsample_trilinear3d(input, output_size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*output_size)) : c10::nullopt, align_corners, scale_factors);
}
at::Tensor upsample_trilinear3d_symint(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec_upsample_trilinear3d(input, output_size, align_corners, scale_factors);
}
at::Tensor upsample_bicubic2d(const at::Tensor & input, at::OptionalIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec_upsample_bicubic2d(input, output_size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*output_size)) : c10::nullopt, align_corners, scale_factors);
}
at::Tensor upsample_bicubic2d_symint(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec_upsample_bicubic2d(input, output_size, align_corners, scale_factors);
}
at::Tensor _upsample_bicubic2d_aa(const at::Tensor & input, at::OptionalIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec__upsample_bicubic2d_aa(input, output_size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*output_size)) : c10::nullopt, align_corners, scale_factors);
}
at::Tensor _upsample_bicubic2d_aa_symint(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec__upsample_bicubic2d_aa(input, output_size, align_corners, scale_factors);
}
at::Tensor upsample_nearest1d(const at::Tensor & input, at::OptionalIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec_upsample_nearest1d(input, output_size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*output_size)) : c10::nullopt, scale_factors);
}
at::Tensor upsample_nearest1d_symint(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec_upsample_nearest1d(input, output_size, scale_factors);
}
at::Tensor _upsample_nearest_exact1d(const at::Tensor & input, at::OptionalIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec__upsample_nearest_exact1d(input, output_size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*output_size)) : c10::nullopt, scale_factors);
}
at::Tensor _upsample_nearest_exact1d_symint(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec__upsample_nearest_exact1d(input, output_size, scale_factors);
}
at::Tensor upsample_nearest2d(const at::Tensor & input, at::OptionalIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec_upsample_nearest2d(input, output_size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*output_size)) : c10::nullopt, scale_factors);
}
at::Tensor upsample_nearest2d_symint(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec_upsample_nearest2d(input, output_size, scale_factors);
}
at::Tensor _upsample_nearest_exact2d(const at::Tensor & input, at::OptionalIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec__upsample_nearest_exact2d(input, output_size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*output_size)) : c10::nullopt, scale_factors);
}
at::Tensor _upsample_nearest_exact2d_symint(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec__upsample_nearest_exact2d(input, output_size, scale_factors);
}
at::Tensor upsample_nearest3d(const at::Tensor & input, at::OptionalIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec_upsample_nearest3d(input, output_size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*output_size)) : c10::nullopt, scale_factors);
}
at::Tensor upsample_nearest3d_symint(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec_upsample_nearest3d(input, output_size, scale_factors);
}
at::Tensor _upsample_nearest_exact3d(const at::Tensor & input, at::OptionalIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec__upsample_nearest_exact3d(input, output_size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*output_size)) : c10::nullopt, scale_factors);
}
at::Tensor _upsample_nearest_exact3d_symint(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_CompositeImplicitAutograd_vec__upsample_nearest_exact3d(input, output_size, scale_factors);
}
at::Tensor thnn_conv2d(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
return wrapper_CompositeImplicitAutograd__thnn_conv2d(self, weight, c10::fromIntArrayRefSlow(kernel_size), bias, c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(padding));
}
at::Tensor thnn_conv2d_symint(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding) {
return wrapper_CompositeImplicitAutograd__thnn_conv2d(self, weight, kernel_size, bias, stride, padding);
}
at::Tensor & thnn_conv2d_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
return wrapper_CompositeImplicitAutograd_out_thnn_conv2d_out(self, weight, c10::fromIntArrayRefSlow(kernel_size), bias, c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(padding), out);
}
at::Tensor & thnn_conv2d_outf(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_thnn_conv2d_out(self, weight, c10::fromIntArrayRefSlow(kernel_size), bias, c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(padding), out);
}
at::Tensor & thnn_conv2d_symint_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding) {
return wrapper_CompositeImplicitAutograd_out_thnn_conv2d_out(self, weight, kernel_size, bias, stride, padding, out);
}
at::Tensor & thnn_conv2d_symint_outf(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_thnn_conv2d_out(self, weight, kernel_size, bias, stride, padding, out);
}
at::Tensor slow_conv3d(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
return wrapper_CompositeImplicitAutograd__slow_conv3d(self, weight, c10::fromIntArrayRefSlow(kernel_size), bias, c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(padding));
}
at::Tensor slow_conv3d_symint(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding) {
return wrapper_CompositeImplicitAutograd__slow_conv3d(self, weight, kernel_size, bias, stride, padding);
}
at::Tensor & slow_conv3d_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
return wrapper_CompositeImplicitAutograd_out_slow_conv3d_out(self, weight, c10::fromIntArrayRefSlow(kernel_size), bias, c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(padding), out);
}
at::Tensor & slow_conv3d_outf(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_slow_conv3d_out(self, weight, c10::fromIntArrayRefSlow(kernel_size), bias, c10::fromIntArrayRefSlow(stride), c10::fromIntArrayRefSlow(padding), out);
}
at::Tensor & slow_conv3d_symint_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding) {
return wrapper_CompositeImplicitAutograd_out_slow_conv3d_out(self, weight, kernel_size, bias, stride, padding, out);
}
at::Tensor & slow_conv3d_symint_outf(const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_slow_conv3d_out(self, weight, kernel_size, bias, stride, padding, out);
}
at::Tensor column_stack(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd__column_stack(tensors);
}
at::Tensor & column_stack_out(at::Tensor & out, at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd_out_column_stack_out(tensors, out);
}
at::Tensor & column_stack_outf(at::TensorList tensors, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_column_stack_out(tensors, out);
}
at::Tensor isfinite(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__isfinite(self);
}
at::Tensor _add_batch_dim(const at::Tensor & self, int64_t batch_dim, int64_t level) {
return wrapper_CompositeImplicitAutograd___add_batch_dim(self, batch_dim, level);
}
at::Tensor _remove_batch_dim(const at::Tensor & self, int64_t level, int64_t batch_size, int64_t out_dim) {
return wrapper_CompositeImplicitAutograd___remove_batch_dim(self, level, batch_size, out_dim);
}
at::Tensor special_expm1(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__special_expm1(self);
}
at::Tensor & special_expm1_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_special_expm1_out(self, out);
}
at::Tensor & special_expm1_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_expm1_out(self, out);
}
at::Tensor special_exp2(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__special_exp2(self);
}
at::Tensor & special_exp2_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_special_exp2_out(self, out);
}
at::Tensor & special_exp2_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_exp2_out(self, out);
}
at::Tensor special_psi(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__special_psi(self);
}
at::Tensor & special_psi_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_special_psi_out(self, out);
}
at::Tensor & special_psi_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_psi_out(self, out);
}
at::Tensor special_digamma(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__special_digamma(self);
}
at::Tensor & special_digamma_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_special_digamma_out(self, out);
}
at::Tensor & special_digamma_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_digamma_out(self, out);
}
at::Tensor special_gammaln(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__special_gammaln(self);
}
at::Tensor & special_gammaln_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_special_gammaln_out(self, out);
}
at::Tensor & special_gammaln_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_gammaln_out(self, out);
}
at::Tensor special_erf(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__special_erf(self);
}
at::Tensor & special_erf_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_special_erf_out(self, out);
}
at::Tensor & special_erf_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_erf_out(self, out);
}
at::Tensor special_erfc(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__special_erfc(self);
}
at::Tensor & special_erfc_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_special_erfc_out(self, out);
}
at::Tensor & special_erfc_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_erfc_out(self, out);
}
at::Tensor special_erfinv(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__special_erfinv(self);
}
at::Tensor & special_erfinv_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_special_erfinv_out(self, out);
}
at::Tensor & special_erfinv_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_erfinv_out(self, out);
}
at::Tensor special_ndtr(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__special_ndtr(self);
}
at::Tensor & special_ndtr_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_special_ndtr_out(self, out);
}
at::Tensor & special_ndtr_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_ndtr_out(self, out);
}
at::Tensor special_xlogy(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__special_xlogy(self, other);
}
at::Tensor & special_xlogy_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_out_special_xlogy_out(self, other, out);
}
at::Tensor & special_xlogy_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_xlogy_out(self, other, out);
}
at::Tensor special_xlogy(const at::Scalar & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_self_scalar_special_xlogy(self, other);
}
at::Tensor & special_xlogy_out(at::Tensor & out, const at::Scalar & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_self_scalar_out_special_xlogy_out(self, other, out);
}
at::Tensor & special_xlogy_outf(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_self_scalar_out_special_xlogy_out(self, other, out);
}
at::Tensor special_xlogy(const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_other_scalar_special_xlogy(self, other);
}
at::Tensor & special_xlogy_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_CompositeImplicitAutograd_other_scalar_out_special_xlogy_out(self, other, out);
}
at::Tensor & special_xlogy_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_other_scalar_out_special_xlogy_out(self, other, out);
}
at::Tensor special_i0(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__special_i0(self);
}
at::Tensor & special_i0_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_special_i0_out(self, out);
}
at::Tensor & special_i0_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_i0_out(self, out);
}
at::Tensor special_logit(const at::Tensor & self, c10::optional<double> eps) {
return wrapper_CompositeImplicitAutograd__special_logit(self, eps);
}
at::Tensor & special_logit_out(at::Tensor & out, const at::Tensor & self, c10::optional<double> eps) {
return wrapper_CompositeImplicitAutograd_out_special_logit_out(self, eps, out);
}
at::Tensor & special_logit_outf(const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_logit_out(self, eps, out);
}
at::Tensor special_polygamma(int64_t n, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__special_polygamma(n, self);
}
at::Tensor & special_polygamma_out(at::Tensor & out, int64_t n, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_special_polygamma_out(n, self, out);
}
at::Tensor & special_polygamma_outf(int64_t n, const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_polygamma_out(n, self, out);
}
at::Tensor special_logsumexp(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd__special_logsumexp(self, dim, keepdim);
}
at::Tensor & special_logsumexp_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper_CompositeImplicitAutograd_out_special_logsumexp_out(self, dim, keepdim, out);
}
at::Tensor & special_logsumexp_outf(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_logsumexp_out(self, dim, keepdim, out);
}
at::Tensor special_expit(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__special_expit(self);
}
at::Tensor & special_expit_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_special_expit_out(self, out);
}
at::Tensor & special_expit_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_expit_out(self, out);
}
at::Tensor special_sinc(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__special_sinc(self);
}
at::Tensor & special_sinc_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_special_sinc_out(self, out);
}
at::Tensor & special_sinc_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_sinc_out(self, out);
}
at::Tensor special_round(const at::Tensor & self, int64_t decimals) {
return wrapper_CompositeImplicitAutograd__special_round(self, decimals);
}
at::Tensor & special_round_out(at::Tensor & out, const at::Tensor & self, int64_t decimals) {
return wrapper_CompositeImplicitAutograd_out_special_round_out(self, decimals, out);
}
at::Tensor & special_round_outf(const at::Tensor & self, int64_t decimals, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_round_out(self, decimals, out);
}
at::Tensor special_log1p(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__special_log1p(self);
}
at::Tensor & special_log1p_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_special_log1p_out(self, out);
}
at::Tensor & special_log1p_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_log1p_out(self, out);
}
at::Tensor special_log_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd__special_log_softmax(self, dim, dtype);
}
at::Tensor special_gammainc(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__special_gammainc(self, other);
}
at::Tensor & special_gammainc_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_out_special_gammainc_out(self, other, out);
}
at::Tensor & special_gammainc_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_gammainc_out(self, other, out);
}
at::Tensor special_gammaincc(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__special_gammaincc(self, other);
}
at::Tensor & special_gammaincc_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_out_special_gammaincc_out(self, other, out);
}
at::Tensor & special_gammaincc_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_gammaincc_out(self, other, out);
}
at::Tensor special_multigammaln(const at::Tensor & self, int64_t p) {
return wrapper_CompositeImplicitAutograd__special_multigammaln(self, p);
}
at::Tensor & special_multigammaln_out(at::Tensor & out, const at::Tensor & self, int64_t p) {
return wrapper_CompositeImplicitAutograd_out_special_multigammaln_out(self, p, out);
}
at::Tensor & special_multigammaln_outf(const at::Tensor & self, int64_t p, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_special_multigammaln_out(self, p, out);
}
at::Tensor special_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd__special_softmax(self, dim, dtype);
}
at::Tensor fft_fft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_fft(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm);
}
at::Tensor fft_fft_symint(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_fft(self, n, dim, norm);
}
at::Tensor & fft_fft_out(at::Tensor & out, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_fft_out(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_fft_outf(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_fft_out(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_fft_symint_out(at::Tensor & out, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_fft_out(self, n, dim, norm, out);
}
at::Tensor & fft_fft_symint_outf(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_fft_out(self, n, dim, norm, out);
}
at::Tensor fft_ifft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_ifft(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm);
}
at::Tensor fft_ifft_symint(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_ifft(self, n, dim, norm);
}
at::Tensor & fft_ifft_out(at::Tensor & out, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_ifft_out(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_ifft_outf(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_ifft_out(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_ifft_symint_out(at::Tensor & out, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_ifft_out(self, n, dim, norm, out);
}
at::Tensor & fft_ifft_symint_outf(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_ifft_out(self, n, dim, norm, out);
}
at::Tensor fft_rfft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_rfft(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm);
}
at::Tensor fft_rfft_symint(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_rfft(self, n, dim, norm);
}
at::Tensor & fft_rfft_out(at::Tensor & out, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_rfft_out(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_rfft_outf(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_rfft_out(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_rfft_symint_out(at::Tensor & out, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_rfft_out(self, n, dim, norm, out);
}
at::Tensor & fft_rfft_symint_outf(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_rfft_out(self, n, dim, norm, out);
}
at::Tensor fft_irfft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_irfft(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm);
}
at::Tensor fft_irfft_symint(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_irfft(self, n, dim, norm);
}
at::Tensor & fft_irfft_out(at::Tensor & out, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_irfft_out(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_irfft_outf(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_irfft_out(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_irfft_symint_out(at::Tensor & out, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_irfft_out(self, n, dim, norm, out);
}
at::Tensor & fft_irfft_symint_outf(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_irfft_out(self, n, dim, norm, out);
}
at::Tensor fft_hfft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_hfft(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm);
}
at::Tensor fft_hfft_symint(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_hfft(self, n, dim, norm);
}
at::Tensor & fft_hfft_out(at::Tensor & out, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_hfft_out(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_hfft_outf(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_hfft_out(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_hfft_symint_out(at::Tensor & out, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_hfft_out(self, n, dim, norm, out);
}
at::Tensor & fft_hfft_symint_outf(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_hfft_out(self, n, dim, norm, out);
}
at::Tensor fft_ihfft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_ihfft(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm);
}
at::Tensor fft_ihfft_symint(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_ihfft(self, n, dim, norm);
}
at::Tensor & fft_ihfft_out(at::Tensor & out, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_ihfft_out(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_ihfft_outf(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_ihfft_out(self, n.has_value() ? c10::make_optional(c10::SymInt(*n)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_ihfft_symint_out(at::Tensor & out, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_ihfft_out(self, n, dim, norm, out);
}
at::Tensor & fft_ihfft_symint_outf(const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_ihfft_out(self, n, dim, norm, out);
}
at::Tensor fft_fft2(const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_fft2(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm);
}
at::Tensor fft_fft2_symint(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_fft2(self, s, dim, norm);
}
at::Tensor & fft_fft2_out(at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_fft2_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_fft2_outf(const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_fft2_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_fft2_symint_out(at::Tensor & out, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_fft2_out(self, s, dim, norm, out);
}
at::Tensor & fft_fft2_symint_outf(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_fft2_out(self, s, dim, norm, out);
}
at::Tensor fft_ifft2(const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_ifft2(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm);
}
at::Tensor fft_ifft2_symint(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_ifft2(self, s, dim, norm);
}
at::Tensor & fft_ifft2_out(at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_ifft2_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_ifft2_outf(const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_ifft2_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_ifft2_symint_out(at::Tensor & out, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_ifft2_out(self, s, dim, norm, out);
}
at::Tensor & fft_ifft2_symint_outf(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_ifft2_out(self, s, dim, norm, out);
}
at::Tensor fft_rfft2(const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_rfft2(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm);
}
at::Tensor fft_rfft2_symint(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_rfft2(self, s, dim, norm);
}
at::Tensor & fft_rfft2_out(at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_rfft2_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_rfft2_outf(const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_rfft2_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_rfft2_symint_out(at::Tensor & out, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_rfft2_out(self, s, dim, norm, out);
}
at::Tensor & fft_rfft2_symint_outf(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_rfft2_out(self, s, dim, norm, out);
}
at::Tensor fft_irfft2(const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_irfft2(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm);
}
at::Tensor fft_irfft2_symint(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_irfft2(self, s, dim, norm);
}
at::Tensor & fft_irfft2_out(at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_irfft2_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_irfft2_outf(const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_irfft2_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_irfft2_symint_out(at::Tensor & out, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_irfft2_out(self, s, dim, norm, out);
}
at::Tensor & fft_irfft2_symint_outf(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_irfft2_out(self, s, dim, norm, out);
}
at::Tensor fft_hfft2(const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_hfft2(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm);
}
at::Tensor fft_hfft2_symint(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_hfft2(self, s, dim, norm);
}
const at::Tensor & fft_hfft2_out(const at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_hfft2_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
const at::Tensor & fft_hfft2_outf(const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_hfft2_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
const at::Tensor & fft_hfft2_symint_out(const at::Tensor & out, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_hfft2_out(self, s, dim, norm, out);
}
const at::Tensor & fft_hfft2_symint_outf(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_hfft2_out(self, s, dim, norm, out);
}
at::Tensor fft_ihfft2(const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_ihfft2(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm);
}
at::Tensor fft_ihfft2_symint(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_ihfft2(self, s, dim, norm);
}
const at::Tensor & fft_ihfft2_out(const at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_ihfft2_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
const at::Tensor & fft_ihfft2_outf(const at::Tensor & self, at::OptionalIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_ihfft2_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
const at::Tensor & fft_ihfft2_symint_out(const at::Tensor & out, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_ihfft2_out(self, s, dim, norm, out);
}
const at::Tensor & fft_ihfft2_symint_outf(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_ihfft2_out(self, s, dim, norm, out);
}
at::Tensor fft_fftn(const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_fftn(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm);
}
at::Tensor fft_fftn_symint(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_fftn(self, s, dim, norm);
}
at::Tensor & fft_fftn_out(at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_fftn_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_fftn_outf(const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_fftn_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_fftn_symint_out(at::Tensor & out, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_fftn_out(self, s, dim, norm, out);
}
at::Tensor & fft_fftn_symint_outf(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_fftn_out(self, s, dim, norm, out);
}
at::Tensor fft_ifftn(const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_ifftn(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm);
}
at::Tensor fft_ifftn_symint(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_ifftn(self, s, dim, norm);
}
at::Tensor & fft_ifftn_out(at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_ifftn_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_ifftn_outf(const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_ifftn_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_ifftn_symint_out(at::Tensor & out, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_ifftn_out(self, s, dim, norm, out);
}
at::Tensor & fft_ifftn_symint_outf(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_ifftn_out(self, s, dim, norm, out);
}
at::Tensor fft_rfftn(const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_rfftn(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm);
}
at::Tensor fft_rfftn_symint(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_rfftn(self, s, dim, norm);
}
at::Tensor & fft_rfftn_out(at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_rfftn_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_rfftn_outf(const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_rfftn_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_rfftn_symint_out(at::Tensor & out, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_rfftn_out(self, s, dim, norm, out);
}
at::Tensor & fft_rfftn_symint_outf(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_rfftn_out(self, s, dim, norm, out);
}
at::Tensor fft_irfftn(const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_irfftn(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm);
}
at::Tensor fft_irfftn_symint(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_irfftn(self, s, dim, norm);
}
at::Tensor & fft_irfftn_out(at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_irfftn_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_irfftn_outf(const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_irfftn_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
at::Tensor & fft_irfftn_symint_out(at::Tensor & out, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_irfftn_out(self, s, dim, norm, out);
}
at::Tensor & fft_irfftn_symint_outf(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_irfftn_out(self, s, dim, norm, out);
}
at::Tensor fft_hfftn(const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_hfftn(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm);
}
at::Tensor fft_hfftn_symint(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_hfftn(self, s, dim, norm);
}
const at::Tensor & fft_hfftn_out(const at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_hfftn_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
const at::Tensor & fft_hfftn_outf(const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_hfftn_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
const at::Tensor & fft_hfftn_symint_out(const at::Tensor & out, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_hfftn_out(self, s, dim, norm, out);
}
const at::Tensor & fft_hfftn_symint_outf(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_hfftn_out(self, s, dim, norm, out);
}
at::Tensor fft_ihfftn(const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_ihfftn(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm);
}
at::Tensor fft_ihfftn_symint(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd__fft_ihfftn(self, s, dim, norm);
}
const at::Tensor & fft_ihfftn_out(const at::Tensor & out, const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_ihfftn_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
const at::Tensor & fft_ihfftn_outf(const at::Tensor & self, at::OptionalIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_ihfftn_out(self, s.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*s)) : c10::nullopt, dim, norm, out);
}
const at::Tensor & fft_ihfftn_symint_out(const at::Tensor & out, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_CompositeImplicitAutograd_out_fft_ihfftn_out(self, s, dim, norm, out);
}
const at::Tensor & fft_ihfftn_symint_outf(const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_fft_ihfftn_out(self, s, dim, norm, out);
}
at::Tensor fft_fftshift(const at::Tensor & self, at::OptionalIntArrayRef dim) {
return wrapper_CompositeImplicitAutograd__fft_fftshift(self, dim);
}
at::Tensor fft_ifftshift(const at::Tensor & self, at::OptionalIntArrayRef dim) {
return wrapper_CompositeImplicitAutograd__fft_ifftshift(self, dim);
}
at::Tensor linalg_cholesky(const at::Tensor & self, bool upper) {
return wrapper_CompositeImplicitAutograd__linalg_cholesky(self, upper);
}
at::Tensor & linalg_cholesky_out(at::Tensor & out, const at::Tensor & self, bool upper) {
return wrapper_CompositeImplicitAutograd_out_linalg_cholesky_out(self, upper, out);
}
at::Tensor & linalg_cholesky_outf(const at::Tensor & self, bool upper, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_cholesky_out(self, upper, out);
}
::std::tuple<at::Tensor,at::Tensor> linalg_lu_factor(const at::Tensor & A, bool pivot) {
return wrapper_CompositeImplicitAutograd__linalg_lu_factor(A, pivot);
}
::std::tuple<at::Tensor &,at::Tensor &> linalg_lu_factor_out(at::Tensor & LU, at::Tensor & pivots, const at::Tensor & A, bool pivot) {
return wrapper_CompositeImplicitAutograd_out_linalg_lu_factor_out(A, pivot, LU, pivots);
}
::std::tuple<at::Tensor &,at::Tensor &> linalg_lu_factor_outf(const at::Tensor & A, bool pivot, at::Tensor & LU, at::Tensor & pivots) {
return wrapper_CompositeImplicitAutograd_out_linalg_lu_factor_out(A, pivot, LU, pivots);
}
at::Tensor linalg_det(const at::Tensor & A) {
return wrapper_CompositeImplicitAutograd__linalg_det(A);
}
at::Tensor & linalg_det_out(at::Tensor & out, const at::Tensor & A) {
return wrapper_CompositeImplicitAutograd_out_linalg_det_out(A, out);
}
at::Tensor & linalg_det_outf(const at::Tensor & A, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_det_out(A, out);
}
at::Tensor det(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__det(self);
}
::std::tuple<at::Tensor,at::Tensor> linalg_ldl_factor(const at::Tensor & self, bool hermitian) {
return wrapper_CompositeImplicitAutograd__linalg_ldl_factor(self, hermitian);
}
::std::tuple<at::Tensor &,at::Tensor &> linalg_ldl_factor_out(at::Tensor & LD, at::Tensor & pivots, const at::Tensor & self, bool hermitian) {
return wrapper_CompositeImplicitAutograd_out_linalg_ldl_factor_out(self, hermitian, LD, pivots);
}
::std::tuple<at::Tensor &,at::Tensor &> linalg_ldl_factor_outf(const at::Tensor & self, bool hermitian, at::Tensor & LD, at::Tensor & pivots) {
return wrapper_CompositeImplicitAutograd_out_linalg_ldl_factor_out(self, hermitian, LD, pivots);
}
at::Tensor linalg_matmul(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__linalg_matmul(self, other);
}
at::Tensor & linalg_matmul_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_out_linalg_matmul_out(self, other, out);
}
at::Tensor & linalg_matmul_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_matmul_out(self, other, out);
}
at::Tensor linalg_vecdot(const at::Tensor & x, const at::Tensor & y, int64_t dim) {
return wrapper_CompositeImplicitAutograd__linalg_vecdot(x, y, dim);
}
at::Tensor & linalg_vecdot_out(at::Tensor & out, const at::Tensor & x, const at::Tensor & y, int64_t dim) {
return wrapper_CompositeImplicitAutograd_out_linalg_vecdot_out(x, y, dim, out);
}
at::Tensor & linalg_vecdot_outf(const at::Tensor & x, const at::Tensor & y, int64_t dim, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_vecdot_out(x, y, dim, out);
}
::std::tuple<at::Tensor,at::Tensor> linalg_slogdet(const at::Tensor & A) {
return wrapper_CompositeImplicitAutograd__linalg_slogdet(A);
}
::std::tuple<at::Tensor &,at::Tensor &> linalg_slogdet_out(at::Tensor & sign, at::Tensor & logabsdet, const at::Tensor & A) {
return wrapper_CompositeImplicitAutograd_out_linalg_slogdet_out(A, sign, logabsdet);
}
::std::tuple<at::Tensor &,at::Tensor &> linalg_slogdet_outf(const at::Tensor & A, at::Tensor & sign, at::Tensor & logabsdet) {
return wrapper_CompositeImplicitAutograd_out_linalg_slogdet_out(A, sign, logabsdet);
}
::std::tuple<at::Tensor,at::Tensor> slogdet(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__slogdet(self);
}
::std::tuple<at::Tensor &,at::Tensor &> slogdet_out(at::Tensor & sign, at::Tensor & logabsdet, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_slogdet_out(self, sign, logabsdet);
}
::std::tuple<at::Tensor &,at::Tensor &> slogdet_outf(const at::Tensor & self, at::Tensor & sign, at::Tensor & logabsdet) {
return wrapper_CompositeImplicitAutograd_out_slogdet_out(self, sign, logabsdet);
}
at::Tensor logdet(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__logdet(self);
}
at::Tensor linalg_eigvals(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__linalg_eigvals(self);
}
at::Tensor & linalg_eigvals_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_linalg_eigvals_out(self, out);
}
at::Tensor & linalg_eigvals_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_eigvals_out(self, out);
}
::std::tuple<at::Tensor,at::Tensor> linalg_eigh(const at::Tensor & self, c10::string_view UPLO) {
return wrapper_CompositeImplicitAutograd__linalg_eigh(self, UPLO);
}
::std::tuple<at::Tensor &,at::Tensor &> linalg_eigh_out(at::Tensor & eigvals, at::Tensor & eigvecs, const at::Tensor & self, c10::string_view UPLO) {
return wrapper_CompositeImplicitAutograd_eigvals_linalg_eigh_out(self, UPLO, eigvals, eigvecs);
}
::std::tuple<at::Tensor &,at::Tensor &> linalg_eigh_outf(const at::Tensor & self, c10::string_view UPLO, at::Tensor & eigvals, at::Tensor & eigvecs) {
return wrapper_CompositeImplicitAutograd_eigvals_linalg_eigh_out(self, UPLO, eigvals, eigvecs);
}
at::Tensor linalg_eigvalsh(const at::Tensor & self, c10::string_view UPLO) {
return wrapper_CompositeImplicitAutograd__linalg_eigvalsh(self, UPLO);
}
at::Tensor & linalg_eigvalsh_out(at::Tensor & out, const at::Tensor & self, c10::string_view UPLO) {
return wrapper_CompositeImplicitAutograd_out_linalg_eigvalsh_out(self, UPLO, out);
}
at::Tensor & linalg_eigvalsh_outf(const at::Tensor & self, c10::string_view UPLO, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_eigvalsh_out(self, UPLO, out);
}
at::Tensor linalg_inv(const at::Tensor & A) {
return wrapper_CompositeImplicitAutograd__linalg_inv(A);
}
at::Tensor & linalg_inv_out(at::Tensor & out, const at::Tensor & A) {
return wrapper_CompositeImplicitAutograd_out_linalg_inv_out(A, out);
}
at::Tensor & linalg_inv_outf(const at::Tensor & A, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_inv_out(A, out);
}
at::Tensor inverse(const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd__inverse(self);
}
at::Tensor & inverse_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_CompositeImplicitAutograd_out_inverse_out(self, out);
}
at::Tensor & inverse_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_inverse_out(self, out);
}
at::Tensor inner(const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd__inner(self, other);
}
at::Tensor & inner_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_CompositeImplicitAutograd_out_inner_out(self, other, out);
}
at::Tensor & inner_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_inner_out(self, other, out);
}
at::Tensor outer(const at::Tensor & self, const at::Tensor & vec2) {
return wrapper_CompositeImplicitAutograd__outer(self, vec2);
}
at::Tensor & outer_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & vec2) {
return wrapper_CompositeImplicitAutograd_out_outer_out(self, vec2, out);
}
at::Tensor & outer_outf(const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_outer_out(self, vec2, out);
}
at::Tensor ger(const at::Tensor & self, const at::Tensor & vec2) {
return wrapper_CompositeImplicitAutograd__ger(self, vec2);
}
at::Tensor & ger_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & vec2) {
return wrapper_CompositeImplicitAutograd_out_ger_out(self, vec2, out);
}
at::Tensor & ger_outf(const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_ger_out(self, vec2, out);
}
at::Tensor linalg_norm(const at::Tensor & self, const c10::optional<at::Scalar> & ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd__linalg_norm(self, ord, dim, keepdim, dtype);
}
at::Tensor & linalg_norm_out(at::Tensor & out, const at::Tensor & self, const c10::optional<at::Scalar> & ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_out_linalg_norm_out(self, ord, dim, keepdim, dtype, out);
}
at::Tensor & linalg_norm_outf(const at::Tensor & self, const c10::optional<at::Scalar> & ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_norm_out(self, ord, dim, keepdim, dtype, out);
}
at::Tensor linalg_norm(const at::Tensor & self, c10::string_view ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_ord_str_linalg_norm(self, ord, dim, keepdim, dtype);
}
at::Tensor & linalg_norm_out(at::Tensor & out, const at::Tensor & self, c10::string_view ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_ord_str_out_linalg_norm_out(self, ord, dim, keepdim, dtype, out);
}
at::Tensor & linalg_norm_outf(const at::Tensor & self, c10::string_view ord, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_ord_str_out_linalg_norm_out(self, ord, dim, keepdim, dtype, out);
}
at::Tensor linalg_matrix_norm(const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd__linalg_matrix_norm(self, ord, dim, keepdim, dtype);
}
at::Tensor & linalg_matrix_norm_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_out_linalg_matrix_norm_out(self, ord, dim, keepdim, dtype, out);
}
at::Tensor & linalg_matrix_norm_outf(const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_matrix_norm_out(self, ord, dim, keepdim, dtype, out);
}
at::Tensor linalg_matrix_norm(const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_str_ord_linalg_matrix_norm(self, ord, dim, keepdim, dtype);
}
at::Tensor & linalg_matrix_norm_out(at::Tensor & out, const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_CompositeImplicitAutograd_str_ord_out_linalg_matrix_norm_out(self, ord, dim, keepdim, dtype, out);
}
at::Tensor & linalg_matrix_norm_outf(const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_str_ord_out_linalg_matrix_norm_out(self, ord, dim, keepdim, dtype, out);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> linalg_svd(const at::Tensor & A, bool full_matrices, c10::optional<c10::string_view> driver) {
return wrapper_CompositeImplicitAutograd__linalg_svd(A, full_matrices, driver);
}
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_svd_out(at::Tensor & U, at::Tensor & S, at::Tensor & Vh, const at::Tensor & A, bool full_matrices, c10::optional<c10::string_view> driver) {
return wrapper_CompositeImplicitAutograd_U_linalg_svd_out(A, full_matrices, driver, U, S, Vh);
}
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_svd_outf(const at::Tensor & A, bool full_matrices, c10::optional<c10::string_view> driver, at::Tensor & U, at::Tensor & S, at::Tensor & Vh) {
return wrapper_CompositeImplicitAutograd_U_linalg_svd_out(A, full_matrices, driver, U, S, Vh);
}
at::Tensor linalg_svdvals(const at::Tensor & A, c10::optional<c10::string_view> driver) {
return wrapper_CompositeImplicitAutograd__linalg_svdvals(A, driver);
}
at::Tensor & linalg_svdvals_out(at::Tensor & out, const at::Tensor & A, c10::optional<c10::string_view> driver) {
return wrapper_CompositeImplicitAutograd_out_linalg_svdvals_out(A, driver, out);
}
at::Tensor & linalg_svdvals_outf(const at::Tensor & A, c10::optional<c10::string_view> driver, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_svdvals_out(A, driver, out);
}
at::Tensor linalg_cond(const at::Tensor & self, const c10::optional<at::Scalar> & p) {
return wrapper_CompositeImplicitAutograd__linalg_cond(self, p);
}
at::Tensor & linalg_cond_out(at::Tensor & out, const at::Tensor & self, const c10::optional<at::Scalar> & p) {
return wrapper_CompositeImplicitAutograd_out_linalg_cond_out(self, p, out);
}
at::Tensor & linalg_cond_outf(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_cond_out(self, p, out);
}
at::Tensor linalg_cond(const at::Tensor & self, c10::string_view p) {
return wrapper_CompositeImplicitAutograd_p_str_linalg_cond(self, p);
}
at::Tensor & linalg_cond_out(at::Tensor & out, const at::Tensor & self, c10::string_view p) {
return wrapper_CompositeImplicitAutograd_p_str_out_linalg_cond_out(self, p, out);
}
at::Tensor & linalg_cond_outf(const at::Tensor & self, c10::string_view p, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_p_str_out_linalg_cond_out(self, p, out);
}
at::Tensor linalg_pinv(const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian) {
return wrapper_CompositeImplicitAutograd_atol_rtol_float_linalg_pinv(self, atol, rtol, hermitian);
}
at::Tensor & linalg_pinv_out(at::Tensor & out, const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian) {
return wrapper_CompositeImplicitAutograd_atol_rtol_float_out_linalg_pinv_out(self, atol, rtol, hermitian, out);
}
at::Tensor & linalg_pinv_outf(const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_atol_rtol_float_out_linalg_pinv_out(self, atol, rtol, hermitian, out);
}
at::Tensor linalg_pinv(const at::Tensor & self, double rcond, bool hermitian) {
return wrapper_CompositeImplicitAutograd__linalg_pinv(self, rcond, hermitian);
}
at::Tensor & linalg_pinv_out(at::Tensor & out, const at::Tensor & self, double rcond, bool hermitian) {
return wrapper_CompositeImplicitAutograd_out_linalg_pinv_out(self, rcond, hermitian, out);
}
at::Tensor & linalg_pinv_outf(const at::Tensor & self, double rcond, bool hermitian, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_pinv_out(self, rcond, hermitian, out);
}
at::Tensor linalg_pinv(const at::Tensor & self, const at::Tensor & rcond, bool hermitian) {
return wrapper_CompositeImplicitAutograd_rcond_tensor_linalg_pinv(self, rcond, hermitian);
}
at::Tensor & linalg_pinv_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & rcond, bool hermitian) {
return wrapper_CompositeImplicitAutograd_out_rcond_tensor_linalg_pinv_out(self, rcond, hermitian, out);
}
at::Tensor & linalg_pinv_outf(const at::Tensor & self, const at::Tensor & rcond, bool hermitian, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_rcond_tensor_linalg_pinv_out(self, rcond, hermitian, out);
}
::std::tuple<at::Tensor,at::Tensor> linalg_solve_ex(const at::Tensor & A, const at::Tensor & B, bool left, bool check_errors) {
return wrapper_CompositeImplicitAutograd__linalg_solve_ex(A, B, left, check_errors);
}
::std::tuple<at::Tensor &,at::Tensor &> linalg_solve_ex_out(at::Tensor & result, at::Tensor & info, const at::Tensor & A, const at::Tensor & B, bool left, bool check_errors) {
return wrapper_CompositeImplicitAutograd_out_linalg_solve_ex_out(A, B, left, check_errors, result, info);
}
::std::tuple<at::Tensor &,at::Tensor &> linalg_solve_ex_outf(const at::Tensor & A, const at::Tensor & B, bool left, bool check_errors, at::Tensor & result, at::Tensor & info) {
return wrapper_CompositeImplicitAutograd_out_linalg_solve_ex_out(A, B, left, check_errors, result, info);
}
at::Tensor linalg_solve(const at::Tensor & A, const at::Tensor & B, bool left) {
return wrapper_CompositeImplicitAutograd__linalg_solve(A, B, left);
}
at::Tensor & linalg_solve_out(at::Tensor & out, const at::Tensor & A, const at::Tensor & B, bool left) {
return wrapper_CompositeImplicitAutograd_out_linalg_solve_out(A, B, left, out);
}
at::Tensor & linalg_solve_outf(const at::Tensor & A, const at::Tensor & B, bool left, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_solve_out(A, B, left, out);
}
at::Tensor linalg_tensorinv(const at::Tensor & self, int64_t ind) {
return wrapper_CompositeImplicitAutograd__linalg_tensorinv(self, ind);
}
at::Tensor & linalg_tensorinv_out(at::Tensor & out, const at::Tensor & self, int64_t ind) {
return wrapper_CompositeImplicitAutograd_out_linalg_tensorinv_out(self, ind, out);
}
at::Tensor & linalg_tensorinv_outf(const at::Tensor & self, int64_t ind, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_tensorinv_out(self, ind, out);
}
at::Tensor linalg_tensorsolve(const at::Tensor & self, const at::Tensor & other, at::OptionalIntArrayRef dims) {
return wrapper_CompositeImplicitAutograd__linalg_tensorsolve(self, other, dims);
}
at::Tensor & linalg_tensorsolve_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, at::OptionalIntArrayRef dims) {
return wrapper_CompositeImplicitAutograd_out_linalg_tensorsolve_out(self, other, dims, out);
}
at::Tensor & linalg_tensorsolve_outf(const at::Tensor & self, const at::Tensor & other, at::OptionalIntArrayRef dims, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_tensorsolve_out(self, other, dims, out);
}
at::Tensor linalg_matrix_power(const at::Tensor & self, int64_t n) {
return wrapper_CompositeImplicitAutograd__linalg_matrix_power(self, n);
}
at::Tensor & linalg_matrix_power_out(at::Tensor & out, const at::Tensor & self, int64_t n) {
return wrapper_CompositeImplicitAutograd_out_linalg_matrix_power_out(self, n, out);
}
at::Tensor & linalg_matrix_power_outf(const at::Tensor & self, int64_t n, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_matrix_power_out(self, n, out);
}
at::Tensor linalg_matrix_rank(const at::Tensor & input, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian) {
return wrapper_CompositeImplicitAutograd_atol_rtol_tensor_linalg_matrix_rank(input, atol, rtol, hermitian);
}
at::Tensor & linalg_matrix_rank_out(at::Tensor & out, const at::Tensor & input, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian) {
return wrapper_CompositeImplicitAutograd_atol_rtol_tensor_out_linalg_matrix_rank_out(input, atol, rtol, hermitian, out);
}
at::Tensor & linalg_matrix_rank_outf(const at::Tensor & input, const c10::optional<at::Tensor> & atol, const c10::optional<at::Tensor> & rtol, bool hermitian, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_atol_rtol_tensor_out_linalg_matrix_rank_out(input, atol, rtol, hermitian, out);
}
at::Tensor linalg_matrix_rank(const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian) {
return wrapper_CompositeImplicitAutograd_atol_rtol_float_linalg_matrix_rank(self, atol, rtol, hermitian);
}
at::Tensor & linalg_matrix_rank_out(at::Tensor & out, const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian) {
return wrapper_CompositeImplicitAutograd_atol_rtol_float_out_linalg_matrix_rank_out(self, atol, rtol, hermitian, out);
}
at::Tensor & linalg_matrix_rank_outf(const at::Tensor & self, c10::optional<double> atol, c10::optional<double> rtol, bool hermitian, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_atol_rtol_float_out_linalg_matrix_rank_out(self, atol, rtol, hermitian, out);
}
at::Tensor linalg_matrix_rank(const at::Tensor & self, double tol, bool hermitian) {
return wrapper_CompositeImplicitAutograd__linalg_matrix_rank(self, tol, hermitian);
}
at::Tensor & linalg_matrix_rank_out(at::Tensor & out, const at::Tensor & self, double tol, bool hermitian) {
return wrapper_CompositeImplicitAutograd_out_linalg_matrix_rank_out(self, tol, hermitian, out);
}
at::Tensor & linalg_matrix_rank_outf(const at::Tensor & self, double tol, bool hermitian, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_matrix_rank_out(self, tol, hermitian, out);
}
at::Tensor linalg_matrix_rank(const at::Tensor & input, const at::Tensor & tol, bool hermitian) {
return wrapper_CompositeImplicitAutograd_tol_tensor_linalg_matrix_rank(input, tol, hermitian);
}
at::Tensor & linalg_matrix_rank_out(at::Tensor & out, const at::Tensor & input, const at::Tensor & tol, bool hermitian) {
return wrapper_CompositeImplicitAutograd_out_tol_tensor_linalg_matrix_rank_out(input, tol, hermitian, out);
}
at::Tensor & linalg_matrix_rank_outf(const at::Tensor & input, const at::Tensor & tol, bool hermitian, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_tol_tensor_linalg_matrix_rank_out(input, tol, hermitian, out);
}
at::Tensor linalg_multi_dot(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd__linalg_multi_dot(tensors);
}
at::Tensor & linalg_multi_dot_out(at::Tensor & out, at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd_out_linalg_multi_dot_out(tensors, out);
}
at::Tensor & linalg_multi_dot_outf(at::TensorList tensors, at::Tensor & out) {
return wrapper_CompositeImplicitAutograd_out_linalg_multi_dot_out(tensors, out);
}
at::Tensor nested_to_padded_tensor(const at::Tensor & self, double padding, at::OptionalIntArrayRef output_size) {
return wrapper_CompositeImplicitAutograd__nested_to_padded_tensor(self, padding, output_size);
}
at::Tensor _test_serialization_subcmul(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_CompositeImplicitAutograd___test_serialization_subcmul(self, other, alpha);
}
at::Tensor _test_string_default(const at::Tensor & dummy, c10::string_view a, c10::string_view b) {
return wrapper_CompositeImplicitAutograd___test_string_default(dummy, a, b);
}
at::Tensor _test_ambiguous_defaults(const at::Tensor & dummy, int64_t a, int64_t b) {
return wrapper_CompositeImplicitAutograd_a__test_ambiguous_defaults(dummy, a, b);
}
at::Tensor _test_ambiguous_defaults(const at::Tensor & dummy, int64_t a, c10::string_view b) {
return wrapper_CompositeImplicitAutograd_b__test_ambiguous_defaults(dummy, a, b);
}
at::Tensor _test_autograd_multiple_dispatch(const at::Tensor & self, bool b) {
return wrapper_CompositeImplicitAutograd_ntonly__test_autograd_multiple_dispatch(self, b);
}
at::Tensor pad_sequence(at::TensorList sequences, bool batch_first, double padding_value) {
return wrapper_CompositeImplicitAutograd__pad_sequence(sequences, batch_first, padding_value);
}
at::Tensor flatten_dense_tensors(at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd__flatten_dense_tensors(tensors);
}
::std::vector<at::Tensor> unflatten_dense_tensors(const at::Tensor & flat, at::TensorList tensors) {
return wrapper_CompositeImplicitAutograd__unflatten_dense_tensors(flat, tensors);
}
at::Tensor scaled_dot_product_attention(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & attn_mask, double dropout_p, bool is_causal, c10::optional<double> scale) {
return wrapper_CompositeImplicitAutograd__scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal, scale);
}
::std::tuple<at::Tensor,at::Tensor> _scaled_dot_product_attention_math(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & attn_mask, double dropout_p, bool is_causal, const c10::optional<at::Tensor> & dropout_mask, c10::optional<double> scale) {
return wrapper_CompositeImplicitAutograd___scaled_dot_product_attention_math(query, key, value, attn_mask, dropout_p, is_causal, dropout_mask, scale);
}
void _propagate_xla_data(const at::Tensor & input, const at::Tensor & output) {
return wrapper_CompositeImplicitAutograd___propagate_xla_data(input, output);
}
} // namespace compositeimplicitautograd
} // namespace at
