#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
// @generated by torchgen/gen.py from RegisterFunctionalization.cpp

#include <ATen/core/LegacyTypeDispatch.h>
#include <ATen/EmptyTensor.h>
#include <ATen/FunctionalTensorWrapper.h>
#include <ATen/FunctionalInverses.h>
#include <ATen/MemoryOverlap.h>
#include <torch/library.h>

#ifndef AT_PER_OPERATOR_HEADERS
#include <ATen/Operators.h>
#include <ATen/NativeFunctions.h>
#else
// needed for the meta tensor calls to get stride info in functionalization
#include <ATen/ops/empty_strided_native.h>
// needed for special handling of copy_().
// See Note [functionalizating copy_() and not preserving strides]
#include <ATen/ops/to_ops.h>
#include <ATen/ops/expand_copy_ops.h>

#include <ATen/ops/_cudnn_rnn_flatten_weight_native.h>
#include <ATen/ops/_cudnn_rnn_flatten_weight_ops.h>
#include <ATen/ops/_cudnn_rnn_flatten_weight_native.h>
#include <ATen/ops/_cudnn_rnn_flatten_weight_ops.h>
#include <ATen/ops/abs_native.h>
#include <ATen/ops/abs_ops.h>
#include <ATen/ops/abs_native.h>
#include <ATen/ops/abs_ops.h>
#include <ATen/ops/abs_native.h>
#include <ATen/ops/abs_ops.h>
#include <ATen/ops/absolute_native.h>
#include <ATen/ops/absolute_ops.h>
#include <ATen/ops/absolute_native.h>
#include <ATen/ops/absolute_ops.h>
#include <ATen/ops/absolute_native.h>
#include <ATen/ops/absolute_ops.h>
#include <ATen/ops/_conj_physical_native.h>
#include <ATen/ops/_conj_physical_ops.h>
#include <ATen/ops/_conj_physical_native.h>
#include <ATen/ops/_conj_physical_ops.h>
#include <ATen/ops/acos_native.h>
#include <ATen/ops/acos_ops.h>
#include <ATen/ops/acos_native.h>
#include <ATen/ops/acos_ops.h>
#include <ATen/ops/acos_native.h>
#include <ATen/ops/acos_ops.h>
#include <ATen/ops/arccos_native.h>
#include <ATen/ops/arccos_ops.h>
#include <ATen/ops/arccos_native.h>
#include <ATen/ops/arccos_ops.h>
#include <ATen/ops/arccos_native.h>
#include <ATen/ops/arccos_ops.h>
#include <ATen/ops/affine_grid_generator_native.h>
#include <ATen/ops/affine_grid_generator_ops.h>
#include <ATen/ops/affine_grid_generator_native.h>
#include <ATen/ops/affine_grid_generator_ops.h>
#include <ATen/ops/arange_native.h>
#include <ATen/ops/arange_ops.h>
#include <ATen/ops/arange_native.h>
#include <ATen/ops/arange_ops.h>
#include <ATen/ops/arange_native.h>
#include <ATen/ops/arange_ops.h>
#include <ATen/ops/arange_native.h>
#include <ATen/ops/arange_ops.h>
#include <ATen/ops/arccosh_native.h>
#include <ATen/ops/arccosh_ops.h>
#include <ATen/ops/arccosh_native.h>
#include <ATen/ops/arccosh_ops.h>
#include <ATen/ops/arccosh_native.h>
#include <ATen/ops/arccosh_ops.h>
#include <ATen/ops/arcsinh_native.h>
#include <ATen/ops/arcsinh_ops.h>
#include <ATen/ops/arcsinh_native.h>
#include <ATen/ops/arcsinh_ops.h>
#include <ATen/ops/arcsinh_native.h>
#include <ATen/ops/arcsinh_ops.h>
#include <ATen/ops/atanh_native.h>
#include <ATen/ops/atanh_ops.h>
#include <ATen/ops/atanh_native.h>
#include <ATen/ops/atanh_ops.h>
#include <ATen/ops/atanh_native.h>
#include <ATen/ops/atanh_ops.h>
#include <ATen/ops/asin_native.h>
#include <ATen/ops/asin_ops.h>
#include <ATen/ops/asin_native.h>
#include <ATen/ops/asin_ops.h>
#include <ATen/ops/asin_native.h>
#include <ATen/ops/asin_ops.h>
#include <ATen/ops/binary_cross_entropy_backward_native.h>
#include <ATen/ops/binary_cross_entropy_backward_ops.h>
#include <ATen/ops/binary_cross_entropy_backward_native.h>
#include <ATen/ops/binary_cross_entropy_backward_ops.h>
#include <ATen/ops/binary_cross_entropy_with_logits_native.h>
#include <ATen/ops/binary_cross_entropy_with_logits_ops.h>
#include <ATen/ops/binary_cross_entropy_with_logits_native.h>
#include <ATen/ops/binary_cross_entropy_with_logits_ops.h>
#include <ATen/ops/logical_not_native.h>
#include <ATen/ops/logical_not_ops.h>
#include <ATen/ops/logical_not_native.h>
#include <ATen/ops/logical_not_ops.h>
#include <ATen/ops/logical_not_native.h>
#include <ATen/ops/logical_not_ops.h>
#include <ATen/ops/logical_and_native.h>
#include <ATen/ops/logical_and_ops.h>
#include <ATen/ops/logical_and_native.h>
#include <ATen/ops/logical_and_ops.h>
#include <ATen/ops/logical_and_native.h>
#include <ATen/ops/logical_and_ops.h>
#include <ATen/ops/concatenate_native.h>
#include <ATen/ops/concatenate_ops.h>
#include <ATen/ops/concatenate_native.h>
#include <ATen/ops/concatenate_ops.h>
#include <ATen/ops/concatenate_native.h>
#include <ATen/ops/concatenate_ops.h>
#include <ATen/ops/concatenate_native.h>
#include <ATen/ops/concatenate_ops.h>
#include <ATen/ops/block_diag_native.h>
#include <ATen/ops/block_diag_ops.h>
#include <ATen/ops/block_diag_native.h>
#include <ATen/ops/block_diag_ops.h>
#include <ATen/ops/chain_matmul_native.h>
#include <ATen/ops/chain_matmul_ops.h>
#include <ATen/ops/chain_matmul_native.h>
#include <ATen/ops/chain_matmul_ops.h>
#include <ATen/ops/convolution_backward_native.h>
#include <ATen/ops/convolution_backward_ops.h>
#include <ATen/ops/convolution_backward_native.h>
#include <ATen/ops/convolution_backward_ops.h>
#include <ATen/ops/_copy_from_native.h>
#include <ATen/ops/_copy_from_ops.h>
#include <ATen/ops/_copy_from_native.h>
#include <ATen/ops/_copy_from_ops.h>
#include <ATen/ops/cosh_native.h>
#include <ATen/ops/cosh_ops.h>
#include <ATen/ops/cosh_native.h>
#include <ATen/ops/cosh_ops.h>
#include <ATen/ops/cosh_native.h>
#include <ATen/ops/cosh_ops.h>
#include <ATen/ops/cudnn_convolution_transpose_native.h>
#include <ATen/ops/cudnn_convolution_transpose_ops.h>
#include <ATen/ops/cudnn_convolution_transpose_native.h>
#include <ATen/ops/cudnn_convolution_transpose_ops.h>
#include <ATen/ops/_mps_convolution_transpose_native.h>
#include <ATen/ops/_mps_convolution_transpose_ops.h>
#include <ATen/ops/_mps_convolution_transpose_native.h>
#include <ATen/ops/_mps_convolution_transpose_ops.h>
#include <ATen/ops/cudnn_grid_sampler_native.h>
#include <ATen/ops/cudnn_grid_sampler_ops.h>
#include <ATen/ops/cudnn_grid_sampler_native.h>
#include <ATen/ops/cudnn_grid_sampler_ops.h>
#include <ATen/ops/cumprod_native.h>
#include <ATen/ops/cumprod_ops.h>
#include <ATen/ops/cumprod_native.h>
#include <ATen/ops/cumprod_ops.h>
#include <ATen/ops/cumprod_native.h>
#include <ATen/ops/cumprod_ops.h>
#include <ATen/ops/cumprod_native.h>
#include <ATen/ops/cumprod_ops.h>
#include <ATen/ops/cumprod_native.h>
#include <ATen/ops/cumprod_ops.h>
#include <ATen/ops/cumprod_native.h>
#include <ATen/ops/cumprod_ops.h>
#include <ATen/ops/diag_embed_native.h>
#include <ATen/ops/diag_embed_ops.h>
#include <ATen/ops/diag_embed_native.h>
#include <ATen/ops/diag_embed_ops.h>
#include <ATen/ops/diagonal_backward_native.h>
#include <ATen/ops/diagonal_backward_ops.h>
#include <ATen/ops/diagonal_backward_native.h>
#include <ATen/ops/diagonal_backward_ops.h>
#include <ATen/ops/div_native.h>
#include <ATen/ops/div_ops.h>
#include <ATen/ops/div_native.h>
#include <ATen/ops/div_ops.h>
#include <ATen/ops/div_native.h>
#include <ATen/ops/div_ops.h>
#include <ATen/ops/div_native.h>
#include <ATen/ops/div_ops.h>
#include <ATen/ops/div_native.h>
#include <ATen/ops/div_ops.h>
#include <ATen/ops/div_native.h>
#include <ATen/ops/div_ops.h>
#include <ATen/ops/div_native.h>
#include <ATen/ops/div_ops.h>
#include <ATen/ops/div_native.h>
#include <ATen/ops/div_ops.h>
#include <ATen/ops/div_native.h>
#include <ATen/ops/div_ops.h>
#include <ATen/ops/div_native.h>
#include <ATen/ops/div_ops.h>
#include <ATen/ops/div_native.h>
#include <ATen/ops/div_ops.h>
#include <ATen/ops/div_native.h>
#include <ATen/ops/div_ops.h>
#include <ATen/ops/_embedding_bag_native.h>
#include <ATen/ops/_embedding_bag_ops.h>
#include <ATen/ops/_embedding_bag_native.h>
#include <ATen/ops/_embedding_bag_ops.h>
#include <ATen/ops/_embedding_bag_per_sample_weights_backward_native.h>
#include <ATen/ops/_embedding_bag_per_sample_weights_backward_ops.h>
#include <ATen/ops/_embedding_bag_per_sample_weights_backward_native.h>
#include <ATen/ops/_embedding_bag_per_sample_weights_backward_ops.h>
#include <ATen/ops/new_full_native.h>
#include <ATen/ops/new_full_ops.h>
#include <ATen/ops/new_full_native.h>
#include <ATen/ops/new_full_ops.h>
#include <ATen/ops/empty_quantized_native.h>
#include <ATen/ops/empty_quantized_ops.h>
#include <ATen/ops/empty_quantized_native.h>
#include <ATen/ops/empty_quantized_ops.h>
#include <ATen/ops/empty_strided_native.h>
#include <ATen/ops/empty_strided_ops.h>
#include <ATen/ops/empty_strided_native.h>
#include <ATen/ops/empty_strided_ops.h>
#include <ATen/ops/exp_native.h>
#include <ATen/ops/exp_ops.h>
#include <ATen/ops/exp_native.h>
#include <ATen/ops/exp_ops.h>
#include <ATen/ops/exp_native.h>
#include <ATen/ops/exp_ops.h>
#include <ATen/ops/expm1_native.h>
#include <ATen/ops/expm1_ops.h>
#include <ATen/ops/expm1_native.h>
#include <ATen/ops/expm1_ops.h>
#include <ATen/ops/expm1_native.h>
#include <ATen/ops/expm1_ops.h>
#include <ATen/ops/fill_native.h>
#include <ATen/ops/fill_ops.h>
#include <ATen/ops/fill_native.h>
#include <ATen/ops/fill_ops.h>
#include <ATen/ops/fill_native.h>
#include <ATen/ops/fill_ops.h>
#include <ATen/ops/fill_native.h>
#include <ATen/ops/fill_ops.h>
#include <ATen/ops/fill_native.h>
#include <ATen/ops/fill_ops.h>
#include <ATen/ops/fill_native.h>
#include <ATen/ops/fill_ops.h>
#include <ATen/ops/floor_native.h>
#include <ATen/ops/floor_ops.h>
#include <ATen/ops/floor_native.h>
#include <ATen/ops/floor_ops.h>
#include <ATen/ops/floor_native.h>
#include <ATen/ops/floor_ops.h>
#include <ATen/ops/floor_divide_native.h>
#include <ATen/ops/floor_divide_ops.h>
#include <ATen/ops/floor_divide_native.h>
#include <ATen/ops/floor_divide_ops.h>
#include <ATen/ops/floor_divide_native.h>
#include <ATen/ops/floor_divide_ops.h>
#include <ATen/ops/floor_divide_native.h>
#include <ATen/ops/floor_divide_ops.h>
#include <ATen/ops/floor_divide_native.h>
#include <ATen/ops/floor_divide_ops.h>
#include <ATen/ops/floor_divide_native.h>
#include <ATen/ops/floor_divide_ops.h>
#include <ATen/ops/full_native.h>
#include <ATen/ops/full_ops.h>
#include <ATen/ops/full_native.h>
#include <ATen/ops/full_ops.h>
#include <ATen/ops/full_native.h>
#include <ATen/ops/full_ops.h>
#include <ATen/ops/full_native.h>
#include <ATen/ops/full_ops.h>
#include <ATen/ops/from_file_native.h>
#include <ATen/ops/from_file_ops.h>
#include <ATen/ops/from_file_native.h>
#include <ATen/ops/from_file_ops.h>
#include <ATen/ops/grid_sampler_2d_native.h>
#include <ATen/ops/grid_sampler_2d_ops.h>
#include <ATen/ops/grid_sampler_2d_native.h>
#include <ATen/ops/grid_sampler_2d_ops.h>
#include <ATen/ops/grid_sampler_3d_native.h>
#include <ATen/ops/grid_sampler_3d_ops.h>
#include <ATen/ops/grid_sampler_3d_native.h>
#include <ATen/ops/grid_sampler_3d_ops.h>
#include <ATen/ops/hamming_window_native.h>
#include <ATen/ops/hamming_window_ops.h>
#include <ATen/ops/hamming_window_native.h>
#include <ATen/ops/hamming_window_ops.h>
#include <ATen/ops/hamming_window_native.h>
#include <ATen/ops/hamming_window_ops.h>
#include <ATen/ops/hamming_window_native.h>
#include <ATen/ops/hamming_window_ops.h>
#include <ATen/ops/hamming_window_native.h>
#include <ATen/ops/hamming_window_ops.h>
#include <ATen/ops/hamming_window_native.h>
#include <ATen/ops/hamming_window_ops.h>
#include <ATen/ops/hamming_window_native.h>
#include <ATen/ops/hamming_window_ops.h>
#include <ATen/ops/hamming_window_native.h>
#include <ATen/ops/hamming_window_ops.h>
#include <ATen/ops/native_group_norm_native.h>
#include <ATen/ops/native_group_norm_ops.h>
#include <ATen/ops/native_group_norm_native.h>
#include <ATen/ops/native_group_norm_ops.h>
#include <ATen/ops/_fft_c2r_native.h>
#include <ATen/ops/_fft_c2r_ops.h>
#include <ATen/ops/_fft_c2r_native.h>
#include <ATen/ops/_fft_c2r_ops.h>
#include <ATen/ops/isnan_native.h>
#include <ATen/ops/isnan_ops.h>
#include <ATen/ops/isnan_native.h>
#include <ATen/ops/isnan_ops.h>
#include <ATen/ops/ldexp_native.h>
#include <ATen/ops/ldexp_ops.h>
#include <ATen/ops/ldexp_native.h>
#include <ATen/ops/ldexp_ops.h>
#include <ATen/ops/ldexp_native.h>
#include <ATen/ops/ldexp_ops.h>
#include <ATen/ops/log2_native.h>
#include <ATen/ops/log2_ops.h>
#include <ATen/ops/log2_native.h>
#include <ATen/ops/log2_ops.h>
#include <ATen/ops/log2_native.h>
#include <ATen/ops/log2_ops.h>
#include <ATen/ops/logaddexp2_native.h>
#include <ATen/ops/logaddexp2_ops.h>
#include <ATen/ops/logaddexp2_native.h>
#include <ATen/ops/logaddexp2_ops.h>
#include <ATen/ops/xlogy_native.h>
#include <ATen/ops/xlogy_ops.h>
#include <ATen/ops/xlogy_native.h>
#include <ATen/ops/xlogy_ops.h>
#include <ATen/ops/xlogy_native.h>
#include <ATen/ops/xlogy_ops.h>
#include <ATen/ops/xlogy_native.h>
#include <ATen/ops/xlogy_ops.h>
#include <ATen/ops/xlogy_native.h>
#include <ATen/ops/xlogy_ops.h>
#include <ATen/ops/xlogy_native.h>
#include <ATen/ops/xlogy_ops.h>
#include <ATen/ops/xlogy_native.h>
#include <ATen/ops/xlogy_ops.h>
#include <ATen/ops/xlogy_native.h>
#include <ATen/ops/xlogy_ops.h>
#include <ATen/ops/logspace_native.h>
#include <ATen/ops/logspace_ops.h>
#include <ATen/ops/logspace_native.h>
#include <ATen/ops/logspace_ops.h>
#include <ATen/ops/logspace_native.h>
#include <ATen/ops/logspace_ops.h>
#include <ATen/ops/logspace_native.h>
#include <ATen/ops/logspace_ops.h>
#include <ATen/ops/logspace_native.h>
#include <ATen/ops/logspace_ops.h>
#include <ATen/ops/logspace_native.h>
#include <ATen/ops/logspace_ops.h>
#include <ATen/ops/logspace_native.h>
#include <ATen/ops/logspace_ops.h>
#include <ATen/ops/logspace_native.h>
#include <ATen/ops/logspace_ops.h>
#include <ATen/ops/matrix_power_native.h>
#include <ATen/ops/matrix_power_ops.h>
#include <ATen/ops/matrix_power_native.h>
#include <ATen/ops/matrix_power_ops.h>
#include <ATen/ops/_aminmax_native.h>
#include <ATen/ops/_aminmax_ops.h>
#include <ATen/ops/_aminmax_native.h>
#include <ATen/ops/_aminmax_ops.h>
#include <ATen/ops/_aminmax_native.h>
#include <ATen/ops/_aminmax_ops.h>
#include <ATen/ops/_aminmax_native.h>
#include <ATen/ops/_aminmax_ops.h>
#include <ATen/ops/aminmax_native.h>
#include <ATen/ops/aminmax_ops.h>
#include <ATen/ops/aminmax_native.h>
#include <ATen/ops/aminmax_ops.h>
#include <ATen/ops/_compute_linear_combination_native.h>
#include <ATen/ops/_compute_linear_combination_ops.h>
#include <ATen/ops/_compute_linear_combination_native.h>
#include <ATen/ops/_compute_linear_combination_ops.h>
#include <ATen/ops/mkldnn_max_pool3d_backward_native.h>
#include <ATen/ops/mkldnn_max_pool3d_backward_ops.h>
#include <ATen/ops/mkldnn_max_pool3d_backward_native.h>
#include <ATen/ops/mkldnn_max_pool3d_backward_ops.h>
#include <ATen/ops/min_native.h>
#include <ATen/ops/min_ops.h>
#include <ATen/ops/min_native.h>
#include <ATen/ops/min_ops.h>
#include <ATen/ops/min_native.h>
#include <ATen/ops/min_ops.h>
#include <ATen/ops/min_native.h>
#include <ATen/ops/min_ops.h>
#include <ATen/ops/mps_convolution_backward_native.h>
#include <ATen/ops/mps_convolution_backward_ops.h>
#include <ATen/ops/mps_convolution_backward_native.h>
#include <ATen/ops/mps_convolution_backward_ops.h>
#include <ATen/ops/miopen_rnn_native.h>
#include <ATen/ops/miopen_rnn_ops.h>
#include <ATen/ops/miopen_rnn_native.h>
#include <ATen/ops/miopen_rnn_ops.h>
#include <ATen/ops/mv_native.h>
#include <ATen/ops/mv_ops.h>
#include <ATen/ops/mv_native.h>
#include <ATen/ops/mv_ops.h>
#include <ATen/ops/_native_batch_norm_legit_native.h>
#include <ATen/ops/_native_batch_norm_legit_ops.h>
#include <ATen/ops/_native_batch_norm_legit_native.h>
#include <ATen/ops/_native_batch_norm_legit_ops.h>
#include <ATen/ops/_native_batch_norm_legit_native.h>
#include <ATen/ops/_native_batch_norm_legit_ops.h>
#include <ATen/ops/_native_batch_norm_legit_native.h>
#include <ATen/ops/_native_batch_norm_legit_ops.h>
#include <ATen/ops/_native_batch_norm_legit_native.h>
#include <ATen/ops/_native_batch_norm_legit_ops.h>
#include <ATen/ops/batch_norm_stats_native.h>
#include <ATen/ops/batch_norm_stats_ops.h>
#include <ATen/ops/batch_norm_stats_native.h>
#include <ATen/ops/batch_norm_stats_ops.h>
#include <ATen/ops/batch_norm_backward_elemt_native.h>
#include <ATen/ops/batch_norm_backward_elemt_ops.h>
#include <ATen/ops/batch_norm_backward_elemt_native.h>
#include <ATen/ops/batch_norm_backward_elemt_ops.h>
#include <ATen/ops/_euclidean_dist_native.h>
#include <ATen/ops/_euclidean_dist_ops.h>
#include <ATen/ops/_euclidean_dist_native.h>
#include <ATen/ops/_euclidean_dist_ops.h>
#include <ATen/ops/_cdist_forward_native.h>
#include <ATen/ops/_cdist_forward_ops.h>
#include <ATen/ops/_cdist_forward_native.h>
#include <ATen/ops/_cdist_forward_ops.h>
#include <ATen/ops/_cdist_backward_native.h>
#include <ATen/ops/_cdist_backward_ops.h>
#include <ATen/ops/_cdist_backward_native.h>
#include <ATen/ops/_cdist_backward_ops.h>
#include <ATen/ops/pixel_unshuffle_native.h>
#include <ATen/ops/pixel_unshuffle_ops.h>
#include <ATen/ops/pixel_unshuffle_native.h>
#include <ATen/ops/pixel_unshuffle_ops.h>
#include <ATen/ops/rad2deg_native.h>
#include <ATen/ops/rad2deg_ops.h>
#include <ATen/ops/rad2deg_native.h>
#include <ATen/ops/rad2deg_ops.h>
#include <ATen/ops/rad2deg_native.h>
#include <ATen/ops/rad2deg_ops.h>
#include <ATen/ops/scalar_tensor_native.h>
#include <ATen/ops/scalar_tensor_ops.h>
#include <ATen/ops/scalar_tensor_native.h>
#include <ATen/ops/scalar_tensor_ops.h>
#include <ATen/ops/rand_native.h>
#include <ATen/ops/rand_ops.h>
#include <ATen/ops/rand_native.h>
#include <ATen/ops/rand_ops.h>
#include <ATen/ops/rand_native.h>
#include <ATen/ops/rand_ops.h>
#include <ATen/ops/rand_native.h>
#include <ATen/ops/rand_ops.h>
#include <ATen/ops/rand_native.h>
#include <ATen/ops/rand_ops.h>
#include <ATen/ops/rand_native.h>
#include <ATen/ops/rand_ops.h>
#include <ATen/ops/rand_native.h>
#include <ATen/ops/rand_ops.h>
#include <ATen/ops/rand_native.h>
#include <ATen/ops/rand_ops.h>
#include <ATen/ops/rand_like_native.h>
#include <ATen/ops/rand_like_ops.h>
#include <ATen/ops/rand_like_native.h>
#include <ATen/ops/rand_like_ops.h>
#include <ATen/ops/relu_native.h>
#include <ATen/ops/relu_ops.h>
#include <ATen/ops/relu_native.h>
#include <ATen/ops/relu_ops.h>
#include <ATen/ops/relu_native.h>
#include <ATen/ops/relu_ops.h>
#include <ATen/ops/logit_native.h>
#include <ATen/ops/logit_ops.h>
#include <ATen/ops/logit_native.h>
#include <ATen/ops/logit_ops.h>
#include <ATen/ops/logit_native.h>
#include <ATen/ops/logit_ops.h>
#include <ATen/ops/select_scatter_native.h>
#include <ATen/ops/select_scatter_ops.h>
#include <ATen/ops/select_scatter_native.h>
#include <ATen/ops/select_scatter_ops.h>
#include <ATen/ops/softmax_native.h>
#include <ATen/ops/softmax_ops.h>
#include <ATen/ops/softmax_native.h>
#include <ATen/ops/softmax_ops.h>
#include <ATen/ops/stack_native.h>
#include <ATen/ops/stack_ops.h>
#include <ATen/ops/stack_native.h>
#include <ATen/ops/stack_ops.h>
#include <ATen/ops/vstack_native.h>
#include <ATen/ops/vstack_ops.h>
#include <ATen/ops/vstack_native.h>
#include <ATen/ops/vstack_ops.h>
#include <ATen/ops/nansum_native.h>
#include <ATen/ops/nansum_ops.h>
#include <ATen/ops/nansum_native.h>
#include <ATen/ops/nansum_ops.h>
#include <ATen/ops/sqrt_native.h>
#include <ATen/ops/sqrt_ops.h>
#include <ATen/ops/sqrt_native.h>
#include <ATen/ops/sqrt_ops.h>
#include <ATen/ops/sqrt_native.h>
#include <ATen/ops/sqrt_ops.h>
#include <ATen/ops/prod_native.h>
#include <ATen/ops/prod_ops.h>
#include <ATen/ops/prod_native.h>
#include <ATen/ops/prod_ops.h>
#include <ATen/ops/prod_native.h>
#include <ATen/ops/prod_ops.h>
#include <ATen/ops/prod_native.h>
#include <ATen/ops/prod_ops.h>
#include <ATen/ops/prod_native.h>
#include <ATen/ops/prod_ops.h>
#include <ATen/ops/prod_native.h>
#include <ATen/ops/prod_ops.h>
#include <ATen/ops/threshold_backward_native.h>
#include <ATen/ops/threshold_backward_ops.h>
#include <ATen/ops/threshold_backward_native.h>
#include <ATen/ops/threshold_backward_ops.h>
#include <ATen/ops/_transform_bias_rescale_qkv_native.h>
#include <ATen/ops/_transform_bias_rescale_qkv_ops.h>
#include <ATen/ops/_transform_bias_rescale_qkv_native.h>
#include <ATen/ops/_transform_bias_rescale_qkv_ops.h>
#include <ATen/ops/_nested_from_padded_native.h>
#include <ATen/ops/_nested_from_padded_ops.h>
#include <ATen/ops/_nested_from_padded_native.h>
#include <ATen/ops/_nested_from_padded_ops.h>
#include <ATen/ops/_nested_tensor_size_native.h>
#include <ATen/ops/_nested_tensor_size_ops.h>
#include <ATen/ops/_nested_tensor_size_native.h>
#include <ATen/ops/_nested_tensor_size_ops.h>
#include <ATen/ops/_nested_view_from_buffer_copy_native.h>
#include <ATen/ops/_nested_view_from_buffer_copy_ops.h>
#include <ATen/ops/_nested_view_from_buffer_copy_native.h>
#include <ATen/ops/_nested_view_from_buffer_copy_ops.h>
#include <ATen/ops/trunc_native.h>
#include <ATen/ops/trunc_ops.h>
#include <ATen/ops/trunc_native.h>
#include <ATen/ops/trunc_ops.h>
#include <ATen/ops/trunc_native.h>
#include <ATen/ops/trunc_ops.h>
#include <ATen/ops/unique_dim_consecutive_native.h>
#include <ATen/ops/unique_dim_consecutive_ops.h>
#include <ATen/ops/unique_dim_consecutive_native.h>
#include <ATen/ops/unique_dim_consecutive_ops.h>
#include <ATen/ops/where_native.h>
#include <ATen/ops/where_ops.h>
#include <ATen/ops/where_native.h>
#include <ATen/ops/where_ops.h>
#include <ATen/ops/_weight_norm_interface_backward_native.h>
#include <ATen/ops/_weight_norm_interface_backward_ops.h>
#include <ATen/ops/_weight_norm_interface_backward_native.h>
#include <ATen/ops/_weight_norm_interface_backward_ops.h>
#include <ATen/ops/_sample_dirichlet_native.h>
#include <ATen/ops/_sample_dirichlet_ops.h>
#include <ATen/ops/_sample_dirichlet_native.h>
#include <ATen/ops/_sample_dirichlet_ops.h>
#include <ATen/ops/binomial_native.h>
#include <ATen/ops/binomial_ops.h>
#include <ATen/ops/binomial_native.h>
#include <ATen/ops/binomial_ops.h>
#include <ATen/ops/native_norm_native.h>
#include <ATen/ops/native_norm_ops.h>
#include <ATen/ops/native_norm_native.h>
#include <ATen/ops/native_norm_ops.h>
#include <ATen/ops/native_norm_native.h>
#include <ATen/ops/native_norm_ops.h>
#include <ATen/ops/native_norm_native.h>
#include <ATen/ops/native_norm_ops.h>
#include <ATen/ops/_sparse_sum_native.h>
#include <ATen/ops/_sparse_sum_ops.h>
#include <ATen/ops/_sparse_sum_native.h>
#include <ATen/ops/_sparse_sum_ops.h>
#include <ATen/ops/_sparse_sum_backward_native.h>
#include <ATen/ops/_sparse_sum_backward_ops.h>
#include <ATen/ops/_sparse_sum_backward_native.h>
#include <ATen/ops/_sparse_sum_backward_ops.h>
#include <ATen/ops/_sparse_softmax_native.h>
#include <ATen/ops/_sparse_softmax_ops.h>
#include <ATen/ops/_sparse_softmax_native.h>
#include <ATen/ops/_sparse_softmax_ops.h>
#include <ATen/ops/clone_native.h>
#include <ATen/ops/clone_ops.h>
#include <ATen/ops/clone_native.h>
#include <ATen/ops/clone_ops.h>
#include <ATen/ops/resize_as_native.h>
#include <ATen/ops/resize_as_ops.h>
#include <ATen/ops/resize_as_native.h>
#include <ATen/ops/resize_as_ops.h>
#include <ATen/ops/resize_as_native.h>
#include <ATen/ops/resize_as_ops.h>
#include <ATen/ops/zero_native.h>
#include <ATen/ops/zero_ops.h>
#include <ATen/ops/zero_native.h>
#include <ATen/ops/zero_ops.h>
#include <ATen/ops/zero_native.h>
#include <ATen/ops/zero_ops.h>
#include <ATen/ops/heaviside_native.h>
#include <ATen/ops/heaviside_ops.h>
#include <ATen/ops/heaviside_native.h>
#include <ATen/ops/heaviside_ops.h>
#include <ATen/ops/heaviside_native.h>
#include <ATen/ops/heaviside_ops.h>
#include <ATen/ops/addmm_native.h>
#include <ATen/ops/addmm_ops.h>
#include <ATen/ops/addmm_native.h>
#include <ATen/ops/addmm_ops.h>
#include <ATen/ops/addmm_native.h>
#include <ATen/ops/addmm_ops.h>
#include <ATen/ops/_scaled_mm_native.h>
#include <ATen/ops/_scaled_mm_ops.h>
#include <ATen/ops/_scaled_mm_native.h>
#include <ATen/ops/_scaled_mm_ops.h>
#include <ATen/ops/_sparse_coo_tensor_with_dims_native.h>
#include <ATen/ops/_sparse_coo_tensor_with_dims_ops.h>
#include <ATen/ops/_sparse_coo_tensor_with_dims_native.h>
#include <ATen/ops/_sparse_coo_tensor_with_dims_ops.h>
#include <ATen/ops/_sparse_coo_tensor_with_dims_and_tensors_native.h>
#include <ATen/ops/_sparse_coo_tensor_with_dims_and_tensors_ops.h>
#include <ATen/ops/_sparse_coo_tensor_with_dims_and_tensors_native.h>
#include <ATen/ops/_sparse_coo_tensor_with_dims_and_tensors_ops.h>
#include <ATen/ops/sparse_resize_and_clear_native.h>
#include <ATen/ops/sparse_resize_and_clear_ops.h>
#include <ATen/ops/sparse_resize_and_clear_native.h>
#include <ATen/ops/sparse_resize_and_clear_ops.h>
#include <ATen/ops/sparse_resize_and_clear_native.h>
#include <ATen/ops/sparse_resize_and_clear_ops.h>
#include <ATen/ops/hspmm_native.h>
#include <ATen/ops/hspmm_ops.h>
#include <ATen/ops/hspmm_native.h>
#include <ATen/ops/hspmm_ops.h>
#include <ATen/ops/_to_sparse_bsc_native.h>
#include <ATen/ops/_to_sparse_bsc_ops.h>
#include <ATen/ops/_to_sparse_bsc_native.h>
#include <ATen/ops/_to_sparse_bsc_ops.h>
#include <ATen/ops/to_mkldnn_native.h>
#include <ATen/ops/to_mkldnn_ops.h>
#include <ATen/ops/to_mkldnn_native.h>
#include <ATen/ops/to_mkldnn_ops.h>
#include <ATen/ops/mkldnn_reorder_conv3d_weight_native.h>
#include <ATen/ops/mkldnn_reorder_conv3d_weight_ops.h>
#include <ATen/ops/mkldnn_reorder_conv3d_weight_native.h>
#include <ATen/ops/mkldnn_reorder_conv3d_weight_ops.h>
#include <ATen/ops/q_per_channel_scales_native.h>
#include <ATen/ops/q_per_channel_scales_ops.h>
#include <ATen/ops/q_per_channel_scales_native.h>
#include <ATen/ops/q_per_channel_scales_ops.h>
#include <ATen/ops/int_repr_native.h>
#include <ATen/ops/int_repr_ops.h>
#include <ATen/ops/int_repr_native.h>
#include <ATen/ops/int_repr_ops.h>
#include <ATen/ops/_make_per_tensor_quantized_tensor_native.h>
#include <ATen/ops/_make_per_tensor_quantized_tensor_ops.h>
#include <ATen/ops/_make_per_tensor_quantized_tensor_native.h>
#include <ATen/ops/_make_per_tensor_quantized_tensor_ops.h>
#include <ATen/ops/_thnn_fused_lstm_cell_native.h>
#include <ATen/ops/_thnn_fused_lstm_cell_ops.h>
#include <ATen/ops/_thnn_fused_lstm_cell_native.h>
#include <ATen/ops/_thnn_fused_lstm_cell_ops.h>
#include <ATen/ops/_thnn_fused_lstm_cell_backward_impl_native.h>
#include <ATen/ops/_thnn_fused_lstm_cell_backward_impl_ops.h>
#include <ATen/ops/_thnn_fused_lstm_cell_backward_impl_native.h>
#include <ATen/ops/_thnn_fused_lstm_cell_backward_impl_ops.h>
#include <ATen/ops/masked_fill_native.h>
#include <ATen/ops/masked_fill_ops.h>
#include <ATen/ops/masked_fill_native.h>
#include <ATen/ops/masked_fill_ops.h>
#include <ATen/ops/masked_fill_native.h>
#include <ATen/ops/masked_fill_ops.h>
#include <ATen/ops/masked_fill_native.h>
#include <ATen/ops/masked_fill_ops.h>
#include <ATen/ops/masked_fill_native.h>
#include <ATen/ops/masked_fill_ops.h>
#include <ATen/ops/masked_fill_native.h>
#include <ATen/ops/masked_fill_ops.h>
#include <ATen/ops/_masked_softmax_native.h>
#include <ATen/ops/_masked_softmax_ops.h>
#include <ATen/ops/_masked_softmax_native.h>
#include <ATen/ops/_masked_softmax_ops.h>
#include <ATen/ops/bitwise_right_shift_native.h>
#include <ATen/ops/bitwise_right_shift_ops.h>
#include <ATen/ops/bitwise_right_shift_native.h>
#include <ATen/ops/bitwise_right_shift_ops.h>
#include <ATen/ops/bitwise_right_shift_native.h>
#include <ATen/ops/bitwise_right_shift_ops.h>
#include <ATen/ops/bitwise_right_shift_native.h>
#include <ATen/ops/bitwise_right_shift_ops.h>
#include <ATen/ops/bitwise_right_shift_native.h>
#include <ATen/ops/bitwise_right_shift_ops.h>
#include <ATen/ops/bitwise_right_shift_native.h>
#include <ATen/ops/bitwise_right_shift_ops.h>
#include <ATen/ops/bitwise_right_shift_native.h>
#include <ATen/ops/bitwise_right_shift_ops.h>
#include <ATen/ops/bitwise_right_shift_native.h>
#include <ATen/ops/bitwise_right_shift_ops.h>
#include <ATen/ops/cauchy_native.h>
#include <ATen/ops/cauchy_ops.h>
#include <ATen/ops/cauchy_native.h>
#include <ATen/ops/cauchy_ops.h>
#include <ATen/ops/cauchy_native.h>
#include <ATen/ops/cauchy_ops.h>
#include <ATen/ops/log_normal_native.h>
#include <ATen/ops/log_normal_ops.h>
#include <ATen/ops/log_normal_native.h>
#include <ATen/ops/log_normal_ops.h>
#include <ATen/ops/log_normal_native.h>
#include <ATen/ops/log_normal_ops.h>
#include <ATen/ops/diag_native.h>
#include <ATen/ops/diag_ops.h>
#include <ATen/ops/diag_native.h>
#include <ATen/ops/diag_ops.h>
#include <ATen/ops/ne_native.h>
#include <ATen/ops/ne_ops.h>
#include <ATen/ops/ne_native.h>
#include <ATen/ops/ne_ops.h>
#include <ATen/ops/ne_native.h>
#include <ATen/ops/ne_ops.h>
#include <ATen/ops/ne_native.h>
#include <ATen/ops/ne_ops.h>
#include <ATen/ops/ne_native.h>
#include <ATen/ops/ne_ops.h>
#include <ATen/ops/ne_native.h>
#include <ATen/ops/ne_ops.h>
#include <ATen/ops/not_equal_native.h>
#include <ATen/ops/not_equal_ops.h>
#include <ATen/ops/not_equal_native.h>
#include <ATen/ops/not_equal_ops.h>
#include <ATen/ops/not_equal_native.h>
#include <ATen/ops/not_equal_ops.h>
#include <ATen/ops/not_equal_native.h>
#include <ATen/ops/not_equal_ops.h>
#include <ATen/ops/not_equal_native.h>
#include <ATen/ops/not_equal_ops.h>
#include <ATen/ops/not_equal_native.h>
#include <ATen/ops/not_equal_ops.h>
#include <ATen/ops/addcmul_native.h>
#include <ATen/ops/addcmul_ops.h>
#include <ATen/ops/addcmul_native.h>
#include <ATen/ops/addcmul_ops.h>
#include <ATen/ops/addcmul_native.h>
#include <ATen/ops/addcmul_ops.h>
#include <ATen/ops/ormqr_native.h>
#include <ATen/ops/ormqr_ops.h>
#include <ATen/ops/ormqr_native.h>
#include <ATen/ops/ormqr_ops.h>
#include <ATen/ops/lu_unpack_native.h>
#include <ATen/ops/lu_unpack_ops.h>
#include <ATen/ops/lu_unpack_native.h>
#include <ATen/ops/lu_unpack_ops.h>
#include <ATen/ops/dist_native.h>
#include <ATen/ops/dist_ops.h>
#include <ATen/ops/dist_native.h>
#include <ATen/ops/dist_ops.h>
#include <ATen/ops/arctan2_native.h>
#include <ATen/ops/arctan2_ops.h>
#include <ATen/ops/arctan2_native.h>
#include <ATen/ops/arctan2_ops.h>
#include <ATen/ops/arctan2_native.h>
#include <ATen/ops/arctan2_ops.h>
#include <ATen/ops/histc_native.h>
#include <ATen/ops/histc_ops.h>
#include <ATen/ops/histc_native.h>
#include <ATen/ops/histc_ops.h>
#include <ATen/ops/_histogramdd_from_bin_cts_native.h>
#include <ATen/ops/_histogramdd_from_bin_cts_ops.h>
#include <ATen/ops/_histogramdd_from_bin_cts_native.h>
#include <ATen/ops/_histogramdd_from_bin_cts_ops.h>
#include <ATen/ops/hypot_native.h>
#include <ATen/ops/hypot_ops.h>
#include <ATen/ops/hypot_native.h>
#include <ATen/ops/hypot_ops.h>
#include <ATen/ops/hypot_native.h>
#include <ATen/ops/hypot_ops.h>
#include <ATen/ops/min_native.h>
#include <ATen/ops/min_ops.h>
#include <ATen/ops/min_native.h>
#include <ATen/ops/min_ops.h>
#include <ATen/ops/min_native.h>
#include <ATen/ops/min_ops.h>
#include <ATen/ops/min_native.h>
#include <ATen/ops/min_ops.h>
#include <ATen/ops/msort_native.h>
#include <ATen/ops/msort_ops.h>
#include <ATen/ops/msort_native.h>
#include <ATen/ops/msort_ops.h>
#include <ATen/ops/_foreach_add_native.h>
#include <ATen/ops/_foreach_add_ops.h>
#include <ATen/ops/_foreach_add_native.h>
#include <ATen/ops/_foreach_add_ops.h>
#include <ATen/ops/_foreach_add_native.h>
#include <ATen/ops/_foreach_add_ops.h>
#include <ATen/ops/_foreach_add_native.h>
#include <ATen/ops/_foreach_add_ops.h>
#include <ATen/ops/_foreach_add_native.h>
#include <ATen/ops/_foreach_add_ops.h>
#include <ATen/ops/_foreach_add_native.h>
#include <ATen/ops/_foreach_add_ops.h>
#include <ATen/ops/_foreach_add_native.h>
#include <ATen/ops/_foreach_add_ops.h>
#include <ATen/ops/_foreach_add_native.h>
#include <ATen/ops/_foreach_add_ops.h>
#include <ATen/ops/_foreach_add_native.h>
#include <ATen/ops/_foreach_add_ops.h>
#include <ATen/ops/_foreach_add_native.h>
#include <ATen/ops/_foreach_add_ops.h>
#include <ATen/ops/_foreach_add_native.h>
#include <ATen/ops/_foreach_add_ops.h>
#include <ATen/ops/_foreach_add_native.h>
#include <ATen/ops/_foreach_add_ops.h>
#include <ATen/ops/_foreach_clamp_min_native.h>
#include <ATen/ops/_foreach_clamp_min_ops.h>
#include <ATen/ops/_foreach_clamp_min_native.h>
#include <ATen/ops/_foreach_clamp_min_ops.h>
#include <ATen/ops/_foreach_clamp_min_native.h>
#include <ATen/ops/_foreach_clamp_min_ops.h>
#include <ATen/ops/_foreach_clamp_min_native.h>
#include <ATen/ops/_foreach_clamp_min_ops.h>
#include <ATen/ops/_foreach_clamp_min_native.h>
#include <ATen/ops/_foreach_clamp_min_ops.h>
#include <ATen/ops/_foreach_clamp_min_native.h>
#include <ATen/ops/_foreach_clamp_min_ops.h>
#include <ATen/ops/_foreach_clamp_min_native.h>
#include <ATen/ops/_foreach_clamp_min_ops.h>
#include <ATen/ops/_foreach_clamp_min_native.h>
#include <ATen/ops/_foreach_clamp_min_ops.h>
#include <ATen/ops/_foreach_clamp_min_native.h>
#include <ATen/ops/_foreach_clamp_min_ops.h>
#include <ATen/ops/_foreach_addcdiv_native.h>
#include <ATen/ops/_foreach_addcdiv_ops.h>
#include <ATen/ops/_foreach_addcdiv_native.h>
#include <ATen/ops/_foreach_addcdiv_ops.h>
#include <ATen/ops/_foreach_addcdiv_native.h>
#include <ATen/ops/_foreach_addcdiv_ops.h>
#include <ATen/ops/_foreach_addcdiv_native.h>
#include <ATen/ops/_foreach_addcdiv_ops.h>
#include <ATen/ops/_foreach_addcdiv_native.h>
#include <ATen/ops/_foreach_addcdiv_ops.h>
#include <ATen/ops/_foreach_addcdiv_native.h>
#include <ATen/ops/_foreach_addcdiv_ops.h>
#include <ATen/ops/_foreach_addcdiv_native.h>
#include <ATen/ops/_foreach_addcdiv_ops.h>
#include <ATen/ops/_foreach_addcdiv_native.h>
#include <ATen/ops/_foreach_addcdiv_ops.h>
#include <ATen/ops/_foreach_addcdiv_native.h>
#include <ATen/ops/_foreach_addcdiv_ops.h>
#include <ATen/ops/_foreach_addcmul_native.h>
#include <ATen/ops/_foreach_addcmul_ops.h>
#include <ATen/ops/_foreach_addcmul_native.h>
#include <ATen/ops/_foreach_addcmul_ops.h>
#include <ATen/ops/_foreach_addcmul_native.h>
#include <ATen/ops/_foreach_addcmul_ops.h>
#include <ATen/ops/_foreach_addcmul_native.h>
#include <ATen/ops/_foreach_addcmul_ops.h>
#include <ATen/ops/_foreach_addcmul_native.h>
#include <ATen/ops/_foreach_addcmul_ops.h>
#include <ATen/ops/_foreach_addcmul_native.h>
#include <ATen/ops/_foreach_addcmul_ops.h>
#include <ATen/ops/_foreach_addcmul_native.h>
#include <ATen/ops/_foreach_addcmul_ops.h>
#include <ATen/ops/_foreach_addcmul_native.h>
#include <ATen/ops/_foreach_addcmul_ops.h>
#include <ATen/ops/_foreach_addcmul_native.h>
#include <ATen/ops/_foreach_addcmul_ops.h>
#include <ATen/ops/_foreach_asin_native.h>
#include <ATen/ops/_foreach_asin_ops.h>
#include <ATen/ops/_foreach_asin_native.h>
#include <ATen/ops/_foreach_asin_ops.h>
#include <ATen/ops/_foreach_asin_native.h>
#include <ATen/ops/_foreach_asin_ops.h>
#include <ATen/ops/_foreach_ceil_native.h>
#include <ATen/ops/_foreach_ceil_ops.h>
#include <ATen/ops/_foreach_ceil_native.h>
#include <ATen/ops/_foreach_ceil_ops.h>
#include <ATen/ops/_foreach_ceil_native.h>
#include <ATen/ops/_foreach_ceil_ops.h>
#include <ATen/ops/_foreach_cosh_native.h>
#include <ATen/ops/_foreach_cosh_ops.h>
#include <ATen/ops/_foreach_cosh_native.h>
#include <ATen/ops/_foreach_cosh_ops.h>
#include <ATen/ops/_foreach_cosh_native.h>
#include <ATen/ops/_foreach_cosh_ops.h>
#include <ATen/ops/_foreach_log1p_native.h>
#include <ATen/ops/_foreach_log1p_ops.h>
#include <ATen/ops/_foreach_log1p_native.h>
#include <ATen/ops/_foreach_log1p_ops.h>
#include <ATen/ops/_foreach_log1p_native.h>
#include <ATen/ops/_foreach_log1p_ops.h>
#include <ATen/ops/_foreach_log2_native.h>
#include <ATen/ops/_foreach_log2_ops.h>
#include <ATen/ops/_foreach_log2_native.h>
#include <ATen/ops/_foreach_log2_ops.h>
#include <ATen/ops/_foreach_log2_native.h>
#include <ATen/ops/_foreach_log2_ops.h>
#include <ATen/ops/_foreach_pow_native.h>
#include <ATen/ops/_foreach_pow_ops.h>
#include <ATen/ops/_foreach_pow_native.h>
#include <ATen/ops/_foreach_pow_ops.h>
#include <ATen/ops/_foreach_pow_native.h>
#include <ATen/ops/_foreach_pow_ops.h>
#include <ATen/ops/_foreach_pow_native.h>
#include <ATen/ops/_foreach_pow_ops.h>
#include <ATen/ops/_foreach_pow_native.h>
#include <ATen/ops/_foreach_pow_ops.h>
#include <ATen/ops/_foreach_pow_native.h>
#include <ATen/ops/_foreach_pow_ops.h>
#include <ATen/ops/_foreach_pow_native.h>
#include <ATen/ops/_foreach_pow_ops.h>
#include <ATen/ops/_foreach_pow_native.h>
#include <ATen/ops/_foreach_pow_ops.h>
#include <ATen/ops/_foreach_pow_native.h>
#include <ATen/ops/_foreach_pow_ops.h>
#include <ATen/ops/_foreach_round_native.h>
#include <ATen/ops/_foreach_round_ops.h>
#include <ATen/ops/_foreach_round_native.h>
#include <ATen/ops/_foreach_round_ops.h>
#include <ATen/ops/_foreach_round_native.h>
#include <ATen/ops/_foreach_round_ops.h>
#include <ATen/ops/_foreach_zero_native.h>
#include <ATen/ops/_foreach_zero_ops.h>
#include <ATen/ops/_foreach_zero_native.h>
#include <ATen/ops/_foreach_zero_ops.h>
#include <ATen/ops/_foreach_zero_native.h>
#include <ATen/ops/_foreach_zero_ops.h>
#include <ATen/ops/bucketize_native.h>
#include <ATen/ops/bucketize_ops.h>
#include <ATen/ops/bucketize_native.h>
#include <ATen/ops/bucketize_ops.h>
#include <ATen/ops/bucketize_native.h>
#include <ATen/ops/bucketize_ops.h>
#include <ATen/ops/bucketize_native.h>
#include <ATen/ops/bucketize_ops.h>
#include <ATen/ops/mse_loss_native.h>
#include <ATen/ops/mse_loss_ops.h>
#include <ATen/ops/mse_loss_native.h>
#include <ATen/ops/mse_loss_ops.h>
#include <ATen/ops/multi_margin_loss_native.h>
#include <ATen/ops/multi_margin_loss_ops.h>
#include <ATen/ops/multi_margin_loss_native.h>
#include <ATen/ops/multi_margin_loss_ops.h>
#include <ATen/ops/multilabel_margin_loss_native.h>
#include <ATen/ops/multilabel_margin_loss_ops.h>
#include <ATen/ops/multilabel_margin_loss_native.h>
#include <ATen/ops/multilabel_margin_loss_ops.h>
#include <ATen/ops/multilabel_margin_loss_forward_native.h>
#include <ATen/ops/multilabel_margin_loss_forward_ops.h>
#include <ATen/ops/multilabel_margin_loss_forward_native.h>
#include <ATen/ops/multilabel_margin_loss_forward_ops.h>
#include <ATen/ops/nll_loss2d_forward_native.h>
#include <ATen/ops/nll_loss2d_forward_ops.h>
#include <ATen/ops/nll_loss2d_forward_native.h>
#include <ATen/ops/nll_loss2d_forward_ops.h>
#include <ATen/ops/soft_margin_loss_native.h>
#include <ATen/ops/soft_margin_loss_ops.h>
#include <ATen/ops/soft_margin_loss_native.h>
#include <ATen/ops/soft_margin_loss_ops.h>
#include <ATen/ops/glu_backward_native.h>
#include <ATen/ops/glu_backward_ops.h>
#include <ATen/ops/glu_backward_native.h>
#include <ATen/ops/glu_backward_ops.h>
#include <ATen/ops/glu_backward_jvp_native.h>
#include <ATen/ops/glu_backward_jvp_ops.h>
#include <ATen/ops/glu_backward_jvp_native.h>
#include <ATen/ops/glu_backward_jvp_ops.h>
#include <ATen/ops/hardtanh_native.h>
#include <ATen/ops/hardtanh_ops.h>
#include <ATen/ops/hardtanh_native.h>
#include <ATen/ops/hardtanh_ops.h>
#include <ATen/ops/hardtanh_native.h>
#include <ATen/ops/hardtanh_ops.h>
#include <ATen/ops/hardtanh_backward_native.h>
#include <ATen/ops/hardtanh_backward_ops.h>
#include <ATen/ops/hardtanh_backward_native.h>
#include <ATen/ops/hardtanh_backward_ops.h>
#include <ATen/ops/leaky_relu_native.h>
#include <ATen/ops/leaky_relu_ops.h>
#include <ATen/ops/leaky_relu_native.h>
#include <ATen/ops/leaky_relu_ops.h>
#include <ATen/ops/leaky_relu_native.h>
#include <ATen/ops/leaky_relu_ops.h>
#include <ATen/ops/log_sigmoid_backward_native.h>
#include <ATen/ops/log_sigmoid_backward_ops.h>
#include <ATen/ops/log_sigmoid_backward_native.h>
#include <ATen/ops/log_sigmoid_backward_ops.h>
#include <ATen/ops/softplus_native.h>
#include <ATen/ops/softplus_ops.h>
#include <ATen/ops/softplus_native.h>
#include <ATen/ops/softplus_ops.h>
#include <ATen/ops/adaptive_avg_pool2d_native.h>
#include <ATen/ops/adaptive_avg_pool2d_ops.h>
#include <ATen/ops/adaptive_avg_pool2d_native.h>
#include <ATen/ops/adaptive_avg_pool2d_ops.h>
#include <ATen/ops/_adaptive_avg_pool3d_backward_native.h>
#include <ATen/ops/_adaptive_avg_pool3d_backward_ops.h>
#include <ATen/ops/_adaptive_avg_pool3d_backward_native.h>
#include <ATen/ops/_adaptive_avg_pool3d_backward_ops.h>
#include <ATen/ops/adaptive_max_pool2d_backward_native.h>
#include <ATen/ops/adaptive_max_pool2d_backward_ops.h>
#include <ATen/ops/adaptive_max_pool2d_backward_native.h>
#include <ATen/ops/adaptive_max_pool2d_backward_ops.h>
#include <ATen/ops/fractional_max_pool2d_backward_native.h>
#include <ATen/ops/fractional_max_pool2d_backward_ops.h>
#include <ATen/ops/fractional_max_pool2d_backward_native.h>
#include <ATen/ops/fractional_max_pool2d_backward_ops.h>
#include <ATen/ops/fractional_max_pool3d_backward_native.h>
#include <ATen/ops/fractional_max_pool3d_backward_ops.h>
#include <ATen/ops/fractional_max_pool3d_backward_native.h>
#include <ATen/ops/fractional_max_pool3d_backward_ops.h>
#include <ATen/ops/max_pool3d_with_indices_native.h>
#include <ATen/ops/max_pool3d_with_indices_ops.h>
#include <ATen/ops/max_pool3d_with_indices_native.h>
#include <ATen/ops/max_pool3d_with_indices_ops.h>
#include <ATen/ops/max_pool3d_with_indices_backward_native.h>
#include <ATen/ops/max_pool3d_with_indices_backward_ops.h>
#include <ATen/ops/max_pool3d_with_indices_backward_native.h>
#include <ATen/ops/max_pool3d_with_indices_backward_ops.h>
#include <ATen/ops/max_unpool2d_native.h>
#include <ATen/ops/max_unpool2d_ops.h>
#include <ATen/ops/max_unpool2d_native.h>
#include <ATen/ops/max_unpool2d_ops.h>
#include <ATen/ops/reflection_pad2d_backward_native.h>
#include <ATen/ops/reflection_pad2d_backward_ops.h>
#include <ATen/ops/reflection_pad2d_backward_native.h>
#include <ATen/ops/reflection_pad2d_backward_ops.h>
#include <ATen/ops/upsample_bilinear2d_native.h>
#include <ATen/ops/upsample_bilinear2d_ops.h>
#include <ATen/ops/upsample_bilinear2d_native.h>
#include <ATen/ops/upsample_bilinear2d_ops.h>
#include <ATen/ops/upsample_bilinear2d_backward_native.h>
#include <ATen/ops/upsample_bilinear2d_backward_ops.h>
#include <ATen/ops/upsample_bilinear2d_backward_native.h>
#include <ATen/ops/upsample_bilinear2d_backward_ops.h>
#include <ATen/ops/_upsample_bilinear2d_aa_native.h>
#include <ATen/ops/_upsample_bilinear2d_aa_ops.h>
#include <ATen/ops/_upsample_bilinear2d_aa_native.h>
#include <ATen/ops/_upsample_bilinear2d_aa_ops.h>
#include <ATen/ops/upsample_trilinear3d_backward_native.h>
#include <ATen/ops/upsample_trilinear3d_backward_ops.h>
#include <ATen/ops/upsample_trilinear3d_backward_native.h>
#include <ATen/ops/upsample_trilinear3d_backward_ops.h>
#include <ATen/ops/_upsample_nearest_exact3d_native.h>
#include <ATen/ops/_upsample_nearest_exact3d_ops.h>
#include <ATen/ops/_upsample_nearest_exact3d_native.h>
#include <ATen/ops/_upsample_nearest_exact3d_ops.h>
#include <ATen/ops/upsample_nearest3d_backward_native.h>
#include <ATen/ops/upsample_nearest3d_backward_ops.h>
#include <ATen/ops/upsample_nearest3d_backward_native.h>
#include <ATen/ops/upsample_nearest3d_backward_ops.h>
#include <ATen/ops/logit_backward_native.h>
#include <ATen/ops/logit_backward_ops.h>
#include <ATen/ops/logit_backward_native.h>
#include <ATen/ops/logit_backward_ops.h>
#include <ATen/ops/thnn_conv2d_native.h>
#include <ATen/ops/thnn_conv2d_ops.h>
#include <ATen/ops/thnn_conv2d_native.h>
#include <ATen/ops/thnn_conv2d_ops.h>
#include <ATen/ops/_slow_conv2d_backward_native.h>
#include <ATen/ops/_slow_conv2d_backward_ops.h>
#include <ATen/ops/_slow_conv2d_backward_native.h>
#include <ATen/ops/_slow_conv2d_backward_ops.h>
#include <ATen/ops/slow_conv3d_native.h>
#include <ATen/ops/slow_conv3d_ops.h>
#include <ATen/ops/slow_conv3d_native.h>
#include <ATen/ops/slow_conv3d_ops.h>
#include <ATen/ops/slow_conv3d_forward_native.h>
#include <ATen/ops/slow_conv3d_forward_ops.h>
#include <ATen/ops/slow_conv3d_forward_native.h>
#include <ATen/ops/slow_conv3d_forward_ops.h>
#include <ATen/ops/slow_conv_dilated3d_native.h>
#include <ATen/ops/slow_conv_dilated3d_ops.h>
#include <ATen/ops/slow_conv_dilated3d_native.h>
#include <ATen/ops/slow_conv_dilated3d_ops.h>
#include <ATen/ops/special_log_ndtr_native.h>
#include <ATen/ops/special_log_ndtr_ops.h>
#include <ATen/ops/special_log_ndtr_native.h>
#include <ATen/ops/special_log_ndtr_ops.h>
#include <ATen/ops/special_exp2_native.h>
#include <ATen/ops/special_exp2_ops.h>
#include <ATen/ops/special_exp2_native.h>
#include <ATen/ops/special_exp2_ops.h>
#include <ATen/ops/special_digamma_native.h>
#include <ATen/ops/special_digamma_ops.h>
#include <ATen/ops/special_digamma_native.h>
#include <ATen/ops/special_digamma_ops.h>
#include <ATen/ops/special_gammaln_native.h>
#include <ATen/ops/special_gammaln_ops.h>
#include <ATen/ops/special_gammaln_native.h>
#include <ATen/ops/special_gammaln_ops.h>
#include <ATen/ops/special_erfcx_native.h>
#include <ATen/ops/special_erfcx_ops.h>
#include <ATen/ops/special_erfcx_native.h>
#include <ATen/ops/special_erfcx_ops.h>
#include <ATen/ops/special_xlog1py_native.h>
#include <ATen/ops/special_xlog1py_ops.h>
#include <ATen/ops/special_xlog1py_native.h>
#include <ATen/ops/special_xlog1py_ops.h>
#include <ATen/ops/special_xlog1py_native.h>
#include <ATen/ops/special_xlog1py_ops.h>
#include <ATen/ops/special_xlog1py_native.h>
#include <ATen/ops/special_xlog1py_ops.h>
#include <ATen/ops/special_xlog1py_native.h>
#include <ATen/ops/special_xlog1py_ops.h>
#include <ATen/ops/special_xlog1py_native.h>
#include <ATen/ops/special_xlog1py_ops.h>
#include <ATen/ops/special_i1_native.h>
#include <ATen/ops/special_i1_ops.h>
#include <ATen/ops/special_i1_native.h>
#include <ATen/ops/special_i1_ops.h>
#include <ATen/ops/special_i1e_native.h>
#include <ATen/ops/special_i1e_ops.h>
#include <ATen/ops/special_i1e_native.h>
#include <ATen/ops/special_i1e_ops.h>
#include <ATen/ops/fft_fft_native.h>
#include <ATen/ops/fft_fft_ops.h>
#include <ATen/ops/fft_fft_native.h>
#include <ATen/ops/fft_fft_ops.h>
#include <ATen/ops/fft_rfft_native.h>
#include <ATen/ops/fft_rfft_ops.h>
#include <ATen/ops/fft_rfft_native.h>
#include <ATen/ops/fft_rfft_ops.h>
#include <ATen/ops/fft_hfft_native.h>
#include <ATen/ops/fft_hfft_ops.h>
#include <ATen/ops/fft_hfft_native.h>
#include <ATen/ops/fft_hfft_ops.h>
#include <ATen/ops/fft_hfft2_native.h>
#include <ATen/ops/fft_hfft2_ops.h>
#include <ATen/ops/fft_hfft2_native.h>
#include <ATen/ops/fft_hfft2_ops.h>
#include <ATen/ops/fft_ifftn_native.h>
#include <ATen/ops/fft_ifftn_ops.h>
#include <ATen/ops/fft_ifftn_native.h>
#include <ATen/ops/fft_ifftn_ops.h>
#include <ATen/ops/fft_rfftn_native.h>
#include <ATen/ops/fft_rfftn_ops.h>
#include <ATen/ops/fft_rfftn_native.h>
#include <ATen/ops/fft_rfftn_ops.h>
#include <ATen/ops/fft_hfftn_native.h>
#include <ATen/ops/fft_hfftn_ops.h>
#include <ATen/ops/fft_hfftn_native.h>
#include <ATen/ops/fft_hfftn_ops.h>
#include <ATen/ops/fft_fftfreq_native.h>
#include <ATen/ops/fft_fftfreq_ops.h>
#include <ATen/ops/fft_fftfreq_native.h>
#include <ATen/ops/fft_fftfreq_ops.h>
#include <ATen/ops/linalg_det_native.h>
#include <ATen/ops/linalg_det_ops.h>
#include <ATen/ops/linalg_det_native.h>
#include <ATen/ops/linalg_det_ops.h>
#include <ATen/ops/linalg_ldl_factor_ex_native.h>
#include <ATen/ops/linalg_ldl_factor_ex_ops.h>
#include <ATen/ops/linalg_ldl_factor_ex_native.h>
#include <ATen/ops/linalg_ldl_factor_ex_ops.h>
#include <ATen/ops/linalg_lstsq_native.h>
#include <ATen/ops/linalg_lstsq_ops.h>
#include <ATen/ops/linalg_lstsq_native.h>
#include <ATen/ops/linalg_lstsq_ops.h>
#include <ATen/ops/linalg_matrix_exp_native.h>
#include <ATen/ops/linalg_matrix_exp_ops.h>
#include <ATen/ops/linalg_matrix_exp_native.h>
#include <ATen/ops/linalg_matrix_exp_ops.h>
#include <ATen/ops/linalg_slogdet_native.h>
#include <ATen/ops/linalg_slogdet_ops.h>
#include <ATen/ops/linalg_slogdet_native.h>
#include <ATen/ops/linalg_slogdet_ops.h>
#include <ATen/ops/_linalg_eigh_native.h>
#include <ATen/ops/_linalg_eigh_ops.h>
#include <ATen/ops/_linalg_eigh_native.h>
#include <ATen/ops/_linalg_eigh_ops.h>
#include <ATen/ops/inner_native.h>
#include <ATen/ops/inner_ops.h>
#include <ATen/ops/inner_native.h>
#include <ATen/ops/inner_ops.h>
#include <ATen/ops/linalg_matrix_norm_native.h>
#include <ATen/ops/linalg_matrix_norm_ops.h>
#include <ATen/ops/linalg_matrix_norm_native.h>
#include <ATen/ops/linalg_matrix_norm_ops.h>
#include <ATen/ops/linalg_matrix_norm_native.h>
#include <ATen/ops/linalg_matrix_norm_ops.h>
#include <ATen/ops/linalg_matrix_norm_native.h>
#include <ATen/ops/linalg_matrix_norm_ops.h>
#include <ATen/ops/linalg_tensorinv_native.h>
#include <ATen/ops/linalg_tensorinv_ops.h>
#include <ATen/ops/linalg_tensorinv_native.h>
#include <ATen/ops/linalg_tensorinv_ops.h>
#include <ATen/ops/linalg_matrix_power_native.h>
#include <ATen/ops/linalg_matrix_power_ops.h>
#include <ATen/ops/linalg_matrix_power_native.h>
#include <ATen/ops/linalg_matrix_power_ops.h>
#include <ATen/ops/_make_dual_copy_native.h>
#include <ATen/ops/_make_dual_copy_ops.h>
#include <ATen/ops/_make_dual_copy_native.h>
#include <ATen/ops/_make_dual_copy_ops.h>
#include <ATen/ops/view_as_real_copy_native.h>
#include <ATen/ops/view_as_real_copy_ops.h>
#include <ATen/ops/view_as_real_copy_native.h>
#include <ATen/ops/view_as_real_copy_ops.h>
#include <ATen/ops/view_as_complex_copy_native.h>
#include <ATen/ops/view_as_complex_copy_ops.h>
#include <ATen/ops/view_as_complex_copy_native.h>
#include <ATen/ops/view_as_complex_copy_ops.h>
#include <ATen/ops/_conj_copy_native.h>
#include <ATen/ops/_conj_copy_ops.h>
#include <ATen/ops/_conj_copy_native.h>
#include <ATen/ops/_conj_copy_ops.h>
#include <ATen/ops/_neg_view_copy_native.h>
#include <ATen/ops/_neg_view_copy_ops.h>
#include <ATen/ops/_neg_view_copy_native.h>
#include <ATen/ops/_neg_view_copy_ops.h>
#include <ATen/ops/permute_copy_native.h>
#include <ATen/ops/permute_copy_ops.h>
#include <ATen/ops/permute_copy_native.h>
#include <ATen/ops/permute_copy_ops.h>
#include <ATen/ops/split_copy_native.h>
#include <ATen/ops/split_copy_ops.h>
#include <ATen/ops/split_copy_native.h>
#include <ATen/ops/split_copy_ops.h>
#include <ATen/ops/t_copy_native.h>
#include <ATen/ops/t_copy_ops.h>
#include <ATen/ops/t_copy_native.h>
#include <ATen/ops/t_copy_ops.h>
#include <ATen/ops/ccol_indices_copy_native.h>
#include <ATen/ops/ccol_indices_copy_ops.h>
#include <ATen/ops/ccol_indices_copy_native.h>
#include <ATen/ops/ccol_indices_copy_ops.h>
#include <ATen/ops/unfold_copy_native.h>
#include <ATen/ops/unfold_copy_ops.h>
#include <ATen/ops/unfold_copy_native.h>
#include <ATen/ops/unfold_copy_ops.h>
#include <ATen/ops/_transformer_encoder_layer_fwd_native.h>
#include <ATen/ops/_transformer_encoder_layer_fwd_ops.h>
#include <ATen/ops/_transformer_encoder_layer_fwd_native.h>
#include <ATen/ops/_transformer_encoder_layer_fwd_ops.h>
#include <ATen/ops/_native_multi_head_attention_native.h>
#include <ATen/ops/_native_multi_head_attention_ops.h>
#include <ATen/ops/_native_multi_head_attention_native.h>
#include <ATen/ops/_native_multi_head_attention_ops.h>
#include <ATen/ops/special_modified_bessel_i1_native.h>
#include <ATen/ops/special_modified_bessel_i1_ops.h>
#include <ATen/ops/special_modified_bessel_i1_native.h>
#include <ATen/ops/special_modified_bessel_i1_ops.h>
#include <ATen/ops/special_modified_bessel_k0_native.h>
#include <ATen/ops/special_modified_bessel_k0_ops.h>
#include <ATen/ops/special_modified_bessel_k0_native.h>
#include <ATen/ops/special_modified_bessel_k0_ops.h>
#include <ATen/ops/_foobar_native.h>
#include <ATen/ops/_foobar_ops.h>
#include <ATen/ops/_foobar_native.h>
#include <ATen/ops/_foobar_ops.h>
#include <ATen/ops/view_as_real_native.h>
#include <ATen/ops/view_as_real_ops.h>
#include <ATen/ops/view_as_real_copy_native.h>
#include <ATen/ops/view_as_real_copy_ops.h>
#include <ATen/ops/broadcast_to_native.h>
#include <ATen/ops/broadcast_to_ops.h>
#include <ATen/ops/contiguous_native.h>
#include <ATen/ops/contiguous_ops.h>
#include <ATen/ops/flatten_native.h>
#include <ATen/ops/flatten_ops.h>
#include <ATen/ops/flatten_native.h>
#include <ATen/ops/flatten_ops.h>
#include <ATen/ops/flatten_native.h>
#include <ATen/ops/flatten_ops.h>
#include <ATen/ops/flatten_native.h>
#include <ATen/ops/flatten_ops.h>
#include <ATen/ops/moveaxis_native.h>
#include <ATen/ops/moveaxis_ops.h>
#include <ATen/ops/moveaxis_native.h>
#include <ATen/ops/moveaxis_ops.h>
#include <ATen/ops/mT_native.h>
#include <ATen/ops/mT_ops.h>
#include <ATen/ops/ravel_native.h>
#include <ATen/ops/ravel_ops.h>
#include <ATen/ops/reshape_native.h>
#include <ATen/ops/reshape_ops.h>
#include <ATen/ops/slice_native.h>
#include <ATen/ops/slice_ops.h>
#include <ATen/ops/slice_copy_native.h>
#include <ATen/ops/slice_copy_ops.h>
#include <ATen/ops/hsplit_native.h>
#include <ATen/ops/hsplit_ops.h>
#include <ATen/ops/hsplit_native.h>
#include <ATen/ops/hsplit_ops.h>
#include <ATen/ops/t_native.h>
#include <ATen/ops/t_ops.h>
#include <ATen/ops/t_copy_native.h>
#include <ATen/ops/t_copy_ops.h>
#include <ATen/ops/coalesce_native.h>
#include <ATen/ops/coalesce_ops.h>
#include <ATen/ops/_indices_native.h>
#include <ATen/ops/_indices_ops.h>
#include <ATen/ops/_indices_copy_native.h>
#include <ATen/ops/_indices_copy_ops.h>
#include <ATen/ops/indices_native.h>
#include <ATen/ops/indices_ops.h>
#include <ATen/ops/indices_copy_native.h>
#include <ATen/ops/indices_copy_ops.h>
#include <ATen/ops/crow_indices_native.h>
#include <ATen/ops/crow_indices_ops.h>
#include <ATen/ops/crow_indices_copy_native.h>
#include <ATen/ops/crow_indices_copy_ops.h>
#include <ATen/ops/col_indices_native.h>
#include <ATen/ops/col_indices_ops.h>
#include <ATen/ops/col_indices_copy_native.h>
#include <ATen/ops/col_indices_copy_ops.h>
#include <ATen/ops/_autocast_to_reduced_precision_native.h>
#include <ATen/ops/_autocast_to_reduced_precision_ops.h>
#include <ATen/ops/swapaxes_native.h>
#include <ATen/ops/swapaxes_ops.h>
#include <ATen/ops/unfold_native.h>
#include <ATen/ops/unfold_ops.h>
#include <ATen/ops/unfold_copy_native.h>
#include <ATen/ops/unfold_copy_ops.h>
#include <ATen/ops/_cast_Byte_native.h>
#include <ATen/ops/_cast_Byte_ops.h>
#include <ATen/ops/_cast_Float_native.h>
#include <ATen/ops/_cast_Float_ops.h>
#include <ATen/ops/is_leaf_native.h>
#include <ATen/ops/is_leaf_ops.h>
#include <ATen/ops/_assert_async_native.h>
#include <ATen/ops/_assert_async_ops.h>
#include <ATen/ops/_assert_async_native.h>
#include <ATen/ops/_assert_async_ops.h>
#include <ATen/ops/_use_cudnn_ctc_loss_native.h>
#include <ATen/ops/_use_cudnn_ctc_loss_ops.h>
#include <ATen/ops/_use_cudnn_ctc_loss_native.h>
#include <ATen/ops/_use_cudnn_ctc_loss_ops.h>
#include <ATen/ops/_reshape_from_tensor_native.h>
#include <ATen/ops/_reshape_from_tensor_ops.h>
#include <ATen/ops/dropout_native.h>
#include <ATen/ops/dropout_ops.h>
#include <ATen/ops/dropout_native.h>
#include <ATen/ops/dropout_ops.h>
#include <ATen/ops/adaptive_max_pool1d_native.h>
#include <ATen/ops/adaptive_max_pool1d_ops.h>
#include <ATen/ops/_is_any_true_native.h>
#include <ATen/ops/_is_any_true_ops.h>
#include <ATen/ops/arange_native.h>
#include <ATen/ops/arange_ops.h>
#include <ATen/ops/atleast_2d_native.h>
#include <ATen/ops/atleast_2d_ops.h>
#include <ATen/ops/atleast_2d_native.h>
#include <ATen/ops/atleast_2d_ops.h>
#include <ATen/ops/_batch_norm_impl_index_backward_native.h>
#include <ATen/ops/_batch_norm_impl_index_backward_ops.h>
#include <ATen/ops/_convolution_double_backward_native.h>
#include <ATen/ops/_convolution_double_backward_ops.h>
#include <ATen/ops/conv1d_native.h>
#include <ATen/ops/conv1d_ops.h>
#include <ATen/ops/conv1d_native.h>
#include <ATen/ops/conv1d_ops.h>
#include <ATen/ops/corrcoef_native.h>
#include <ATen/ops/corrcoef_ops.h>
#include <ATen/ops/cummaxmin_backward_native.h>
#include <ATen/ops/cummaxmin_backward_ops.h>
#include <ATen/ops/cumprod_backward_native.h>
#include <ATen/ops/cumprod_backward_ops.h>
#include <ATen/ops/embedding_sparse_backward_native.h>
#include <ATen/ops/embedding_sparse_backward_ops.h>
#include <ATen/ops/_rowwise_prune_native.h>
#include <ATen/ops/_rowwise_prune_ops.h>
#include <ATen/ops/_embedding_bag_sparse_backward_native.h>
#include <ATen/ops/_embedding_bag_sparse_backward_ops.h>
#include <ATen/ops/group_norm_native.h>
#include <ATen/ops/group_norm_ops.h>
#include <ATen/ops/_validate_compressed_sparse_indices_native.h>
#include <ATen/ops/_validate_compressed_sparse_indices_ops.h>
#include <ATen/ops/_unsafe_index_native.h>
#include <ATen/ops/_unsafe_index_ops.h>
#include <ATen/ops/is_distributed_native.h>
#include <ATen/ops/is_distributed_ops.h>
#include <ATen/ops/is_neg_native.h>
#include <ATen/ops/is_neg_ops.h>
#include <ATen/ops/is_same_size_native.h>
#include <ATen/ops/is_same_size_ops.h>
#include <ATen/ops/is_signed_native.h>
#include <ATen/ops/is_signed_ops.h>
#include <ATen/ops/_cslt_compress_native.h>
#include <ATen/ops/_cslt_compress_ops.h>
#include <ATen/ops/fbgemm_linear_fp16_weight_fp32_activation_native.h>
#include <ATen/ops/fbgemm_linear_fp16_weight_fp32_activation_ops.h>
#include <ATen/ops/matrix_exp_backward_native.h>
#include <ATen/ops/matrix_exp_backward_ops.h>
#include <ATen/ops/max_pool2d_native.h>
#include <ATen/ops/max_pool2d_ops.h>
#include <ATen/ops/miopen_convolution_add_relu_native.h>
#include <ATen/ops/miopen_convolution_add_relu_ops.h>
#include <ATen/ops/cosine_similarity_native.h>
#include <ATen/ops/cosine_similarity_ops.h>
#include <ATen/ops/native_channel_shuffle_native.h>
#include <ATen/ops/native_channel_shuffle_ops.h>
#include <ATen/ops/pinverse_native.h>
#include <ATen/ops/pinverse_ops.h>
#include <ATen/ops/rrelu_native.h>
#include <ATen/ops/rrelu_ops.h>
#include <ATen/ops/rrelu_native.h>
#include <ATen/ops/rrelu_ops.h>
#include <ATen/ops/prelu_native.h>
#include <ATen/ops/prelu_ops.h>
#include <ATen/ops/softmax_native.h>
#include <ATen/ops/softmax_ops.h>
#include <ATen/ops/stft_native.h>
#include <ATen/ops/stft_ops.h>
#include <ATen/ops/stft_native.h>
#include <ATen/ops/stft_ops.h>
#include <ATen/ops/tile_native.h>
#include <ATen/ops/tile_ops.h>
#include <ATen/ops/trapezoid_native.h>
#include <ATen/ops/trapezoid_ops.h>
#include <ATen/ops/trapezoid_native.h>
#include <ATen/ops/trapezoid_ops.h>
#include <ATen/ops/trapz_native.h>
#include <ATen/ops/trapz_ops.h>
#include <ATen/ops/trapz_native.h>
#include <ATen/ops/trapz_ops.h>
#include <ATen/ops/_has_compatible_shallow_copy_type_native.h>
#include <ATen/ops/_has_compatible_shallow_copy_type_ops.h>
#include <ATen/ops/vander_native.h>
#include <ATen/ops/vander_ops.h>
#include <ATen/ops/where_native.h>
#include <ATen/ops/where_ops.h>
#include <ATen/ops/where_native.h>
#include <ATen/ops/where_ops.h>
#include <ATen/ops/where_native.h>
#include <ATen/ops/where_ops.h>
#include <ATen/ops/where_native.h>
#include <ATen/ops/where_ops.h>
#include <ATen/ops/_weight_norm_native.h>
#include <ATen/ops/_weight_norm_ops.h>
#include <ATen/ops/_weight_norm_differentiable_backward_native.h>
#include <ATen/ops/_weight_norm_differentiable_backward_ops.h>
#include <ATen/ops/_sparse_sum_native.h>
#include <ATen/ops/_sparse_sum_ops.h>
#include <ATen/ops/_sparse_sum_native.h>
#include <ATen/ops/_sparse_sum_ops.h>
#include <ATen/ops/_sparse_sum_native.h>
#include <ATen/ops/_sparse_sum_ops.h>
#include <ATen/ops/_sparse_softmax_native.h>
#include <ATen/ops/_sparse_softmax_ops.h>
#include <ATen/ops/_sparse_softmax_native.h>
#include <ATen/ops/_sparse_softmax_ops.h>
#include <ATen/ops/_sparse_mm_reduce_impl_native.h>
#include <ATen/ops/_sparse_mm_reduce_impl_ops.h>
#include <ATen/ops/sparse_csr_tensor_native.h>
#include <ATen/ops/sparse_csr_tensor_ops.h>
#include <ATen/ops/sparse_bsc_tensor_native.h>
#include <ATen/ops/sparse_bsc_tensor_ops.h>
#include <ATen/ops/sparse_csr_tensor_native.h>
#include <ATen/ops/sparse_csr_tensor_ops.h>
#include <ATen/ops/sparse_bsc_tensor_native.h>
#include <ATen/ops/sparse_bsc_tensor_ops.h>
#include <ATen/ops/_sparse_csr_tensor_unsafe_native.h>
#include <ATen/ops/_sparse_csr_tensor_unsafe_ops.h>
#include <ATen/ops/_sparse_bsc_tensor_unsafe_native.h>
#include <ATen/ops/_sparse_bsc_tensor_unsafe_ops.h>
#include <ATen/ops/_validate_sparse_bsr_tensor_args_native.h>
#include <ATen/ops/_validate_sparse_bsr_tensor_args_ops.h>
#include <ATen/ops/sparse_dim_native.h>
#include <ATen/ops/sparse_dim_ops.h>
#include <ATen/ops/is_coalesced_native.h>
#include <ATen/ops/is_coalesced_ops.h>
#include <ATen/ops/to_sparse_native.h>
#include <ATen/ops/to_sparse_ops.h>
#include <ATen/ops/to_sparse_native.h>
#include <ATen/ops/to_sparse_ops.h>
#include <ATen/ops/to_sparse_bsr_native.h>
#include <ATen/ops/to_sparse_bsr_ops.h>
#include <ATen/ops/_to_sparse_semi_structured_native.h>
#include <ATen/ops/_to_sparse_semi_structured_ops.h>
#include <ATen/ops/q_scale_native.h>
#include <ATen/ops/q_scale_ops.h>
#include <ATen/ops/_saturate_weight_to_fp16_native.h>
#include <ATen/ops/_saturate_weight_to_fp16_ops.h>
#include <ATen/ops/item_native.h>
#include <ATen/ops/item_ops.h>
#include <ATen/ops/_thnn_fused_lstm_cell_backward_native.h>
#include <ATen/ops/_thnn_fused_lstm_cell_backward_ops.h>
#include <ATen/ops/_thnn_differentiable_lstm_cell_backward_native.h>
#include <ATen/ops/_thnn_differentiable_lstm_cell_backward_ops.h>
#include <ATen/ops/rnn_relu_native.h>
#include <ATen/ops/rnn_relu_ops.h>
#include <ATen/ops/rnn_relu_native.h>
#include <ATen/ops/rnn_relu_ops.h>
#include <ATen/ops/quantized_lstm_cell_native.h>
#include <ATen/ops/quantized_lstm_cell_ops.h>
#include <ATen/ops/quantized_rnn_relu_cell_native.h>
#include <ATen/ops/quantized_rnn_relu_cell_ops.h>
#include <ATen/ops/_pack_padded_sequence_backward_native.h>
#include <ATen/ops/_pack_padded_sequence_backward_ops.h>
#include <ATen/ops/histogramdd_native.h>
#include <ATen/ops/histogramdd_ops.h>
#include <ATen/ops/histogramdd_native.h>
#include <ATen/ops/histogramdd_ops.h>
#include <ATen/ops/histogramdd_native.h>
#include <ATen/ops/histogramdd_ops.h>
#include <ATen/ops/_foreach_pow_native.h>
#include <ATen/ops/_foreach_pow_ops.h>
#include <ATen/ops/l1_loss_native.h>
#include <ATen/ops/l1_loss_ops.h>
#include <ATen/ops/upsample_bilinear2d_native.h>
#include <ATen/ops/upsample_bilinear2d_ops.h>
#include <ATen/ops/_upsample_bilinear2d_aa_native.h>
#include <ATen/ops/_upsample_bilinear2d_aa_ops.h>
#include <ATen/ops/_upsample_nearest_exact3d_native.h>
#include <ATen/ops/_upsample_nearest_exact3d_ops.h>
#include <ATen/ops/_slow_conv2d_backward_native.h>
#include <ATen/ops/_slow_conv2d_backward_ops.h>
#include <ATen/ops/isfinite_native.h>
#include <ATen/ops/isfinite_ops.h>
#include <ATen/ops/special_log_softmax_native.h>
#include <ATen/ops/special_log_softmax_ops.h>
#include <ATen/ops/_nested_tensor_softmax_with_shape_native.h>
#include <ATen/ops/_nested_tensor_softmax_with_shape_ops.h>
#include <ATen/ops/scaled_dot_product_attention_native.h>
#include <ATen/ops/scaled_dot_product_attention_ops.h>
#include <ATen/ops/_fused_sdp_choice_native.h>
#include <ATen/ops/_fused_sdp_choice_ops.h>
#include <ATen/ops/_scaled_dot_product_attention_math_native.h>
#include <ATen/ops/_scaled_dot_product_attention_math_ops.h>
#include <ATen/ops/_fill_mem_eff_dropout_mask_native.h>
#include <ATen/ops/_fill_mem_eff_dropout_mask_ops.h>
#endif

namespace at {
namespace functionalization {

// This keyset is used by functionalization when it calls into meta kernels
// to accurately propagate stride metadata.
// Exclude any modes: the purpose of calling into meta kernels is only as an implementation
// detail to perform shape inference, and we don't want any modal keys to run.
// Specifically, we want to prevent functionalization and Python modes from running.
constexpr auto exclude_keys_for_meta_dispatch =
    c10::functorch_transforms_ks |
    c10::DispatchKeySet({
        c10::DispatchKey::FuncTorchDynamicLayerBackMode,
        c10::DispatchKey::FuncTorchDynamicLayerFrontMode,
        c10::DispatchKey::Python,
        c10::DispatchKey::PreDispatch,

    });

// Helper around at::has_internal_overlap.
// The ATen util is used in hot-path eager mode: it's always fast,
// but might return TOO_HARD sometimes.
// During functionalization, we're ok taking a bit longer
// to detect memory overlap.
inline bool has_internal_overlap_helper(const at::Tensor t) {
  auto has_overlap = at::has_internal_overlap(t);
  if (has_overlap == at::MemOverlap::Yes) return true;
  if (has_overlap == at::MemOverlap::No) return false;
  return false;
}


inline Tensor to_meta(const Tensor& t) {
    if (!t.defined()) return t;
    return at::native::empty_strided_meta_symint(t.sym_sizes(), t.sym_strides(),
/*dtype=*/c10::make_optional(t.scalar_type()), /*layout=*/c10::make_optional(t.layout()),
/*device=*/c10::make_optional(c10::Device(kMeta)), /*pin_memory=*/c10::nullopt);
}

inline c10::optional<Tensor> to_meta(const c10::optional<Tensor>& t) {
  if (t.has_value()) {
    return c10::make_optional<Tensor>(to_meta(*t));
  }
  return c10::nullopt;
}

inline std::vector<Tensor> to_meta(at::ITensorListRef t_list) {
  std::vector<Tensor> outputs;
  outputs.reserve(t_list.size());
  for (const auto& tensor : t_list) {
    outputs.push_back(to_meta(tensor));
  }
  return outputs;
}

inline c10::List<Tensor> to_meta(const c10::List<Tensor>& t_list) {
  c10::List<Tensor> outputs;
  outputs.reserve(t_list.size());
  for (const auto i : c10::irange(t_list.size())) {
    outputs.push_back(to_meta(t_list[i]));
  }
  return outputs;
}

inline c10::List<c10::optional<Tensor>> to_meta(const c10::List<c10::optional<Tensor>>& t_list) {
  c10::List<c10::optional<Tensor>> outputs;
  outputs.reserve(t_list.size());
  for (const auto i : c10::irange(t_list.size())) {
    outputs.push_back(to_meta(t_list[i]));
  }
  return outputs;
}



    at::Tensor & _cudnn_rnn_flatten_weight_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList weight_arr, int64_t weight_stride0, c10::SymInt input_size, int64_t mode, c10::SymInt hidden_size, c10::SymInt proj_size, int64_t num_layers, bool batch_first, bool bidirectional, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto weight_arr_meta = to_meta(weight_arr);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_cudnn_rnn_flatten_weight_out::call(weight_arr_meta, weight_stride0, input_size, mode, hidden_size, proj_size, num_layers, batch_first, bidirectional, out_meta);
      }
      
      ::std::vector<at::Tensor> weight_arr_;
      if (at::functionalization::impl::isFunctionalTensor(weight_arr)) {
        at::functionalization::impl::sync(weight_arr);
        weight_arr_ = at::functionalization::impl::from_functional_tensor(weight_arr);
      } else {
        weight_arr_ = weight_arr.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(weight_arr))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_cudnn_rnn_flatten_weight_out::call(weight_arr_, weight_stride0, input_size, mode, hidden_size, proj_size, num_layers, batch_first, bidirectional, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_cudnn_rnn_flatten_weight::call(weight_arr_, weight_stride0, input_size, mode, hidden_size, proj_size, num_layers, batch_first, bidirectional);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & abs_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::abs_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::abs_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::abs::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & abs_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::abs_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::abs_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::abs::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & absolute_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::absolute_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::absolute_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::absolute::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & absolute_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::absolute_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::absolute_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::absolute::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & _conj_physical_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_conj_physical_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_conj_physical_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_conj_physical::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & acos_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::acos_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::acos_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::acos::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & acos_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::acos_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::acos_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::acos::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & arccos_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arccos_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arccos_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arccos::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & arccos_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arccos_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arccos_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arccos::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & affine_grid_generator_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & theta, c10::SymIntArrayRef size, bool align_corners, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto theta_meta = to_meta(theta);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::affine_grid_generator_out::call(theta_meta, size, align_corners, out_meta);
      }
      
      at::Tensor theta_;
      if (at::functionalization::impl::isFunctionalTensor(theta)) {
        at::functionalization::impl::sync(theta);
        theta_ = at::functionalization::impl::from_functional_tensor(theta);
      } else {
        theta_ = theta;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || theta.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(theta))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::affine_grid_generator_out::call(theta_, size, align_corners, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::affine_grid_generator::call(theta_, size, align_corners);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & arange_out_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & end, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arange_out::call(end, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arange_out::call(end, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arange::call(end, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & arange_out_start_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arange_start_out::call(start, end, step, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arange_start_out::call(start, end, step, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arange_start_step::call(start, end, step, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & arccosh_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arccosh_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arccosh_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arccosh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & arccosh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arccosh_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arccosh_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arccosh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & arcsinh_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arcsinh_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arcsinh_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arcsinh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & arcsinh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arcsinh_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arcsinh_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arcsinh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & atanh_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::atanh_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::atanh_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::atanh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & atanh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::atanh_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::atanh_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::atanh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & asin_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::asin_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::asin_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::asin::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & asin_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::asin_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::asin_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::asin::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & binary_cross_entropy_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto weight_meta = to_meta(weight);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::binary_cross_entropy_backward_grad_input::call(grad_output_meta, self_meta, target_meta, weight_meta, reduction, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::binary_cross_entropy_backward_grad_input::call(grad_output_, self_, target_, weight_, reduction, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::binary_cross_entropy_backward::call(grad_output_, self_, target_, weight_, reduction);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & binary_cross_entropy_with_logits_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & pos_weight, int64_t reduction, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto weight_meta = to_meta(weight);
        auto pos_weight_meta = to_meta(pos_weight);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::binary_cross_entropy_with_logits_out::call(self_meta, target_meta, weight_meta, pos_weight_meta, reduction, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> pos_weight_;
      if (at::functionalization::impl::isFunctionalTensor(pos_weight)) {
        at::functionalization::impl::sync(pos_weight);
        pos_weight_ = at::functionalization::impl::from_functional_tensor(pos_weight);
      } else {
        pos_weight_ = pos_weight;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(pos_weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::binary_cross_entropy_with_logits_out::call(self_, target_, weight_, pos_weight_, reduction, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::binary_cross_entropy_with_logits::call(self_, target_, weight_, pos_weight_, reduction);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & logical_not_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logical_not_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logical_not_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logical_not::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & logical_not_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logical_not_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logical_not_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logical_not::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & logical_and_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logical_and_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logical_and_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logical_and::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & logical_and_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logical_and_::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logical_and_::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logical_and::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & concatenate_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::concatenate_out::call(tensors_meta, dim, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::concatenate_out::call(tensors_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::concatenate::call(tensors_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & concatenate_out_names_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::concatenate_names_out::call(tensors_meta, dim, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::concatenate_names_out::call(tensors_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::concatenate_names::call(tensors_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & block_diag_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::block_diag_out::call(tensors_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::block_diag_out::call(tensors_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::block_diag::call(tensors_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & chain_matmul_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList matrices, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto matrices_meta = to_meta(matrices);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::chain_matmul_out::call(matrices_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> matrices_;
      if (at::functionalization::impl::isFunctionalTensor(matrices)) {
        at::functionalization::impl::sync(matrices);
        matrices_ = at::functionalization::impl::from_functional_tensor(matrices);
      } else {
        matrices_ = matrices.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(matrices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::chain_matmul_out::call(matrices_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::chain_matmul::call(matrices_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> convolution_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, at::OptionalSymIntArrayRef bias_sizes, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::convolution_backward_out::call(grad_output_meta, input_meta, weight_meta, bias_sizes, stride, padding, dilation, transposed, output_padding, groups, output_mask, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || input.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::convolution_backward_out::call(grad_output_, input_, weight_, bias_sizes, stride, padding, dilation, transposed, output_padding, groups, output_mask, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::convolution_backward::call(grad_output_, input_, weight_, bias_sizes, stride, padding, dilation, transposed, output_padding, groups, output_mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & _copy_from_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & dst, bool non_blocking, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto dst_meta = to_meta(dst);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_copy_from_out::call(self_meta, dst_meta, non_blocking, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor dst_;
      if (at::functionalization::impl::isFunctionalTensor(dst)) {
        at::functionalization::impl::sync(dst);
        dst_ = at::functionalization::impl::from_functional_tensor(dst);
      } else {
        dst_ = dst;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || dst.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(dst))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_copy_from_out::call(self_, dst_, non_blocking, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_copy_from::call(self_, dst_, non_blocking);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cosh_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cosh_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cosh_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cosh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cosh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cosh_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cosh_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cosh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & cudnn_convolution_transpose_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, bool benchmark, bool deterministic, bool allow_tf32, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cudnn_convolution_transpose_out::call(self_meta, weight_meta, padding, output_padding, stride, dilation, groups, benchmark, deterministic, allow_tf32, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cudnn_convolution_transpose_out::call(self_, weight_, padding, output_padding, stride, dilation, groups, benchmark, deterministic, allow_tf32, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cudnn_convolution_transpose::call(self_, weight_, padding, output_padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _mps_convolution_transpose_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef output_padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_mps_convolution_transpose_out::call(self_meta, weight_meta, padding, output_padding, stride, dilation, groups, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_mps_convolution_transpose_out::call(self_, weight_, padding, output_padding, stride, dilation, groups, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_mps_convolution_transpose::call(self_, weight_, padding, output_padding, stride, dilation, groups);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cudnn_grid_sampler_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grid, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grid_meta = to_meta(grid);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cudnn_grid_sampler_out::call(self_meta, grid_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grid_;
      if (at::functionalization::impl::isFunctionalTensor(grid)) {
        at::functionalization::impl::sync(grid);
        grid_ = at::functionalization::impl::from_functional_tensor(grid);
      } else {
        grid_ = grid;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || grid.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(grid))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cudnn_grid_sampler_out::call(self_, grid_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cudnn_grid_sampler::call(self_, grid_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cumprod_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cumprod_out::call(self_meta, dim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cumprod_out::call(self_, dim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cumprod::call(self_, dim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cumprod_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cumprod_::call(self_meta, dim, dtype);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cumprod_::call(self_, dim, dtype);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cumprod::call(self_, dim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & cumprod_out_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cumprod_dimname_out::call(self_meta, dim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cumprod_dimname_out::call(self_, dim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cumprod_dimname::call(self_, dim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cumprod__dimname(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cumprod__dimname::call(self_meta, dim, dtype);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cumprod__dimname::call(self_, dim, dtype);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cumprod_dimname::call(self_, dim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & diag_embed_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::diag_embed_out::call(self_meta, offset, dim1, dim2, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::diag_embed_out::call(self_, offset, dim1, dim2, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::diag_embed::call(self_, offset, dim1, dim2);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & diagonal_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t offset, int64_t dim1, int64_t dim2, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::diagonal_backward_out::call(grad_output_meta, input_sizes, offset, dim1, dim2, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::diagonal_backward_out::call(grad_output_, input_sizes, offset, dim1, dim2, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::diagonal_backward::call(grad_output_, input_sizes, offset, dim1, dim2);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & div_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::div_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::div_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::div_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & div__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::div__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::div__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::div_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & div_out_out_mode(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::div_out_mode::call(self_meta, other_meta, rounding_mode, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::div_out_mode::call(self_, other_, rounding_mode, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::div_Tensor_mode::call(self_, other_, rounding_mode);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & div__Tensor_mode(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::div__Tensor_mode::call(self_meta, other_meta, rounding_mode);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::div__Tensor_mode::call(self_, other_, rounding_mode);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::div_Tensor_mode::call(self_, other_, rounding_mode);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & div_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::div_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::div_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::div_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & div__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::div__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::div__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::div_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & div_out_Scalar_mode_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::div_Scalar_mode_out::call(self_meta, other, rounding_mode, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::div_Scalar_mode_out::call(self_, other, rounding_mode, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::div_Scalar_mode::call(self_, other, rounding_mode);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & div__Scalar_mode(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::div__Scalar_mode::call(self_meta, other, rounding_mode);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::div__Scalar_mode::call(self_, other, rounding_mode);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::div_Scalar_mode::call(self_, other, rounding_mode);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> _embedding_bag_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto weight_meta = to_meta(weight);
        auto indices_meta = to_meta(indices);
        auto offsets_meta = to_meta(offsets);
        auto per_sample_weights_meta = to_meta(per_sample_weights);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        auto out3_meta = to_meta(out3);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_embedding_bag_out::call(weight_meta, indices_meta, offsets_meta, scale_grad_by_freq, mode, sparse, per_sample_weights_meta, include_last_offset, padding_idx, out0_meta, out1_meta, out2_meta, out3_meta);
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor offsets_;
      if (at::functionalization::impl::isFunctionalTensor(offsets)) {
        at::functionalization::impl::sync(offsets);
        offsets_ = at::functionalization::impl::from_functional_tensor(offsets);
      } else {
        offsets_ = offsets;
      }
      
      c10::optional<at::Tensor> per_sample_weights_;
      if (at::functionalization::impl::isFunctionalTensor(per_sample_weights)) {
        at::functionalization::impl::sync(per_sample_weights);
        per_sample_weights_ = at::functionalization::impl::from_functional_tensor(per_sample_weights);
      } else {
        per_sample_weights_ = per_sample_weights;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      
      at::Tensor out3_;
      if (at::functionalization::impl::isFunctionalTensor(out3)) {
        at::functionalization::impl::sync(out3);
        out3_ = at::functionalization::impl::from_functional_tensor(out3);
      } else {
        out3_ = out3;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2) && at::functionalization::impl::isFunctionalTensor(out3))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || weight.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA || offsets.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(offsets) || at::functionalization::impl::isFunctionalTensor(per_sample_weights))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_embedding_bag_out::call(weight_, indices_, offsets_, scale_grad_by_freq, mode, sparse, per_sample_weights_, include_last_offset, padding_idx, out0_, out1_, out2_, out3_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_embedding_bag::call(weight_, indices_, offsets_, scale_grad_by_freq, mode, sparse, per_sample_weights_, include_last_offset, padding_idx);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
  at::functionalization::impl::propagate_xla_data(out3, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(out3, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(out3);
  at::functionalization::impl::sync(out3);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3);
      }
    }

    at::Tensor & _embedding_bag_per_sample_weights_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, int64_t mode, int64_t padding_idx, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_meta = to_meta(grad);
        auto weight_meta = to_meta(weight);
        auto indices_meta = to_meta(indices);
        auto offsets_meta = to_meta(offsets);
        auto offset2bag_meta = to_meta(offset2bag);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_embedding_bag_per_sample_weights_backward_out::call(grad_meta, weight_meta, indices_meta, offsets_meta, offset2bag_meta, mode, padding_idx, out_meta);
      }
      
      at::Tensor grad_;
      if (at::functionalization::impl::isFunctionalTensor(grad)) {
        at::functionalization::impl::sync(grad);
        grad_ = at::functionalization::impl::from_functional_tensor(grad);
      } else {
        grad_ = grad;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor offsets_;
      if (at::functionalization::impl::isFunctionalTensor(offsets)) {
        at::functionalization::impl::sync(offsets);
        offsets_ = at::functionalization::impl::from_functional_tensor(offsets);
      } else {
        offsets_ = offsets;
      }
      
      at::Tensor offset2bag_;
      if (at::functionalization::impl::isFunctionalTensor(offset2bag)) {
        at::functionalization::impl::sync(offset2bag);
        offset2bag_ = at::functionalization::impl::from_functional_tensor(offset2bag);
      } else {
        offset2bag_ = offset2bag;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA || offsets.device().type() == c10::DeviceType::XLA || offset2bag.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(offsets) || at::functionalization::impl::isFunctionalTensor(offset2bag))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_embedding_bag_per_sample_weights_backward_out::call(grad_, weight_, indices_, offsets_, offset2bag_, mode, padding_idx, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_embedding_bag_per_sample_weights_backward::call(grad_, weight_, indices_, offsets_, offset2bag_, mode, padding_idx);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & new_full_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef size, const at::Scalar & fill_value, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::new_full_out::call(self_meta, size, fill_value, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::new_full_out::call(self_, size, fill_value, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::new_full::call(self_, size, fill_value, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & empty_quantized_out_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, const at::Tensor & qtensor, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto qtensor_meta = to_meta(qtensor);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::empty_quantized_out::call(size, qtensor_meta, memory_format, out_meta);
      }
      
      at::Tensor qtensor_;
      if (at::functionalization::impl::isFunctionalTensor(qtensor)) {
        at::functionalization::impl::sync(qtensor);
        qtensor_ = at::functionalization::impl::from_functional_tensor(qtensor);
      } else {
        qtensor_ = qtensor;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || qtensor.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(qtensor))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::empty_quantized_out::call(size, qtensor_, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::empty_quantized::call(size, qtensor_, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & empty_strided_out_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::empty_strided_out::call(size, stride, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::empty_strided_out::call(size, stride, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::empty_strided::call(size, stride, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & exp_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::exp_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::exp_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::exp::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & exp_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::exp_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::exp_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::exp::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & expm1_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::expm1_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::expm1_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::expm1::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & expm1_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::expm1_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::expm1_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::expm1::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & fill_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & value, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fill_Scalar_out::call(self_meta, value, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fill_Scalar_out::call(self_, value, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fill_Scalar::call(self_, value);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fill__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & value) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fill__Scalar::call(self_meta, value);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fill__Scalar::call(self_, value);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fill_Scalar::call(self_, value);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & fill_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & value, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto value_meta = to_meta(value);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fill_Tensor_out::call(self_meta, value_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor value_;
      if (at::functionalization::impl::isFunctionalTensor(value)) {
        at::functionalization::impl::sync(value);
        value_ = at::functionalization::impl::from_functional_tensor(value);
      } else {
        value_ = value;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || value.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(value))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fill_Tensor_out::call(self_, value_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fill_Tensor::call(self_, value_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fill__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & value) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto value_meta = to_meta(value);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fill__Tensor::call(self_meta, value_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor value_;
      if (at::functionalization::impl::isFunctionalTensor(value)) {
        at::functionalization::impl::sync(value);
        value_ = at::functionalization::impl::from_functional_tensor(value);
      } else {
        value_ = value;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || value.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(value))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fill__Tensor::call(self_, value_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fill_Tensor::call(self_, value_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & floor_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::floor_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::floor_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::floor::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & floor_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::floor_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::floor_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::floor::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & floor_divide_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::floor_divide_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::floor_divide_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::floor_divide::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & floor_divide__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::floor_divide__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::floor_divide__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::floor_divide::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & floor_divide_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::floor_divide_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::floor_divide_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::floor_divide_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & floor_divide__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::floor_divide__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::floor_divide__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::floor_divide_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & full_out_names_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::DimnameList> names, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::full_names_out::call(size, fill_value, names, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::full_names_out::call(size, fill_value, names, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::full_names::call(size, fill_value, names, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & full_out_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, const at::Scalar & fill_value, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::full_out::call(size, fill_value, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::full_out::call(size, fill_value, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::full::call(size, fill_value, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & from_file_out_out(c10::DispatchKeySet dispatchKeySet, c10::string_view filename, c10::optional<bool> shared, c10::optional<int64_t> size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::from_file_out::call(filename, shared, size, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::from_file_out::call(filename, shared, size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::from_file::call(filename, shared, size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & grid_sampler_2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto grid_meta = to_meta(grid);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::grid_sampler_2d_out::call(input_meta, grid_meta, interpolation_mode, padding_mode, align_corners, out_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor grid_;
      if (at::functionalization::impl::isFunctionalTensor(grid)) {
        at::functionalization::impl::sync(grid);
        grid_ = at::functionalization::impl::from_functional_tensor(grid);
      } else {
        grid_ = grid;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || grid.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(grid))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::grid_sampler_2d_out::call(input_, grid_, interpolation_mode, padding_mode, align_corners, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::grid_sampler_2d::call(input_, grid_, interpolation_mode, padding_mode, align_corners);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & grid_sampler_3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto grid_meta = to_meta(grid);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::grid_sampler_3d_out::call(input_meta, grid_meta, interpolation_mode, padding_mode, align_corners, out_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor grid_;
      if (at::functionalization::impl::isFunctionalTensor(grid)) {
        at::functionalization::impl::sync(grid);
        grid_ = at::functionalization::impl::from_functional_tensor(grid);
      } else {
        grid_ = grid;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || grid.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(grid))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::grid_sampler_3d_out::call(input_, grid_, interpolation_mode, padding_mode, align_corners, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::grid_sampler_3d::call(input_, grid_, interpolation_mode, padding_mode, align_corners);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hamming_window_out_out(c10::DispatchKeySet dispatchKeySet, int64_t window_length, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hamming_window_out::call(window_length, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hamming_window_out::call(window_length, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hamming_window::call(window_length, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hamming_window_out_periodic_out(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hamming_window_periodic_out::call(window_length, periodic, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hamming_window_periodic_out::call(window_length, periodic, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hamming_window_periodic::call(window_length, periodic, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hamming_window_out_periodic_alpha_out(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, double alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hamming_window_periodic_alpha_out::call(window_length, periodic, alpha, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hamming_window_periodic_alpha_out::call(window_length, periodic, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hamming_window_periodic_alpha::call(window_length, periodic, alpha, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hamming_window_out_periodic_alpha_beta_out(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, double alpha, double beta, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hamming_window_periodic_alpha_beta_out::call(window_length, periodic, alpha, beta, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hamming_window_periodic_alpha_beta_out::call(window_length, periodic, alpha, beta, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hamming_window_periodic_alpha_beta::call(window_length, periodic, alpha, beta, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_group_norm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, c10::SymInt N, c10::SymInt C, c10::SymInt HxW, int64_t group, double eps, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::native_group_norm_out::call(input_meta, weight_meta, bias_meta, N, C, HxW, group, eps, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::native_group_norm_out::call(input_, weight_, bias_, N, C, HxW, group, eps, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::native_group_norm::call(input_, weight_, bias_, N, C, HxW, group, eps);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & _fft_c2r_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, c10::SymInt last_dim_size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_fft_c2r_out::call(self_meta, dim, normalization, last_dim_size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_fft_c2r_out::call(self_, dim, normalization, last_dim_size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_fft_c2r::call(self_, dim, normalization, last_dim_size);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & isnan_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::isnan_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::isnan_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::isnan::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & ldexp_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ldexp_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ldexp_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ldexp_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & ldexp_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ldexp_::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ldexp_::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ldexp_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & log2_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::log2_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::log2_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::log2::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & log2_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::log2_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::log2_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::log2::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & logaddexp2_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logaddexp2_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logaddexp2_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logaddexp2::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & xlogy_out_OutTensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::xlogy_OutTensor::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::xlogy_OutTensor::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::xlogy_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & xlogy__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::xlogy__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::xlogy__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::xlogy_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & xlogy_out_OutScalar_Self(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::xlogy_OutScalar_Self::call(self, other_meta, out_meta);
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::xlogy_OutScalar_Self::call(self, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::xlogy_Scalar_Self::call(self, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & xlogy_out_OutScalar_Other(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::xlogy_OutScalar_Other::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::xlogy_OutScalar_Other::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::xlogy_Scalar_Other::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & xlogy__Scalar_Other(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::xlogy__Scalar_Other::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::xlogy__Scalar_Other::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::xlogy_Scalar_Other::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & logspace_out_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, int64_t steps, double base, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logspace_out::call(start, end, steps, base, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logspace_out::call(start, end, steps, base, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logspace::call(start, end, steps, base, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & logspace_out_Tensor_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & start, const at::Tensor & end, int64_t steps, double base, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto start_meta = to_meta(start);
        auto end_meta = to_meta(end);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logspace_Tensor_Tensor_out::call(start_meta, end_meta, steps, base, out_meta);
      }
      
      at::Tensor start_;
      if (at::functionalization::impl::isFunctionalTensor(start)) {
        at::functionalization::impl::sync(start);
        start_ = at::functionalization::impl::from_functional_tensor(start);
      } else {
        start_ = start;
      }
      
      at::Tensor end_;
      if (at::functionalization::impl::isFunctionalTensor(end)) {
        at::functionalization::impl::sync(end);
        end_ = at::functionalization::impl::from_functional_tensor(end);
      } else {
        end_ = end;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || start.device().type() == c10::DeviceType::XLA || end.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(start) || at::functionalization::impl::isFunctionalTensor(end))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logspace_Tensor_Tensor_out::call(start_, end_, steps, base, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logspace_Tensor_Tensor::call(start_, end_, steps, base, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & logspace_out_Tensor_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & start, const at::Scalar & end, int64_t steps, double base, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto start_meta = to_meta(start);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logspace_Tensor_Scalar_out::call(start_meta, end, steps, base, out_meta);
      }
      
      at::Tensor start_;
      if (at::functionalization::impl::isFunctionalTensor(start)) {
        at::functionalization::impl::sync(start);
        start_ = at::functionalization::impl::from_functional_tensor(start);
      } else {
        start_ = start;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || start.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(start))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logspace_Tensor_Scalar_out::call(start_, end, steps, base, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logspace_Tensor_Scalar::call(start_, end, steps, base, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & logspace_out_Scalar_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Tensor & end, int64_t steps, double base, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto end_meta = to_meta(end);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logspace_Scalar_Tensor_out::call(start, end_meta, steps, base, out_meta);
      }
      
      at::Tensor end_;
      if (at::functionalization::impl::isFunctionalTensor(end)) {
        at::functionalization::impl::sync(end);
        end_ = at::functionalization::impl::from_functional_tensor(end);
      } else {
        end_ = end;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || end.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(end))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logspace_Scalar_Tensor_out::call(start, end_, steps, base, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logspace_Scalar_Tensor::call(start, end_, steps, base, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & matrix_power_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::matrix_power_out::call(self_meta, n, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::matrix_power_out::call(self_, n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::matrix_power::call(self_, n);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _aminmax_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_aminmax_out::call(self_meta, out0_meta, out1_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_aminmax_out::call(self_, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_aminmax::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _aminmax_out_dim_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_aminmax_dim_out::call(self_meta, dim, keepdim, out0_meta, out1_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_aminmax_dim_out::call(self_, dim, keepdim, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_aminmax_dim::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> aminmax_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & min, at::Tensor & max) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto min_meta = to_meta(min);
        auto max_meta = to_meta(max);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::aminmax_out::call(self_meta, dim, keepdim, min_meta, max_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor min_;
      if (at::functionalization::impl::isFunctionalTensor(min)) {
        at::functionalization::impl::sync(min);
        min_ = at::functionalization::impl::from_functional_tensor(min);
      } else {
        min_ = min;
      }
      
      at::Tensor max_;
      if (at::functionalization::impl::isFunctionalTensor(max)) {
        at::functionalization::impl::sync(max);
        max_ = at::functionalization::impl::from_functional_tensor(max);
      } else {
        max_ = max;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(min) && at::functionalization::impl::isFunctionalTensor(max))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::aminmax_out::call(self_, dim, keepdim, min_, max_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(min, max);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::aminmax::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(min, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(min, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(min);
  at::functionalization::impl::sync(min);
  at::functionalization::impl::propagate_xla_data(max, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(max, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(max);
  at::functionalization::impl::sync(max);
    return ::std::tuple<at::Tensor &,at::Tensor &>(min, max);
      }
    }

    at::Tensor & _compute_linear_combination_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & coefficients, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto coefficients_meta = to_meta(coefficients);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_compute_linear_combination_out::call(input_meta, coefficients_meta, out_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor coefficients_;
      if (at::functionalization::impl::isFunctionalTensor(coefficients)) {
        at::functionalization::impl::sync(coefficients);
        coefficients_ = at::functionalization::impl::from_functional_tensor(coefficients);
      } else {
        coefficients_ = coefficients;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || coefficients.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(coefficients))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_compute_linear_combination_out::call(input_, coefficients_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_compute_linear_combination::call(input_, coefficients_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mkldnn_max_pool3d_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto output_meta = to_meta(output);
        auto input_meta = to_meta(input);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_max_pool3d_backward_out::call(grad_output_meta, output_meta, input_meta, kernel_size, stride, padding, dilation, ceil_mode, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || output.device().type() == c10::DeviceType::XLA || input.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(output) || at::functionalization::impl::isFunctionalTensor(input))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mkldnn_max_pool3d_backward_out::call(grad_output_, output_, input_, kernel_size, stride, padding, dilation, ceil_mode, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_max_pool3d_backward::call(grad_output_, output_, input_, kernel_size, stride, padding, dilation, ceil_mode);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> min_out_dim_min(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto min_meta = to_meta(min);
        auto min_indices_meta = to_meta(min_indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::min_dim_min::call(self_meta, dim, keepdim, min_meta, min_indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor min_;
      if (at::functionalization::impl::isFunctionalTensor(min)) {
        at::functionalization::impl::sync(min);
        min_ = at::functionalization::impl::from_functional_tensor(min);
      } else {
        min_ = min;
      }
      
      at::Tensor min_indices_;
      if (at::functionalization::impl::isFunctionalTensor(min_indices)) {
        at::functionalization::impl::sync(min_indices);
        min_indices_ = at::functionalization::impl::from_functional_tensor(min_indices);
      } else {
        min_indices_ = min_indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(min) && at::functionalization::impl::isFunctionalTensor(min_indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::min_dim_min::call(self_, dim, keepdim, min_, min_indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(min, min_indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::min_dim::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(min, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(min, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(min);
  at::functionalization::impl::sync(min);
  at::functionalization::impl::propagate_xla_data(min_indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(min_indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(min_indices);
  at::functionalization::impl::sync(min_indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(min, min_indices);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> min_out_names_dim_min(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto min_meta = to_meta(min);
        auto min_indices_meta = to_meta(min_indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::min_names_dim_min::call(self_meta, dim, keepdim, min_meta, min_indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor min_;
      if (at::functionalization::impl::isFunctionalTensor(min)) {
        at::functionalization::impl::sync(min);
        min_ = at::functionalization::impl::from_functional_tensor(min);
      } else {
        min_ = min;
      }
      
      at::Tensor min_indices_;
      if (at::functionalization::impl::isFunctionalTensor(min_indices)) {
        at::functionalization::impl::sync(min_indices);
        min_indices_ = at::functionalization::impl::from_functional_tensor(min_indices);
      } else {
        min_indices_ = min_indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(min) && at::functionalization::impl::isFunctionalTensor(min_indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::min_names_dim_min::call(self_, dim, keepdim, min_, min_indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(min, min_indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::min_names_dim::call(self_, dim, keepdim);
        }
          at::functionalization::impl::propagate_xla_data(min, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(min, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(min);
  at::functionalization::impl::sync(min);
  at::functionalization::impl::propagate_xla_data(min_indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(min_indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(min_indices);
  at::functionalization::impl::sync(min_indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(min, min_indices);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> mps_convolution_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto grad_output_meta = to_meta(grad_output);
        auto weight_meta = to_meta(weight);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mps_convolution_backward_out::call(self_meta, grad_output_meta, weight_meta, padding, stride, dilation, groups, output_mask, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || grad_output.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::mps_convolution_backward_out::call(self_, grad_output_, weight_, padding, stride, dilation, groups, output_mask, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mps_convolution_backward::call(self_, grad_output_, weight_, padding, stride, dilation, groups, output_mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> miopen_rnn_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3, at::Tensor & out4) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto hx_meta = to_meta(hx);
        auto cx_meta = to_meta(cx);
        auto dropout_state_meta = to_meta(dropout_state);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        auto out3_meta = to_meta(out3);
        auto out4_meta = to_meta(out4);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::miopen_rnn_out::call(input_meta, weight_meta, weight_stride0, hx_meta, cx_meta, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state_meta, out0_meta, out1_meta, out2_meta, out3_meta, out4_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      ::std::vector<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight.vec();
      }
      
      at::Tensor hx_;
      if (at::functionalization::impl::isFunctionalTensor(hx)) {
        at::functionalization::impl::sync(hx);
        hx_ = at::functionalization::impl::from_functional_tensor(hx);
      } else {
        hx_ = hx;
      }
      
      c10::optional<at::Tensor> cx_;
      if (at::functionalization::impl::isFunctionalTensor(cx)) {
        at::functionalization::impl::sync(cx);
        cx_ = at::functionalization::impl::from_functional_tensor(cx);
      } else {
        cx_ = cx;
      }
      
      c10::optional<at::Tensor> dropout_state_;
      if (at::functionalization::impl::isFunctionalTensor(dropout_state)) {
        at::functionalization::impl::sync(dropout_state);
        dropout_state_ = at::functionalization::impl::from_functional_tensor(dropout_state);
      } else {
        dropout_state_ = dropout_state;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      
      at::Tensor out3_;
      if (at::functionalization::impl::isFunctionalTensor(out3)) {
        at::functionalization::impl::sync(out3);
        out3_ = at::functionalization::impl::from_functional_tensor(out3);
      } else {
        out3_ = out3;
      }
      
      at::Tensor out4_;
      if (at::functionalization::impl::isFunctionalTensor(out4)) {
        at::functionalization::impl::sync(out4);
        out4_ = at::functionalization::impl::from_functional_tensor(out4);
      } else {
        out4_ = out4;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2) && at::functionalization::impl::isFunctionalTensor(out3) && at::functionalization::impl::isFunctionalTensor(out4))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA || hx.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(hx) || at::functionalization::impl::isFunctionalTensor(cx) || at::functionalization::impl::isFunctionalTensor(dropout_state))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::miopen_rnn_out::call(input_, weight_, weight_stride0, hx_, cx_, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state_, out0_, out1_, out2_, out3_, out4_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3, out4);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::miopen_rnn::call(input_, weight_, weight_stride0, hx_, cx_, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state_);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
  at::functionalization::impl::propagate_xla_data(out3, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(out3, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(out3);
  at::functionalization::impl::sync(out3);
  at::functionalization::impl::propagate_xla_data(out4, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(out4, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(out4);
  at::functionalization::impl::sync(out4);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3, out4);
      }
    }

    at::Tensor & mv_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto vec_meta = to_meta(vec);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mv_out::call(self_meta, vec_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor vec_;
      if (at::functionalization::impl::isFunctionalTensor(vec)) {
        at::functionalization::impl::sync(vec);
        vec_ = at::functionalization::impl::from_functional_tensor(vec);
      } else {
        vec_ = vec;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || vec.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(vec))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mv_out::call(self_, vec_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mv::call(self_, vec_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _native_batch_norm_legit_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, at::Tensor & running_mean, at::Tensor & running_var, bool training, double momentum, double eps, at::Tensor & out, at::Tensor & save_mean, at::Tensor & save_invstd) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto running_mean_meta = to_meta(running_mean);
        auto running_var_meta = to_meta(running_var);
        auto out_meta = to_meta(out);
        auto save_mean_meta = to_meta(save_mean);
        auto save_invstd_meta = to_meta(save_invstd);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_native_batch_norm_legit_out::call(input_meta, weight_meta, bias_meta, running_mean_meta, running_var_meta, training, momentum, eps, out_meta, save_mean_meta, save_invstd_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor running_mean_;
      if (at::functionalization::impl::isFunctionalTensor(running_mean)) {
        at::functionalization::impl::sync(running_mean);
        running_mean_ = at::functionalization::impl::from_functional_tensor(running_mean);
      } else {
        running_mean_ = running_mean;
      }
      
      at::Tensor running_var_;
      if (at::functionalization::impl::isFunctionalTensor(running_var)) {
        at::functionalization::impl::sync(running_var);
        running_var_ = at::functionalization::impl::from_functional_tensor(running_var);
      } else {
        running_var_ = running_var;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      
      at::Tensor save_mean_;
      if (at::functionalization::impl::isFunctionalTensor(save_mean)) {
        at::functionalization::impl::sync(save_mean);
        save_mean_ = at::functionalization::impl::from_functional_tensor(save_mean);
      } else {
        save_mean_ = save_mean;
      }
      
      at::Tensor save_invstd_;
      if (at::functionalization::impl::isFunctionalTensor(save_invstd)) {
        at::functionalization::impl::sync(save_invstd);
        save_invstd_ = at::functionalization::impl::from_functional_tensor(save_invstd);
      } else {
        save_invstd_ = save_invstd;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(running_mean) && at::functionalization::impl::isFunctionalTensor(running_var) && at::functionalization::impl::isFunctionalTensor(out) && at::functionalization::impl::isFunctionalTensor(save_mean) && at::functionalization::impl::isFunctionalTensor(save_invstd))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_native_batch_norm_legit_out::call(input_, weight_, bias_, running_mean_, running_var_, training, momentum, eps, out_, save_mean_, save_invstd_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out, save_mean, save_invstd);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_native_batch_norm_legit_functional::call(input_, weight_, bias_, running_mean_, running_var_, training, momentum, eps);
        }
          at::functionalization::impl::propagate_xla_data(running_mean, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(running_mean, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(running_mean);
  at::functionalization::impl::sync(running_mean);
  at::functionalization::impl::propagate_xla_data(running_var, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(running_var, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(running_var);
  at::functionalization::impl::sync(running_var);
  at::functionalization::impl::propagate_xla_data(out, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
  at::functionalization::impl::propagate_xla_data(save_mean, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(save_mean, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(save_mean);
  at::functionalization::impl::sync(save_mean);
  at::functionalization::impl::propagate_xla_data(save_invstd, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(save_invstd, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(save_invstd);
  at::functionalization::impl::sync(save_invstd);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out, save_mean, save_invstd);
      }
    }

    ::std::tuple<at::Tensor,at::Tensor,at::Tensor> _native_batch_norm_legit(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, at::Tensor & running_mean, at::Tensor & running_var, bool training, double momentum, double eps) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto running_mean_meta = to_meta(running_mean);
        auto running_var_meta = to_meta(running_var);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_native_batch_norm_legit::call(input_meta, weight_meta, bias_meta, running_mean_meta, running_var_meta, training, momentum, eps);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor running_mean_;
      if (at::functionalization::impl::isFunctionalTensor(running_mean)) {
        at::functionalization::impl::sync(running_mean);
        running_mean_ = at::functionalization::impl::from_functional_tensor(running_mean);
      } else {
        running_mean_ = running_mean;
      }
      
      at::Tensor running_var_;
      if (at::functionalization::impl::isFunctionalTensor(running_var)) {
        at::functionalization::impl::sync(running_var);
        running_var_ = at::functionalization::impl::from_functional_tensor(running_var);
      } else {
        running_var_ = running_var;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(running_mean) && at::functionalization::impl::isFunctionalTensor(running_var))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_native_batch_norm_legit::call(input_, weight_, bias_, running_mean_, running_var_, training, momentum, eps);
         return ::std::tuple<at::Tensor,at::Tensor,at::Tensor>(std::get<0>(tmp_output), std::get<1>(tmp_output), std::get<2>(tmp_output));;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_native_batch_norm_legit_functional::call(input_, weight_, bias_, running_mean_, running_var_, training, momentum, eps);
        }
          auto output_0 = at::functionalization::impl::to_functional_tensor(std::get<0>(tmp_output));
  auto output_1 = at::functionalization::impl::to_functional_tensor(std::get<1>(tmp_output));
  auto output_2 = at::functionalization::impl::to_functional_tensor(std::get<2>(tmp_output));
  at::functionalization::impl::propagate_xla_data(running_mean, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(running_mean, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(running_mean);
  at::functionalization::impl::sync(running_mean);
  at::functionalization::impl::propagate_xla_data(running_var, std::get<4>(tmp_output));
  at::functionalization::impl::replace_(running_var, std::get<4>(tmp_output));
  at::functionalization::impl::commit_update(running_var);
  at::functionalization::impl::sync(running_var);
    return ::std::tuple<at::Tensor,at::Tensor,at::Tensor>(output_0, output_1, output_2);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _native_batch_norm_legit_out_no_stats_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, bool training, double momentum, double eps, at::Tensor & out, at::Tensor & save_mean, at::Tensor & save_invstd) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        auto save_mean_meta = to_meta(save_mean);
        auto save_invstd_meta = to_meta(save_invstd);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_native_batch_norm_legit_no_stats_out::call(input_meta, weight_meta, bias_meta, training, momentum, eps, out_meta, save_mean_meta, save_invstd_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      
      at::Tensor save_mean_;
      if (at::functionalization::impl::isFunctionalTensor(save_mean)) {
        at::functionalization::impl::sync(save_mean);
        save_mean_ = at::functionalization::impl::from_functional_tensor(save_mean);
      } else {
        save_mean_ = save_mean;
      }
      
      at::Tensor save_invstd_;
      if (at::functionalization::impl::isFunctionalTensor(save_invstd)) {
        at::functionalization::impl::sync(save_invstd);
        save_invstd_ = at::functionalization::impl::from_functional_tensor(save_invstd);
      } else {
        save_invstd_ = save_invstd;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out) && at::functionalization::impl::isFunctionalTensor(save_mean) && at::functionalization::impl::isFunctionalTensor(save_invstd))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_native_batch_norm_legit_no_stats_out::call(input_, weight_, bias_, training, momentum, eps, out_, save_mean_, save_invstd_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out, save_mean, save_invstd);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_native_batch_norm_legit_no_stats::call(input_, weight_, bias_, training, momentum, eps);
        }
          at::functionalization::impl::propagate_xla_data(out, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
  at::functionalization::impl::propagate_xla_data(save_mean, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(save_mean, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(save_mean);
  at::functionalization::impl::sync(save_mean);
  at::functionalization::impl::propagate_xla_data(save_invstd, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(save_invstd, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(save_invstd);
  at::functionalization::impl::sync(save_invstd);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out, save_mean, save_invstd);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> batch_norm_stats_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double eps, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_meta = to_meta(input);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::batch_norm_stats_out::call(input_meta, eps, out0_meta, out1_meta);
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::batch_norm_stats_out::call(input_, eps, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::batch_norm_stats::call(input_, eps);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & batch_norm_backward_elemt_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & weight, const at::Tensor & sum_dy, const at::Tensor & sum_dy_xmu, const at::Tensor & count, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_out_meta = to_meta(grad_out);
        auto input_meta = to_meta(input);
        auto mean_meta = to_meta(mean);
        auto invstd_meta = to_meta(invstd);
        auto weight_meta = to_meta(weight);
        auto sum_dy_meta = to_meta(sum_dy);
        auto sum_dy_xmu_meta = to_meta(sum_dy_xmu);
        auto count_meta = to_meta(count);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::batch_norm_backward_elemt_out::call(grad_out_meta, input_meta, mean_meta, invstd_meta, weight_meta, sum_dy_meta, sum_dy_xmu_meta, count_meta, out_meta);
      }
      
      at::Tensor grad_out_;
      if (at::functionalization::impl::isFunctionalTensor(grad_out)) {
        at::functionalization::impl::sync(grad_out);
        grad_out_ = at::functionalization::impl::from_functional_tensor(grad_out);
      } else {
        grad_out_ = grad_out;
      }
      
      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor mean_;
      if (at::functionalization::impl::isFunctionalTensor(mean)) {
        at::functionalization::impl::sync(mean);
        mean_ = at::functionalization::impl::from_functional_tensor(mean);
      } else {
        mean_ = mean;
      }
      
      at::Tensor invstd_;
      if (at::functionalization::impl::isFunctionalTensor(invstd)) {
        at::functionalization::impl::sync(invstd);
        invstd_ = at::functionalization::impl::from_functional_tensor(invstd);
      } else {
        invstd_ = invstd;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor sum_dy_;
      if (at::functionalization::impl::isFunctionalTensor(sum_dy)) {
        at::functionalization::impl::sync(sum_dy);
        sum_dy_ = at::functionalization::impl::from_functional_tensor(sum_dy);
      } else {
        sum_dy_ = sum_dy;
      }
      
      at::Tensor sum_dy_xmu_;
      if (at::functionalization::impl::isFunctionalTensor(sum_dy_xmu)) {
        at::functionalization::impl::sync(sum_dy_xmu);
        sum_dy_xmu_ = at::functionalization::impl::from_functional_tensor(sum_dy_xmu);
      } else {
        sum_dy_xmu_ = sum_dy_xmu;
      }
      
      at::Tensor count_;
      if (at::functionalization::impl::isFunctionalTensor(count)) {
        at::functionalization::impl::sync(count);
        count_ = at::functionalization::impl::from_functional_tensor(count);
      } else {
        count_ = count;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_out.device().type() == c10::DeviceType::XLA || input.device().type() == c10::DeviceType::XLA || mean.device().type() == c10::DeviceType::XLA || invstd.device().type() == c10::DeviceType::XLA || sum_dy.device().type() == c10::DeviceType::XLA || sum_dy_xmu.device().type() == c10::DeviceType::XLA || count.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_out) || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(mean) || at::functionalization::impl::isFunctionalTensor(invstd) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(sum_dy) || at::functionalization::impl::isFunctionalTensor(sum_dy_xmu) || at::functionalization::impl::isFunctionalTensor(count))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::batch_norm_backward_elemt_out::call(grad_out_, input_, mean_, invstd_, weight_, sum_dy_, sum_dy_xmu_, count_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::batch_norm_backward_elemt::call(grad_out_, input_, mean_, invstd_, weight_, sum_dy_, sum_dy_xmu_, count_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _euclidean_dist_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x1_meta = to_meta(x1);
        auto x2_meta = to_meta(x2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_euclidean_dist_out::call(x1_meta, x2_meta, out_meta);
      }
      
      at::Tensor x1_;
      if (at::functionalization::impl::isFunctionalTensor(x1)) {
        at::functionalization::impl::sync(x1);
        x1_ = at::functionalization::impl::from_functional_tensor(x1);
      } else {
        x1_ = x1;
      }
      
      at::Tensor x2_;
      if (at::functionalization::impl::isFunctionalTensor(x2)) {
        at::functionalization::impl::sync(x2);
        x2_ = at::functionalization::impl::from_functional_tensor(x2);
      } else {
        x2_ = x2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x1.device().type() == c10::DeviceType::XLA || x2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x1) || at::functionalization::impl::isFunctionalTensor(x2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_euclidean_dist_out::call(x1_, x2_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_euclidean_dist::call(x1_, x2_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _cdist_forward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto x1_meta = to_meta(x1);
        auto x2_meta = to_meta(x2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_cdist_forward_out::call(x1_meta, x2_meta, p, compute_mode, out_meta);
      }
      
      at::Tensor x1_;
      if (at::functionalization::impl::isFunctionalTensor(x1)) {
        at::functionalization::impl::sync(x1);
        x1_ = at::functionalization::impl::from_functional_tensor(x1);
      } else {
        x1_ = x1;
      }
      
      at::Tensor x2_;
      if (at::functionalization::impl::isFunctionalTensor(x2)) {
        at::functionalization::impl::sync(x2);
        x2_ = at::functionalization::impl::from_functional_tensor(x2);
      } else {
        x2_ = x2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || x1.device().type() == c10::DeviceType::XLA || x2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(x1) || at::functionalization::impl::isFunctionalTensor(x2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_cdist_forward_out::call(x1_, x2_, p, compute_mode, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_cdist_forward::call(x1_, x2_, p, compute_mode);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _cdist_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & x1, const at::Tensor & x2, double p, const at::Tensor & cdist, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_meta = to_meta(grad);
        auto x1_meta = to_meta(x1);
        auto x2_meta = to_meta(x2);
        auto cdist_meta = to_meta(cdist);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_cdist_backward_out::call(grad_meta, x1_meta, x2_meta, p, cdist_meta, out_meta);
      }
      
      at::Tensor grad_;
      if (at::functionalization::impl::isFunctionalTensor(grad)) {
        at::functionalization::impl::sync(grad);
        grad_ = at::functionalization::impl::from_functional_tensor(grad);
      } else {
        grad_ = grad;
      }
      
      at::Tensor x1_;
      if (at::functionalization::impl::isFunctionalTensor(x1)) {
        at::functionalization::impl::sync(x1);
        x1_ = at::functionalization::impl::from_functional_tensor(x1);
      } else {
        x1_ = x1;
      }
      
      at::Tensor x2_;
      if (at::functionalization::impl::isFunctionalTensor(x2)) {
        at::functionalization::impl::sync(x2);
        x2_ = at::functionalization::impl::from_functional_tensor(x2);
      } else {
        x2_ = x2;
      }
      
      at::Tensor cdist_;
      if (at::functionalization::impl::isFunctionalTensor(cdist)) {
        at::functionalization::impl::sync(cdist);
        cdist_ = at::functionalization::impl::from_functional_tensor(cdist);
      } else {
        cdist_ = cdist;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad.device().type() == c10::DeviceType::XLA || x1.device().type() == c10::DeviceType::XLA || x2.device().type() == c10::DeviceType::XLA || cdist.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad) || at::functionalization::impl::isFunctionalTensor(x1) || at::functionalization::impl::isFunctionalTensor(x2) || at::functionalization::impl::isFunctionalTensor(cdist))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_cdist_backward_out::call(grad_, x1_, x2_, p, cdist_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_cdist_backward::call(grad_, x1_, x2_, p, cdist_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & pixel_unshuffle_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t downscale_factor, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::pixel_unshuffle_out::call(self_meta, downscale_factor, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::pixel_unshuffle_out::call(self_, downscale_factor, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::pixel_unshuffle::call(self_, downscale_factor);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & rad2deg_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rad2deg_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rad2deg_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rad2deg::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & rad2deg_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rad2deg_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rad2deg_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rad2deg::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & scalar_tensor_out_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & s, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::scalar_tensor_out::call(s, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::scalar_tensor_out::call(s, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::scalar_tensor::call(s, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & rand_out_names_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, c10::optional<at::DimnameList> names, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rand_names_out::call(size, names, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rand_names_out::call(size, names, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rand_names::call(size, names, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & rand_out_generator_with_names_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rand_generator_with_names_out::call(size, generator, names, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rand_generator_with_names_out::call(size, generator, names, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rand_generator_with_names::call(size, generator, names, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & rand_out_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rand_out::call(size, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rand_out::call(size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rand::call(size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & rand_out_generator_out(c10::DispatchKeySet dispatchKeySet, c10::SymIntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rand_generator_out::call(size, generator, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rand_generator_out::call(size, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rand_generator::call(size, generator, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & rand_like_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::rand_like_out::call(self_meta, memory_format, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::rand_like_out::call(self_, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::rand_like::call(self_, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & relu_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::relu_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::relu_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::relu::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & relu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::relu_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::relu_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::relu::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & logit_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logit_out::call(self_meta, eps, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logit_out::call(self_, eps, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logit::call(self_, eps);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & logit_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<double> eps) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logit_::call(self_meta, eps);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logit_::call(self_, eps);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logit::call(self_, eps);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & select_scatter_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & src, int64_t dim, c10::SymInt index, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto src_meta = to_meta(src);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::select_scatter_out::call(self_meta, src_meta, dim, index, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || src.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(src))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::select_scatter_out::call(self_, src_, dim, index, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::select_scatter::call(self_, src_, dim, index);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & softmax_out_int_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::softmax_int_out::call(self_meta, dim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::softmax_int_out::call(self_, dim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::softmax_int::call(self_, dim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & stack_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::stack_out::call(tensors_meta, dim, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::stack_out::call(tensors_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::stack::call(tensors_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & vstack_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto tensors_meta = to_meta(tensors);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::vstack_out::call(tensors_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> tensors_;
      if (at::functionalization::impl::isFunctionalTensor(tensors)) {
        at::functionalization::impl::sync(tensors);
        tensors_ = at::functionalization::impl::from_functional_tensor(tensors);
      } else {
        tensors_ = tensors.vec();
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensors))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::vstack_out::call(tensors_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::vstack::call(tensors_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & nansum_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nansum_out::call(self_meta, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::nansum_out::call(self_, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nansum::call(self_, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sqrt_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sqrt_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sqrt_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sqrt::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & sqrt_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sqrt_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sqrt_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sqrt::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & prod_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::prod_out::call(self_meta, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::prod_out::call(self_, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::prod::call(self_, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & prod_out_int_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::prod_int_out::call(self_meta, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::prod_int_out::call(self_, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::prod_dim_int::call(self_, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & prod_out_Dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::prod_Dimname_out::call(self_meta, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::prod_Dimname_out::call(self_, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::prod_dim_Dimname::call(self_, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & threshold_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::threshold_backward_grad_input::call(grad_output_meta, self_meta, threshold, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::threshold_backward_grad_input::call(grad_output_, self_, threshold, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::threshold_backward::call(grad_output_, self_, threshold);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _transform_bias_rescale_qkv_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & qkv, const at::Tensor & qkv_bias, int64_t num_heads, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto qkv_meta = to_meta(qkv);
        auto qkv_bias_meta = to_meta(qkv_bias);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_transform_bias_rescale_qkv_out::call(qkv_meta, qkv_bias_meta, num_heads, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor qkv_;
      if (at::functionalization::impl::isFunctionalTensor(qkv)) {
        at::functionalization::impl::sync(qkv);
        qkv_ = at::functionalization::impl::from_functional_tensor(qkv);
      } else {
        qkv_ = qkv;
      }
      
      at::Tensor qkv_bias_;
      if (at::functionalization::impl::isFunctionalTensor(qkv_bias)) {
        at::functionalization::impl::sync(qkv_bias);
        qkv_bias_ = at::functionalization::impl::from_functional_tensor(qkv_bias);
      } else {
        qkv_bias_ = qkv_bias;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || qkv.device().type() == c10::DeviceType::XLA || qkv_bias.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(qkv) || at::functionalization::impl::isFunctionalTensor(qkv_bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_transform_bias_rescale_qkv_out::call(qkv_, qkv_bias_, num_heads, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_transform_bias_rescale_qkv::call(qkv_, qkv_bias_, num_heads);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & _nested_from_padded_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & padded, const at::Tensor & cpu_nested_shape_example, bool fuse_transform_0213, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto padded_meta = to_meta(padded);
        auto cpu_nested_shape_example_meta = to_meta(cpu_nested_shape_example);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_nested_from_padded_out::call(padded_meta, cpu_nested_shape_example_meta, fuse_transform_0213, out_meta);
      }
      
      at::Tensor padded_;
      if (at::functionalization::impl::isFunctionalTensor(padded)) {
        at::functionalization::impl::sync(padded);
        padded_ = at::functionalization::impl::from_functional_tensor(padded);
      } else {
        padded_ = padded;
      }
      
      at::Tensor cpu_nested_shape_example_;
      if (at::functionalization::impl::isFunctionalTensor(cpu_nested_shape_example)) {
        at::functionalization::impl::sync(cpu_nested_shape_example);
        cpu_nested_shape_example_ = at::functionalization::impl::from_functional_tensor(cpu_nested_shape_example);
      } else {
        cpu_nested_shape_example_ = cpu_nested_shape_example;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || padded.device().type() == c10::DeviceType::XLA || cpu_nested_shape_example.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(padded) || at::functionalization::impl::isFunctionalTensor(cpu_nested_shape_example))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_nested_from_padded_out::call(padded_, cpu_nested_shape_example_, fuse_transform_0213, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_nested_from_padded::call(padded_, cpu_nested_shape_example_, fuse_transform_0213);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _nested_tensor_size_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_nested_tensor_size_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_nested_tensor_size_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_nested_tensor_size::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _nested_view_from_buffer_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & nested_size, const at::Tensor & nested_strides, const at::Tensor & offsets, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto nested_size_meta = to_meta(nested_size);
        auto nested_strides_meta = to_meta(nested_strides);
        auto offsets_meta = to_meta(offsets);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_nested_view_from_buffer_copy_out::call(self_meta, nested_size_meta, nested_strides_meta, offsets_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor nested_size_;
      if (at::functionalization::impl::isFunctionalTensor(nested_size)) {
        at::functionalization::impl::sync(nested_size);
        nested_size_ = at::functionalization::impl::from_functional_tensor(nested_size);
      } else {
        nested_size_ = nested_size;
      }
      
      at::Tensor nested_strides_;
      if (at::functionalization::impl::isFunctionalTensor(nested_strides)) {
        at::functionalization::impl::sync(nested_strides);
        nested_strides_ = at::functionalization::impl::from_functional_tensor(nested_strides);
      } else {
        nested_strides_ = nested_strides;
      }
      
      at::Tensor offsets_;
      if (at::functionalization::impl::isFunctionalTensor(offsets)) {
        at::functionalization::impl::sync(offsets);
        offsets_ = at::functionalization::impl::from_functional_tensor(offsets);
      } else {
        offsets_ = offsets;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || nested_size.device().type() == c10::DeviceType::XLA || nested_strides.device().type() == c10::DeviceType::XLA || offsets.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(nested_size) || at::functionalization::impl::isFunctionalTensor(nested_strides) || at::functionalization::impl::isFunctionalTensor(offsets))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_nested_view_from_buffer_copy_out::call(self_, nested_size_, nested_strides_, offsets_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_nested_view_from_buffer_copy::call(self_, nested_size_, nested_strides_, offsets_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & trunc_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::trunc_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::trunc_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::trunc::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & trunc_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::trunc_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::trunc_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::trunc::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> unique_dim_consecutive_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool return_inverse, bool return_counts, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::unique_dim_consecutive_out::call(self_meta, dim, return_inverse, return_counts, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::unique_dim_consecutive_out::call(self_, dim, return_inverse, return_counts, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::unique_dim_consecutive::call(self_, dim, return_inverse, return_counts);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & where_out_self_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & condition, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto condition_meta = to_meta(condition);
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::where_self_out::call(condition_meta, self_meta, other_meta, out_meta);
      }
      
      at::Tensor condition_;
      if (at::functionalization::impl::isFunctionalTensor(condition)) {
        at::functionalization::impl::sync(condition);
        condition_ = at::functionalization::impl::from_functional_tensor(condition);
      } else {
        condition_ = condition;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || condition.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(condition) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::where_self_out::call(condition_, self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::where_self::call(condition_, self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _weight_norm_interface_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_w_meta = to_meta(grad_w);
        auto saved_v_meta = to_meta(saved_v);
        auto saved_g_meta = to_meta(saved_g);
        auto saved_norms_meta = to_meta(saved_norms);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_weight_norm_interface_backward_out::call(grad_w_meta, saved_v_meta, saved_g_meta, saved_norms_meta, dim, out0_meta, out1_meta);
      }
      
      at::Tensor grad_w_;
      if (at::functionalization::impl::isFunctionalTensor(grad_w)) {
        at::functionalization::impl::sync(grad_w);
        grad_w_ = at::functionalization::impl::from_functional_tensor(grad_w);
      } else {
        grad_w_ = grad_w;
      }
      
      at::Tensor saved_v_;
      if (at::functionalization::impl::isFunctionalTensor(saved_v)) {
        at::functionalization::impl::sync(saved_v);
        saved_v_ = at::functionalization::impl::from_functional_tensor(saved_v);
      } else {
        saved_v_ = saved_v;
      }
      
      at::Tensor saved_g_;
      if (at::functionalization::impl::isFunctionalTensor(saved_g)) {
        at::functionalization::impl::sync(saved_g);
        saved_g_ = at::functionalization::impl::from_functional_tensor(saved_g);
      } else {
        saved_g_ = saved_g;
      }
      
      at::Tensor saved_norms_;
      if (at::functionalization::impl::isFunctionalTensor(saved_norms)) {
        at::functionalization::impl::sync(saved_norms);
        saved_norms_ = at::functionalization::impl::from_functional_tensor(saved_norms);
      } else {
        saved_norms_ = saved_norms;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_w.device().type() == c10::DeviceType::XLA || saved_v.device().type() == c10::DeviceType::XLA || saved_g.device().type() == c10::DeviceType::XLA || saved_norms.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_w) || at::functionalization::impl::isFunctionalTensor(saved_v) || at::functionalization::impl::isFunctionalTensor(saved_g) || at::functionalization::impl::isFunctionalTensor(saved_norms))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_weight_norm_interface_backward_out::call(grad_w_, saved_v_, saved_g_, saved_norms_, dim, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_weight_norm_interface_backward::call(grad_w_, saved_v_, saved_g_, saved_norms_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & _sample_dirichlet_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sample_dirichlet_out::call(self_meta, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sample_dirichlet_out::call(self_, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sample_dirichlet::call(self_, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & binomial_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & count, const at::Tensor & prob, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto count_meta = to_meta(count);
        auto prob_meta = to_meta(prob);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::binomial_out::call(count_meta, prob_meta, generator, out_meta);
      }
      
      at::Tensor count_;
      if (at::functionalization::impl::isFunctionalTensor(count)) {
        at::functionalization::impl::sync(count);
        count_ = at::functionalization::impl::from_functional_tensor(count);
      } else {
        count_ = count;
      }
      
      at::Tensor prob_;
      if (at::functionalization::impl::isFunctionalTensor(prob)) {
        at::functionalization::impl::sync(prob);
        prob_ = at::functionalization::impl::from_functional_tensor(prob);
      } else {
        prob_ = prob;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || count.device().type() == c10::DeviceType::XLA || prob.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(count) || at::functionalization::impl::isFunctionalTensor(prob))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::binomial_out::call(count_, prob_, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::binomial::call(count_, prob_, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & native_norm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & p, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::native_norm_out::call(self_meta, p, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::native_norm_out::call(self_, p, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::native_norm::call(self_, p);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & native_norm_out_ScalarOpt_dim_dtype_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::native_norm_ScalarOpt_dim_dtype_out::call(self_meta, p, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::native_norm_ScalarOpt_dim_dtype_out::call(self_, p, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::native_norm_ScalarOpt_dim_dtype::call(self_, p, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _sparse_sum_out_dim_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sparse_sum_dim_out::call(self_meta, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sparse_sum_dim_out::call(self_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sparse_sum_dim::call(self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _sparse_sum_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, at::IntArrayRef dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_meta = to_meta(grad);
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sparse_sum_backward_out::call(grad_meta, self_meta, dim, out_meta);
      }
      
      at::Tensor grad_;
      if (at::functionalization::impl::isFunctionalTensor(grad)) {
        at::functionalization::impl::sync(grad);
        grad_ = at::functionalization::impl::from_functional_tensor(grad);
      } else {
        grad_ = grad;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sparse_sum_backward_out::call(grad_, self_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sparse_sum_backward::call(grad_, self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _sparse_softmax_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sparse_softmax_out::call(self_meta, dim, half_to_float, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sparse_softmax_out::call(self_, dim, half_to_float, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sparse_softmax::call(self_, dim, half_to_float);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & clone_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::clone_out::call(self_meta, memory_format, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::clone_out::call(self_, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::clone::call(self_, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & resize_as_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & the_template, c10::optional<at::MemoryFormat> memory_format, const at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto the_template_meta = to_meta(the_template);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::resize_as_out::call(self_meta, the_template_meta, memory_format, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor the_template_;
      if (at::functionalization::impl::isFunctionalTensor(the_template)) {
        at::functionalization::impl::sync(the_template);
        the_template_ = at::functionalization::impl::from_functional_tensor(the_template);
      } else {
        the_template_ = the_template;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || the_template.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(the_template))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::resize_as_out::call(self_, the_template_, memory_format, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::resize_as::call(self_, the_template_, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & resize_as_(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & the_template, c10::optional<at::MemoryFormat> memory_format) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto the_template_meta = to_meta(the_template);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::resize_as_::call(self_meta, the_template_meta, memory_format);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor the_template_;
      if (at::functionalization::impl::isFunctionalTensor(the_template)) {
        at::functionalization::impl::sync(the_template);
        the_template_ = at::functionalization::impl::from_functional_tensor(the_template);
      } else {
        the_template_ = the_template;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || the_template.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(the_template))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::resize_as_::call(self_, the_template_, memory_format);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::resize_as::call(self_, the_template_, memory_format);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & zero_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::zero_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::zero_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::zero::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & zero_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::zero_::call(self_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::zero_::call(self_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::zero::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & heaviside_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & values, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::heaviside_out::call(self_meta, values_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || values.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(values))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::heaviside_out::call(self_, values_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::heaviside::call(self_, values_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & heaviside_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & values) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto values_meta = to_meta(values);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::heaviside_::call(self_meta, values_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || values.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(values))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::heaviside_::call(self_, values_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::heaviside::call(self_, values_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & addmm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mat1_meta = to_meta(mat1);
        auto mat2_meta = to_meta(mat2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::addmm_out::call(self_meta, mat1_meta, mat2_meta, beta, alpha, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mat1_;
      if (at::functionalization::impl::isFunctionalTensor(mat1)) {
        at::functionalization::impl::sync(mat1);
        mat1_ = at::functionalization::impl::from_functional_tensor(mat1);
      } else {
        mat1_ = mat1;
      }
      
      at::Tensor mat2_;
      if (at::functionalization::impl::isFunctionalTensor(mat2)) {
        at::functionalization::impl::sync(mat2);
        mat2_ = at::functionalization::impl::from_functional_tensor(mat2);
      } else {
        mat2_ = mat2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mat1.device().type() == c10::DeviceType::XLA || mat2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mat1) || at::functionalization::impl::isFunctionalTensor(mat2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::addmm_out::call(self_, mat1_, mat2_, beta, alpha, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::addmm::call(self_, mat1_, mat2_, beta, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & addmm_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mat1_meta = to_meta(mat1);
        auto mat2_meta = to_meta(mat2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::addmm_::call(self_meta, mat1_meta, mat2_meta, beta, alpha);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mat1_;
      if (at::functionalization::impl::isFunctionalTensor(mat1)) {
        at::functionalization::impl::sync(mat1);
        mat1_ = at::functionalization::impl::from_functional_tensor(mat1);
      } else {
        mat1_ = mat1;
      }
      
      at::Tensor mat2_;
      if (at::functionalization::impl::isFunctionalTensor(mat2)) {
        at::functionalization::impl::sync(mat2);
        mat2_ = at::functionalization::impl::from_functional_tensor(mat2);
      } else {
        mat2_ = mat2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || mat1.device().type() == c10::DeviceType::XLA || mat2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(mat1) || at::functionalization::impl::isFunctionalTensor(mat2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::addmm_::call(self_, mat1_, mat2_, beta, alpha);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::addmm::call(self_, mat1_, mat2_, beta, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _scaled_mm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, const c10::optional<at::Tensor> & bias, c10::optional<at::ScalarType> out_dtype, const c10::optional<at::Tensor> & scale_a, const c10::optional<at::Tensor> & scale_b, const c10::optional<at::Tensor> & scale_result, bool use_fast_accum, at::Tensor & out, at::Tensor & out_amax) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mat2_meta = to_meta(mat2);
        auto bias_meta = to_meta(bias);
        auto scale_a_meta = to_meta(scale_a);
        auto scale_b_meta = to_meta(scale_b);
        auto scale_result_meta = to_meta(scale_result);
        auto out_meta = to_meta(out);
        auto out_amax_meta = to_meta(out_amax);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_scaled_mm_out::call(self_meta, mat2_meta, bias_meta, out_dtype, scale_a_meta, scale_b_meta, scale_result_meta, use_fast_accum, out_meta, out_amax_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mat2_;
      if (at::functionalization::impl::isFunctionalTensor(mat2)) {
        at::functionalization::impl::sync(mat2);
        mat2_ = at::functionalization::impl::from_functional_tensor(mat2);
      } else {
        mat2_ = mat2;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      c10::optional<at::Tensor> scale_a_;
      if (at::functionalization::impl::isFunctionalTensor(scale_a)) {
        at::functionalization::impl::sync(scale_a);
        scale_a_ = at::functionalization::impl::from_functional_tensor(scale_a);
      } else {
        scale_a_ = scale_a;
      }
      
      c10::optional<at::Tensor> scale_b_;
      if (at::functionalization::impl::isFunctionalTensor(scale_b)) {
        at::functionalization::impl::sync(scale_b);
        scale_b_ = at::functionalization::impl::from_functional_tensor(scale_b);
      } else {
        scale_b_ = scale_b;
      }
      
      c10::optional<at::Tensor> scale_result_;
      if (at::functionalization::impl::isFunctionalTensor(scale_result)) {
        at::functionalization::impl::sync(scale_result);
        scale_result_ = at::functionalization::impl::from_functional_tensor(scale_result);
      } else {
        scale_result_ = scale_result;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      
      at::Tensor out_amax_;
      if (at::functionalization::impl::isFunctionalTensor(out_amax)) {
        at::functionalization::impl::sync(out_amax);
        out_amax_ = at::functionalization::impl::from_functional_tensor(out_amax);
      } else {
        out_amax_ = out_amax;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out) && at::functionalization::impl::isFunctionalTensor(out_amax))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mat2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mat2) || at::functionalization::impl::isFunctionalTensor(bias) || at::functionalization::impl::isFunctionalTensor(scale_a) || at::functionalization::impl::isFunctionalTensor(scale_b) || at::functionalization::impl::isFunctionalTensor(scale_result))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_scaled_mm_out::call(self_, mat2_, bias_, out_dtype, scale_a_, scale_b_, scale_result_, use_fast_accum, out_, out_amax_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out, out_amax);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_scaled_mm::call(self_, mat2_, bias_, out_dtype, scale_a_, scale_b_, scale_result_, use_fast_accum);
        }
          at::functionalization::impl::propagate_xla_data(out, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
  at::functionalization::impl::propagate_xla_data(out_amax, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out_amax, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out_amax);
  at::functionalization::impl::sync(out_amax);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out, out_amax);
      }
    }

    at::Tensor & _sparse_coo_tensor_with_dims_out_out(c10::DispatchKeySet dispatchKeySet, int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sparse_coo_tensor_with_dims_out::call(sparse_dim, dense_dim, size, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sparse_coo_tensor_with_dims_out::call(sparse_dim, dense_dim, size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sparse_coo_tensor_with_dims::call(sparse_dim, dense_dim, size, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _sparse_coo_tensor_with_dims_and_tensors_out_out(c10::DispatchKeySet dispatchKeySet, int64_t sparse_dim, int64_t dense_dim, c10::SymIntArrayRef size, const at::Tensor & indices, const at::Tensor & values, c10::optional<bool> is_coalesced, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto indices_meta = to_meta(indices);
        auto values_meta = to_meta(values);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_sparse_coo_tensor_with_dims_and_tensors_out::call(sparse_dim, dense_dim, size, indices_meta, values_meta, is_coalesced, out_meta);
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor values_;
      if (at::functionalization::impl::isFunctionalTensor(values)) {
        at::functionalization::impl::sync(values);
        values_ = at::functionalization::impl::from_functional_tensor(values);
      } else {
        values_ = values;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || indices.device().type() == c10::DeviceType::XLA || values.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(values))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_sparse_coo_tensor_with_dims_and_tensors_out::call(sparse_dim, dense_dim, size, indices_, values_, is_coalesced, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_sparse_coo_tensor_with_dims_and_tensors::call(sparse_dim, dense_dim, size, indices_, values_, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt, is_coalesced);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & sparse_resize_and_clear_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim, const at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sparse_resize_and_clear_out::call(self_meta, size, sparse_dim, dense_dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sparse_resize_and_clear_out::call(self_, size, sparse_dim, dense_dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sparse_resize_and_clear::call(self_, size, sparse_dim, dense_dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & sparse_resize_and_clear_(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::sparse_resize_and_clear_::call(self_meta, size, sparse_dim, dense_dim);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::sparse_resize_and_clear_::call(self_, size, sparse_dim, dense_dim);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::sparse_resize_and_clear::call(self_, size, sparse_dim, dense_dim);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & hspmm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mat1, const at::Tensor & mat2, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto mat1_meta = to_meta(mat1);
        auto mat2_meta = to_meta(mat2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hspmm_out::call(mat1_meta, mat2_meta, out_meta);
      }
      
      at::Tensor mat1_;
      if (at::functionalization::impl::isFunctionalTensor(mat1)) {
        at::functionalization::impl::sync(mat1);
        mat1_ = at::functionalization::impl::from_functional_tensor(mat1);
      } else {
        mat1_ = mat1;
      }
      
      at::Tensor mat2_;
      if (at::functionalization::impl::isFunctionalTensor(mat2)) {
        at::functionalization::impl::sync(mat2);
        mat2_ = at::functionalization::impl::from_functional_tensor(mat2);
      } else {
        mat2_ = mat2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || mat1.device().type() == c10::DeviceType::XLA || mat2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(mat1) || at::functionalization::impl::isFunctionalTensor(mat2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hspmm_out::call(mat1_, mat2_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hspmm::call(mat1_, mat2_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _to_sparse_bsc_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef blocksize, c10::optional<int64_t> dense_dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_to_sparse_bsc_out::call(self_meta, blocksize, dense_dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_to_sparse_bsc_out::call(self_, blocksize, dense_dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_to_sparse_bsc::call(self_, blocksize, dense_dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & to_mkldnn_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::to_mkldnn_out::call(self_meta, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::to_mkldnn_out::call(self_, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::to_mkldnn::call(self_, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mkldnn_reorder_conv3d_weight_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef padding, c10::SymIntArrayRef stride, c10::SymIntArrayRef dilation, c10::SymInt groups, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mkldnn_reorder_conv3d_weight_out::call(self_meta, padding, stride, dilation, groups, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mkldnn_reorder_conv3d_weight_out::call(self_, padding, stride, dilation, groups, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mkldnn_reorder_conv3d_weight::call(self_, padding, stride, dilation, groups);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & q_per_channel_scales_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::q_per_channel_scales_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::q_per_channel_scales_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::q_per_channel_scales::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & int_repr_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::int_repr_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::int_repr_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::int_repr::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _make_per_tensor_quantized_tensor_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double scale, int64_t zero_point, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_make_per_tensor_quantized_tensor_out::call(self_meta, scale, zero_point, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_make_per_tensor_quantized_tensor_out::call(self_, scale, zero_point, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_make_per_tensor_quantized_tensor::call(self_, scale, zero_point);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _thnn_fused_lstm_cell_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & cx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto input_gates_meta = to_meta(input_gates);
        auto hidden_gates_meta = to_meta(hidden_gates);
        auto cx_meta = to_meta(cx);
        auto input_bias_meta = to_meta(input_bias);
        auto hidden_bias_meta = to_meta(hidden_bias);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_thnn_fused_lstm_cell_out::call(input_gates_meta, hidden_gates_meta, cx_meta, input_bias_meta, hidden_bias_meta, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor input_gates_;
      if (at::functionalization::impl::isFunctionalTensor(input_gates)) {
        at::functionalization::impl::sync(input_gates);
        input_gates_ = at::functionalization::impl::from_functional_tensor(input_gates);
      } else {
        input_gates_ = input_gates;
      }
      
      at::Tensor hidden_gates_;
      if (at::functionalization::impl::isFunctionalTensor(hidden_gates)) {
        at::functionalization::impl::sync(hidden_gates);
        hidden_gates_ = at::functionalization::impl::from_functional_tensor(hidden_gates);
      } else {
        hidden_gates_ = hidden_gates;
      }
      
      at::Tensor cx_;
      if (at::functionalization::impl::isFunctionalTensor(cx)) {
        at::functionalization::impl::sync(cx);
        cx_ = at::functionalization::impl::from_functional_tensor(cx);
      } else {
        cx_ = cx;
      }
      
      c10::optional<at::Tensor> input_bias_;
      if (at::functionalization::impl::isFunctionalTensor(input_bias)) {
        at::functionalization::impl::sync(input_bias);
        input_bias_ = at::functionalization::impl::from_functional_tensor(input_bias);
      } else {
        input_bias_ = input_bias;
      }
      
      c10::optional<at::Tensor> hidden_bias_;
      if (at::functionalization::impl::isFunctionalTensor(hidden_bias)) {
        at::functionalization::impl::sync(hidden_bias);
        hidden_bias_ = at::functionalization::impl::from_functional_tensor(hidden_bias);
      } else {
        hidden_bias_ = hidden_bias;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || input_gates.device().type() == c10::DeviceType::XLA || hidden_gates.device().type() == c10::DeviceType::XLA || cx.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(input_gates) || at::functionalization::impl::isFunctionalTensor(hidden_gates) || at::functionalization::impl::isFunctionalTensor(cx) || at::functionalization::impl::isFunctionalTensor(input_bias) || at::functionalization::impl::isFunctionalTensor(hidden_bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_thnn_fused_lstm_cell_out::call(input_gates_, hidden_gates_, cx_, input_bias_, hidden_bias_, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_thnn_fused_lstm_cell::call(input_gates_, hidden_gates_, cx_, input_bias_, hidden_bias_);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _thnn_fused_lstm_cell_backward_impl_out_out(c10::DispatchKeySet dispatchKeySet, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & cx, const at::Tensor & cy, const at::Tensor & workspace, bool has_bias, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_hy_meta = to_meta(grad_hy);
        auto grad_cy_meta = to_meta(grad_cy);
        auto cx_meta = to_meta(cx);
        auto cy_meta = to_meta(cy);
        auto workspace_meta = to_meta(workspace);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_thnn_fused_lstm_cell_backward_impl_out::call(grad_hy_meta, grad_cy_meta, cx_meta, cy_meta, workspace_meta, has_bias, out0_meta, out1_meta, out2_meta);
      }
      
      c10::optional<at::Tensor> grad_hy_;
      if (at::functionalization::impl::isFunctionalTensor(grad_hy)) {
        at::functionalization::impl::sync(grad_hy);
        grad_hy_ = at::functionalization::impl::from_functional_tensor(grad_hy);
      } else {
        grad_hy_ = grad_hy;
      }
      
      c10::optional<at::Tensor> grad_cy_;
      if (at::functionalization::impl::isFunctionalTensor(grad_cy)) {
        at::functionalization::impl::sync(grad_cy);
        grad_cy_ = at::functionalization::impl::from_functional_tensor(grad_cy);
      } else {
        grad_cy_ = grad_cy;
      }
      
      at::Tensor cx_;
      if (at::functionalization::impl::isFunctionalTensor(cx)) {
        at::functionalization::impl::sync(cx);
        cx_ = at::functionalization::impl::from_functional_tensor(cx);
      } else {
        cx_ = cx;
      }
      
      at::Tensor cy_;
      if (at::functionalization::impl::isFunctionalTensor(cy)) {
        at::functionalization::impl::sync(cy);
        cy_ = at::functionalization::impl::from_functional_tensor(cy);
      } else {
        cy_ = cy;
      }
      
      at::Tensor workspace_;
      if (at::functionalization::impl::isFunctionalTensor(workspace)) {
        at::functionalization::impl::sync(workspace);
        workspace_ = at::functionalization::impl::from_functional_tensor(workspace);
      } else {
        workspace_ = workspace;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || cx.device().type() == c10::DeviceType::XLA || cy.device().type() == c10::DeviceType::XLA || workspace.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_hy) || at::functionalization::impl::isFunctionalTensor(grad_cy) || at::functionalization::impl::isFunctionalTensor(cx) || at::functionalization::impl::isFunctionalTensor(cy) || at::functionalization::impl::isFunctionalTensor(workspace))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_thnn_fused_lstm_cell_backward_impl_out::call(grad_hy_, grad_cy_, cx_, cy_, workspace_, has_bias, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_thnn_fused_lstm_cell_backward_impl::call(grad_hy_, grad_cy_, cx_, cy_, workspace_, has_bias);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & masked_fill_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, const at::Scalar & value, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mask_meta = to_meta(mask);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::masked_fill_Scalar_out::call(self_meta, mask_meta, value, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mask.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mask))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::masked_fill_Scalar_out::call(self_, mask_, value, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::masked_fill_Scalar::call(self_, mask_, value);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & masked_fill__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mask_meta = to_meta(mask);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::masked_fill__Scalar::call(self_meta, mask_meta, value);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || mask.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(mask))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::masked_fill__Scalar::call(self_, mask_, value);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::masked_fill_Scalar::call(self_, mask_, value);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & masked_fill_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, const at::Tensor & value, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mask_meta = to_meta(mask);
        auto value_meta = to_meta(value);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::masked_fill_Tensor_out::call(self_meta, mask_meta, value_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor value_;
      if (at::functionalization::impl::isFunctionalTensor(value)) {
        at::functionalization::impl::sync(value);
        value_ = at::functionalization::impl::from_functional_tensor(value);
      } else {
        value_ = value;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mask.device().type() == c10::DeviceType::XLA || value.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mask) || at::functionalization::impl::isFunctionalTensor(value))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::masked_fill_Tensor_out::call(self_, mask_, value_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::masked_fill_Tensor::call(self_, mask_, value_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & masked_fill__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mask_meta = to_meta(mask);
        auto value_meta = to_meta(value);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::masked_fill__Tensor::call(self_meta, mask_meta, value_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor value_;
      if (at::functionalization::impl::isFunctionalTensor(value)) {
        at::functionalization::impl::sync(value);
        value_ = at::functionalization::impl::from_functional_tensor(value);
      } else {
        value_ = value;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || mask.device().type() == c10::DeviceType::XLA || value.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(mask) || at::functionalization::impl::isFunctionalTensor(value))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::masked_fill__Tensor::call(self_, mask_, value_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::masked_fill_Tensor::call(self_, mask_, value_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & _masked_softmax_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, c10::optional<int64_t> dim, c10::optional<int64_t> mask_type, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto mask_meta = to_meta(mask);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_masked_softmax_out::call(self_meta, mask_meta, dim, mask_type, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || mask.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(mask))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_masked_softmax_out::call(self_, mask_, dim, mask_type, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_masked_softmax::call(self_, mask_, dim, mask_type);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_right_shift_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_right_shift_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_right_shift_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_right_shift_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_right_shift__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_right_shift__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_right_shift__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_right_shift_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bitwise_right_shift_out_Tensor_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_right_shift_Tensor_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_right_shift_Tensor_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_right_shift_Tensor_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bitwise_right_shift__Tensor_Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_right_shift__Tensor_Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_right_shift__Tensor_Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_right_shift_Tensor_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & bitwise_right_shift_out_Scalar_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bitwise_right_shift_Scalar_Tensor_out::call(self, other_meta, out_meta);
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bitwise_right_shift_Scalar_Tensor_out::call(self, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bitwise_right_shift_Scalar_Tensor::call(self, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cauchy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double median, double sigma, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cauchy_out::call(self_meta, median, sigma, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cauchy_out::call(self_, median, sigma, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cauchy::call(self_, median, sigma, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & cauchy_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double median, double sigma, c10::optional<at::Generator> generator) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::cauchy_::call(self_meta, median, sigma, generator);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::cauchy_::call(self_, median, sigma, generator);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::cauchy::call(self_, median, sigma, generator);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & log_normal_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::log_normal_out::call(self_meta, mean, std, generator, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::log_normal_out::call(self_, mean, std, generator, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::log_normal::call(self_, mean, std, generator);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & log_normal_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::log_normal_::call(self_meta, mean, std, generator);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::log_normal_::call(self_, mean, std, generator);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::log_normal::call(self_, mean, std, generator);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & diag_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::diag_out::call(self_meta, diagonal, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::diag_out::call(self_, diagonal, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::diag::call(self_, diagonal);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & ne_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ne_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ne_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ne_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & ne__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ne__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ne__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ne_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & ne_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ne_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ne_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ne_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & ne__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ne__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ne__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ne_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & not_equal_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::not_equal_Scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::not_equal_Scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::not_equal_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & not_equal__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::not_equal__Scalar::call(self_meta, other);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::not_equal__Scalar::call(self_, other);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::not_equal_Scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & not_equal_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::not_equal_Tensor_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::not_equal_Tensor_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::not_equal_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & not_equal__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::not_equal__Tensor::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::not_equal__Tensor::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::not_equal_Tensor::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & addcmul_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::addcmul_out::call(self_meta, tensor1_meta, tensor2_meta, value, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1;
      }
      
      at::Tensor tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || tensor1.device().type() == c10::DeviceType::XLA || tensor2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::addcmul_out::call(self_, tensor1_, tensor2_, value, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::addcmul::call(self_, tensor1_, tensor2_, value);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & addcmul_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::addcmul_::call(self_meta, tensor1_meta, tensor2_meta, value);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1;
      }
      
      at::Tensor tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || tensor1.device().type() == c10::DeviceType::XLA || tensor2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::addcmul_::call(self_, tensor1_, tensor2_, value);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::addcmul::call(self_, tensor1_, tensor2_, value);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & ormqr_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto input2_meta = to_meta(input2);
        auto input3_meta = to_meta(input3);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ormqr_out::call(self_meta, input2_meta, input3_meta, left, transpose, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor input2_;
      if (at::functionalization::impl::isFunctionalTensor(input2)) {
        at::functionalization::impl::sync(input2);
        input2_ = at::functionalization::impl::from_functional_tensor(input2);
      } else {
        input2_ = input2;
      }
      
      at::Tensor input3_;
      if (at::functionalization::impl::isFunctionalTensor(input3)) {
        at::functionalization::impl::sync(input3);
        input3_ = at::functionalization::impl::from_functional_tensor(input3);
      } else {
        input3_ = input3;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || input2.device().type() == c10::DeviceType::XLA || input3.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(input2) || at::functionalization::impl::isFunctionalTensor(input3))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ormqr_out::call(self_, input2_, input3_, left, transpose, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ormqr::call(self_, input2_, input3_, left, transpose);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> lu_unpack_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots, at::Tensor & P, at::Tensor & L, at::Tensor & U) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto LU_data_meta = to_meta(LU_data);
        auto LU_pivots_meta = to_meta(LU_pivots);
        auto P_meta = to_meta(P);
        auto L_meta = to_meta(L);
        auto U_meta = to_meta(U);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::lu_unpack_out::call(LU_data_meta, LU_pivots_meta, unpack_data, unpack_pivots, P_meta, L_meta, U_meta);
      }
      
      at::Tensor LU_data_;
      if (at::functionalization::impl::isFunctionalTensor(LU_data)) {
        at::functionalization::impl::sync(LU_data);
        LU_data_ = at::functionalization::impl::from_functional_tensor(LU_data);
      } else {
        LU_data_ = LU_data;
      }
      
      at::Tensor LU_pivots_;
      if (at::functionalization::impl::isFunctionalTensor(LU_pivots)) {
        at::functionalization::impl::sync(LU_pivots);
        LU_pivots_ = at::functionalization::impl::from_functional_tensor(LU_pivots);
      } else {
        LU_pivots_ = LU_pivots;
      }
      
      at::Tensor P_;
      if (at::functionalization::impl::isFunctionalTensor(P)) {
        at::functionalization::impl::sync(P);
        P_ = at::functionalization::impl::from_functional_tensor(P);
      } else {
        P_ = P;
      }
      
      at::Tensor L_;
      if (at::functionalization::impl::isFunctionalTensor(L)) {
        at::functionalization::impl::sync(L);
        L_ = at::functionalization::impl::from_functional_tensor(L);
      } else {
        L_ = L;
      }
      
      at::Tensor U_;
      if (at::functionalization::impl::isFunctionalTensor(U)) {
        at::functionalization::impl::sync(U);
        U_ = at::functionalization::impl::from_functional_tensor(U);
      } else {
        U_ = U;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(P) && at::functionalization::impl::isFunctionalTensor(L) && at::functionalization::impl::isFunctionalTensor(U))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || LU_data.device().type() == c10::DeviceType::XLA || LU_pivots.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(LU_data) || at::functionalization::impl::isFunctionalTensor(LU_pivots))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::lu_unpack_out::call(LU_data_, LU_pivots_, unpack_data, unpack_pivots, P_, L_, U_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(P, L, U);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::lu_unpack::call(LU_data_, LU_pivots_, unpack_data, unpack_pivots);
        }
          at::functionalization::impl::propagate_xla_data(P, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(P, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(P);
  at::functionalization::impl::sync(P);
  at::functionalization::impl::propagate_xla_data(L, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(L, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(L);
  at::functionalization::impl::sync(L);
  at::functionalization::impl::propagate_xla_data(U, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(U, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(U);
  at::functionalization::impl::sync(U);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(P, L, U);
      }
    }

    at::Tensor & dist_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & p, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::dist_out::call(self_meta, other_meta, p, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::dist_out::call(self_, other_, p, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::dist::call(self_, other_, p);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & arctan2_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arctan2_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arctan2_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arctan2::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & arctan2_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::arctan2_::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::arctan2_::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::arctan2::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & histc_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t bins, const at::Scalar & min, const at::Scalar & max, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::histc_out::call(self_meta, bins, min, max, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::histc_out::call(self_, bins, min, max, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::histc::call(self_, bins, min, max);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _histogramdd_from_bin_cts_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_histogramdd_from_bin_cts_out::call(self_meta, bins, range, weight_meta, density, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_histogramdd_from_bin_cts_out::call(self_, bins, range, weight_, density, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_histogramdd_from_bin_cts::call(self_, bins, range, weight_, density);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hypot_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hypot_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hypot_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hypot::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hypot_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hypot_::call(self_meta, other_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hypot_::call(self_, other_);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hypot::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & min_out_unary_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::min_unary_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::min_unary_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::min::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & min_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::min_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::min_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::min_other::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & msort_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::msort_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::msort_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::msort::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    void _foreach_add_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_add_Scalar_out::call(self_meta, scalar, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_add_Scalar_out::call(self_, scalar, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_add_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_add__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_add__Scalar::call(self_meta, scalar);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_add__Scalar::call(self_, scalar);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_add_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_add_out_List_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, const at::Scalar & alpha, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_add_List_out::call(self_meta, other_meta, alpha, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_add_List_out::call(self_, other_, alpha, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_add_List::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_add__List(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_add__List::call(self_meta, other_meta, alpha);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_add__List::call(self_, other_, alpha);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_add_List::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_add_out_ScalarList_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_add_ScalarList_out::call(self_meta, scalars, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_add_ScalarList_out::call(self_, scalars, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_add_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_add__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_add__ScalarList::call(self_meta, scalars);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_add__ScalarList::call(self_, scalars);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_add_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_add_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Tensor & other, const at::Scalar & alpha, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_add_Tensor_out::call(self_meta, other_meta, alpha, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_add_Tensor_out::call(self_, other_, alpha, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_add_Tensor::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_add__Tensor(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Tensor & other, const at::Scalar & alpha) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_add__Tensor::call(self_meta, other_meta, alpha);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_add__Tensor::call(self_, other_, alpha);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_add_Tensor::call(self_, other_, alpha);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_clamp_min_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_clamp_min_Scalar_out::call(self_meta, scalar, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_clamp_min_Scalar_out::call(self_, scalar, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_clamp_min_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_clamp_min__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_clamp_min__Scalar::call(self_meta, scalar);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_clamp_min__Scalar::call(self_, scalar);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_clamp_min_Scalar::call(self_, scalar);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_clamp_min_out_List_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_clamp_min_List_out::call(self_meta, other_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_clamp_min_List_out::call(self_, other_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_clamp_min_List::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_clamp_min__List(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_clamp_min__List::call(self_meta, other_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_clamp_min__List::call(self_, other_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_clamp_min_List::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_clamp_min_out_ScalarList_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_clamp_min_ScalarList_out::call(self_meta, scalars, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_clamp_min_ScalarList_out::call(self_, scalars, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_clamp_min_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_clamp_min__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_clamp_min__ScalarList::call(self_meta, scalars);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_clamp_min__ScalarList::call(self_, scalars);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_clamp_min_ScalarList::call(self_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_addcdiv_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_addcdiv_Scalar_out::call(self_meta, tensor1_meta, tensor2_meta, value, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1.vec();
      }
      
      ::std::vector<at::Tensor> tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_addcdiv_Scalar_out::call(self_, tensor1_, tensor2_, value, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_addcdiv_Scalar::call(self_, tensor1_, tensor2_, value);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_addcdiv__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_addcdiv__Scalar::call(self_meta, tensor1_meta, tensor2_meta, value);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1.vec();
      }
      
      ::std::vector<at::Tensor> tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_addcdiv__Scalar::call(self_, tensor1_, tensor2_, value);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_addcdiv_Scalar::call(self_, tensor1_, tensor2_, value);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_addcdiv_out_ScalarList_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_addcdiv_ScalarList_out::call(self_meta, tensor1_meta, tensor2_meta, scalars, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1.vec();
      }
      
      ::std::vector<at::Tensor> tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_addcdiv_ScalarList_out::call(self_, tensor1_, tensor2_, scalars, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_addcdiv_ScalarList::call(self_, tensor1_, tensor2_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_addcdiv__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_addcdiv__ScalarList::call(self_meta, tensor1_meta, tensor2_meta, scalars);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1.vec();
      }
      
      ::std::vector<at::Tensor> tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_addcdiv__ScalarList::call(self_, tensor1_, tensor2_, scalars);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_addcdiv_ScalarList::call(self_, tensor1_, tensor2_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_addcdiv_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Tensor & scalars, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        auto scalars_meta = to_meta(scalars);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_addcdiv_Tensor_out::call(self_meta, tensor1_meta, tensor2_meta, scalars_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1.vec();
      }
      
      ::std::vector<at::Tensor> tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2.vec();
      }
      
      at::Tensor scalars_;
      if (at::functionalization::impl::isFunctionalTensor(scalars)) {
        at::functionalization::impl::sync(scalars);
        scalars_ = at::functionalization::impl::from_functional_tensor(scalars);
      } else {
        scalars_ = scalars;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || scalars.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2) || at::functionalization::impl::isFunctionalTensor(scalars))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_addcdiv_Tensor_out::call(self_, tensor1_, tensor2_, scalars_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_addcdiv_Tensor::call(self_, tensor1_, tensor2_, scalars_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_addcdiv__Tensor(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Tensor & scalars) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        auto scalars_meta = to_meta(scalars);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_addcdiv__Tensor::call(self_meta, tensor1_meta, tensor2_meta, scalars_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1.vec();
      }
      
      ::std::vector<at::Tensor> tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2.vec();
      }
      
      at::Tensor scalars_;
      if (at::functionalization::impl::isFunctionalTensor(scalars)) {
        at::functionalization::impl::sync(scalars);
        scalars_ = at::functionalization::impl::from_functional_tensor(scalars);
      } else {
        scalars_ = scalars;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || scalars.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2) || at::functionalization::impl::isFunctionalTensor(scalars))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_addcdiv__Tensor::call(self_, tensor1_, tensor2_, scalars_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_addcdiv_Tensor::call(self_, tensor1_, tensor2_, scalars_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_addcmul_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_addcmul_Scalar_out::call(self_meta, tensor1_meta, tensor2_meta, value, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1.vec();
      }
      
      ::std::vector<at::Tensor> tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_addcmul_Scalar_out::call(self_, tensor1_, tensor2_, value, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_addcmul_Scalar::call(self_, tensor1_, tensor2_, value);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_addcmul__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_addcmul__Scalar::call(self_meta, tensor1_meta, tensor2_meta, value);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1.vec();
      }
      
      ::std::vector<at::Tensor> tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_addcmul__Scalar::call(self_, tensor1_, tensor2_, value);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_addcmul_Scalar::call(self_, tensor1_, tensor2_, value);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_addcmul_out_ScalarList_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_addcmul_ScalarList_out::call(self_meta, tensor1_meta, tensor2_meta, scalars, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1.vec();
      }
      
      ::std::vector<at::Tensor> tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_addcmul_ScalarList_out::call(self_, tensor1_, tensor2_, scalars, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_addcmul_ScalarList::call(self_, tensor1_, tensor2_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_addcmul__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_addcmul__ScalarList::call(self_meta, tensor1_meta, tensor2_meta, scalars);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1.vec();
      }
      
      ::std::vector<at::Tensor> tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_addcmul__ScalarList::call(self_, tensor1_, tensor2_, scalars);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_addcmul_ScalarList::call(self_, tensor1_, tensor2_, scalars);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_addcmul_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Tensor & scalars, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        auto scalars_meta = to_meta(scalars);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_addcmul_Tensor_out::call(self_meta, tensor1_meta, tensor2_meta, scalars_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1.vec();
      }
      
      ::std::vector<at::Tensor> tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2.vec();
      }
      
      at::Tensor scalars_;
      if (at::functionalization::impl::isFunctionalTensor(scalars)) {
        at::functionalization::impl::sync(scalars);
        scalars_ = at::functionalization::impl::from_functional_tensor(scalars);
      } else {
        scalars_ = scalars;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || scalars.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2) || at::functionalization::impl::isFunctionalTensor(scalars))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_addcmul_Tensor_out::call(self_, tensor1_, tensor2_, scalars_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_addcmul_Tensor::call(self_, tensor1_, tensor2_, scalars_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_addcmul__Tensor(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Tensor & scalars) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto tensor1_meta = to_meta(tensor1);
        auto tensor2_meta = to_meta(tensor2);
        auto scalars_meta = to_meta(scalars);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_addcmul__Tensor::call(self_meta, tensor1_meta, tensor2_meta, scalars_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> tensor1_;
      if (at::functionalization::impl::isFunctionalTensor(tensor1)) {
        at::functionalization::impl::sync(tensor1);
        tensor1_ = at::functionalization::impl::from_functional_tensor(tensor1);
      } else {
        tensor1_ = tensor1.vec();
      }
      
      ::std::vector<at::Tensor> tensor2_;
      if (at::functionalization::impl::isFunctionalTensor(tensor2)) {
        at::functionalization::impl::sync(tensor2);
        tensor2_ = at::functionalization::impl::from_functional_tensor(tensor2);
      } else {
        tensor2_ = tensor2.vec();
      }
      
      at::Tensor scalars_;
      if (at::functionalization::impl::isFunctionalTensor(scalars)) {
        at::functionalization::impl::sync(scalars);
        scalars_ = at::functionalization::impl::from_functional_tensor(scalars);
      } else {
        scalars_ = scalars;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || scalars.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(tensor1) || at::functionalization::impl::isFunctionalTensor(tensor2) || at::functionalization::impl::isFunctionalTensor(scalars))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_addcmul__Tensor::call(self_, tensor1_, tensor2_, scalars_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_addcmul_Tensor::call(self_, tensor1_, tensor2_, scalars_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_asin_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_asin_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_asin_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_asin::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_asin_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_asin_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_asin_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_asin::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_ceil_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_ceil_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_ceil_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_ceil::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_ceil_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_ceil_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_ceil_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_ceil::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_cosh_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_cosh_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_cosh_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_cosh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_cosh_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_cosh_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_cosh_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_cosh::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_log1p_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_log1p_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_log1p_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_log1p::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_log1p_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_log1p_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_log1p_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_log1p::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_log2_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_log2_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_log2_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_log2::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_log2_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_log2_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_log2_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_log2::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_pow_out_List_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList exponent, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto exponent_meta = to_meta(exponent);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_pow_List_out::call(self_meta, exponent_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> exponent_;
      if (at::functionalization::impl::isFunctionalTensor(exponent)) {
        at::functionalization::impl::sync(exponent);
        exponent_ = at::functionalization::impl::from_functional_tensor(exponent);
      } else {
        exponent_ = exponent.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(exponent))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_pow_List_out::call(self_, exponent_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_pow_List::call(self_, exponent_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_pow__List(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList exponent) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto exponent_meta = to_meta(exponent);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_pow__List::call(self_meta, exponent_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> exponent_;
      if (at::functionalization::impl::isFunctionalTensor(exponent)) {
        at::functionalization::impl::sync(exponent);
        exponent_ = at::functionalization::impl::from_functional_tensor(exponent);
      } else {
        exponent_ = exponent.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(exponent))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_pow__List::call(self_, exponent_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_pow_List::call(self_, exponent_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_pow_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & exponent, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_pow_Scalar_out::call(self_meta, exponent, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_pow_Scalar_out::call(self_, exponent, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_pow_Scalar::call(self_, exponent);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_pow__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & exponent) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_pow__Scalar::call(self_meta, exponent);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_pow__Scalar::call(self_, exponent);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_pow_Scalar::call(self_, exponent);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_pow_out_ScalarList_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> exponent, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_pow_ScalarList_out::call(self_meta, exponent, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_pow_ScalarList_out::call(self_, exponent, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_pow_ScalarList::call(self_, exponent);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_pow__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> exponent) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_pow__ScalarList::call(self_meta, exponent);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_pow__ScalarList::call(self_, exponent);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_pow_ScalarList::call(self_, exponent);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_round_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_round_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_round_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_round::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_round_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_round_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_round_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_round::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    void _foreach_zero_out_out(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_zero_out::call(self_meta, out_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_zero_out::call(self_, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_zero::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    void _foreach_zero_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foreach_zero_::call(self_meta);
      }
      
      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::_foreach_zero_::call(self_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foreach_zero::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    
      }
    }

    at::Tensor & bucketize_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & boundaries, bool out_int32, bool right, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto boundaries_meta = to_meta(boundaries);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bucketize_Tensor_out::call(self_meta, boundaries_meta, out_int32, right, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor boundaries_;
      if (at::functionalization::impl::isFunctionalTensor(boundaries)) {
        at::functionalization::impl::sync(boundaries);
        boundaries_ = at::functionalization::impl::from_functional_tensor(boundaries);
      } else {
        boundaries_ = boundaries;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || boundaries.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(boundaries))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bucketize_Tensor_out::call(self_, boundaries_, out_int32, right, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bucketize_Tensor::call(self_, boundaries_, out_int32, right);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & bucketize_out_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & boundaries, bool out_int32, bool right, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto boundaries_meta = to_meta(boundaries);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::bucketize_Scalar_out::call(self, boundaries_meta, out_int32, right, out_meta);
      }
      
      at::Tensor boundaries_;
      if (at::functionalization::impl::isFunctionalTensor(boundaries)) {
        at::functionalization::impl::sync(boundaries);
        boundaries_ = at::functionalization::impl::from_functional_tensor(boundaries);
      } else {
        boundaries_ = boundaries;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || boundaries.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(boundaries))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::bucketize_Scalar_out::call(self, boundaries_, out_int32, right, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::bucketize_Scalar::call(self, boundaries_, out_int32, right);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & mse_loss_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::mse_loss_out::call(self_meta, target_meta, reduction, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::mse_loss_out::call(self_, target_, reduction, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::mse_loss::call(self_, target_, reduction);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & multi_margin_loss_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto weight_meta = to_meta(weight);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::multi_margin_loss_out::call(self_meta, target_meta, p, margin, weight_meta, reduction, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::multi_margin_loss_out::call(self_, target_, p, margin, weight_, reduction, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::multi_margin_loss::call(self_, target_, p, margin, weight_, reduction);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & multilabel_margin_loss_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::multilabel_margin_loss_out::call(self_meta, target_meta, reduction, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::multilabel_margin_loss_out::call(self_, target_, reduction, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::multilabel_margin_loss::call(self_, target_, reduction);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> multilabel_margin_loss_forward_out_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & output, at::Tensor & is_target) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto output_meta = to_meta(output);
        auto is_target_meta = to_meta(is_target);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::multilabel_margin_loss_forward_output::call(self_meta, target_meta, reduction, output_meta, is_target_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor is_target_;
      if (at::functionalization::impl::isFunctionalTensor(is_target)) {
        at::functionalization::impl::sync(is_target);
        is_target_ = at::functionalization::impl::from_functional_tensor(is_target);
      } else {
        is_target_ = is_target;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(output) && at::functionalization::impl::isFunctionalTensor(is_target))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::multilabel_margin_loss_forward_output::call(self_, target_, reduction, output_, is_target_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(output, is_target);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::multilabel_margin_loss_forward::call(self_, target_, reduction);
        }
          at::functionalization::impl::propagate_xla_data(output, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(output, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(output);
  at::functionalization::impl::sync(output);
  at::functionalization::impl::propagate_xla_data(is_target, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(is_target, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(is_target);
  at::functionalization::impl::sync(is_target);
    return ::std::tuple<at::Tensor &,at::Tensor &>(output, is_target);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> nll_loss2d_forward_out_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & output, at::Tensor & total_weight) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto weight_meta = to_meta(weight);
        auto output_meta = to_meta(output);
        auto total_weight_meta = to_meta(total_weight);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::nll_loss2d_forward_output::call(self_meta, target_meta, weight_meta, reduction, ignore_index, output_meta, total_weight_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      c10::optional<at::Tensor> weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      
      at::Tensor total_weight_;
      if (at::functionalization::impl::isFunctionalTensor(total_weight)) {
        at::functionalization::impl::sync(total_weight);
        total_weight_ = at::functionalization::impl::from_functional_tensor(total_weight);
      } else {
        total_weight_ = total_weight;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(output) && at::functionalization::impl::isFunctionalTensor(total_weight))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::nll_loss2d_forward_output::call(self_, target_, weight_, reduction, ignore_index, output_, total_weight_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(output, total_weight);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::nll_loss2d_forward::call(self_, target_, weight_, reduction, ignore_index);
        }
          at::functionalization::impl::propagate_xla_data(output, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(output, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(output);
  at::functionalization::impl::sync(output);
  at::functionalization::impl::propagate_xla_data(total_weight, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(total_weight, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(total_weight);
  at::functionalization::impl::sync(total_weight);
    return ::std::tuple<at::Tensor &,at::Tensor &>(output, total_weight);
      }
    }

    at::Tensor & soft_margin_loss_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto target_meta = to_meta(target);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::soft_margin_loss_out::call(self_meta, target_meta, reduction, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor target_;
      if (at::functionalization::impl::isFunctionalTensor(target)) {
        at::functionalization::impl::sync(target);
        target_ = at::functionalization::impl::from_functional_tensor(target);
      } else {
        target_ = target;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || target.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(target))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::soft_margin_loss_out::call(self_, target_, reduction, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::soft_margin_loss::call(self_, target_, reduction);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & glu_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, int64_t dim, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::glu_backward_grad_input::call(grad_output_meta, self_meta, dim, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::glu_backward_grad_input::call(grad_output_, self_, dim, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::glu_backward::call(grad_output_, self_, dim);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & glu_backward_jvp_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_x, const at::Tensor & grad_glu, const at::Tensor & x, const at::Tensor & dgrad_glu, const at::Tensor & dx, int64_t dim, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_x_meta = to_meta(grad_x);
        auto grad_glu_meta = to_meta(grad_glu);
        auto x_meta = to_meta(x);
        auto dgrad_glu_meta = to_meta(dgrad_glu);
        auto dx_meta = to_meta(dx);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::glu_backward_jvp_out::call(grad_x_meta, grad_glu_meta, x_meta, dgrad_glu_meta, dx_meta, dim, out_meta);
      }
      
      at::Tensor grad_x_;
      if (at::functionalization::impl::isFunctionalTensor(grad_x)) {
        at::functionalization::impl::sync(grad_x);
        grad_x_ = at::functionalization::impl::from_functional_tensor(grad_x);
      } else {
        grad_x_ = grad_x;
      }
      
      at::Tensor grad_glu_;
      if (at::functionalization::impl::isFunctionalTensor(grad_glu)) {
        at::functionalization::impl::sync(grad_glu);
        grad_glu_ = at::functionalization::impl::from_functional_tensor(grad_glu);
      } else {
        grad_glu_ = grad_glu;
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor dgrad_glu_;
      if (at::functionalization::impl::isFunctionalTensor(dgrad_glu)) {
        at::functionalization::impl::sync(dgrad_glu);
        dgrad_glu_ = at::functionalization::impl::from_functional_tensor(dgrad_glu);
      } else {
        dgrad_glu_ = dgrad_glu;
      }
      
      at::Tensor dx_;
      if (at::functionalization::impl::isFunctionalTensor(dx)) {
        at::functionalization::impl::sync(dx);
        dx_ = at::functionalization::impl::from_functional_tensor(dx);
      } else {
        dx_ = dx;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_x.device().type() == c10::DeviceType::XLA || grad_glu.device().type() == c10::DeviceType::XLA || x.device().type() == c10::DeviceType::XLA || dgrad_glu.device().type() == c10::DeviceType::XLA || dx.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_x) || at::functionalization::impl::isFunctionalTensor(grad_glu) || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(dgrad_glu) || at::functionalization::impl::isFunctionalTensor(dx))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::glu_backward_jvp_out::call(grad_x_, grad_glu_, x_, dgrad_glu_, dx_, dim, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::glu_backward_jvp::call(grad_x_, grad_glu_, x_, dgrad_glu_, dx_, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hardtanh_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hardtanh_out::call(self_meta, min_val, max_val, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hardtanh_out::call(self_, min_val, max_val, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hardtanh::call(self_, min_val, max_val);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & hardtanh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hardtanh_::call(self_meta, min_val, max_val);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hardtanh_::call(self_, min_val, max_val);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hardtanh::call(self_, min_val, max_val);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & hardtanh_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::hardtanh_backward_grad_input::call(grad_output_meta, self_meta, min_val, max_val, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::hardtanh_backward_grad_input::call(grad_output_, self_, min_val, max_val, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::hardtanh_backward::call(grad_output_, self_, min_val, max_val);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & leaky_relu_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & negative_slope, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::leaky_relu_out::call(self_meta, negative_slope, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::leaky_relu_out::call(self_, negative_slope, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::leaky_relu::call(self_, negative_slope);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & leaky_relu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & negative_slope) {
      if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::leaky_relu_::call(self_meta, negative_slope);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::leaky_relu_::call(self_, negative_slope);
         return self;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::leaky_relu::call(self_, negative_slope);
        }
          at::functionalization::impl::propagate_xla_data(self, tmp_output);
  at::functionalization::impl::replace_(self, tmp_output);
  at::functionalization::impl::commit_update(self);
  at::functionalization::impl::sync(self);
    return self;
      }
    }

    at::Tensor & log_sigmoid_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto buffer_meta = to_meta(buffer);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::log_sigmoid_backward_grad_input::call(grad_output_meta, self_meta, buffer_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor buffer_;
      if (at::functionalization::impl::isFunctionalTensor(buffer)) {
        at::functionalization::impl::sync(buffer);
        buffer_ = at::functionalization::impl::from_functional_tensor(buffer);
      } else {
        buffer_ = buffer;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || buffer.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(buffer))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::log_sigmoid_backward_grad_input::call(grad_output_, self_, buffer_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::log_sigmoid_backward::call(grad_output_, self_, buffer_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & softplus_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::softplus_out::call(self_meta, beta, threshold, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::softplus_out::call(self_, beta, threshold, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::softplus::call(self_, beta, threshold);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & adaptive_avg_pool2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::adaptive_avg_pool2d_out::call(self_meta, output_size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::adaptive_avg_pool2d_out::call(self_, output_size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::adaptive_avg_pool2d::call(self_, output_size);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _adaptive_avg_pool3d_backward_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_adaptive_avg_pool3d_backward_out::call(grad_output_meta, self_meta, out_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_adaptive_avg_pool3d_backward_out::call(grad_output_, self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_adaptive_avg_pool3d_backward::call(grad_output_, self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & adaptive_max_pool2d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::adaptive_max_pool2d_backward_grad_input::call(grad_output_meta, self_meta, indices_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::adaptive_max_pool2d_backward_grad_input::call(grad_output_, self_, indices_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::adaptive_max_pool2d_backward::call(grad_output_, self_, indices_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & fractional_max_pool2d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fractional_max_pool2d_backward_grad_input::call(grad_output_meta, self_meta, kernel_size, output_size, indices_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fractional_max_pool2d_backward_grad_input::call(grad_output_, self_, kernel_size, output_size, indices_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fractional_max_pool2d_backward::call(grad_output_, self_, kernel_size, output_size, indices_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & fractional_max_pool3d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fractional_max_pool3d_backward_grad_input::call(grad_output_meta, self_meta, kernel_size, output_size, indices_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fractional_max_pool3d_backward_grad_input::call(grad_output_, self_, kernel_size, output_size, indices_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fractional_max_pool3d_backward::call(grad_output_, self_, kernel_size, output_size, indices_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> max_pool3d_with_indices_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out, at::Tensor & indices) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        auto indices_meta = to_meta(indices);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::max_pool3d_with_indices_out::call(self_meta, kernel_size, stride, padding, dilation, ceil_mode, out_meta, indices_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out) && at::functionalization::impl::isFunctionalTensor(indices))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::max_pool3d_with_indices_out::call(self_, kernel_size, stride, padding, dilation, ceil_mode, out_, indices_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out, indices);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::max_pool3d_with_indices::call(self_, kernel_size, stride, padding, dilation, ceil_mode);
        }
          at::functionalization::impl::propagate_xla_data(out, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
  at::functionalization::impl::propagate_xla_data(indices, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(indices, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(indices);
  at::functionalization::impl::sync(indices);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out, indices);
      }
    }

    at::Tensor & max_pool3d_with_indices_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::max_pool3d_with_indices_backward_grad_input::call(grad_output_meta, self_meta, kernel_size, stride, padding, dilation, ceil_mode, indices_meta, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::max_pool3d_with_indices_backward_grad_input::call(grad_output_, self_, kernel_size, stride, padding, dilation, ceil_mode, indices_, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::max_pool3d_with_indices_backward::call(grad_output_, self_, kernel_size, stride, padding, dilation, ceil_mode, indices_);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & max_unpool2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::max_unpool2d_out::call(self_meta, indices_meta, output_size, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || indices.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(indices))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::max_unpool2d_out::call(self_, indices_, output_size, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::max_unpool2d::call(self_, indices_, output_size);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & reflection_pad2d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::reflection_pad2d_backward_grad_input::call(grad_output_meta, self_meta, padding, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::reflection_pad2d_backward_grad_input::call(grad_output_, self_, padding, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::reflection_pad2d_backward::call(grad_output_, self_, padding);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & upsample_bilinear2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::upsample_bilinear2d_out::call(self_meta, output_size, align_corners, scales_h, scales_w, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::upsample_bilinear2d_out::call(self_, output_size, align_corners, scales_h, scales_w, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::upsample_bilinear2d::call(self_, output_size, align_corners, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & upsample_bilinear2d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::upsample_bilinear2d_backward_grad_input::call(grad_output_meta, output_size, input_size, align_corners, scales_h, scales_w, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::upsample_bilinear2d_backward_grad_input::call(grad_output_, output_size, input_size, align_corners, scales_h, scales_w, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::upsample_bilinear2d_backward::call(grad_output_, output_size, input_size, align_corners, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & _upsample_bilinear2d_aa_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_upsample_bilinear2d_aa_out::call(self_meta, output_size, align_corners, scales_h, scales_w, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_upsample_bilinear2d_aa_out::call(self_, output_size, align_corners, scales_h, scales_w, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_upsample_bilinear2d_aa::call(self_, output_size, align_corners, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & upsample_trilinear3d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::upsample_trilinear3d_backward_grad_input::call(grad_output_meta, output_size, input_size, align_corners, scales_d, scales_h, scales_w, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::upsample_trilinear3d_backward_grad_input::call(grad_output_, output_size, input_size, align_corners, scales_d, scales_h, scales_w, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::upsample_trilinear3d_backward::call(grad_output_, output_size, input_size, align_corners, scales_d, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & _upsample_nearest_exact3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymIntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_upsample_nearest_exact3d_out::call(self_meta, output_size, scales_d, scales_h, scales_w, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_upsample_nearest_exact3d_out::call(self_, output_size, scales_d, scales_h, scales_w, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_upsample_nearest_exact3d::call(self_, output_size, scales_d, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & upsample_nearest3d_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::upsample_nearest3d_backward_grad_input::call(grad_output_meta, output_size, input_size, scales_d, scales_h, scales_w, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::upsample_nearest3d_backward_grad_input::call(grad_output_, output_size, input_size, scales_d, scales_h, scales_w, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::upsample_nearest3d_backward::call(grad_output_, output_size, input_size, scales_d, scales_h, scales_w);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & logit_backward_out_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps, at::Tensor & grad_input) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto grad_input_meta = to_meta(grad_input);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::logit_backward_grad_input::call(grad_output_meta, self_meta, eps, grad_input_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor grad_input_;
      if (at::functionalization::impl::isFunctionalTensor(grad_input)) {
        at::functionalization::impl::sync(grad_input);
        grad_input_ = at::functionalization::impl::from_functional_tensor(grad_input);
      } else {
        grad_input_ = grad_input;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(grad_input))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::logit_backward_grad_input::call(grad_output_, self_, eps, grad_input_);
         return grad_input;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::logit_backward::call(grad_output_, self_, eps);
        }
          at::functionalization::impl::propagate_xla_data(grad_input, tmp_output);
  at::functionalization::impl::replace_(grad_input, tmp_output);
  at::functionalization::impl::commit_update(grad_input);
  at::functionalization::impl::sync(grad_input);
    return grad_input;
      }
    }

    at::Tensor & thnn_conv2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::thnn_conv2d_out::call(self_meta, weight_meta, kernel_size, bias_meta, stride, padding, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::thnn_conv2d_out::call(self_, weight_, kernel_size, bias_, stride, padding, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::thnn_conv2d::call(self_, weight_, kernel_size, bias_, stride, padding);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> _slow_conv2d_backward_out_output_mask_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto grad_output_meta = to_meta(grad_output);
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        auto out2_meta = to_meta(out2);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_slow_conv2d_backward_output_mask_out::call(grad_output_meta, self_meta, weight_meta, kernel_size, stride, padding, output_mask, out0_meta, out1_meta, out2_meta);
      }
      
      at::Tensor grad_output_;
      if (at::functionalization::impl::isFunctionalTensor(grad_output)) {
        at::functionalization::impl::sync(grad_output);
        grad_output_ = at::functionalization::impl::from_functional_tensor(grad_output);
      } else {
        grad_output_ = grad_output;
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      
      at::Tensor out2_;
      if (at::functionalization::impl::isFunctionalTensor(out2)) {
        at::functionalization::impl::sync(out2);
        out2_ = at::functionalization::impl::from_functional_tensor(out2);
      } else {
        out2_ = out2;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1) && at::functionalization::impl::isFunctionalTensor(out2))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || grad_output.device().type() == c10::DeviceType::XLA || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(grad_output) || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::_slow_conv2d_backward_output_mask_out::call(grad_output_, self_, weight_, kernel_size, stride, padding, output_mask, out0_, out1_, out2_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_slow_conv2d_backward_output_mask::call(grad_output_, self_, weight_, kernel_size, stride, padding, output_mask);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
  at::functionalization::impl::propagate_xla_data(out2, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(out2, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(out2);
  at::functionalization::impl::sync(out2);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
      }
    }

    at::Tensor & slow_conv3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::slow_conv3d_out::call(self_meta, weight_meta, kernel_size, bias_meta, stride, padding, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::slow_conv3d_out::call(self_, weight_, kernel_size, bias_, stride, padding, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::slow_conv3d::call(self_, weight_, kernel_size, bias_, stride, padding);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & slow_conv3d_forward_out_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, at::Tensor & output) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto output_meta = to_meta(output);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::slow_conv3d_forward_output::call(self_meta, weight_meta, kernel_size, bias_meta, stride, padding, output_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor output_;
      if (at::functionalization::impl::isFunctionalTensor(output)) {
        at::functionalization::impl::sync(output);
        output_ = at::functionalization::impl::from_functional_tensor(output);
      } else {
        output_ = output;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(output))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::slow_conv3d_forward_output::call(self_, weight_, kernel_size, bias_, stride, padding, output_);
         return output;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::slow_conv3d_forward::call(self_, weight_, kernel_size, bias_, stride, padding);
        }
          at::functionalization::impl::propagate_xla_data(output, tmp_output);
  at::functionalization::impl::replace_(output, tmp_output);
  at::functionalization::impl::commit_update(output);
  at::functionalization::impl::sync(output);
    return output;
      }
    }

    at::Tensor & slow_conv_dilated3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, c10::SymIntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::slow_conv_dilated3d_out::call(self_meta, weight_meta, kernel_size, bias_meta, stride, padding, dilation, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      c10::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || weight.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::slow_conv_dilated3d_out::call(self_, weight_, kernel_size, bias_, stride, padding, dilation, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::slow_conv_dilated3d::call(self_, weight_, kernel_size, bias_, stride, padding, dilation);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_log_ndtr_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_log_ndtr_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_log_ndtr_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_log_ndtr::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_exp2_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_exp2_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_exp2_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_exp2::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_digamma_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_digamma_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_digamma_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_digamma::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_gammaln_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_gammaln_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_gammaln_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_gammaln::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_erfcx_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_erfcx_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_erfcx_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_erfcx::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_xlog1py_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_xlog1py_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_xlog1py_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_xlog1py::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_xlog1py_out_self_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_xlog1py_self_scalar_out::call(self, other_meta, out_meta);
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_xlog1py_self_scalar_out::call(self, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_xlog1py_self_scalar::call(self, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_xlog1py_out_other_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_xlog1py_other_scalar_out::call(self_meta, other, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_xlog1py_other_scalar_out::call(self_, other, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_xlog1py_other_scalar::call(self_, other);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_i1_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_i1_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_i1_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_i1::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_i1e_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_i1e_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_i1e_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_i1e::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_fft_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_fft_out::call(self_meta, n, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_fft_out::call(self_, n, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_fft::call(self_, n, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_rfft_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_rfft_out::call(self_meta, n, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_rfft_out::call(self_, n, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_rfft::call(self_, n, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_hfft_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<c10::SymInt> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_hfft_out::call(self_meta, n, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_hfft_out::call(self_, n, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_hfft::call(self_, n, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & fft_hfft2_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_hfft2_out::call(self_meta, s, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_hfft2_out::call(self_, s, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_hfft2::call(self_, s, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_ifftn_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_ifftn_out::call(self_meta, s, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_ifftn_out::call(self_, s, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_ifftn::call(self_, s, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_rfftn_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_rfftn_out::call(self_meta, s, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_rfftn_out::call(self_, s, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_rfftn::call(self_, s, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    const at::Tensor & fft_hfftn_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::OptionalSymIntArrayRef s, at::OptionalIntArrayRef dim, c10::optional<c10::string_view> norm, const at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_hfftn_out::call(self_meta, s, dim, norm, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_hfftn_out::call(self_, s, dim, norm, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_hfftn::call(self_, s, dim, norm);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & fft_fftfreq_out_out(c10::DispatchKeySet dispatchKeySet, int64_t n, double d, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::fft_fftfreq_out::call(n, d, out_meta);
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false) && (false)) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::fft_fftfreq_out::call(n, d, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::fft_fftfreq::call(n, d, out_.scalar_type(), out_.layout(), out_.device(), c10::nullopt);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_det_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_det_out::call(A_meta, out_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_det_out::call(A_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_det::call(A_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_ldl_factor_ex_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool hermitian, bool check_errors, at::Tensor & LD, at::Tensor & pivots, at::Tensor & info) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto LD_meta = to_meta(LD);
        auto pivots_meta = to_meta(pivots);
        auto info_meta = to_meta(info);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_ldl_factor_ex_out::call(self_meta, hermitian, check_errors, LD_meta, pivots_meta, info_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor LD_;
      if (at::functionalization::impl::isFunctionalTensor(LD)) {
        at::functionalization::impl::sync(LD);
        LD_ = at::functionalization::impl::from_functional_tensor(LD);
      } else {
        LD_ = LD;
      }
      
      at::Tensor pivots_;
      if (at::functionalization::impl::isFunctionalTensor(pivots)) {
        at::functionalization::impl::sync(pivots);
        pivots_ = at::functionalization::impl::from_functional_tensor(pivots);
      } else {
        pivots_ = pivots;
      }
      
      at::Tensor info_;
      if (at::functionalization::impl::isFunctionalTensor(info)) {
        at::functionalization::impl::sync(info);
        info_ = at::functionalization::impl::from_functional_tensor(info);
      } else {
        info_ = info;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(LD) && at::functionalization::impl::isFunctionalTensor(pivots) && at::functionalization::impl::isFunctionalTensor(info))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::linalg_ldl_factor_ex_out::call(self_, hermitian, check_errors, LD_, pivots_, info_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(LD, pivots, info);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_ldl_factor_ex::call(self_, hermitian, check_errors);
        }
          at::functionalization::impl::propagate_xla_data(LD, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(LD, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(LD);
  at::functionalization::impl::sync(LD);
  at::functionalization::impl::propagate_xla_data(pivots, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(pivots, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(pivots);
  at::functionalization::impl::sync(pivots);
  at::functionalization::impl::propagate_xla_data(info, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(info, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(info);
  at::functionalization::impl::sync(info);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(LD, pivots, info);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> linalg_lstsq_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver, at::Tensor & solution, at::Tensor & residuals, at::Tensor & rank, at::Tensor & singular_values) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto b_meta = to_meta(b);
        auto solution_meta = to_meta(solution);
        auto residuals_meta = to_meta(residuals);
        auto rank_meta = to_meta(rank);
        auto singular_values_meta = to_meta(singular_values);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_lstsq_out::call(self_meta, b_meta, rcond, driver, solution_meta, residuals_meta, rank_meta, singular_values_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor b_;
      if (at::functionalization::impl::isFunctionalTensor(b)) {
        at::functionalization::impl::sync(b);
        b_ = at::functionalization::impl::from_functional_tensor(b);
      } else {
        b_ = b;
      }
      
      at::Tensor solution_;
      if (at::functionalization::impl::isFunctionalTensor(solution)) {
        at::functionalization::impl::sync(solution);
        solution_ = at::functionalization::impl::from_functional_tensor(solution);
      } else {
        solution_ = solution;
      }
      
      at::Tensor residuals_;
      if (at::functionalization::impl::isFunctionalTensor(residuals)) {
        at::functionalization::impl::sync(residuals);
        residuals_ = at::functionalization::impl::from_functional_tensor(residuals);
      } else {
        residuals_ = residuals;
      }
      
      at::Tensor rank_;
      if (at::functionalization::impl::isFunctionalTensor(rank)) {
        at::functionalization::impl::sync(rank);
        rank_ = at::functionalization::impl::from_functional_tensor(rank);
      } else {
        rank_ = rank;
      }
      
      at::Tensor singular_values_;
      if (at::functionalization::impl::isFunctionalTensor(singular_values)) {
        at::functionalization::impl::sync(singular_values);
        singular_values_ = at::functionalization::impl::from_functional_tensor(singular_values);
      } else {
        singular_values_ = singular_values;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(solution) && at::functionalization::impl::isFunctionalTensor(residuals) && at::functionalization::impl::isFunctionalTensor(rank) && at::functionalization::impl::isFunctionalTensor(singular_values))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || b.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(b))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output = at::_ops::linalg_lstsq_out::call(self_, b_, rcond, driver, solution_, residuals_, rank_, singular_values_);
         return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(solution, residuals, rank, singular_values);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_lstsq::call(self_, b_, rcond, driver);
        }
          at::functionalization::impl::propagate_xla_data(solution, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(solution, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(solution);
  at::functionalization::impl::sync(solution);
  at::functionalization::impl::propagate_xla_data(residuals, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(residuals, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(residuals);
  at::functionalization::impl::sync(residuals);
  at::functionalization::impl::propagate_xla_data(rank, std::get<2>(tmp_output));
  at::functionalization::impl::replace_(rank, std::get<2>(tmp_output));
  at::functionalization::impl::commit_update(rank);
  at::functionalization::impl::sync(rank);
  at::functionalization::impl::propagate_xla_data(singular_values, std::get<3>(tmp_output));
  at::functionalization::impl::replace_(singular_values, std::get<3>(tmp_output));
  at::functionalization::impl::commit_update(singular_values);
  at::functionalization::impl::sync(singular_values);
    return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(solution, residuals, rank, singular_values);
      }
    }

    at::Tensor & linalg_matrix_exp_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_matrix_exp_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_matrix_exp_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_matrix_exp::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> linalg_slogdet_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, at::Tensor & sign, at::Tensor & logabsdet) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto sign_meta = to_meta(sign);
        auto logabsdet_meta = to_meta(logabsdet);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_slogdet_out::call(A_meta, sign_meta, logabsdet_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor sign_;
      if (at::functionalization::impl::isFunctionalTensor(sign)) {
        at::functionalization::impl::sync(sign);
        sign_ = at::functionalization::impl::from_functional_tensor(sign);
      } else {
        sign_ = sign;
      }
      
      at::Tensor logabsdet_;
      if (at::functionalization::impl::isFunctionalTensor(logabsdet)) {
        at::functionalization::impl::sync(logabsdet);
        logabsdet_ = at::functionalization::impl::from_functional_tensor(logabsdet);
      } else {
        logabsdet_ = logabsdet;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(sign) && at::functionalization::impl::isFunctionalTensor(logabsdet))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::linalg_slogdet_out::call(A_, sign_, logabsdet_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(sign, logabsdet);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_slogdet::call(A_);
        }
          at::functionalization::impl::propagate_xla_data(sign, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(sign, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(sign);
  at::functionalization::impl::sync(sign);
  at::functionalization::impl::propagate_xla_data(logabsdet, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(logabsdet, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(logabsdet);
  at::functionalization::impl::sync(logabsdet);
    return ::std::tuple<at::Tensor &,at::Tensor &>(sign, logabsdet);
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _linalg_eigh_out_eigenvalues(c10::DispatchKeySet dispatchKeySet, const at::Tensor & A, c10::string_view UPLO, bool compute_v, at::Tensor & eigenvalues, at::Tensor & eigenvectors) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto A_meta = to_meta(A);
        auto eigenvalues_meta = to_meta(eigenvalues);
        auto eigenvectors_meta = to_meta(eigenvectors);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_linalg_eigh_eigenvalues::call(A_meta, UPLO, compute_v, eigenvalues_meta, eigenvectors_meta);
      }
      
      at::Tensor A_;
      if (at::functionalization::impl::isFunctionalTensor(A)) {
        at::functionalization::impl::sync(A);
        A_ = at::functionalization::impl::from_functional_tensor(A);
      } else {
        A_ = A;
      }
      
      at::Tensor eigenvalues_;
      if (at::functionalization::impl::isFunctionalTensor(eigenvalues)) {
        at::functionalization::impl::sync(eigenvalues);
        eigenvalues_ = at::functionalization::impl::from_functional_tensor(eigenvalues);
      } else {
        eigenvalues_ = eigenvalues;
      }
      
      at::Tensor eigenvectors_;
      if (at::functionalization::impl::isFunctionalTensor(eigenvectors)) {
        at::functionalization::impl::sync(eigenvectors);
        eigenvectors_ = at::functionalization::impl::from_functional_tensor(eigenvectors);
      } else {
        eigenvectors_ = eigenvectors;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(eigenvalues) && at::functionalization::impl::isFunctionalTensor(eigenvectors))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || A.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(A))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_linalg_eigh_eigenvalues::call(A_, UPLO, compute_v, eigenvalues_, eigenvectors_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(eigenvalues, eigenvectors);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_linalg_eigh::call(A_, UPLO, compute_v);
        }
          at::functionalization::impl::propagate_xla_data(eigenvalues, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(eigenvalues, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(eigenvalues);
  at::functionalization::impl::sync(eigenvalues);
  at::functionalization::impl::propagate_xla_data(eigenvectors, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(eigenvectors, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(eigenvectors);
  at::functionalization::impl::sync(eigenvectors);
    return ::std::tuple<at::Tensor &,at::Tensor &>(eigenvalues, eigenvectors);
      }
    }

    at::Tensor & inner_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto other_meta = to_meta(other);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::inner_out::call(self_meta, other_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor other_;
      if (at::functionalization::impl::isFunctionalTensor(other)) {
        at::functionalization::impl::sync(other);
        other_ = at::functionalization::impl::from_functional_tensor(other);
      } else {
        other_ = other;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA || other.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self) || at::functionalization::impl::isFunctionalTensor(other))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::inner_out::call(self_, other_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::inner::call(self_, other_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_matrix_norm_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_matrix_norm_out::call(self_meta, ord, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_matrix_norm_out::call(self_, ord, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_matrix_norm::call(self_, ord, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_matrix_norm_out_str_ord_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_matrix_norm_str_ord_out::call(self_meta, ord, dim, keepdim, dtype, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_matrix_norm_str_ord_out::call(self_, ord, dim, keepdim, dtype, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_matrix_norm_str_ord::call(self_, ord, dim, keepdim, dtype);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_tensorinv_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t ind, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_tensorinv_out::call(self_meta, ind, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_tensorinv_out::call(self_, ind, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_tensorinv::call(self_, ind);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & linalg_matrix_power_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::linalg_matrix_power_out::call(self_meta, n, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::linalg_matrix_power_out::call(self_, n, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::linalg_matrix_power::call(self_, n);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _make_dual_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & primal, const at::Tensor & tangent, int64_t level, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto primal_meta = to_meta(primal);
        auto tangent_meta = to_meta(tangent);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_make_dual_copy_out::call(primal_meta, tangent_meta, level, out_meta);
      }
      
      at::Tensor primal_;
      if (at::functionalization::impl::isFunctionalTensor(primal)) {
        at::functionalization::impl::sync(primal);
        primal_ = at::functionalization::impl::from_functional_tensor(primal);
      } else {
        primal_ = primal;
      }
      
      at::Tensor tangent_;
      if (at::functionalization::impl::isFunctionalTensor(tangent)) {
        at::functionalization::impl::sync(tangent);
        tangent_ = at::functionalization::impl::from_functional_tensor(tangent);
      } else {
        tangent_ = tangent;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || primal.device().type() == c10::DeviceType::XLA || tangent.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(primal) || at::functionalization::impl::isFunctionalTensor(tangent))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_make_dual_copy_out::call(primal_, tangent_, level, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_make_dual_copy::call(primal_, tangent_, level);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & view_as_real_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::view_as_real_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::view_as_real_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::view_as_real_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & view_as_complex_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::view_as_complex_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::view_as_complex_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::view_as_complex_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _conj_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_conj_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_conj_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_conj_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _neg_view_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_neg_view_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_neg_view_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_neg_view_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & permute_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dims, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::permute_copy_out::call(self_meta, dims, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::permute_copy_out::call(self_, dims, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::permute_copy::call(self_, dims);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    void split_copy_out_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::SymInt split_size, int64_t dim, at::TensorList out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::split_copy_Tensor_out::call(self_meta, split_size, dim, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      ::std::vector<at::Tensor> out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out.vec();
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::_ops::split_copy_Tensor_out::call(self_, split_size, dim, out_);
         ;
        }
      } else {
        ::std::vector<at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::split_copy_Tensor::call(self_, split_size, dim);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    
      }
    }

    at::Tensor & t_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::t_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::t_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::t_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & ccol_indices_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::ccol_indices_copy_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::ccol_indices_copy_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::ccol_indices_copy::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & unfold_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dimension, int64_t size, int64_t step, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::unfold_copy_out::call(self_meta, dimension, size, step, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::unfold_copy_out::call(self_, dimension, size, step, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::unfold_copy::call(self_, dimension, size, step);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _transformer_encoder_layer_fwd_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & src, int64_t embed_dim, int64_t num_heads, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, bool use_gelu, bool norm_first, double eps, const at::Tensor & norm_weight_1, const at::Tensor & norm_bias_1, const at::Tensor & norm_weight_2, const at::Tensor & norm_bias_2, const at::Tensor & ffn_weight_1, const at::Tensor & ffn_bias_1, const at::Tensor & ffn_weight_2, const at::Tensor & ffn_bias_2, const c10::optional<at::Tensor> & mask, c10::optional<int64_t> mask_type, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto src_meta = to_meta(src);
        auto qkv_weight_meta = to_meta(qkv_weight);
        auto qkv_bias_meta = to_meta(qkv_bias);
        auto proj_weight_meta = to_meta(proj_weight);
        auto proj_bias_meta = to_meta(proj_bias);
        auto norm_weight_1_meta = to_meta(norm_weight_1);
        auto norm_bias_1_meta = to_meta(norm_bias_1);
        auto norm_weight_2_meta = to_meta(norm_weight_2);
        auto norm_bias_2_meta = to_meta(norm_bias_2);
        auto ffn_weight_1_meta = to_meta(ffn_weight_1);
        auto ffn_bias_1_meta = to_meta(ffn_bias_1);
        auto ffn_weight_2_meta = to_meta(ffn_weight_2);
        auto ffn_bias_2_meta = to_meta(ffn_bias_2);
        auto mask_meta = to_meta(mask);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_transformer_encoder_layer_fwd_out::call(src_meta, embed_dim, num_heads, qkv_weight_meta, qkv_bias_meta, proj_weight_meta, proj_bias_meta, use_gelu, norm_first, eps, norm_weight_1_meta, norm_bias_1_meta, norm_weight_2_meta, norm_bias_2_meta, ffn_weight_1_meta, ffn_bias_1_meta, ffn_weight_2_meta, ffn_bias_2_meta, mask_meta, mask_type, out_meta);
      }
      
      at::Tensor src_;
      if (at::functionalization::impl::isFunctionalTensor(src)) {
        at::functionalization::impl::sync(src);
        src_ = at::functionalization::impl::from_functional_tensor(src);
      } else {
        src_ = src;
      }
      
      at::Tensor qkv_weight_;
      if (at::functionalization::impl::isFunctionalTensor(qkv_weight)) {
        at::functionalization::impl::sync(qkv_weight);
        qkv_weight_ = at::functionalization::impl::from_functional_tensor(qkv_weight);
      } else {
        qkv_weight_ = qkv_weight;
      }
      
      at::Tensor qkv_bias_;
      if (at::functionalization::impl::isFunctionalTensor(qkv_bias)) {
        at::functionalization::impl::sync(qkv_bias);
        qkv_bias_ = at::functionalization::impl::from_functional_tensor(qkv_bias);
      } else {
        qkv_bias_ = qkv_bias;
      }
      
      at::Tensor proj_weight_;
      if (at::functionalization::impl::isFunctionalTensor(proj_weight)) {
        at::functionalization::impl::sync(proj_weight);
        proj_weight_ = at::functionalization::impl::from_functional_tensor(proj_weight);
      } else {
        proj_weight_ = proj_weight;
      }
      
      at::Tensor proj_bias_;
      if (at::functionalization::impl::isFunctionalTensor(proj_bias)) {
        at::functionalization::impl::sync(proj_bias);
        proj_bias_ = at::functionalization::impl::from_functional_tensor(proj_bias);
      } else {
        proj_bias_ = proj_bias;
      }
      
      at::Tensor norm_weight_1_;
      if (at::functionalization::impl::isFunctionalTensor(norm_weight_1)) {
        at::functionalization::impl::sync(norm_weight_1);
        norm_weight_1_ = at::functionalization::impl::from_functional_tensor(norm_weight_1);
      } else {
        norm_weight_1_ = norm_weight_1;
      }
      
      at::Tensor norm_bias_1_;
      if (at::functionalization::impl::isFunctionalTensor(norm_bias_1)) {
        at::functionalization::impl::sync(norm_bias_1);
        norm_bias_1_ = at::functionalization::impl::from_functional_tensor(norm_bias_1);
      } else {
        norm_bias_1_ = norm_bias_1;
      }
      
      at::Tensor norm_weight_2_;
      if (at::functionalization::impl::isFunctionalTensor(norm_weight_2)) {
        at::functionalization::impl::sync(norm_weight_2);
        norm_weight_2_ = at::functionalization::impl::from_functional_tensor(norm_weight_2);
      } else {
        norm_weight_2_ = norm_weight_2;
      }
      
      at::Tensor norm_bias_2_;
      if (at::functionalization::impl::isFunctionalTensor(norm_bias_2)) {
        at::functionalization::impl::sync(norm_bias_2);
        norm_bias_2_ = at::functionalization::impl::from_functional_tensor(norm_bias_2);
      } else {
        norm_bias_2_ = norm_bias_2;
      }
      
      at::Tensor ffn_weight_1_;
      if (at::functionalization::impl::isFunctionalTensor(ffn_weight_1)) {
        at::functionalization::impl::sync(ffn_weight_1);
        ffn_weight_1_ = at::functionalization::impl::from_functional_tensor(ffn_weight_1);
      } else {
        ffn_weight_1_ = ffn_weight_1;
      }
      
      at::Tensor ffn_bias_1_;
      if (at::functionalization::impl::isFunctionalTensor(ffn_bias_1)) {
        at::functionalization::impl::sync(ffn_bias_1);
        ffn_bias_1_ = at::functionalization::impl::from_functional_tensor(ffn_bias_1);
      } else {
        ffn_bias_1_ = ffn_bias_1;
      }
      
      at::Tensor ffn_weight_2_;
      if (at::functionalization::impl::isFunctionalTensor(ffn_weight_2)) {
        at::functionalization::impl::sync(ffn_weight_2);
        ffn_weight_2_ = at::functionalization::impl::from_functional_tensor(ffn_weight_2);
      } else {
        ffn_weight_2_ = ffn_weight_2;
      }
      
      at::Tensor ffn_bias_2_;
      if (at::functionalization::impl::isFunctionalTensor(ffn_bias_2)) {
        at::functionalization::impl::sync(ffn_bias_2);
        ffn_bias_2_ = at::functionalization::impl::from_functional_tensor(ffn_bias_2);
      } else {
        ffn_bias_2_ = ffn_bias_2;
      }
      
      c10::optional<at::Tensor> mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || src.device().type() == c10::DeviceType::XLA || qkv_weight.device().type() == c10::DeviceType::XLA || qkv_bias.device().type() == c10::DeviceType::XLA || proj_weight.device().type() == c10::DeviceType::XLA || proj_bias.device().type() == c10::DeviceType::XLA || norm_weight_1.device().type() == c10::DeviceType::XLA || norm_bias_1.device().type() == c10::DeviceType::XLA || norm_weight_2.device().type() == c10::DeviceType::XLA || norm_bias_2.device().type() == c10::DeviceType::XLA || ffn_weight_1.device().type() == c10::DeviceType::XLA || ffn_bias_1.device().type() == c10::DeviceType::XLA || ffn_weight_2.device().type() == c10::DeviceType::XLA || ffn_bias_2.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(src) || at::functionalization::impl::isFunctionalTensor(qkv_weight) || at::functionalization::impl::isFunctionalTensor(qkv_bias) || at::functionalization::impl::isFunctionalTensor(proj_weight) || at::functionalization::impl::isFunctionalTensor(proj_bias) || at::functionalization::impl::isFunctionalTensor(norm_weight_1) || at::functionalization::impl::isFunctionalTensor(norm_bias_1) || at::functionalization::impl::isFunctionalTensor(norm_weight_2) || at::functionalization::impl::isFunctionalTensor(norm_bias_2) || at::functionalization::impl::isFunctionalTensor(ffn_weight_1) || at::functionalization::impl::isFunctionalTensor(ffn_bias_1) || at::functionalization::impl::isFunctionalTensor(ffn_weight_2) || at::functionalization::impl::isFunctionalTensor(ffn_bias_2) || at::functionalization::impl::isFunctionalTensor(mask))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_transformer_encoder_layer_fwd_out::call(src_, embed_dim, num_heads, qkv_weight_, qkv_bias_, proj_weight_, proj_bias_, use_gelu, norm_first, eps, norm_weight_1_, norm_bias_1_, norm_weight_2_, norm_bias_2_, ffn_weight_1_, ffn_bias_1_, ffn_weight_2_, ffn_bias_2_, mask_, mask_type, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_transformer_encoder_layer_fwd::call(src_, embed_dim, num_heads, qkv_weight_, qkv_bias_, proj_weight_, proj_bias_, use_gelu, norm_first, eps, norm_weight_1_, norm_bias_1_, norm_weight_2_, norm_bias_2_, ffn_weight_1_, ffn_bias_1_, ffn_weight_2_, ffn_bias_2_, mask_, mask_type);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    ::std::tuple<at::Tensor &,at::Tensor &> _native_multi_head_attention_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, int64_t embed_dim, int64_t num_head, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, const c10::optional<at::Tensor> & mask, bool need_weights, bool average_attn_weights, c10::optional<int64_t> mask_type, at::Tensor & out0, at::Tensor & out1) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto query_meta = to_meta(query);
        auto key_meta = to_meta(key);
        auto value_meta = to_meta(value);
        auto qkv_weight_meta = to_meta(qkv_weight);
        auto qkv_bias_meta = to_meta(qkv_bias);
        auto proj_weight_meta = to_meta(proj_weight);
        auto proj_bias_meta = to_meta(proj_bias);
        auto mask_meta = to_meta(mask);
        auto out0_meta = to_meta(out0);
        auto out1_meta = to_meta(out1);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_native_multi_head_attention_out::call(query_meta, key_meta, value_meta, embed_dim, num_head, qkv_weight_meta, qkv_bias_meta, proj_weight_meta, proj_bias_meta, mask_meta, need_weights, average_attn_weights, mask_type, out0_meta, out1_meta);
      }
      
      at::Tensor query_;
      if (at::functionalization::impl::isFunctionalTensor(query)) {
        at::functionalization::impl::sync(query);
        query_ = at::functionalization::impl::from_functional_tensor(query);
      } else {
        query_ = query;
      }
      
      at::Tensor key_;
      if (at::functionalization::impl::isFunctionalTensor(key)) {
        at::functionalization::impl::sync(key);
        key_ = at::functionalization::impl::from_functional_tensor(key);
      } else {
        key_ = key;
      }
      
      at::Tensor value_;
      if (at::functionalization::impl::isFunctionalTensor(value)) {
        at::functionalization::impl::sync(value);
        value_ = at::functionalization::impl::from_functional_tensor(value);
      } else {
        value_ = value;
      }
      
      at::Tensor qkv_weight_;
      if (at::functionalization::impl::isFunctionalTensor(qkv_weight)) {
        at::functionalization::impl::sync(qkv_weight);
        qkv_weight_ = at::functionalization::impl::from_functional_tensor(qkv_weight);
      } else {
        qkv_weight_ = qkv_weight;
      }
      
      at::Tensor qkv_bias_;
      if (at::functionalization::impl::isFunctionalTensor(qkv_bias)) {
        at::functionalization::impl::sync(qkv_bias);
        qkv_bias_ = at::functionalization::impl::from_functional_tensor(qkv_bias);
      } else {
        qkv_bias_ = qkv_bias;
      }
      
      at::Tensor proj_weight_;
      if (at::functionalization::impl::isFunctionalTensor(proj_weight)) {
        at::functionalization::impl::sync(proj_weight);
        proj_weight_ = at::functionalization::impl::from_functional_tensor(proj_weight);
      } else {
        proj_weight_ = proj_weight;
      }
      
      at::Tensor proj_bias_;
      if (at::functionalization::impl::isFunctionalTensor(proj_bias)) {
        at::functionalization::impl::sync(proj_bias);
        proj_bias_ = at::functionalization::impl::from_functional_tensor(proj_bias);
      } else {
        proj_bias_ = proj_bias;
      }
      
      c10::optional<at::Tensor> mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
      
      at::Tensor out0_;
      if (at::functionalization::impl::isFunctionalTensor(out0)) {
        at::functionalization::impl::sync(out0);
        out0_ = at::functionalization::impl::from_functional_tensor(out0);
      } else {
        out0_ = out0;
      }
      
      at::Tensor out1_;
      if (at::functionalization::impl::isFunctionalTensor(out1)) {
        at::functionalization::impl::sync(out1);
        out1_ = at::functionalization::impl::from_functional_tensor(out1);
      } else {
        out1_ = out1;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out0) && at::functionalization::impl::isFunctionalTensor(out1))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || query.device().type() == c10::DeviceType::XLA || key.device().type() == c10::DeviceType::XLA || value.device().type() == c10::DeviceType::XLA || qkv_weight.device().type() == c10::DeviceType::XLA || qkv_bias.device().type() == c10::DeviceType::XLA || proj_weight.device().type() == c10::DeviceType::XLA || proj_bias.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(query) || at::functionalization::impl::isFunctionalTensor(key) || at::functionalization::impl::isFunctionalTensor(value) || at::functionalization::impl::isFunctionalTensor(qkv_weight) || at::functionalization::impl::isFunctionalTensor(qkv_bias) || at::functionalization::impl::isFunctionalTensor(proj_weight) || at::functionalization::impl::isFunctionalTensor(proj_bias) || at::functionalization::impl::isFunctionalTensor(mask))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         ::std::tuple<at::Tensor,at::Tensor> tmp_output = at::_ops::_native_multi_head_attention_out::call(query_, key_, value_, embed_dim, num_head, qkv_weight_, qkv_bias_, proj_weight_, proj_bias_, mask_, need_weights, average_attn_weights, mask_type, out0_, out1_);
         return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);;
        }
      } else {
        ::std::tuple<at::Tensor,at::Tensor> tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_native_multi_head_attention::call(query_, key_, value_, embed_dim, num_head, qkv_weight_, qkv_bias_, proj_weight_, proj_bias_, mask_, need_weights, average_attn_weights, mask_type);
        }
          at::functionalization::impl::propagate_xla_data(out0, std::get<0>(tmp_output));
  at::functionalization::impl::replace_(out0, std::get<0>(tmp_output));
  at::functionalization::impl::commit_update(out0);
  at::functionalization::impl::sync(out0);
  at::functionalization::impl::propagate_xla_data(out1, std::get<1>(tmp_output));
  at::functionalization::impl::replace_(out1, std::get<1>(tmp_output));
  at::functionalization::impl::commit_update(out1);
  at::functionalization::impl::sync(out1);
    return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
      }
    }

    at::Tensor & special_modified_bessel_i1_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_modified_bessel_i1_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_modified_bessel_i1_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_modified_bessel_i1::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & special_modified_bessel_k0_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::special_modified_bessel_k0_out::call(self_meta, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::special_modified_bessel_k0_out::call(self_, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::special_modified_bessel_k0::call(self_);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor & _foobar_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool arg1, bool arg2, bool arg3, at::Tensor & out) {
      if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technically all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at::_ops::_foobar_out::call(self_meta, arg1, arg2, arg3, out_meta);
      }
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
      if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        // We want to disable this check if there are any XLA tensors.
        // cpu_tensor.copy_(xla_tensor) is valid code.
        if (!(false || self.device().type() == c10::DeviceType::XLA) && (false || at::functionalization::impl::isFunctionalTensor(self))) {
         // case 1: trying to mutate a non functional tensor with a functional tensor is an error
         TORCH_INTERNAL_ASSERT(false,
           "mutating a non-functional tensor with a functional tensor is not allowed.",
           " Please ensure that all of your inputs are wrapped inside of a functionalize() call.");
        } else {
         // case 2: arguments are not functional tensors, so we no-op and redispatch.
         at::AutoDispatchSkipFunctionalize guard;
         at::Tensor tmp_output = at::_ops::_foobar_out::call(self_, arg1, arg2, arg3, out_);
         return out;;
        }
      } else {
        at::Tensor tmp_output;
        {
          at::AutoDispatchSkipFunctionalize guard;
          tmp_output = at::_ops::_foobar::call(self_, arg1, arg2, arg3);
        }
          at::functionalization::impl::propagate_xla_data(out, tmp_output);
  at::functionalization::impl::replace_(out, tmp_output);
  at::functionalization::impl::commit_update(out);
  at::functionalization::impl::sync(out);
    return out;
      }
    }

    at::Tensor view_as_real(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::view_as_real::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::view_as_real::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::view_as_real::call(self_);
        } else {
          tmp_output = at::_ops::view_as_real_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::view_as_real::call(base);
          } else {
            return at::_ops::view_as_real_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::view_as_real_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor slice_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<c10::SymInt> start, c10::optional<c10::SymInt> end, c10::SymInt step) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::slice_Tensor::call(self_, dim, start, end, step);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::slice_Tensor::call(self_meta, dim, start, end, step);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::slice_Tensor::call(self_, dim, start, end, step);
        } else {
          tmp_output = at::_ops::slice_copy_Tensor::call(self_, dim, start, end, step);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, dim = dim, start = start, end = end, step = step](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::slice_Tensor::call(base, dim, start, end, step);
          } else {
            return at::_ops::slice_copy_Tensor::call(base, dim, start, end, step);
          }
        },
        [inverse_return_mode = inverse_return_mode, dim = dim, start = start, end = end, step = step](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::slice_Tensor_inverse(base, mutated_view, inverse_return_mode, dim, start, end, step);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor t(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::t::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::t::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::t::call(self_);
        } else {
          tmp_output = at::_ops::t_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::t::call(base);
          } else {
            return at::_ops::t_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::t_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor & t_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::t_::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::t::call(base);
          } else {
            return at::_ops::t_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::t_inverse(base, mutated_view, inverse_return_mode);
        }
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::t_::call(self_meta);
      }
      // This function adds the above view meta to the current tensor and replays them off the base,
      // mutating the size/stride info of the current FunctionalTensorWrapper.
      // Because of this, we need to make sure to run the reference shape function above,
      // BEFORE doing this (otherwise we'll end up runnin the reference function using the wrong sizes/strides)
      at::functionalization::impl::mutate_view_meta(self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      // XLA/LTC don't implement the logic to propagate strides correctly, so we need to rely
      // on a reference implementation here (instead of relying on the output from the forward lambda
      // having the correct stride info)
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(self, reference_tensor_output);
      }
      return self;
    }

    at::Tensor _indices(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::_indices::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::_indices::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::_indices::call(self_);
        } else {
          tmp_output = at::_ops::_indices_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::_indices::call(base);
          } else {
            return at::_ops::_indices_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::_indices_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor indices(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::indices::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::indices::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::indices::call(self_);
        } else {
          tmp_output = at::_ops::indices_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::indices::call(base);
          } else {
            return at::_ops::indices_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::indices_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor crow_indices(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::crow_indices::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::crow_indices::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::crow_indices::call(self_);
        } else {
          tmp_output = at::_ops::crow_indices_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::crow_indices::call(base);
          } else {
            return at::_ops::crow_indices_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::crow_indices_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor col_indices(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::col_indices::call(self_);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::col_indices::call(self_meta);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::col_indices::call(self_);
        } else {
          tmp_output = at::_ops::col_indices_copy::call(self_);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::col_indices::call(base);
          } else {
            return at::_ops::col_indices_copy::call(base);
          }
        },
        [inverse_return_mode = inverse_return_mode](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::col_indices_inverse(base, mutated_view, inverse_return_mode);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

    at::Tensor unfold(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dimension, int64_t size, int64_t step) {
      
      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      if (!at::functionalization::impl::isFunctionalTensor(self)) {
        // functionalization is re-entrant, but will no-op if it wasn't passed a FunctionalTensorWrapper.
        at::AutoDispatchSkipFunctionalize guard;
        return at::_ops::unfold::call(self_, dimension, size, step);
      }
      auto reapply_views = at::functionalization::impl::getFunctionalizationReapplyViewsTLS();
      auto inverse_return_mode = (
          reapply_views ? at::functionalization::InverseReturnMode::ViewOrScatterInverse
            : at::functionalization::InverseReturnMode::NeverView
      );
      auto compute_reference_meta =
        self.key_set().has_backend(c10::BackendComponent::XLABit) ||
        self.key_set().has_backend(c10::BackendComponent::LazyBit);
      at::Tensor reference_tensor_output;
      if (compute_reference_meta) {
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        reference_tensor_output = at::_ops::unfold::call(self_meta, dimension, size, step);
      }
      at::Tensor tmp_output;
      {
        at::AutoDispatchSkipFunctionalize guard;
        if (reapply_views) {
          tmp_output = at::_ops::unfold::call(self_, dimension, size, step);
        } else {
          tmp_output = at::_ops::unfold_copy::call(self_, dimension, size, step);
        }
      }
      at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
        [reapply_views = reapply_views, dimension = dimension, size = size, step = step](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
          if (reapply_views) {
            return at::_ops::unfold::call(base, dimension, size, step);
          } else {
            return at::_ops::unfold_copy::call(base, dimension, size, step);
          }
        },
        [inverse_return_mode = inverse_return_mode, dimension = dimension, size = size, step = step](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
          return at::functionalization::FunctionalInverses::unfold_inverse(base, mutated_view, inverse_return_mode, dimension, size, step);
        },
        /*is_multi_output=*/false
      );
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
      // See  Note [Propagating strides in the functionalization pass]
      if (compute_reference_meta) {
        at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
      }
      return out;
    }

}  // namespace functionalization

namespace {

TORCH_LIBRARY_IMPL(aten, Functionalize, m) {
  m.impl("_cudnn_rnn_flatten_weight.out", TORCH_FN(functionalization::_cudnn_rnn_flatten_weight_out_out));
  m.impl("abs.out", TORCH_FN(functionalization::abs_out_out));
  m.impl("abs_", TORCH_FN(functionalization::abs_));
  m.impl("_conj_physical.out", TORCH_FN(functionalization::_conj_physical_out_out));
  m.impl("acos.out", TORCH_FN(functionalization::acos_out_out));
  m.impl("acos_", TORCH_FN(functionalization::acos_));
  m.impl("affine_grid_generator.out", TORCH_FN(functionalization::affine_grid_generator_out_out));
  m.impl("arange.out", TORCH_FN(functionalization::arange_out_out));
  m.impl("arange.start_out", TORCH_FN(functionalization::arange_out_start_out));
  m.impl("atanh.out", TORCH_FN(functionalization::atanh_out_out));
  m.impl("atanh_", TORCH_FN(functionalization::atanh_));
  m.impl("asin.out", TORCH_FN(functionalization::asin_out_out));
  m.impl("asin_", TORCH_FN(functionalization::asin_));
  m.impl("binary_cross_entropy_backward.grad_input", TORCH_FN(functionalization::binary_cross_entropy_backward_out_grad_input));
  m.impl("binary_cross_entropy_with_logits.out", TORCH_FN(functionalization::binary_cross_entropy_with_logits_out_out));
  m.impl("logical_not.out", TORCH_FN(functionalization::logical_not_out_out));
  m.impl("logical_not_", TORCH_FN(functionalization::logical_not_));
  m.impl("logical_and.out", TORCH_FN(functionalization::logical_and_out_out));
  m.impl("logical_and_", TORCH_FN(functionalization::logical_and_));
  m.impl("block_diag.out", TORCH_FN(functionalization::block_diag_out_out));
  m.impl("convolution_backward.out", TORCH_FN(functionalization::convolution_backward_out_out));
  m.impl("_copy_from.out", TORCH_FN(functionalization::_copy_from_out_out));
  m.impl("cosh.out", TORCH_FN(functionalization::cosh_out_out));
  m.impl("cosh_", TORCH_FN(functionalization::cosh_));
  m.impl("cudnn_convolution_transpose.out", TORCH_FN(functionalization::cudnn_convolution_transpose_out_out));
  m.impl("_mps_convolution_transpose.out", TORCH_FN(functionalization::_mps_convolution_transpose_out_out));
  m.impl("cudnn_grid_sampler.out", TORCH_FN(functionalization::cudnn_grid_sampler_out_out));
  m.impl("cumprod.out", TORCH_FN(functionalization::cumprod_out_out));
  m.impl("cumprod_", TORCH_FN(functionalization::cumprod_));
  m.impl("diag_embed.out", TORCH_FN(functionalization::diag_embed_out_out));
  m.impl("diagonal_backward.out", TORCH_FN(functionalization::diagonal_backward_out_out));
  m.impl("div.out", TORCH_FN(functionalization::div_out_out));
  m.impl("div_.Tensor", TORCH_FN(functionalization::div__Tensor));
  m.impl("div.out_mode", TORCH_FN(functionalization::div_out_out_mode));
  m.impl("div_.Tensor_mode", TORCH_FN(functionalization::div__Tensor_mode));
  m.impl("div.Scalar_out", TORCH_FN(functionalization::div_out_Scalar_out));
  m.impl("div_.Scalar", TORCH_FN(functionalization::div__Scalar));
  m.impl("div.Scalar_mode_out", TORCH_FN(functionalization::div_out_Scalar_mode_out));
  m.impl("div_.Scalar_mode", TORCH_FN(functionalization::div__Scalar_mode));
  m.impl("_embedding_bag.out", TORCH_FN(functionalization::_embedding_bag_out_out));
  m.impl("_embedding_bag_per_sample_weights_backward.out", TORCH_FN(functionalization::_embedding_bag_per_sample_weights_backward_out_out));
  m.impl("new_full.out", TORCH_FN(functionalization::new_full_out_out));
  m.impl("empty_quantized.out", TORCH_FN(functionalization::empty_quantized_out_out));
  m.impl("empty_strided.out", TORCH_FN(functionalization::empty_strided_out_out));
  m.impl("exp.out", TORCH_FN(functionalization::exp_out_out));
  m.impl("exp_", TORCH_FN(functionalization::exp_));
  m.impl("expm1.out", TORCH_FN(functionalization::expm1_out_out));
  m.impl("expm1_", TORCH_FN(functionalization::expm1_));
  m.impl("fill.Scalar_out", TORCH_FN(functionalization::fill_out_Scalar_out));
  m.impl("fill_.Scalar", TORCH_FN(functionalization::fill__Scalar));
  m.impl("fill.Tensor_out", TORCH_FN(functionalization::fill_out_Tensor_out));
  m.impl("fill_.Tensor", TORCH_FN(functionalization::fill__Tensor));
  m.impl("floor.out", TORCH_FN(functionalization::floor_out_out));
  m.impl("floor_", TORCH_FN(functionalization::floor_));
  m.impl("floor_divide.out", TORCH_FN(functionalization::floor_divide_out_out));
  m.impl("floor_divide_.Tensor", TORCH_FN(functionalization::floor_divide__Tensor));
  m.impl("floor_divide.Scalar_out", TORCH_FN(functionalization::floor_divide_out_Scalar_out));
  m.impl("floor_divide_.Scalar", TORCH_FN(functionalization::floor_divide__Scalar));
  m.impl("full.names_out", TORCH_FN(functionalization::full_out_names_out));
  m.impl("full.out", TORCH_FN(functionalization::full_out_out));
  m.impl("from_file.out", TORCH_FN(functionalization::from_file_out_out));
  m.impl("grid_sampler_2d.out", TORCH_FN(functionalization::grid_sampler_2d_out_out));
  m.impl("grid_sampler_3d.out", TORCH_FN(functionalization::grid_sampler_3d_out_out));
  m.impl("hamming_window.out", TORCH_FN(functionalization::hamming_window_out_out));
  m.impl("hamming_window.periodic_out", TORCH_FN(functionalization::hamming_window_out_periodic_out));
  m.impl("hamming_window.periodic_alpha_out", TORCH_FN(functionalization::hamming_window_out_periodic_alpha_out));
  m.impl("hamming_window.periodic_alpha_beta_out", TORCH_FN(functionalization::hamming_window_out_periodic_alpha_beta_out));
  m.impl("native_group_norm.out", TORCH_FN(functionalization::native_group_norm_out_out));
  m.impl("_fft_c2r.out", TORCH_FN(functionalization::_fft_c2r_out_out));
  m.impl("isnan.out", TORCH_FN(functionalization::isnan_out_out));
  m.impl("log2.out", TORCH_FN(functionalization::log2_out_out));
  m.impl("log2_", TORCH_FN(functionalization::log2_));
  m.impl("logaddexp2.out", TORCH_FN(functionalization::logaddexp2_out_out));
  m.impl("xlogy.OutTensor", TORCH_FN(functionalization::xlogy_out_OutTensor));
  m.impl("xlogy_.Tensor", TORCH_FN(functionalization::xlogy__Tensor));
  m.impl("xlogy.OutScalar_Self", TORCH_FN(functionalization::xlogy_out_OutScalar_Self));
  m.impl("xlogy.OutScalar_Other", TORCH_FN(functionalization::xlogy_out_OutScalar_Other));
  m.impl("xlogy_.Scalar_Other", TORCH_FN(functionalization::xlogy__Scalar_Other));
  m.impl("logspace.out", TORCH_FN(functionalization::logspace_out_out));
  m.impl("logspace.Tensor_Tensor_out", TORCH_FN(functionalization::logspace_out_Tensor_Tensor_out));
  m.impl("logspace.Tensor_Scalar_out", TORCH_FN(functionalization::logspace_out_Tensor_Scalar_out));
  m.impl("logspace.Scalar_Tensor_out", TORCH_FN(functionalization::logspace_out_Scalar_Tensor_out));
  m.impl("_aminmax.out", TORCH_FN(functionalization::_aminmax_out_out));
  m.impl("_aminmax.dim_out", TORCH_FN(functionalization::_aminmax_out_dim_out));
  m.impl("aminmax.out", TORCH_FN(functionalization::aminmax_out_out));
  m.impl("_compute_linear_combination.out", TORCH_FN(functionalization::_compute_linear_combination_out_out));
  m.impl("mkldnn_max_pool3d_backward.out", TORCH_FN(functionalization::mkldnn_max_pool3d_backward_out_out));
  m.impl("min.dim_min", TORCH_FN(functionalization::min_out_dim_min));
  m.impl("mps_convolution_backward.out", TORCH_FN(functionalization::mps_convolution_backward_out_out));
  m.impl("miopen_rnn.out", TORCH_FN(functionalization::miopen_rnn_out_out));
  m.impl("mv.out", TORCH_FN(functionalization::mv_out_out));
  m.impl("_native_batch_norm_legit.out", TORCH_FN(functionalization::_native_batch_norm_legit_out_out));
  m.impl("_native_batch_norm_legit", TORCH_FN(functionalization::_native_batch_norm_legit));
  m.impl("_native_batch_norm_legit.no_stats_out", TORCH_FN(functionalization::_native_batch_norm_legit_out_no_stats_out));
  m.impl("batch_norm_stats.out", TORCH_FN(functionalization::batch_norm_stats_out_out));
  m.impl("batch_norm_backward_elemt.out", TORCH_FN(functionalization::batch_norm_backward_elemt_out_out));
  m.impl("_euclidean_dist.out", TORCH_FN(functionalization::_euclidean_dist_out_out));
  m.impl("_cdist_forward.out", TORCH_FN(functionalization::_cdist_forward_out_out));
  m.impl("_cdist_backward.out", TORCH_FN(functionalization::_cdist_backward_out_out));
  m.impl("pixel_unshuffle.out", TORCH_FN(functionalization::pixel_unshuffle_out_out));
  m.impl("rad2deg.out", TORCH_FN(functionalization::rad2deg_out_out));
  m.impl("rad2deg_", TORCH_FN(functionalization::rad2deg_));
  m.impl("scalar_tensor.out", TORCH_FN(functionalization::scalar_tensor_out_out));
  m.impl("rand.names_out", TORCH_FN(functionalization::rand_out_names_out));
  m.impl("rand.generator_with_names_out", TORCH_FN(functionalization::rand_out_generator_with_names_out));
  m.impl("rand.out", TORCH_FN(functionalization::rand_out_out));
  m.impl("rand_like.out", TORCH_FN(functionalization::rand_like_out_out));
  m.impl("relu.out", TORCH_FN(functionalization::relu_out_out));
  m.impl("relu_", TORCH_FN(functionalization::relu_));
  m.impl("logit.out", TORCH_FN(functionalization::logit_out_out));
  m.impl("logit_", TORCH_FN(functionalization::logit_));
  m.impl("select_scatter.out", TORCH_FN(functionalization::select_scatter_out_out));
  m.impl("softmax.int_out", TORCH_FN(functionalization::softmax_out_int_out));
  m.impl("stack.out", TORCH_FN(functionalization::stack_out_out));
  m.impl("nansum.out", TORCH_FN(functionalization::nansum_out_out));
  m.impl("sqrt.out", TORCH_FN(functionalization::sqrt_out_out));
  m.impl("sqrt_", TORCH_FN(functionalization::sqrt_));
  m.impl("prod.out", TORCH_FN(functionalization::prod_out_out));
  m.impl("prod.int_out", TORCH_FN(functionalization::prod_out_int_out));
  m.impl("threshold_backward.grad_input", TORCH_FN(functionalization::threshold_backward_out_grad_input));
  m.impl("_transform_bias_rescale_qkv.out", TORCH_FN(functionalization::_transform_bias_rescale_qkv_out_out));
  m.impl("_nested_from_padded.out", TORCH_FN(functionalization::_nested_from_padded_out_out));
  m.impl("_nested_tensor_size.out", TORCH_FN(functionalization::_nested_tensor_size_out_out));
  m.impl("_nested_view_from_buffer_copy.out", TORCH_FN(functionalization::_nested_view_from_buffer_copy_out_out));
  m.impl("trunc.out", TORCH_FN(functionalization::trunc_out_out));
  m.impl("trunc_", TORCH_FN(functionalization::trunc_));
  m.impl("unique_dim_consecutive.out", TORCH_FN(functionalization::unique_dim_consecutive_out_out));
  m.impl("where.self_out", TORCH_FN(functionalization::where_out_self_out));
  m.impl("_weight_norm_interface_backward.out", TORCH_FN(functionalization::_weight_norm_interface_backward_out_out));
  m.impl("_sample_dirichlet.out", TORCH_FN(functionalization::_sample_dirichlet_out_out));
  m.impl("binomial.out", TORCH_FN(functionalization::binomial_out_out));
  m.impl("native_norm.out", TORCH_FN(functionalization::native_norm_out_out));
  m.impl("native_norm.ScalarOpt_dim_dtype_out", TORCH_FN(functionalization::native_norm_out_ScalarOpt_dim_dtype_out));
  m.impl("_sparse_sum.dim_out", TORCH_FN(functionalization::_sparse_sum_out_dim_out));
  m.impl("_sparse_sum_backward.out", TORCH_FN(functionalization::_sparse_sum_backward_out_out));
  m.impl("_sparse_softmax.out", TORCH_FN(functionalization::_sparse_softmax_out_out));
  m.impl("clone.out", TORCH_FN(functionalization::clone_out_out));
  m.impl("resize_as.out", TORCH_FN(functionalization::resize_as_out_out));
  m.impl("resize_as_", TORCH_FN(functionalization::resize_as_));
  m.impl("zero.out", TORCH_FN(functionalization::zero_out_out));
  m.impl("zero_", TORCH_FN(functionalization::zero_));
  m.impl("heaviside.out", TORCH_FN(functionalization::heaviside_out_out));
  m.impl("heaviside_", TORCH_FN(functionalization::heaviside_));
  m.impl("addmm.out", TORCH_FN(functionalization::addmm_out_out));
  m.impl("addmm_", TORCH_FN(functionalization::addmm_));
  m.impl("_scaled_mm.out", TORCH_FN(functionalization::_scaled_mm_out_out));
  m.impl("_sparse_coo_tensor_with_dims.out", TORCH_FN(functionalization::_sparse_coo_tensor_with_dims_out_out));
  m.impl("_sparse_coo_tensor_with_dims_and_tensors.out", TORCH_FN(functionalization::_sparse_coo_tensor_with_dims_and_tensors_out_out));
  m.impl("sparse_resize_and_clear.out", TORCH_FN(functionalization::sparse_resize_and_clear_out_out));
  m.impl("sparse_resize_and_clear_", TORCH_FN(functionalization::sparse_resize_and_clear_));
  m.impl("hspmm.out", TORCH_FN(functionalization::hspmm_out_out));
  m.impl("_to_sparse_bsc.out", TORCH_FN(functionalization::_to_sparse_bsc_out_out));
  m.impl("to_mkldnn.out", TORCH_FN(functionalization::to_mkldnn_out_out));
  m.impl("mkldnn_reorder_conv3d_weight.out", TORCH_FN(functionalization::mkldnn_reorder_conv3d_weight_out_out));
  m.impl("q_per_channel_scales.out", TORCH_FN(functionalization::q_per_channel_scales_out_out));
  m.impl("int_repr.out", TORCH_FN(functionalization::int_repr_out_out));
  m.impl("_make_per_tensor_quantized_tensor.out", TORCH_FN(functionalization::_make_per_tensor_quantized_tensor_out_out));
  m.impl("_thnn_fused_lstm_cell.out", TORCH_FN(functionalization::_thnn_fused_lstm_cell_out_out));
  m.impl("_thnn_fused_lstm_cell_backward_impl.out", TORCH_FN(functionalization::_thnn_fused_lstm_cell_backward_impl_out_out));
  m.impl("masked_fill.Scalar_out", TORCH_FN(functionalization::masked_fill_out_Scalar_out));
  m.impl("masked_fill_.Scalar", TORCH_FN(functionalization::masked_fill__Scalar));
  m.impl("masked_fill.Tensor_out", TORCH_FN(functionalization::masked_fill_out_Tensor_out));
  m.impl("masked_fill_.Tensor", TORCH_FN(functionalization::masked_fill__Tensor));
  m.impl("_masked_softmax.out", TORCH_FN(functionalization::_masked_softmax_out_out));
  m.impl("bitwise_right_shift.Tensor_out", TORCH_FN(functionalization::bitwise_right_shift_out_Tensor_out));
  m.impl("bitwise_right_shift_.Tensor", TORCH_FN(functionalization::bitwise_right_shift__Tensor));
  m.impl("bitwise_right_shift.Tensor_Scalar_out", TORCH_FN(functionalization::bitwise_right_shift_out_Tensor_Scalar_out));
  m.impl("bitwise_right_shift_.Tensor_Scalar", TORCH_FN(functionalization::bitwise_right_shift__Tensor_Scalar));
  m.impl("bitwise_right_shift.Scalar_Tensor_out", TORCH_FN(functionalization::bitwise_right_shift_out_Scalar_Tensor_out));
  m.impl("cauchy.out", TORCH_FN(functionalization::cauchy_out_out));
  m.impl("cauchy_", TORCH_FN(functionalization::cauchy_));
  m.impl("log_normal.out", TORCH_FN(functionalization::log_normal_out_out));
  m.impl("log_normal_", TORCH_FN(functionalization::log_normal_));
  m.impl("ne.Scalar_out", TORCH_FN(functionalization::ne_out_Scalar_out));
  m.impl("ne_.Scalar", TORCH_FN(functionalization::ne__Scalar));
  m.impl("ne.Tensor_out", TORCH_FN(functionalization::ne_out_Tensor_out));
  m.impl("ne_.Tensor", TORCH_FN(functionalization::ne__Tensor));
  m.impl("addcmul.out", TORCH_FN(functionalization::addcmul_out_out));
  m.impl("addcmul_", TORCH_FN(functionalization::addcmul_));
  m.impl("ormqr.out", TORCH_FN(functionalization::ormqr_out_out));
  m.impl("lu_unpack.out", TORCH_FN(functionalization::lu_unpack_out_out));
  m.impl("dist.out", TORCH_FN(functionalization::dist_out_out));
  m.impl("histc.out", TORCH_FN(functionalization::histc_out_out));
  m.impl("_histogramdd_from_bin_cts.out", TORCH_FN(functionalization::_histogramdd_from_bin_cts_out_out));
  m.impl("hypot.out", TORCH_FN(functionalization::hypot_out_out));
  m.impl("hypot_", TORCH_FN(functionalization::hypot_));
  m.impl("min.unary_out", TORCH_FN(functionalization::min_out_unary_out));
  m.impl("_foreach_add.Scalar_out", TORCH_FN(functionalization::_foreach_add_out_Scalar_out));
  m.impl("_foreach_add_.Scalar", TORCH_FN(functionalization::_foreach_add__Scalar));
  m.impl("_foreach_add.List_out", TORCH_FN(functionalization::_foreach_add_out_List_out));
  m.impl("_foreach_add_.List", TORCH_FN(functionalization::_foreach_add__List));
  m.impl("_foreach_add.ScalarList_out", TORCH_FN(functionalization::_foreach_add_out_ScalarList_out));
  m.impl("_foreach_add_.ScalarList", TORCH_FN(functionalization::_foreach_add__ScalarList));
  m.impl("_foreach_add.Tensor_out", TORCH_FN(functionalization::_foreach_add_out_Tensor_out));
  m.impl("_foreach_add_.Tensor", TORCH_FN(functionalization::_foreach_add__Tensor));
  m.impl("_foreach_clamp_min.Scalar_out", TORCH_FN(functionalization::_foreach_clamp_min_out_Scalar_out));
  m.impl("_foreach_clamp_min_.Scalar", TORCH_FN(functionalization::_foreach_clamp_min__Scalar));
  m.impl("_foreach_clamp_min.List_out", TORCH_FN(functionalization::_foreach_clamp_min_out_List_out));
  m.impl("_foreach_clamp_min_.List", TORCH_FN(functionalization::_foreach_clamp_min__List));
  m.impl("_foreach_clamp_min.ScalarList_out", TORCH_FN(functionalization::_foreach_clamp_min_out_ScalarList_out));
  m.impl("_foreach_clamp_min_.ScalarList", TORCH_FN(functionalization::_foreach_clamp_min__ScalarList));
  m.impl("_foreach_addcdiv.Scalar_out", TORCH_FN(functionalization::_foreach_addcdiv_out_Scalar_out));
  m.impl("_foreach_addcdiv_.Scalar", TORCH_FN(functionalization::_foreach_addcdiv__Scalar));
  m.impl("_foreach_addcdiv.ScalarList_out", TORCH_FN(functionalization::_foreach_addcdiv_out_ScalarList_out));
  m.impl("_foreach_addcdiv_.ScalarList", TORCH_FN(functionalization::_foreach_addcdiv__ScalarList));
  m.impl("_foreach_addcdiv.Tensor_out", TORCH_FN(functionalization::_foreach_addcdiv_out_Tensor_out));
  m.impl("_foreach_addcdiv_.Tensor", TORCH_FN(functionalization::_foreach_addcdiv__Tensor));
  m.impl("_foreach_addcmul.Scalar_out", TORCH_FN(functionalization::_foreach_addcmul_out_Scalar_out));
  m.impl("_foreach_addcmul_.Scalar", TORCH_FN(functionalization::_foreach_addcmul__Scalar));
  m.impl("_foreach_addcmul.ScalarList_out", TORCH_FN(functionalization::_foreach_addcmul_out_ScalarList_out));
  m.impl("_foreach_addcmul_.ScalarList", TORCH_FN(functionalization::_foreach_addcmul__ScalarList));
  m.impl("_foreach_addcmul.Tensor_out", TORCH_FN(functionalization::_foreach_addcmul_out_Tensor_out));
  m.impl("_foreach_addcmul_.Tensor", TORCH_FN(functionalization::_foreach_addcmul__Tensor));
  m.impl("_foreach_asin.out", TORCH_FN(functionalization::_foreach_asin_out_out));
  m.impl("_foreach_asin_", TORCH_FN(functionalization::_foreach_asin_));
  m.impl("_foreach_ceil.out", TORCH_FN(functionalization::_foreach_ceil_out_out));
  m.impl("_foreach_ceil_", TORCH_FN(functionalization::_foreach_ceil_));
  m.impl("_foreach_cosh.out", TORCH_FN(functionalization::_foreach_cosh_out_out));
  m.impl("_foreach_cosh_", TORCH_FN(functionalization::_foreach_cosh_));
  m.impl("_foreach_log1p.out", TORCH_FN(functionalization::_foreach_log1p_out_out));
  m.impl("_foreach_log1p_", TORCH_FN(functionalization::_foreach_log1p_));
  m.impl("_foreach_log2.out", TORCH_FN(functionalization::_foreach_log2_out_out));
  m.impl("_foreach_log2_", TORCH_FN(functionalization::_foreach_log2_));
  m.impl("_foreach_pow.List_out", TORCH_FN(functionalization::_foreach_pow_out_List_out));
  m.impl("_foreach_pow_.List", TORCH_FN(functionalization::_foreach_pow__List));
  m.impl("_foreach_pow.Scalar_out", TORCH_FN(functionalization::_foreach_pow_out_Scalar_out));
  m.impl("_foreach_pow_.Scalar", TORCH_FN(functionalization::_foreach_pow__Scalar));
  m.impl("_foreach_pow.ScalarList_out", TORCH_FN(functionalization::_foreach_pow_out_ScalarList_out));
  m.impl("_foreach_pow_.ScalarList", TORCH_FN(functionalization::_foreach_pow__ScalarList));
  m.impl("_foreach_round.out", TORCH_FN(functionalization::_foreach_round_out_out));
  m.impl("_foreach_round_", TORCH_FN(functionalization::_foreach_round_));
  m.impl("_foreach_zero.out", TORCH_FN(functionalization::_foreach_zero_out_out));
  m.impl("_foreach_zero_", TORCH_FN(functionalization::_foreach_zero_));
  m.impl("bucketize.Tensor_out", TORCH_FN(functionalization::bucketize_out_Tensor_out));
  m.impl("bucketize.Scalar_out", TORCH_FN(functionalization::bucketize_out_Scalar_out));
  m.impl("mse_loss.out", TORCH_FN(functionalization::mse_loss_out_out));
  m.impl("multi_margin_loss.out", TORCH_FN(functionalization::multi_margin_loss_out_out));
  m.impl("multilabel_margin_loss_forward.output", TORCH_FN(functionalization::multilabel_margin_loss_forward_out_output));
  m.impl("nll_loss2d_forward.output", TORCH_FN(functionalization::nll_loss2d_forward_out_output));
  m.impl("soft_margin_loss.out", TORCH_FN(functionalization::soft_margin_loss_out_out));
  m.impl("glu_backward.grad_input", TORCH_FN(functionalization::glu_backward_out_grad_input));
  m.impl("glu_backward_jvp.out", TORCH_FN(functionalization::glu_backward_jvp_out_out));
  m.impl("hardtanh.out", TORCH_FN(functionalization::hardtanh_out_out));
  m.impl("hardtanh_", TORCH_FN(functionalization::hardtanh_));
  m.impl("hardtanh_backward.grad_input", TORCH_FN(functionalization::hardtanh_backward_out_grad_input));
  m.impl("leaky_relu.out", TORCH_FN(functionalization::leaky_relu_out_out));
  m.impl("leaky_relu_", TORCH_FN(functionalization::leaky_relu_));
  m.impl("log_sigmoid_backward.grad_input", TORCH_FN(functionalization::log_sigmoid_backward_out_grad_input));
  m.impl("softplus.out", TORCH_FN(functionalization::softplus_out_out));
  m.impl("adaptive_avg_pool2d.out", TORCH_FN(functionalization::adaptive_avg_pool2d_out_out));
  m.impl("_adaptive_avg_pool3d_backward.out", TORCH_FN(functionalization::_adaptive_avg_pool3d_backward_out_out));
  m.impl("adaptive_max_pool2d_backward.grad_input", TORCH_FN(functionalization::adaptive_max_pool2d_backward_out_grad_input));
  m.impl("fractional_max_pool2d_backward.grad_input", TORCH_FN(functionalization::fractional_max_pool2d_backward_out_grad_input));
  m.impl("fractional_max_pool3d_backward.grad_input", TORCH_FN(functionalization::fractional_max_pool3d_backward_out_grad_input));
  m.impl("max_pool3d_with_indices.out", TORCH_FN(functionalization::max_pool3d_with_indices_out_out));
  m.impl("max_pool3d_with_indices_backward.grad_input", TORCH_FN(functionalization::max_pool3d_with_indices_backward_out_grad_input));
  m.impl("max_unpool2d.out", TORCH_FN(functionalization::max_unpool2d_out_out));
  m.impl("reflection_pad2d_backward.grad_input", TORCH_FN(functionalization::reflection_pad2d_backward_out_grad_input));
  m.impl("upsample_bilinear2d.out", TORCH_FN(functionalization::upsample_bilinear2d_out_out));
  m.impl("upsample_bilinear2d_backward.grad_input", TORCH_FN(functionalization::upsample_bilinear2d_backward_out_grad_input));
  m.impl("_upsample_bilinear2d_aa.out", TORCH_FN(functionalization::_upsample_bilinear2d_aa_out_out));
  m.impl("upsample_trilinear3d_backward.grad_input", TORCH_FN(functionalization::upsample_trilinear3d_backward_out_grad_input));
  m.impl("_upsample_nearest_exact3d.out", TORCH_FN(functionalization::_upsample_nearest_exact3d_out_out));
  m.impl("upsample_nearest3d_backward.grad_input", TORCH_FN(functionalization::upsample_nearest3d_backward_out_grad_input));
  m.impl("logit_backward.grad_input", TORCH_FN(functionalization::logit_backward_out_grad_input));
  m.impl("_slow_conv2d_backward.output_mask_out", TORCH_FN(functionalization::_slow_conv2d_backward_out_output_mask_out));
  m.impl("slow_conv3d_forward.output", TORCH_FN(functionalization::slow_conv3d_forward_out_output));
  m.impl("slow_conv_dilated3d.out", TORCH_FN(functionalization::slow_conv_dilated3d_out_out));
  m.impl("special_log_ndtr.out", TORCH_FN(functionalization::special_log_ndtr_out_out));
  m.impl("special_erfcx.out", TORCH_FN(functionalization::special_erfcx_out_out));
  m.impl("special_xlog1py.out", TORCH_FN(functionalization::special_xlog1py_out_out));
  m.impl("special_xlog1py.self_scalar_out", TORCH_FN(functionalization::special_xlog1py_out_self_scalar_out));
  m.impl("special_xlog1py.other_scalar_out", TORCH_FN(functionalization::special_xlog1py_out_other_scalar_out));
  m.impl("special_i1.out", TORCH_FN(functionalization::special_i1_out_out));
  m.impl("special_i1e.out", TORCH_FN(functionalization::special_i1e_out_out));
  m.impl("fft_fftfreq.out", TORCH_FN(functionalization::fft_fftfreq_out_out));
  m.impl("linalg_ldl_factor_ex.out", TORCH_FN(functionalization::linalg_ldl_factor_ex_out_out));
  m.impl("linalg_lstsq.out", TORCH_FN(functionalization::linalg_lstsq_out_out));
  m.impl("linalg_matrix_exp.out", TORCH_FN(functionalization::linalg_matrix_exp_out_out));
  m.impl("_linalg_eigh.eigenvalues", TORCH_FN(functionalization::_linalg_eigh_out_eigenvalues));
  m.impl("_make_dual_copy.out", TORCH_FN(functionalization::_make_dual_copy_out_out));
  m.impl("view_as_real_copy.out", TORCH_FN(functionalization::view_as_real_copy_out_out));
  m.impl("view_as_complex_copy.out", TORCH_FN(functionalization::view_as_complex_copy_out_out));
  m.impl("_conj_copy.out", TORCH_FN(functionalization::_conj_copy_out_out));
  m.impl("_neg_view_copy.out", TORCH_FN(functionalization::_neg_view_copy_out_out));
  m.impl("permute_copy.out", TORCH_FN(functionalization::permute_copy_out_out));
  m.impl("split_copy.Tensor_out", TORCH_FN(functionalization::split_copy_out_Tensor_out));
  m.impl("t_copy.out", TORCH_FN(functionalization::t_copy_out_out));
  m.impl("ccol_indices_copy.out", TORCH_FN(functionalization::ccol_indices_copy_out_out));
  m.impl("unfold_copy.out", TORCH_FN(functionalization::unfold_copy_out_out));
  m.impl("_transformer_encoder_layer_fwd.out", TORCH_FN(functionalization::_transformer_encoder_layer_fwd_out_out));
  m.impl("_native_multi_head_attention.out", TORCH_FN(functionalization::_native_multi_head_attention_out_out));
  m.impl("special_modified_bessel_i1.out", TORCH_FN(functionalization::special_modified_bessel_i1_out_out));
  m.impl("special_modified_bessel_k0.out", TORCH_FN(functionalization::special_modified_bessel_k0_out_out));
  m.impl("_foobar.out", TORCH_FN(functionalization::_foobar_out_out));
  m.impl("view_as_real", TORCH_FN(functionalization::view_as_real));
  m.impl("slice.Tensor", TORCH_FN(functionalization::slice_Tensor));
  m.impl("t", TORCH_FN(functionalization::t));
  m.impl("t_", TORCH_FN(functionalization::t_));
  m.impl("_indices", TORCH_FN(functionalization::_indices));
  m.impl("indices", TORCH_FN(functionalization::indices));
  m.impl("crow_indices", TORCH_FN(functionalization::crow_indices));
  m.impl("col_indices", TORCH_FN(functionalization::col_indices));
  m.impl("unfold", TORCH_FN(functionalization::unfold));;
}

}  // namespace

} // namespace at
