// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// an external backend might generate file within its code tree
// and check all the source files within the tree with clang-format.
// so, disable it since the backend might have a different config.
// clang-format off

// NOTE: This condition is true for all PyTorch internal libraries, it
//       just excludes external projects such as torch_xla which
//       re-use some of the PyTorch codegen machinery.
#if defined(CAFFE2_BUILD_MAIN_LIB)        || \
    defined(TORCH_CUDA_BUILD_MAIN_LIB)    || \
    defined(TORCH_HIP_BUILD_MAIN_LIB)     || \
    defined(TORCH_CUDA_CU_BUILD_MAIN_LIB) || \
    defined(TORCH_CUDA_CPP_BUILD_MAIN_LIB)
#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
#endif

// @generated by torchgen/gen.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/Tensor.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>


#include <ATen/NativeFunctions.h>
#include <ATen/Functions.h>
#include <ATen/NativeFunctions.h>

// See template file RegisterDispatchDefinitions.ini
namespace at {
// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {
void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      // TODO: avoid the redispatch here
      out.as_strided_(sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}
void check_inplace(const Tensor &self, IntArrayRef sizes, const TensorOptions &options) {
  // These checks are needed on those operators that:
  //   1) don't use 'TensorIterator' (e.g. 'addmm' and 'baddbmm')
  //   2) have particular typing rules (e.g. 'cumsum' and 'cumprod')
  // For other operators (e.g. 'add'), 'TensorIterator' already checks
  // these things separately.
  TORCH_CHECK(options.dtype() == self.dtype(),
      "Bad in-place call: ",
      "input tensor dtype ", self.dtype(), " and output tensor dtype ", options.dtype(), " should match");
  TORCH_CHECK(options.device() == self.device(),
      "Bad in-place call: ",
      "input tensor device ", self.device(), " and output tensor device ", options.device(), " should match");
  TORCH_CHECK(sizes == self.sizes(),
      "Bad in-place call: ",
      "input tensor size ", self.sizes(), " and output tensor size ", sizes, " should match");
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_NestedTensorCPU__native_dropout(const at::Tensor & input, double p, c10::optional<bool> train) {
    // No device check
  // DeviceGuard omitted
  return at::native::native_dropout_nested(input, p, train);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__native_dropout_backward(const at::Tensor & grad_output, const at::Tensor & mask, double scale) {
    // No device check
  // DeviceGuard omitted
  return at::native::native_dropout_backward(grad_output, mask, scale);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__abs(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_abs(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU__abs_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_abs_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__sgn(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_sgn(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU__sgn_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_sgn_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_Tensor_add(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_add_Tensor(self, other, alpha);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU_Tensor_add_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_add__Tensor(self, other, alpha);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__logical_not(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_logical_not(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU__logical_not_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_logical_not_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__bmm(const at::Tensor & self, const at::Tensor & mat2) {
    // No device check
  // DeviceGuard omitted
  return at::native::bmm_nested(self, mat2);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__cat(const at::ITensorListRef & tensors, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::cat_nested(tensors, dim);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_NestedTensorCPU__chunk(const at::Tensor & self, int64_t chunks, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::chunk_nested_tensor(self, chunks, dim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU__copy_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return at::native::copy_nested_(self, src, non_blocking);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__cos(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::cos_nested(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_Tensor_div(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_div_Tensor(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_Scalar_div(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_div_Scalar(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__embedding(const at::Tensor & weight, const at::Tensor & indices, c10::SymInt padding_idx, bool scale_grad_by_freq, bool sparse) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_embedding(weight, indices, padding_idx.guard_int(__FILE__, __LINE__), scale_grad_by_freq, sparse);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__empty_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::empty_like_nested(self, dtype, layout, device, pin_memory, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU_Scalar_fill_(at::Tensor & self, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return at::native::fill_nested_(self, value);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU_Tensor_fill_(at::Tensor & self, const at::Tensor & value) {
    // No device check
  // DeviceGuard omitted
  return at::native::fill_nested_(self, value);
}
} // anonymous namespace
namespace {
bool wrapper_NestedTensorCPU__is_same_size(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::nested_is_same_size(self, other);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_NestedTensorCPU__native_layer_norm(const at::Tensor & input, c10::SymIntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps) {
    // No device check
  // DeviceGuard omitted
  return at::native::nested_layer_norm(input, C10_AS_INTARRAYREF_SLOW(normalized_shape), weight, bias, eps);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_NestedTensorCPU__native_layer_norm_backward(const at::Tensor & grad_out, const at::Tensor & input, c10::SymIntArrayRef normalized_shape, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, ::std::array<bool,3> output_mask) {
    // No device check
  // DeviceGuard omitted
  return at::native::layer_norm_backward_nested(grad_out, input, C10_AS_INTARRAYREF_SLOW(normalized_shape), mean, rstd, weight, bias, output_mask);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__linear(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
    // No device check
  // DeviceGuard omitted
  return at::native::nested_linear(input, weight, bias);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_NestedTensorCPU__linear_backward(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask) {
    // No device check
  // DeviceGuard omitted
  return at::native::nested_linear_backward(self, grad_output, weight, output_mask);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__matmul(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::matmul_nested(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU_out_matmul_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::matmul_out_nested(self, other, out);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_NestedTensorCPU__matmul_backward(const at::Tensor & grad, const at::Tensor & self, const at::Tensor & other, ::std::array<bool,2> mask) {
    // No device check
  // DeviceGuard omitted
  return at::native::matmul_backward_nested(grad, self, other, mask);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_Tensor_mul(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_mul_Tensor(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU_Tensor_mul_(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_mul__Tensor(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_Scalar_mul(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_mul_Scalar(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU_Scalar_mul_(at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_mul__Scalar(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__narrow(const at::Tensor & self, int64_t dim, c10::SymInt start, c10::SymInt length) {
    // No device check
  // DeviceGuard omitted
  return at::native::narrow_nested_symint(self, dim, start, length);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__ones_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::ones_like(self, dtype, layout, device, pin_memory, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU___pin_memory(const at::Tensor & self, c10::optional<at::Device> device) {
    // No device check
  // DeviceGuard omitted
  return at::native::_pin_memory_nested(self, device);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__neg(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_neg(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU__neg_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_neg_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__relu(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_relu(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU__relu_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_relu_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__gelu(const at::Tensor & self, c10::string_view approximate) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_gelu(self, approximate);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU__gelu_(at::Tensor & self, c10::string_view approximate) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_gelu_(self, approximate);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__gelu_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::string_view approximate) {
    // No device check
  // DeviceGuard omitted
  return at::native::gelu_backwards_nested(grad_output, self, approximate);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_int_select(const at::Tensor & self, int64_t dim, c10::SymInt index) {
    // No device check
  // DeviceGuard omitted
  return at::native::select_nested(self, dim, index.guard_int(__FILE__, __LINE__));
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU___nested_select_backward(const at::Tensor & grad_output, const at::Tensor & self, int64_t dim, c10::SymInt index) {
    // No device check
  // DeviceGuard omitted
  return at::native::_nested_select_backward_symint(grad_output, self, dim, index);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__silu(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_silu(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU__silu_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_silu_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__silu_backward(const at::Tensor & grad_output, const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::silu_backward_nested(grad_output, self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__sin(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::sin_nested(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__detach(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::detach(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU___softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
    // No device check
  // DeviceGuard omitted
  return at::native::softmax_nested(self, dim, half_to_float);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU___softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::nested_softmax_backward(grad_output, output, dim, input_dtype);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_NestedTensorCPU__split_with_sizes(const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::split_with_sizes_nested(self, C10_AS_INTARRAYREF_SLOW(split_sizes), dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__squeeze(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::squeeze_nested(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_dim_squeeze(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::squeeze_dim_nested(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_dims_squeeze(const at::Tensor & self, at::IntArrayRef dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::squeeze_dim_nested(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_dim_IntList_sum(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_sum_dim_CPU(self, dim, keepdim, dtype);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU___nested_sum_backward(const at::Tensor & grad, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return at::native::_nested_sum_backward_cpu(grad, self, dim, keepdim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__tanh(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_tanh(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU__tanh_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_tanh_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__threshold_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
    // No device check
  // DeviceGuard omitted
  return at::native::threshold_backwards_nested(grad_output, self, threshold);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_int_transpose(const at::Tensor & self, int64_t dim0, int64_t dim1) {
    // No device check
  // DeviceGuard omitted
  return at::native::transpose_nested(self, dim0, dim1);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_NestedTensorCPU___transform_bias_rescale_qkv(const at::Tensor & qkv, const at::Tensor & qkv_bias, int64_t num_heads) {
    // No device check
  // DeviceGuard omitted
  return at::native::transform_bias_rescale_qkv_cpu(qkv, qkv_bias, num_heads);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU___nested_tensor_size(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::_nested_tensor_size(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU___nested_tensor_strides(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::_nested_tensor_strides(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU___nested_tensor_storage_offsets(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::_nested_tensor_storage_offsets(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU___nested_from_padded_and_nested_example(const at::Tensor & padded, const at::Tensor & nt_example) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_from_padded_and_nested_example(padded, nt_example);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__unsqueeze(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::unsqueeze_nested(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::clone_nested(self, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU__zero_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::zero_nested_(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_Tensor_sub(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_sub_Tensor(self, other, alpha);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__values(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::values_nested(self);
}
} // anonymous namespace
namespace {
::std::vector<at::Tensor> wrapper_NestedTensorCPU_int_unbind(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_unbind(self, dim);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU___to_copy(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, c10::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::native::_to_copy_nested(self, dtype, layout, device, pin_memory, non_blocking, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_Scalar_masked_fill(const at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_masked_fill(self, mask, value);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__view(const at::Tensor & self, c10::SymIntArrayRef size) {
    // No device check
  // DeviceGuard omitted
  return at::native::view_nested(self, C10_AS_INTARRAYREF_SLOW(size));
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_Scalar_eq(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::eq_scalar_nested(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_Scalar_ge(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::ge_scalar_nested(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_Scalar_gt(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return at::native::gt_scalar_nested(self, other);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_NestedTensorCPU__normal_(at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return at::native::normal_nested_(self, mean, std, generator);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_fullcoverage__test_autograd_multiple_dispatch(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return at::native::_test_autograd_multiple_dispatch_fullcoverage(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU_ntonly__test_autograd_multiple_dispatch(const at::Tensor & self, bool b) {
    // No device check
  // DeviceGuard omitted
  return at::native::_test_autograd_multiple_dispatch_ntonly(self, b);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU__to_padded_tensor(const at::Tensor & self, double padding, at::OptionalSymIntArrayRef output_size) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_to_padded_tensor_generic(self, padding, output_size.has_value() ? c10::make_optional(C10_AS_INTARRAYREF_SLOW(*output_size)) : c10::nullopt);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU___nested_tensor_softmax_with_shape(const at::Tensor & self, const at::Tensor & query) {
    // No device check
  // DeviceGuard omitted
  return at::native::NestedTensor_softmax_dropout(self, query);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_NestedTensorCPU___transformer_encoder_layer_fwd(const at::Tensor & src, int64_t embed_dim, int64_t num_heads, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, bool use_gelu, bool norm_first, double eps, const at::Tensor & norm_weight_1, const at::Tensor & norm_bias_1, const at::Tensor & norm_weight_2, const at::Tensor & norm_bias_2, const at::Tensor & ffn_weight_1, const at::Tensor & ffn_bias_1, const at::Tensor & ffn_weight_2, const at::Tensor & ffn_bias_2, const c10::optional<at::Tensor> & mask, c10::optional<int64_t> mask_type) {
    // No device check
  // DeviceGuard omitted
  return at::native::transformer_encoder_layer_forward(src, embed_dim, num_heads, qkv_weight, qkv_bias, proj_weight, proj_bias, use_gelu, norm_first, eps, norm_weight_1, norm_bias_1, norm_weight_2, norm_bias_2, ffn_weight_1, ffn_bias_1, ffn_weight_2, ffn_bias_2, mask, mask_type);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_NestedTensorCPU___native_multi_head_attention(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, int64_t embed_dim, int64_t num_head, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, const c10::optional<at::Tensor> & mask, bool need_weights, bool average_attn_weights, c10::optional<int64_t> mask_type) {
    // No device check
  // DeviceGuard omitted
  return at::native::native_multi_head_attention_cpu(query, key, value, embed_dim, num_head, qkv_weight, qkv_bias, proj_weight, proj_bias, mask, need_weights, average_attn_weights, mask_type);
}
} // anonymous namespace
namespace {
int64_t wrapper_NestedTensorCPU___fused_sdp_choice(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & attn_mask, double dropout_p, bool is_causal, c10::optional<double> scale) {
    // No device check
  // DeviceGuard omitted
  return at::native::_fused_sdp_choice_cpp(query, key, value, attn_mask, dropout_p, is_causal, scale);
}
} // anonymous namespace
TORCH_LIBRARY_IMPL(aten, NestedTensorCPU, m) {
    m.impl("native_dropout",
TORCH_FN(wrapper_NestedTensorCPU__native_dropout));
m.impl("native_dropout_backward",
TORCH_FN(wrapper_NestedTensorCPU__native_dropout_backward));
m.impl("abs",
TORCH_FN(wrapper_NestedTensorCPU__abs));
m.impl("abs_",
TORCH_FN(wrapper_NestedTensorCPU__abs_));
m.impl("sgn",
TORCH_FN(wrapper_NestedTensorCPU__sgn));
m.impl("sgn_",
TORCH_FN(wrapper_NestedTensorCPU__sgn_));
m.impl("add.Tensor",
TORCH_FN(wrapper_NestedTensorCPU_Tensor_add));
m.impl("add_.Tensor",
TORCH_FN(wrapper_NestedTensorCPU_Tensor_add_));
m.impl("logical_not",
TORCH_FN(wrapper_NestedTensorCPU__logical_not));
m.impl("logical_not_",
TORCH_FN(wrapper_NestedTensorCPU__logical_not_));
m.impl("bmm",
TORCH_FN(wrapper_NestedTensorCPU__bmm));
m.impl("cat",
TORCH_FN(wrapper_NestedTensorCPU__cat));
m.impl("chunk",
TORCH_FN(wrapper_NestedTensorCPU__chunk));
m.impl("copy_",
TORCH_FN(wrapper_NestedTensorCPU__copy_));
m.impl("cos",
TORCH_FN(wrapper_NestedTensorCPU__cos));
m.impl("div.Tensor",
TORCH_FN(wrapper_NestedTensorCPU_Tensor_div));
m.impl("div.Scalar",
TORCH_FN(wrapper_NestedTensorCPU_Scalar_div));
m.impl("embedding",
TORCH_FN(wrapper_NestedTensorCPU__embedding));
m.impl("empty_like",
TORCH_FN(wrapper_NestedTensorCPU__empty_like));
m.impl("fill_.Scalar",
TORCH_FN(wrapper_NestedTensorCPU_Scalar_fill_));
m.impl("fill_.Tensor",
TORCH_FN(wrapper_NestedTensorCPU_Tensor_fill_));
m.impl("is_same_size",
TORCH_FN(wrapper_NestedTensorCPU__is_same_size));
m.impl("native_layer_norm",
TORCH_FN(wrapper_NestedTensorCPU__native_layer_norm));
m.impl("native_layer_norm_backward",
TORCH_FN(wrapper_NestedTensorCPU__native_layer_norm_backward));
m.impl("linear",
TORCH_FN(wrapper_NestedTensorCPU__linear));
m.impl("linear_backward",
TORCH_FN(wrapper_NestedTensorCPU__linear_backward));
m.impl("matmul",
TORCH_FN(wrapper_NestedTensorCPU__matmul));
m.impl("matmul.out",
TORCH_FN(wrapper_NestedTensorCPU_out_matmul_out));
m.impl("matmul_backward",
TORCH_FN(wrapper_NestedTensorCPU__matmul_backward));
m.impl("mul.Tensor",
TORCH_FN(wrapper_NestedTensorCPU_Tensor_mul));
m.impl("mul_.Tensor",
TORCH_FN(wrapper_NestedTensorCPU_Tensor_mul_));
m.impl("mul.Scalar",
TORCH_FN(wrapper_NestedTensorCPU_Scalar_mul));
m.impl("mul_.Scalar",
TORCH_FN(wrapper_NestedTensorCPU_Scalar_mul_));
m.impl("narrow",
TORCH_FN(wrapper_NestedTensorCPU__narrow));
m.impl("ones_like",
TORCH_FN(wrapper_NestedTensorCPU__ones_like));
m.impl("_pin_memory",
TORCH_FN(wrapper_NestedTensorCPU___pin_memory));
m.impl("neg",
TORCH_FN(wrapper_NestedTensorCPU__neg));
m.impl("neg_",
TORCH_FN(wrapper_NestedTensorCPU__neg_));
m.impl("relu",
TORCH_FN(wrapper_NestedTensorCPU__relu));
m.impl("relu_",
TORCH_FN(wrapper_NestedTensorCPU__relu_));
m.impl("gelu",
TORCH_FN(wrapper_NestedTensorCPU__gelu));
m.impl("gelu_",
TORCH_FN(wrapper_NestedTensorCPU__gelu_));
m.impl("gelu_backward",
TORCH_FN(wrapper_NestedTensorCPU__gelu_backward));
m.impl("select.int",
TORCH_FN(wrapper_NestedTensorCPU_int_select));
m.impl("_nested_select_backward",
TORCH_FN(wrapper_NestedTensorCPU___nested_select_backward));
m.impl("silu",
TORCH_FN(wrapper_NestedTensorCPU__silu));
m.impl("silu_",
TORCH_FN(wrapper_NestedTensorCPU__silu_));
m.impl("silu_backward",
TORCH_FN(wrapper_NestedTensorCPU__silu_backward));
m.impl("sin",
TORCH_FN(wrapper_NestedTensorCPU__sin));
m.impl("detach",
TORCH_FN(wrapper_NestedTensorCPU__detach));
m.impl("_softmax",
TORCH_FN(wrapper_NestedTensorCPU___softmax));
m.impl("_softmax_backward_data",
TORCH_FN(wrapper_NestedTensorCPU___softmax_backward_data));
m.impl("split_with_sizes",
TORCH_FN(wrapper_NestedTensorCPU__split_with_sizes));
m.impl("squeeze",
TORCH_FN(wrapper_NestedTensorCPU__squeeze));
m.impl("squeeze.dim",
TORCH_FN(wrapper_NestedTensorCPU_dim_squeeze));
m.impl("squeeze.dims",
TORCH_FN(wrapper_NestedTensorCPU_dims_squeeze));
m.impl("sum.dim_IntList",
TORCH_FN(wrapper_NestedTensorCPU_dim_IntList_sum));
m.impl("_nested_sum_backward",
TORCH_FN(wrapper_NestedTensorCPU___nested_sum_backward));
m.impl("tanh",
TORCH_FN(wrapper_NestedTensorCPU__tanh));
m.impl("tanh_",
TORCH_FN(wrapper_NestedTensorCPU__tanh_));
m.impl("threshold_backward",
TORCH_FN(wrapper_NestedTensorCPU__threshold_backward));
m.impl("transpose.int",
TORCH_FN(wrapper_NestedTensorCPU_int_transpose));
m.impl("_transform_bias_rescale_qkv",
TORCH_FN(wrapper_NestedTensorCPU___transform_bias_rescale_qkv));
m.impl("_nested_tensor_size",
TORCH_FN(wrapper_NestedTensorCPU___nested_tensor_size));
m.impl("_nested_tensor_strides",
TORCH_FN(wrapper_NestedTensorCPU___nested_tensor_strides));
m.impl("_nested_tensor_storage_offsets",
TORCH_FN(wrapper_NestedTensorCPU___nested_tensor_storage_offsets));
m.impl("_nested_from_padded_and_nested_example",
TORCH_FN(wrapper_NestedTensorCPU___nested_from_padded_and_nested_example));
m.impl("unsqueeze",
TORCH_FN(wrapper_NestedTensorCPU__unsqueeze));
m.impl("clone",
TORCH_FN(wrapper_NestedTensorCPU__clone));
m.impl("zero_",
TORCH_FN(wrapper_NestedTensorCPU__zero_));
m.impl("sub.Tensor",
TORCH_FN(wrapper_NestedTensorCPU_Tensor_sub));
m.impl("values",
TORCH_FN(wrapper_NestedTensorCPU__values));
m.impl("unbind.int",
TORCH_FN(wrapper_NestedTensorCPU_int_unbind));
m.impl("_to_copy",
TORCH_FN(wrapper_NestedTensorCPU___to_copy));
m.impl("masked_fill.Scalar",
TORCH_FN(wrapper_NestedTensorCPU_Scalar_masked_fill));
m.impl("view",
TORCH_FN(wrapper_NestedTensorCPU__view));
m.impl("eq.Scalar",
TORCH_FN(wrapper_NestedTensorCPU_Scalar_eq));
m.impl("ge.Scalar",
TORCH_FN(wrapper_NestedTensorCPU_Scalar_ge));
m.impl("gt.Scalar",
TORCH_FN(wrapper_NestedTensorCPU_Scalar_gt));
m.impl("normal_",
TORCH_FN(wrapper_NestedTensorCPU__normal_));
m.impl("_test_autograd_multiple_dispatch.fullcoverage",
TORCH_FN(wrapper_NestedTensorCPU_fullcoverage__test_autograd_multiple_dispatch));
m.impl("_test_autograd_multiple_dispatch.ntonly",
TORCH_FN(wrapper_NestedTensorCPU_ntonly__test_autograd_multiple_dispatch));
m.impl("to_padded_tensor",
TORCH_FN(wrapper_NestedTensorCPU__to_padded_tensor));
m.impl("_nested_tensor_softmax_with_shape",
TORCH_FN(wrapper_NestedTensorCPU___nested_tensor_softmax_with_shape));
m.impl("_transformer_encoder_layer_fwd",
TORCH_FN(wrapper_NestedTensorCPU___transformer_encoder_layer_fwd));
m.impl("_native_multi_head_attention",
TORCH_FN(wrapper_NestedTensorCPU___native_multi_head_attention));
m.impl("_fused_sdp_choice",
TORCH_FN(wrapper_NestedTensorCPU___fused_sdp_choice));
};
} // anonymous namespace
namespace nestedtensorcpu {
::std::tuple<at::Tensor,at::Tensor> native_dropout(const at::Tensor & input, double p, c10::optional<bool> train) {
return wrapper_NestedTensorCPU__native_dropout(input, p, train);
}
at::Tensor native_dropout_backward(const at::Tensor & grad_output, const at::Tensor & mask, double scale) {
return wrapper_NestedTensorCPU__native_dropout_backward(grad_output, mask, scale);
}
at::Tensor abs(const at::Tensor & self) {
return wrapper_NestedTensorCPU__abs(self);
}
at::Tensor & abs_(at::Tensor & self) {
return wrapper_NestedTensorCPU__abs_(self);
}
at::Tensor sgn(const at::Tensor & self) {
return wrapper_NestedTensorCPU__sgn(self);
}
at::Tensor & sgn_(at::Tensor & self) {
return wrapper_NestedTensorCPU__sgn_(self);
}
at::Tensor add(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_NestedTensorCPU_Tensor_add(self, other, alpha);
}
at::Tensor & add_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_NestedTensorCPU_Tensor_add_(self, other, alpha);
}
at::Tensor logical_not(const at::Tensor & self) {
return wrapper_NestedTensorCPU__logical_not(self);
}
at::Tensor & logical_not_(at::Tensor & self) {
return wrapper_NestedTensorCPU__logical_not_(self);
}
at::Tensor bmm(const at::Tensor & self, const at::Tensor & mat2) {
return wrapper_NestedTensorCPU__bmm(self, mat2);
}
at::Tensor cat(const at::ITensorListRef & tensors, int64_t dim) {
return wrapper_NestedTensorCPU__cat(tensors, dim);
}
::std::vector<at::Tensor> chunk(const at::Tensor & self, int64_t chunks, int64_t dim) {
return wrapper_NestedTensorCPU__chunk(self, chunks, dim);
}
at::Tensor & copy_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
return wrapper_NestedTensorCPU__copy_(self, src, non_blocking);
}
at::Tensor cos(const at::Tensor & self) {
return wrapper_NestedTensorCPU__cos(self);
}
at::Tensor div(const at::Tensor & self, const at::Tensor & other) {
return wrapper_NestedTensorCPU_Tensor_div(self, other);
}
at::Tensor div(const at::Tensor & self, const at::Scalar & other) {
return wrapper_NestedTensorCPU_Scalar_div(self, other);
}
at::Tensor embedding(const at::Tensor & weight, const at::Tensor & indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
return wrapper_NestedTensorCPU__embedding(weight, indices, padding_idx, scale_grad_by_freq, sparse);
}
at::Tensor embedding_symint(const at::Tensor & weight, const at::Tensor & indices, c10::SymInt padding_idx, bool scale_grad_by_freq, bool sparse) {
return wrapper_NestedTensorCPU__embedding(weight, indices, padding_idx, scale_grad_by_freq, sparse);
}
at::Tensor empty_like(const at::Tensor & self, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_NestedTensorCPU__empty_like(self, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor empty_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_NestedTensorCPU__empty_like(self, dtype, layout, device, pin_memory, memory_format);
}
at::Tensor & fill_(at::Tensor & self, const at::Scalar & value) {
return wrapper_NestedTensorCPU_Scalar_fill_(self, value);
}
at::Tensor & fill_(at::Tensor & self, const at::Tensor & value) {
return wrapper_NestedTensorCPU_Tensor_fill_(self, value);
}
bool is_same_size(const at::Tensor & self, const at::Tensor & other) {
return wrapper_NestedTensorCPU__is_same_size(self, other);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_layer_norm(const at::Tensor & input, at::IntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps) {
return wrapper_NestedTensorCPU__native_layer_norm(input, c10::fromIntArrayRefSlow(normalized_shape), weight, bias, eps);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_layer_norm_symint(const at::Tensor & input, c10::SymIntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps) {
return wrapper_NestedTensorCPU__native_layer_norm(input, normalized_shape, weight, bias, eps);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_layer_norm_backward(const at::Tensor & grad_out, const at::Tensor & input, at::IntArrayRef normalized_shape, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, ::std::array<bool,3> output_mask) {
return wrapper_NestedTensorCPU__native_layer_norm_backward(grad_out, input, c10::fromIntArrayRefSlow(normalized_shape), mean, rstd, weight, bias, output_mask);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_layer_norm_backward_symint(const at::Tensor & grad_out, const at::Tensor & input, c10::SymIntArrayRef normalized_shape, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, ::std::array<bool,3> output_mask) {
return wrapper_NestedTensorCPU__native_layer_norm_backward(grad_out, input, normalized_shape, mean, rstd, weight, bias, output_mask);
}
at::Tensor linear(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
return wrapper_NestedTensorCPU__linear(input, weight, bias);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> linear_backward(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask) {
return wrapper_NestedTensorCPU__linear_backward(self, grad_output, weight, output_mask);
}
at::Tensor matmul(const at::Tensor & self, const at::Tensor & other) {
return wrapper_NestedTensorCPU__matmul(self, other);
}
at::Tensor & matmul_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_NestedTensorCPU_out_matmul_out(self, other, out);
}
at::Tensor & matmul_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_NestedTensorCPU_out_matmul_out(self, other, out);
}
::std::tuple<at::Tensor,at::Tensor> matmul_backward(const at::Tensor & grad, const at::Tensor & self, const at::Tensor & other, ::std::array<bool,2> mask) {
return wrapper_NestedTensorCPU__matmul_backward(grad, self, other, mask);
}
at::Tensor mul(const at::Tensor & self, const at::Tensor & other) {
return wrapper_NestedTensorCPU_Tensor_mul(self, other);
}
at::Tensor & mul_(at::Tensor & self, const at::Tensor & other) {
return wrapper_NestedTensorCPU_Tensor_mul_(self, other);
}
at::Tensor mul(const at::Tensor & self, const at::Scalar & other) {
return wrapper_NestedTensorCPU_Scalar_mul(self, other);
}
at::Tensor & mul_(at::Tensor & self, const at::Scalar & other) {
return wrapper_NestedTensorCPU_Scalar_mul_(self, other);
}
at::Tensor narrow(const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
return wrapper_NestedTensorCPU__narrow(self, dim, start, length);
}
at::Tensor narrow_symint(const at::Tensor & self, int64_t dim, c10::SymInt start, c10::SymInt length) {
return wrapper_NestedTensorCPU__narrow(self, dim, start, length);
}
at::Tensor ones_like(const at::Tensor & self, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_NestedTensorCPU__ones_like(self, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor ones_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_NestedTensorCPU__ones_like(self, dtype, layout, device, pin_memory, memory_format);
}
at::Tensor _pin_memory(const at::Tensor & self, c10::optional<at::Device> device) {
return wrapper_NestedTensorCPU___pin_memory(self, device);
}
at::Tensor neg(const at::Tensor & self) {
return wrapper_NestedTensorCPU__neg(self);
}
at::Tensor & neg_(at::Tensor & self) {
return wrapper_NestedTensorCPU__neg_(self);
}
at::Tensor relu(const at::Tensor & self) {
return wrapper_NestedTensorCPU__relu(self);
}
at::Tensor & relu_(at::Tensor & self) {
return wrapper_NestedTensorCPU__relu_(self);
}
at::Tensor gelu(const at::Tensor & self, c10::string_view approximate) {
return wrapper_NestedTensorCPU__gelu(self, approximate);
}
at::Tensor & gelu_(at::Tensor & self, c10::string_view approximate) {
return wrapper_NestedTensorCPU__gelu_(self, approximate);
}
at::Tensor gelu_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::string_view approximate) {
return wrapper_NestedTensorCPU__gelu_backward(grad_output, self, approximate);
}
at::Tensor select(const at::Tensor & self, int64_t dim, int64_t index) {
return wrapper_NestedTensorCPU_int_select(self, dim, index);
}
at::Tensor select_symint(const at::Tensor & self, int64_t dim, c10::SymInt index) {
return wrapper_NestedTensorCPU_int_select(self, dim, index);
}
at::Tensor _nested_select_backward(const at::Tensor & grad_output, const at::Tensor & self, int64_t dim, int64_t index) {
return wrapper_NestedTensorCPU___nested_select_backward(grad_output, self, dim, index);
}
at::Tensor _nested_select_backward_symint(const at::Tensor & grad_output, const at::Tensor & self, int64_t dim, c10::SymInt index) {
return wrapper_NestedTensorCPU___nested_select_backward(grad_output, self, dim, index);
}
at::Tensor silu(const at::Tensor & self) {
return wrapper_NestedTensorCPU__silu(self);
}
at::Tensor & silu_(at::Tensor & self) {
return wrapper_NestedTensorCPU__silu_(self);
}
at::Tensor silu_backward(const at::Tensor & grad_output, const at::Tensor & self) {
return wrapper_NestedTensorCPU__silu_backward(grad_output, self);
}
at::Tensor sin(const at::Tensor & self) {
return wrapper_NestedTensorCPU__sin(self);
}
at::Tensor detach(const at::Tensor & self) {
return wrapper_NestedTensorCPU__detach(self);
}
at::Tensor _softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
return wrapper_NestedTensorCPU___softmax(self, dim, half_to_float);
}
at::Tensor _softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype) {
return wrapper_NestedTensorCPU___softmax_backward_data(grad_output, output, dim, input_dtype);
}
::std::vector<at::Tensor> split_with_sizes(const at::Tensor & self, at::IntArrayRef split_sizes, int64_t dim) {
return wrapper_NestedTensorCPU__split_with_sizes(self, c10::fromIntArrayRefSlow(split_sizes), dim);
}
::std::vector<at::Tensor> split_with_sizes_symint(const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim) {
return wrapper_NestedTensorCPU__split_with_sizes(self, split_sizes, dim);
}
at::Tensor squeeze(const at::Tensor & self) {
return wrapper_NestedTensorCPU__squeeze(self);
}
at::Tensor squeeze(const at::Tensor & self, int64_t dim) {
return wrapper_NestedTensorCPU_dim_squeeze(self, dim);
}
at::Tensor squeeze(const at::Tensor & self, at::IntArrayRef dim) {
return wrapper_NestedTensorCPU_dims_squeeze(self, dim);
}
at::Tensor sum(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_NestedTensorCPU_dim_IntList_sum(self, dim, keepdim, dtype);
}
at::Tensor _nested_sum_backward(const at::Tensor & grad, const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim) {
return wrapper_NestedTensorCPU___nested_sum_backward(grad, self, dim, keepdim);
}
at::Tensor tanh(const at::Tensor & self) {
return wrapper_NestedTensorCPU__tanh(self);
}
at::Tensor & tanh_(at::Tensor & self) {
return wrapper_NestedTensorCPU__tanh_(self);
}
at::Tensor threshold_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
return wrapper_NestedTensorCPU__threshold_backward(grad_output, self, threshold);
}
at::Tensor transpose(const at::Tensor & self, int64_t dim0, int64_t dim1) {
return wrapper_NestedTensorCPU_int_transpose(self, dim0, dim1);
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _transform_bias_rescale_qkv(const at::Tensor & qkv, const at::Tensor & qkv_bias, int64_t num_heads) {
return wrapper_NestedTensorCPU___transform_bias_rescale_qkv(qkv, qkv_bias, num_heads);
}
at::Tensor _nested_tensor_size(const at::Tensor & self) {
return wrapper_NestedTensorCPU___nested_tensor_size(self);
}
at::Tensor _nested_tensor_strides(const at::Tensor & self) {
return wrapper_NestedTensorCPU___nested_tensor_strides(self);
}
at::Tensor _nested_tensor_storage_offsets(const at::Tensor & self) {
return wrapper_NestedTensorCPU___nested_tensor_storage_offsets(self);
}
at::Tensor _nested_from_padded_and_nested_example(const at::Tensor & padded, const at::Tensor & nt_example) {
return wrapper_NestedTensorCPU___nested_from_padded_and_nested_example(padded, nt_example);
}
at::Tensor unsqueeze(const at::Tensor & self, int64_t dim) {
return wrapper_NestedTensorCPU__unsqueeze(self, dim);
}
at::Tensor clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_NestedTensorCPU__clone(self, memory_format);
}
at::Tensor & zero_(at::Tensor & self) {
return wrapper_NestedTensorCPU__zero_(self);
}
at::Tensor sub(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_NestedTensorCPU_Tensor_sub(self, other, alpha);
}
at::Tensor values(const at::Tensor & self) {
return wrapper_NestedTensorCPU__values(self);
}
::std::vector<at::Tensor> unbind(const at::Tensor & self, int64_t dim) {
return wrapper_NestedTensorCPU_int_unbind(self, dim);
}
at::Tensor _to_copy(const at::Tensor & self, at::TensorOptions options, bool non_blocking, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_NestedTensorCPU___to_copy(self, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), non_blocking, c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}
at::Tensor _to_copy(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_NestedTensorCPU___to_copy(self, dtype, layout, device, pin_memory, non_blocking, memory_format);
}
at::Tensor masked_fill(const at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
return wrapper_NestedTensorCPU_Scalar_masked_fill(self, mask, value);
}
at::Tensor view(const at::Tensor & self, at::IntArrayRef size) {
return wrapper_NestedTensorCPU__view(self, c10::fromIntArrayRefSlow(size));
}
at::Tensor view_symint(const at::Tensor & self, c10::SymIntArrayRef size) {
return wrapper_NestedTensorCPU__view(self, size);
}
at::Tensor eq(const at::Tensor & self, const at::Scalar & other) {
return wrapper_NestedTensorCPU_Scalar_eq(self, other);
}
at::Tensor ge(const at::Tensor & self, const at::Scalar & other) {
return wrapper_NestedTensorCPU_Scalar_ge(self, other);
}
at::Tensor gt(const at::Tensor & self, const at::Scalar & other) {
return wrapper_NestedTensorCPU_Scalar_gt(self, other);
}
at::Tensor & normal_(at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
return wrapper_NestedTensorCPU__normal_(self, mean, std, generator);
}
at::Tensor _test_autograd_multiple_dispatch(const at::Tensor & self) {
return wrapper_NestedTensorCPU_fullcoverage__test_autograd_multiple_dispatch(self);
}
at::Tensor _test_autograd_multiple_dispatch(const at::Tensor & self, bool b) {
return wrapper_NestedTensorCPU_ntonly__test_autograd_multiple_dispatch(self, b);
}
at::Tensor to_padded_tensor(const at::Tensor & self, double padding, at::OptionalIntArrayRef output_size) {
return wrapper_NestedTensorCPU__to_padded_tensor(self, padding, output_size.has_value() ? c10::make_optional(c10::fromIntArrayRefSlow(*output_size)) : c10::nullopt);
}
at::Tensor to_padded_tensor_symint(const at::Tensor & self, double padding, at::OptionalSymIntArrayRef output_size) {
return wrapper_NestedTensorCPU__to_padded_tensor(self, padding, output_size);
}
at::Tensor _nested_tensor_softmax_with_shape(const at::Tensor & self, const at::Tensor & query) {
return wrapper_NestedTensorCPU___nested_tensor_softmax_with_shape(self, query);
}
at::Tensor _transformer_encoder_layer_fwd(const at::Tensor & src, int64_t embed_dim, int64_t num_heads, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, bool use_gelu, bool norm_first, double eps, const at::Tensor & norm_weight_1, const at::Tensor & norm_bias_1, const at::Tensor & norm_weight_2, const at::Tensor & norm_bias_2, const at::Tensor & ffn_weight_1, const at::Tensor & ffn_bias_1, const at::Tensor & ffn_weight_2, const at::Tensor & ffn_bias_2, const c10::optional<at::Tensor> & mask, c10::optional<int64_t> mask_type) {
return wrapper_NestedTensorCPU___transformer_encoder_layer_fwd(src, embed_dim, num_heads, qkv_weight, qkv_bias, proj_weight, proj_bias, use_gelu, norm_first, eps, norm_weight_1, norm_bias_1, norm_weight_2, norm_bias_2, ffn_weight_1, ffn_bias_1, ffn_weight_2, ffn_bias_2, mask, mask_type);
}
::std::tuple<at::Tensor,at::Tensor> _native_multi_head_attention(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, int64_t embed_dim, int64_t num_head, const at::Tensor & qkv_weight, const at::Tensor & qkv_bias, const at::Tensor & proj_weight, const at::Tensor & proj_bias, const c10::optional<at::Tensor> & mask, bool need_weights, bool average_attn_weights, c10::optional<int64_t> mask_type) {
return wrapper_NestedTensorCPU___native_multi_head_attention(query, key, value, embed_dim, num_head, qkv_weight, qkv_bias, proj_weight, proj_bias, mask, need_weights, average_attn_weights, mask_type);
}
int64_t _fused_sdp_choice(const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const c10::optional<at::Tensor> & attn_mask, double dropout_p, bool is_causal, c10::optional<double> scale) {
return wrapper_NestedTensorCPU___fused_sdp_choice(query, key, value, attn_mask, dropout_p, is_causal, scale);
}
} // namespace nestedtensorcpu
} // namespace at
